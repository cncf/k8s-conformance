Oct 29 21:10:07.593: INFO: The --provider flag is not set. Continuing as if --provider=skeleton had been used.
I1029 21:10:07.593780   53132 test_context.go:429] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I1029 21:10:07.593913   53132 e2e.go:129] Starting e2e run "b03d1bf3-fe8d-415b-b960-4a1eb76a778f" on Ginkgo node 1
{"msg":"Test Suite starting","total":19,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1604005806 - Will randomize all specs
Will run 19 of 5234 specs

Oct 29 21:10:07.617: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
Oct 29 21:10:07.620: INFO: Waiting up to 30m0s for all (but 3) nodes to be schedulable
Oct 29 21:10:07.751: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Oct 29 21:10:07.828: INFO: 0 / 0 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Oct 29 21:10:07.828: INFO: expected 0 pod replicas in namespace 'kube-system', 0 are Running and Ready.
Oct 29 21:10:07.828: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Oct 29 21:10:07.852: INFO: e2e test version: v1.19.4-rc.0.27+9dfb4c876bfca7
Oct 29 21:10:07.871: INFO: kube-apiserver version: v1.19.0+d59ce34
Oct 29 21:10:07.871: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
Oct 29 21:10:07.894: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:10:07.896: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename daemonsets
Oct 29 21:10:08.025: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 21:10:08.226: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Oct 29 21:10:08.277: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:10:08.277: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:08.277: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:08.321: INFO: Number of nodes with available pods: 0
Oct 29 21:10:08.321: INFO: Node ip-10-0-142-212.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:10:09.400: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:10:09.400: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:09.401: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:09.422: INFO: Number of nodes with available pods: 0
Oct 29 21:10:09.422: INFO: Node ip-10-0-142-212.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:10:10.400: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:10:10.400: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:10.400: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:10.421: INFO: Number of nodes with available pods: 0
Oct 29 21:10:10.421: INFO: Node ip-10-0-142-212.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:10:11.404: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:10:11.404: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:11.404: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:11.426: INFO: Number of nodes with available pods: 2
Oct 29 21:10:11.426: INFO: Node ip-10-0-142-212.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:10:12.400: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:10:12.401: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:12.401: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:12.421: INFO: Number of nodes with available pods: 4
Oct 29 21:10:12.421: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Oct 29 21:10:12.578: INFO: Wrong image for pod: daemon-set-hs6h6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:12.578: INFO: Wrong image for pod: daemon-set-px9xr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:12.578: INFO: Wrong image for pod: daemon-set-tvld7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:12.578: INFO: Wrong image for pod: daemon-set-zqcw5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:12.601: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:10:12.601: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:12.601: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:13.641: INFO: Wrong image for pod: daemon-set-hs6h6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:13.641: INFO: Wrong image for pod: daemon-set-px9xr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:13.641: INFO: Wrong image for pod: daemon-set-tvld7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:13.641: INFO: Wrong image for pod: daemon-set-zqcw5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:13.701: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:10:13.701: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:13.701: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:14.643: INFO: Wrong image for pod: daemon-set-hs6h6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:14.643: INFO: Pod daemon-set-hs6h6 is not available
Oct 29 21:10:14.643: INFO: Wrong image for pod: daemon-set-px9xr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:14.643: INFO: Wrong image for pod: daemon-set-tvld7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:14.643: INFO: Wrong image for pod: daemon-set-zqcw5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:14.705: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:10:14.705: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:14.705: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:15.641: INFO: Wrong image for pod: daemon-set-hs6h6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:15.641: INFO: Pod daemon-set-hs6h6 is not available
Oct 29 21:10:15.641: INFO: Wrong image for pod: daemon-set-px9xr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:15.641: INFO: Wrong image for pod: daemon-set-tvld7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:15.641: INFO: Wrong image for pod: daemon-set-zqcw5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:15.701: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:10:15.701: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:15.701: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:16.642: INFO: Wrong image for pod: daemon-set-hs6h6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:16.643: INFO: Pod daemon-set-hs6h6 is not available
Oct 29 21:10:16.643: INFO: Wrong image for pod: daemon-set-px9xr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:16.643: INFO: Wrong image for pod: daemon-set-tvld7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:16.643: INFO: Wrong image for pod: daemon-set-zqcw5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:16.704: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:10:16.705: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:16.705: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:17.642: INFO: Pod daemon-set-9stct is not available
Oct 29 21:10:17.642: INFO: Wrong image for pod: daemon-set-px9xr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:17.642: INFO: Wrong image for pod: daemon-set-tvld7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:17.642: INFO: Wrong image for pod: daemon-set-zqcw5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:17.712: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:10:17.712: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:17.712: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:18.641: INFO: Pod daemon-set-9stct is not available
Oct 29 21:10:18.641: INFO: Wrong image for pod: daemon-set-px9xr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:18.641: INFO: Wrong image for pod: daemon-set-tvld7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:18.641: INFO: Wrong image for pod: daemon-set-zqcw5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:18.703: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:10:18.703: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:18.703: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:19.641: INFO: Pod daemon-set-9stct is not available
Oct 29 21:10:19.641: INFO: Wrong image for pod: daemon-set-px9xr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:19.641: INFO: Wrong image for pod: daemon-set-tvld7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:19.642: INFO: Wrong image for pod: daemon-set-zqcw5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:19.713: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:10:19.713: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:19.713: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:20.642: INFO: Wrong image for pod: daemon-set-px9xr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:20.643: INFO: Wrong image for pod: daemon-set-tvld7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:20.643: INFO: Wrong image for pod: daemon-set-zqcw5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:20.643: INFO: Pod daemon-set-zqcw5 is not available
Oct 29 21:10:20.703: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:10:20.703: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:20.703: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:21.641: INFO: Wrong image for pod: daemon-set-px9xr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:21.641: INFO: Wrong image for pod: daemon-set-tvld7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:21.641: INFO: Wrong image for pod: daemon-set-zqcw5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:21.641: INFO: Pod daemon-set-zqcw5 is not available
Oct 29 21:10:21.701: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:10:21.702: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:21.702: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:22.641: INFO: Wrong image for pod: daemon-set-px9xr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:22.641: INFO: Wrong image for pod: daemon-set-tvld7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:22.641: INFO: Wrong image for pod: daemon-set-zqcw5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:22.641: INFO: Pod daemon-set-zqcw5 is not available
Oct 29 21:10:22.702: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:10:22.702: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:22.702: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:23.642: INFO: Wrong image for pod: daemon-set-px9xr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:23.642: INFO: Wrong image for pod: daemon-set-tvld7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:23.642: INFO: Wrong image for pod: daemon-set-zqcw5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:23.642: INFO: Pod daemon-set-zqcw5 is not available
Oct 29 21:10:23.701: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:10:23.702: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:23.702: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:24.642: INFO: Wrong image for pod: daemon-set-px9xr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:24.642: INFO: Wrong image for pod: daemon-set-tvld7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:24.642: INFO: Wrong image for pod: daemon-set-zqcw5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:24.642: INFO: Pod daemon-set-zqcw5 is not available
Oct 29 21:10:24.704: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:10:24.704: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:24.704: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:25.641: INFO: Wrong image for pod: daemon-set-px9xr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:25.641: INFO: Wrong image for pod: daemon-set-tvld7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:25.641: INFO: Wrong image for pod: daemon-set-zqcw5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:25.641: INFO: Pod daemon-set-zqcw5 is not available
Oct 29 21:10:25.702: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:10:25.702: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:25.702: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:26.642: INFO: Wrong image for pod: daemon-set-px9xr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:26.642: INFO: Pod daemon-set-sqh2p is not available
Oct 29 21:10:26.642: INFO: Wrong image for pod: daemon-set-tvld7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:26.704: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:10:26.704: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:26.704: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:27.641: INFO: Wrong image for pod: daemon-set-px9xr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:27.641: INFO: Pod daemon-set-sqh2p is not available
Oct 29 21:10:27.641: INFO: Wrong image for pod: daemon-set-tvld7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:27.701: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:10:27.702: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:27.702: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:28.642: INFO: Wrong image for pod: daemon-set-px9xr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:28.642: INFO: Pod daemon-set-sqh2p is not available
Oct 29 21:10:28.642: INFO: Wrong image for pod: daemon-set-tvld7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:28.703: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:10:28.704: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:28.704: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:29.643: INFO: Wrong image for pod: daemon-set-px9xr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:29.643: INFO: Wrong image for pod: daemon-set-tvld7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:29.704: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:10:29.704: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:29.704: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:30.642: INFO: Wrong image for pod: daemon-set-px9xr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:30.642: INFO: Wrong image for pod: daemon-set-tvld7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:30.703: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:10:30.703: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:30.703: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:31.642: INFO: Wrong image for pod: daemon-set-px9xr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:31.642: INFO: Wrong image for pod: daemon-set-tvld7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:31.642: INFO: Pod daemon-set-tvld7 is not available
Oct 29 21:10:31.705: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:10:31.705: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:31.705: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:32.642: INFO: Pod daemon-set-f4bzs is not available
Oct 29 21:10:32.642: INFO: Wrong image for pod: daemon-set-px9xr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:32.702: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:10:32.702: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:32.702: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:33.642: INFO: Pod daemon-set-f4bzs is not available
Oct 29 21:10:33.642: INFO: Wrong image for pod: daemon-set-px9xr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:33.703: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:10:33.703: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:33.703: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:34.645: INFO: Pod daemon-set-f4bzs is not available
Oct 29 21:10:34.645: INFO: Wrong image for pod: daemon-set-px9xr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:34.705: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:10:34.706: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:34.706: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:35.642: INFO: Wrong image for pod: daemon-set-px9xr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:35.702: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:10:35.702: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:35.702: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:36.643: INFO: Wrong image for pod: daemon-set-px9xr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:36.643: INFO: Pod daemon-set-px9xr is not available
Oct 29 21:10:36.704: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:10:36.704: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:36.704: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:37.642: INFO: Wrong image for pod: daemon-set-px9xr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:37.642: INFO: Pod daemon-set-px9xr is not available
Oct 29 21:10:37.702: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:10:37.702: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:37.702: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:38.642: INFO: Wrong image for pod: daemon-set-px9xr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:38.642: INFO: Pod daemon-set-px9xr is not available
Oct 29 21:10:38.702: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:10:38.702: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:38.702: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:39.642: INFO: Wrong image for pod: daemon-set-px9xr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:39.642: INFO: Pod daemon-set-px9xr is not available
Oct 29 21:10:39.707: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:10:39.707: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:39.707: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:40.642: INFO: Wrong image for pod: daemon-set-px9xr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 29 21:10:40.642: INFO: Pod daemon-set-px9xr is not available
Oct 29 21:10:40.702: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:10:40.702: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:40.702: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:41.642: INFO: Pod daemon-set-wzbrc is not available
Oct 29 21:10:41.703: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:10:41.704: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:41.704: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Oct 29 21:10:41.729: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:10:41.730: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:41.730: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:41.751: INFO: Number of nodes with available pods: 3
Oct 29 21:10:41.751: INFO: Node ip-10-0-157-225.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:10:42.836: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:10:42.837: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:42.837: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:42.858: INFO: Number of nodes with available pods: 3
Oct 29 21:10:42.858: INFO: Node ip-10-0-157-225.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:10:43.830: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:10:43.830: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:43.830: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:43.852: INFO: Number of nodes with available pods: 3
Oct 29 21:10:43.852: INFO: Node ip-10-0-157-225.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:10:44.831: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:10:44.832: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:44.832: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:10:44.853: INFO: Number of nodes with available pods: 4
Oct 29 21:10:44.853: INFO: Number of running nodes: 4, number of available pods: 4
[AfterEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5530, will wait for the garbage collector to delete the pods
Oct 29 21:10:45.057: INFO: Deleting DaemonSet.extensions daemon-set took: 24.020043ms
Oct 29 21:10:45.157: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.108889ms
Oct 29 21:10:57.377: INFO: Number of nodes with available pods: 0
Oct 29 21:10:57.377: INFO: Number of running nodes: 0, number of available pods: 0
Oct 29 21:10:57.398: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5530/daemonsets","resourceVersion":"201015"},"items":null}

Oct 29 21:10:57.420: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5530/pods","resourceVersion":"201015"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:10:57.545: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "daemonsets-5530" for this suite.

 [SLOW TEST:49.714 seconds]
[sig-apps] Daemon set [Serial]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":19,"completed":1,"skipped":73,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:10:57.616: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Oct 29 21:10:57.796: INFO: Waiting up to 1m0s for all (but 3) nodes to be ready
Oct 29 21:10:57.853: INFO: Waiting for terminating namespaces to be deleted...
Oct 29 21:10:57.878: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-142-212.us-east-2.compute.internal before test
Oct 29 21:10:57.921: INFO: aws-ebs-csi-driver-node-5gmf4 from openshift-cluster-csi-drivers started at 2020-10-29 17:30:44 +0000 UTC (3 container statuses recorded)
Oct 29 21:10:57.921: INFO: 	Container csi-driver ready: true, restart count 0
Oct 29 21:10:57.921: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Oct 29 21:10:57.921: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Oct 29 21:10:57.921: INFO: tuned-d6fwx from openshift-cluster-node-tuning-operator started at 2020-10-29 17:30:44 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:57.921: INFO: 	Container tuned ready: true, restart count 0
Oct 29 21:10:57.921: INFO: dns-default-b89gt from openshift-dns started at 2020-10-29 17:30:44 +0000 UTC (3 container statuses recorded)
Oct 29 21:10:57.921: INFO: 	Container dns ready: true, restart count 0
Oct 29 21:10:57.921: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 29 21:10:57.921: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:10:57.921: INFO: node-ca-rq9vr from openshift-image-registry started at 2020-10-29 17:30:44 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:57.921: INFO: 	Container node-ca ready: true, restart count 0
Oct 29 21:10:57.921: INFO: machine-config-daemon-l4r47 from openshift-machine-config-operator started at 2020-10-29 17:30:44 +0000 UTC (2 container statuses recorded)
Oct 29 21:10:57.921: INFO: 	Container machine-config-daemon ready: true, restart count 0
Oct 29 21:10:57.921: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:10:57.921: INFO: osd-patch-subscription-source-1604005200-ccz9j from openshift-marketplace started at 2020-10-29 21:00:09 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:57.921: INFO: 	Container osd-patch-subscription-source ready: false, restart count 0
Oct 29 21:10:57.921: INFO: node-exporter-p2phw from openshift-monitoring started at 2020-10-29 17:30:44 +0000 UTC (2 container statuses recorded)
Oct 29 21:10:57.921: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:10:57.921: INFO: 	Container node-exporter ready: true, restart count 0
Oct 29 21:10:57.921: INFO: sre-dns-latency-exporter-29j4j from openshift-monitoring started at 2020-10-29 20:32:47 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:57.921: INFO: 	Container main ready: true, restart count 0
Oct 29 21:10:57.921: INFO: multus-jst6k from openshift-multus started at 2020-10-29 17:30:43 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:57.921: INFO: 	Container kube-multus ready: true, restart count 0
Oct 29 21:10:57.921: INFO: network-metrics-daemon-xrvxh from openshift-multus started at 2020-10-29 17:30:44 +0000 UTC (2 container statuses recorded)
Oct 29 21:10:57.921: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:10:57.921: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Oct 29 21:10:57.921: INFO: ovs-66jns from openshift-sdn started at 2020-10-29 17:30:43 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:57.921: INFO: 	Container openvswitch ready: true, restart count 0
Oct 29 21:10:57.921: INFO: sdn-lbx27 from openshift-sdn started at 2020-10-29 17:30:44 +0000 UTC (2 container statuses recorded)
Oct 29 21:10:57.921: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Oct 29 21:10:57.921: INFO: 	Container sdn ready: true, restart count 0
Oct 29 21:10:57.921: INFO: alert-pruner-1604005200-gtct6 from openshift-sre-pruning started at 2020-10-29 21:00:09 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:57.921: INFO: 	Container alert-pruner ready: false, restart count 0
Oct 29 21:10:57.921: INFO: builds-pruner-1604005200-8wspq from openshift-sre-pruning started at 2020-10-29 21:00:09 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:57.921: INFO: 	Container builds-pruner ready: false, restart count 0
Oct 29 21:10:57.921: INFO: crd-pruner-1604005200-w76df from openshift-sre-pruning started at 2020-10-29 21:00:09 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:57.921: INFO: 	Container crd-pruner ready: false, restart count 0
Oct 29 21:10:57.921: INFO: image-pruner-1604005200-bp5kw from openshift-sre-pruning started at 2020-10-29 21:00:09 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:57.921: INFO: 	Container image-pruner ready: false, restart count 0
Oct 29 21:10:57.921: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-157-225.us-east-2.compute.internal before test
Oct 29 21:10:57.979: INFO: aws-ebs-csi-driver-node-qvwrc from openshift-cluster-csi-drivers started at 2020-10-29 17:41:11 +0000 UTC (3 container statuses recorded)
Oct 29 21:10:57.979: INFO: 	Container csi-driver ready: true, restart count 0
Oct 29 21:10:57.979: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Oct 29 21:10:57.979: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Oct 29 21:10:57.979: INFO: tuned-4tm5g from openshift-cluster-node-tuning-operator started at 2020-10-29 17:41:11 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:57.979: INFO: 	Container tuned ready: true, restart count 0
Oct 29 21:10:57.979: INFO: dns-default-hr5xr from openshift-dns started at 2020-10-29 17:41:12 +0000 UTC (3 container statuses recorded)
Oct 29 21:10:57.979: INFO: 	Container dns ready: true, restart count 0
Oct 29 21:10:57.979: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 29 21:10:57.979: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:10:57.979: INFO: image-registry-75b75c555d-cm6gf from openshift-image-registry started at 2020-10-29 17:48:47 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:57.979: INFO: 	Container registry ready: true, restart count 0
Oct 29 21:10:57.979: INFO: image-registry-75b75c555d-ksztb from openshift-image-registry started at 2020-10-29 17:48:47 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:57.979: INFO: 	Container registry ready: true, restart count 0
Oct 29 21:10:57.979: INFO: node-ca-6tz25 from openshift-image-registry started at 2020-10-29 17:41:11 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:57.979: INFO: 	Container node-ca ready: true, restart count 0
Oct 29 21:10:57.979: INFO: router-default-5f9d96945d-zxzg4 from openshift-ingress started at 2020-10-29 19:45:00 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:57.979: INFO: 	Container router ready: true, restart count 0
Oct 29 21:10:57.979: INFO: machine-config-daemon-mjcnt from openshift-machine-config-operator started at 2020-10-29 17:41:11 +0000 UTC (2 container statuses recorded)
Oct 29 21:10:57.979: INFO: 	Container machine-config-daemon ready: true, restart count 0
Oct 29 21:10:57.979: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:10:57.979: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-10-29 17:49:01 +0000 UTC (5 container statuses recorded)
Oct 29 21:10:57.979: INFO: 	Container alertmanager ready: true, restart count 0
Oct 29 21:10:57.979: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Oct 29 21:10:57.979: INFO: 	Container config-reloader ready: true, restart count 0
Oct 29 21:10:57.979: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:10:57.979: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:10:57.979: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-10-29 17:49:02 +0000 UTC (5 container statuses recorded)
Oct 29 21:10:57.979: INFO: 	Container alertmanager ready: true, restart count 0
Oct 29 21:10:57.979: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Oct 29 21:10:57.979: INFO: 	Container config-reloader ready: true, restart count 0
Oct 29 21:10:57.979: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:10:57.979: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:10:57.979: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-10-29 17:49:10 +0000 UTC (5 container statuses recorded)
Oct 29 21:10:57.979: INFO: 	Container alertmanager ready: true, restart count 0
Oct 29 21:10:57.979: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Oct 29 21:10:57.979: INFO: 	Container config-reloader ready: true, restart count 0
Oct 29 21:10:57.979: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:10:57.979: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:10:57.979: INFO: grafana-5cfb6b5fbb-gjr2l from openshift-monitoring started at 2020-10-29 17:48:48 +0000 UTC (2 container statuses recorded)
Oct 29 21:10:57.979: INFO: 	Container grafana ready: true, restart count 0
Oct 29 21:10:57.979: INFO: 	Container grafana-proxy ready: true, restart count 0
Oct 29 21:10:57.979: INFO: kube-state-metrics-596748788d-6mblj from openshift-monitoring started at 2020-10-29 17:48:48 +0000 UTC (3 container statuses recorded)
Oct 29 21:10:57.979: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Oct 29 21:10:57.979: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Oct 29 21:10:57.979: INFO: 	Container kube-state-metrics ready: true, restart count 0
Oct 29 21:10:57.979: INFO: node-exporter-d8llk from openshift-monitoring started at 2020-10-29 17:41:13 +0000 UTC (2 container statuses recorded)
Oct 29 21:10:57.979: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:10:57.979: INFO: 	Container node-exporter ready: true, restart count 0
Oct 29 21:10:57.979: INFO: prometheus-adapter-6495ff5885-lrx64 from openshift-monitoring started at 2020-10-29 17:48:47 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:57.979: INFO: 	Container prometheus-adapter ready: true, restart count 0
Oct 29 21:10:57.979: INFO: prometheus-adapter-6495ff5885-lw9vm from openshift-monitoring started at 2020-10-29 17:48:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:57.979: INFO: 	Container prometheus-adapter ready: true, restart count 0
Oct 29 21:10:57.979: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-10-29 17:48:53 +0000 UTC (6 container statuses recorded)
Oct 29 21:10:57.979: INFO: 	Container config-reloader ready: true, restart count 0
Oct 29 21:10:57.979: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:10:57.979: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:10:57.979: INFO: 	Container prometheus ready: true, restart count 1
Oct 29 21:10:57.979: INFO: 	Container prometheus-proxy ready: true, restart count 0
Oct 29 21:10:57.979: INFO: 	Container thanos-sidecar ready: true, restart count 0
Oct 29 21:10:57.979: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-10-29 17:49:00 +0000 UTC (6 container statuses recorded)
Oct 29 21:10:57.979: INFO: 	Container config-reloader ready: true, restart count 0
Oct 29 21:10:57.979: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:10:57.979: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:10:57.979: INFO: 	Container prometheus ready: true, restart count 1
Oct 29 21:10:57.979: INFO: 	Container prometheus-proxy ready: true, restart count 0
Oct 29 21:10:57.979: INFO: 	Container thanos-sidecar ready: true, restart count 0
Oct 29 21:10:57.979: INFO: prometheus-operator-89f4b9855-wlkt2 from openshift-monitoring started at 2020-10-29 17:48:47 +0000 UTC (2 container statuses recorded)
Oct 29 21:10:57.979: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:10:57.979: INFO: 	Container prometheus-operator ready: true, restart count 0
Oct 29 21:10:57.979: INFO: sre-dns-latency-exporter-l58wm from openshift-monitoring started at 2020-10-29 17:41:56 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:57.979: INFO: 	Container main ready: true, restart count 0
Oct 29 21:10:57.979: INFO: sre-stuck-ebs-vols-1-98w59 from openshift-monitoring started at 2020-10-29 17:48:47 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:57.979: INFO: 	Container main ready: true, restart count 0
Oct 29 21:10:57.979: INFO: telemeter-client-577bcc4f8d-jv4m8 from openshift-monitoring started at 2020-10-29 17:48:47 +0000 UTC (3 container statuses recorded)
Oct 29 21:10:57.979: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:10:57.979: INFO: 	Container reload ready: true, restart count 0
Oct 29 21:10:57.979: INFO: 	Container telemeter-client ready: true, restart count 0
Oct 29 21:10:57.979: INFO: multus-str5z from openshift-multus started at 2020-10-29 17:41:12 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:57.979: INFO: 	Container kube-multus ready: true, restart count 0
Oct 29 21:10:57.979: INFO: network-metrics-daemon-gcjdz from openshift-multus started at 2020-10-29 17:41:13 +0000 UTC (2 container statuses recorded)
Oct 29 21:10:57.979: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:10:57.979: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Oct 29 21:10:57.979: INFO: ovs-99tdx from openshift-sdn started at 2020-10-29 17:41:12 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:57.979: INFO: 	Container openvswitch ready: true, restart count 0
Oct 29 21:10:57.979: INFO: sdn-cp5t4 from openshift-sdn started at 2020-10-29 17:41:12 +0000 UTC (2 container statuses recorded)
Oct 29 21:10:57.979: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Oct 29 21:10:57.979: INFO: 	Container sdn ready: true, restart count 0
Oct 29 21:10:57.979: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-158-72.us-east-2.compute.internal before test
Oct 29 21:10:58.025: INFO: sre-build-test-1603995060-2nl7g from openshift-build-test started at 2020-10-29 18:11:00 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:58.025: INFO: 	Container sre-build-test ready: false, restart count 0
Oct 29 21:10:58.025: INFO: sre-build-test-1603998660-hmrqr from openshift-build-test started at 2020-10-29 19:11:03 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:58.025: INFO: 	Container sre-build-test ready: false, restart count 0
Oct 29 21:10:58.025: INFO: sre-build-test-1604002260-284ls from openshift-build-test started at 2020-10-29 20:11:07 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:58.025: INFO: 	Container sre-build-test ready: false, restart count 0
Oct 29 21:10:58.025: INFO: cloud-ingress-operator-798549d69-9fh9n from openshift-cloud-ingress-operator started at 2020-10-29 19:28:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:58.025: INFO: 	Container cloud-ingress-operator ready: true, restart count 0
Oct 29 21:10:58.025: INFO: aws-ebs-csi-driver-node-zfr5l from openshift-cluster-csi-drivers started at 2020-10-29 17:42:13 +0000 UTC (3 container statuses recorded)
Oct 29 21:10:58.025: INFO: 	Container csi-driver ready: true, restart count 0
Oct 29 21:10:58.025: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Oct 29 21:10:58.025: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Oct 29 21:10:58.025: INFO: tuned-jwndg from openshift-cluster-node-tuning-operator started at 2020-10-29 17:42:13 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:58.025: INFO: 	Container tuned ready: true, restart count 0
Oct 29 21:10:58.025: INFO: dns-default-25bcd from openshift-dns started at 2020-10-29 17:42:13 +0000 UTC (3 container statuses recorded)
Oct 29 21:10:58.025: INFO: 	Container dns ready: true, restart count 0
Oct 29 21:10:58.025: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 29 21:10:58.025: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:10:58.025: INFO: node-ca-d275k from openshift-image-registry started at 2020-10-29 17:42:13 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:58.025: INFO: 	Container node-ca ready: true, restart count 0
Oct 29 21:10:58.025: INFO: router-default-5f9d96945d-ckql4 from openshift-ingress started at 2020-10-29 19:44:59 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:58.025: INFO: 	Container router ready: true, restart count 0
Oct 29 21:10:58.025: INFO: machine-config-daemon-6987z from openshift-machine-config-operator started at 2020-10-29 17:42:13 +0000 UTC (2 container statuses recorded)
Oct 29 21:10:58.025: INFO: 	Container machine-config-daemon ready: true, restart count 0
Oct 29 21:10:58.025: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:10:58.025: INFO: node-exporter-pdd9z from openshift-monitoring started at 2020-10-29 17:42:13 +0000 UTC (2 container statuses recorded)
Oct 29 21:10:58.025: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:10:58.025: INFO: 	Container node-exporter ready: true, restart count 0
Oct 29 21:10:58.025: INFO: sre-dns-latency-exporter-krp25 from openshift-monitoring started at 2020-10-29 17:42:51 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:58.025: INFO: 	Container main ready: true, restart count 0
Oct 29 21:10:58.025: INFO: sre-ebs-iops-reporter-1-rrlxd from openshift-monitoring started at 2020-10-29 19:28:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:58.025: INFO: 	Container main ready: true, restart count 0
Oct 29 21:10:58.025: INFO: thanos-querier-7f688468b8-6jh8m from openshift-monitoring started at 2020-10-29 19:28:48 +0000 UTC (5 container statuses recorded)
Oct 29 21:10:58.025: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:10:58.025: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Oct 29 21:10:58.025: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:10:58.025: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:10:58.025: INFO: 	Container thanos-query ready: true, restart count 0
Oct 29 21:10:58.025: INFO: multus-l5sdf from openshift-multus started at 2020-10-29 17:42:13 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:58.025: INFO: 	Container kube-multus ready: true, restart count 0
Oct 29 21:10:58.025: INFO: network-metrics-daemon-nb7sg from openshift-multus started at 2020-10-29 17:42:13 +0000 UTC (2 container statuses recorded)
Oct 29 21:10:58.025: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:10:58.025: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Oct 29 21:10:58.025: INFO: ovs-9ct7r from openshift-sdn started at 2020-10-29 17:42:13 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:58.025: INFO: 	Container openvswitch ready: true, restart count 0
Oct 29 21:10:58.025: INFO: sdn-8kf7x from openshift-sdn started at 2020-10-29 17:42:13 +0000 UTC (2 container statuses recorded)
Oct 29 21:10:58.025: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Oct 29 21:10:58.025: INFO: 	Container sdn ready: true, restart count 0
Oct 29 21:10:58.025: INFO: splunk-forwarder-operator-6c55bb877-mw9lq from openshift-splunk-forwarder-operator started at 2020-10-29 19:28:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:58.025: INFO: 	Container splunk-forwarder-operator ready: true, restart count 0
Oct 29 21:10:58.025: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-234-238.us-east-2.compute.internal before test
Oct 29 21:10:58.069: INFO: cloud-ingress-operator-registry-zpdvx from openshift-cloud-ingress-operator started at 2020-10-29 19:32:07 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:58.069: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:10:58.069: INFO: aws-ebs-csi-driver-node-r7m6f from openshift-cluster-csi-drivers started at 2020-10-29 17:30:47 +0000 UTC (3 container statuses recorded)
Oct 29 21:10:58.069: INFO: 	Container csi-driver ready: true, restart count 0
Oct 29 21:10:58.069: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Oct 29 21:10:58.069: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Oct 29 21:10:58.069: INFO: tuned-6rs49 from openshift-cluster-node-tuning-operator started at 2020-10-29 17:30:47 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:58.069: INFO: 	Container tuned ready: true, restart count 0
Oct 29 21:10:58.069: INFO: downloads-85df645c7c-ft27h from openshift-console started at 2020-10-29 19:31:54 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:58.069: INFO: 	Container download-server ready: true, restart count 0
Oct 29 21:10:58.069: INFO: downloads-85df645c7c-pxr2l from openshift-console started at 2020-10-29 19:28:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:58.069: INFO: 	Container download-server ready: true, restart count 0
Oct 29 21:10:58.069: INFO: dns-default-jxrwk from openshift-dns started at 2020-10-29 17:30:47 +0000 UTC (3 container statuses recorded)
Oct 29 21:10:58.069: INFO: 	Container dns ready: true, restart count 0
Oct 29 21:10:58.069: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 29 21:10:58.069: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:10:58.069: INFO: node-ca-wxm2h from openshift-image-registry started at 2020-10-29 17:30:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:58.069: INFO: 	Container node-ca ready: true, restart count 0
Oct 29 21:10:58.069: INFO: migrator-b8d88f977-j4wqg from openshift-kube-storage-version-migrator started at 2020-10-29 19:28:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:58.069: INFO: 	Container migrator ready: true, restart count 0
Oct 29 21:10:58.069: INFO: machine-config-daemon-5rrfn from openshift-machine-config-operator started at 2020-10-29 17:30:47 +0000 UTC (2 container statuses recorded)
Oct 29 21:10:58.069: INFO: 	Container machine-config-daemon ready: true, restart count 0
Oct 29 21:10:58.069: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:10:58.069: INFO: managed-upgrade-operator-7558857f4-cxqkg from openshift-managed-upgrade-operator started at 2020-10-29 17:48:47 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:58.069: INFO: 	Container managed-upgrade-operator ready: true, restart count 0
Oct 29 21:10:58.069: INFO: managed-upgrade-operator-catalog-g4m2g from openshift-managed-upgrade-operator started at 2020-10-29 19:32:11 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:58.069: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:10:58.069: INFO: certified-operators-pjgrs from openshift-marketplace started at 2020-10-29 19:32:07 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:58.069: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:10:58.069: INFO: community-operators-rss65 from openshift-marketplace started at 2020-10-29 19:32:06 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:58.069: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:10:58.069: INFO: redhat-marketplace-cwfzm from openshift-marketplace started at 2020-10-29 19:32:08 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:58.069: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:10:58.069: INFO: redhat-operators-pjjcr from openshift-marketplace started at 2020-10-29 19:32:11 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:58.069: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:10:58.069: INFO: configure-alertmanager-operator-579dfffff7-nmj5v from openshift-monitoring started at 2020-10-29 17:48:47 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:58.069: INFO: 	Container configure-alertmanager-operator ready: true, restart count 0
Oct 29 21:10:58.069: INFO: configure-alertmanager-operator-registry-zcvq5 from openshift-monitoring started at 2020-10-29 19:32:07 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:58.069: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:10:58.069: INFO: node-exporter-lmwzp from openshift-monitoring started at 2020-10-29 17:30:48 +0000 UTC (2 container statuses recorded)
Oct 29 21:10:58.069: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:10:58.069: INFO: 	Container node-exporter ready: true, restart count 0
Oct 29 21:10:58.069: INFO: openshift-state-metrics-74d99f5845-62ghm from openshift-monitoring started at 2020-10-29 19:31:54 +0000 UTC (3 container statuses recorded)
Oct 29 21:10:58.069: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Oct 29 21:10:58.069: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Oct 29 21:10:58.069: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Oct 29 21:10:58.069: INFO: sre-dns-latency-exporter-5wrqd from openshift-monitoring started at 2020-10-29 17:39:27 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:58.069: INFO: 	Container main ready: true, restart count 0
Oct 29 21:10:58.069: INFO: thanos-querier-7f688468b8-cfd7g from openshift-monitoring started at 2020-10-29 17:48:47 +0000 UTC (5 container statuses recorded)
Oct 29 21:10:58.069: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:10:58.069: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Oct 29 21:10:58.069: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:10:58.069: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:10:58.069: INFO: 	Container thanos-query ready: true, restart count 0
Oct 29 21:10:58.069: INFO: multus-bcgtj from openshift-multus started at 2020-10-29 17:30:47 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:58.069: INFO: 	Container kube-multus ready: true, restart count 0
Oct 29 21:10:58.069: INFO: network-metrics-daemon-5brxv from openshift-multus started at 2020-10-29 17:30:48 +0000 UTC (2 container statuses recorded)
Oct 29 21:10:58.069: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:10:58.070: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Oct 29 21:10:58.070: INFO: rbac-permissions-operator-78d47d8556-92kbv from openshift-rbac-permissions started at 2020-10-29 17:48:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:58.070: INFO: 	Container rbac-permissions-operator ready: true, restart count 0
Oct 29 21:10:58.070: INFO: rbac-permissions-operator-registry-x892d from openshift-rbac-permissions started at 2020-10-29 19:32:08 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:58.070: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:10:58.070: INFO: ovs-9f9tm from openshift-sdn started at 2020-10-29 17:30:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:58.070: INFO: 	Container openvswitch ready: true, restart count 0
Oct 29 21:10:58.070: INFO: sdn-9tj9v from openshift-sdn started at 2020-10-29 17:30:48 +0000 UTC (2 container statuses recorded)
Oct 29 21:10:58.070: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Oct 29 21:10:58.070: INFO: 	Container sdn ready: true, restart count 0
Oct 29 21:10:58.070: INFO: splunk-forwarder-operator-catalog-jg48j from openshift-splunk-forwarder-operator started at 2020-10-29 19:32:10 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:58.070: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:10:58.070: INFO: alert-pruner-1603998000-fgw8n from openshift-sre-pruning started at 2020-10-29 19:00:09 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:58.070: INFO: 	Container alert-pruner ready: false, restart count 0
Oct 29 21:10:58.070: INFO: crd-pruner-1603998000-ljqfl from openshift-sre-pruning started at 2020-10-29 19:00:09 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:58.070: INFO: 	Container crd-pruner ready: false, restart count 0
Oct 29 21:10:58.070: INFO: deployments-pruner-1603998000-vfdcp from openshift-sre-pruning started at 2020-10-29 19:00:09 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:58.070: INFO: 	Container deployments-pruner ready: false, restart count 0
Oct 29 21:10:58.070: INFO: deployments-pruner-1604005200-dfg2v from openshift-sre-pruning started at 2020-10-29 21:00:09 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:58.070: INFO: 	Container deployments-pruner ready: false, restart count 0
Oct 29 21:10:58.070: INFO: managed-velero-operator-8685cc459-lsvfl from openshift-velero started at 2020-10-29 17:48:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:58.070: INFO: 	Container managed-velero-operator ready: true, restart count 0
Oct 29 21:10:58.070: INFO: managed-velero-operator-registry-9bghr from openshift-velero started at 2020-10-29 19:32:07 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:58.070: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:10:58.070: INFO: velero-68656b49f9-zvplm from openshift-velero started at 2020-10-29 17:46:41 +0000 UTC (1 container statuses recorded)
Oct 29 21:10:58.070: INFO: 	Container velero ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 21:20:59.218: INFO: Timed out waiting for the following pods to schedule
Oct 29 21:20:59.218: INFO: openshift-authentication/oauth-openshift-7779c8f48c-5p9nn
Oct 29 21:20:59.218: INFO: openshift-console/console-5bc554f4bf-nfqz6
Oct 29 21:20:59.218: FAIL: Timed out after 10m0s waiting for stable cluster.

Full Stack Trace
k8s.io/kubernetes/test/e2e/scheduling.WaitForStableCluster(0x53ec980, 0xc0035c3340, 0xc000964600, 0x4bcfb27)
	/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:55 +0x405
k8s.io/kubernetes/test/e2e/scheduling.glob..func4.5()
	/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:323 +0xb4
k8s.io/kubernetes/test/e2e.RunE2ETests(0xc003648c00)
	_output/local/go/src/k8s.io/kubernetes/test/e2e/e2e.go:130 +0x345
k8s.io/kubernetes/test/e2e.TestE2E(0xc003648c00)
	_output/local/go/src/k8s.io/kubernetes/test/e2e/e2e_test.go:145 +0x2b
testing.tRunner(0xc003648c00, 0x4dca3c8)
	/usr/lib/golang/src/testing/testing.go:1127 +0xef
created by testing.(*T).Run
	/usr/lib/golang/src/testing/testing.go:1178 +0x386
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
STEP: Collecting events from namespace "sched-pred-1167".
STEP: Found 0 events.
Oct 29 21:20:59.261: INFO: POD  NODE  PHASE  GRACE  CONDITIONS
Oct 29 21:20:59.261: INFO: 
Oct 29 21:20:59.341: INFO: 
Logging node info for node ip-10-0-142-212.us-east-2.compute.internal
Oct 29 21:20:59.363: INFO: Node Info: &Node{ObjectMeta:{ip-10-0-142-212.us-east-2.compute.internal   /api/v1/nodes/ip-10-0-142-212.us-east-2.compute.internal 445eef2f-6888-42a8-af2e-f8f509d046fb 203433 0 2020-10-29 17:30:43 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:m5.xlarge beta.kubernetes.io/os:linux failure-domain.beta.kubernetes.io/region:us-east-2 failure-domain.beta.kubernetes.io/zone:us-east-2a kubernetes.io/arch:amd64 kubernetes.io/hostname:ip-10-0-142-212 kubernetes.io/os:linux node-role.kubernetes.io/worker: node.kubernetes.io/instance-type:m5.xlarge node.openshift.io/os_id:rhcos topology.ebs.csi.aws.com/zone:us-east-2a topology.kubernetes.io/region:us-east-2 topology.kubernetes.io/zone:us-east-2a] map[csi.volume.kubernetes.io/nodeid:{"ebs.csi.aws.com":"i-03781cb9a8092da57"} machine.openshift.io/machine:openshift-machine-api/jeder-461-cncf2-pvrph-worker-us-east-2a-jhsl2 machineconfiguration.openshift.io/currentConfig:rendered-worker-f26c105cafab97113f99101cc57ac048 machineconfiguration.openshift.io/desiredConfig:rendered-worker-f26c105cafab97113f99101cc57ac048 machineconfiguration.openshift.io/reason: machineconfiguration.openshift.io/state:Done volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{nodelink-controller Update v1 2020-10-29 17:30:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machine.openshift.io/machine":{}}}}} {machine-config-controller Update v1 2020-10-29 17:40:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/desiredConfig":{}}}}} {machine-config-daemon Update v1 2020-10-29 17:44:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/currentConfig":{},"f:machineconfiguration.openshift.io/reason":{},"f:machineconfiguration.openshift.io/state":{}}}}} {kube-controller-manager Update v1 2020-10-29 17:44:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{}}},"f:spec":{"f:podCIDR":{},"f:podCIDRs":{".":{},"v:\"10.128.3.0/24\"":{}}}}} {kubelet Update v1 2020-10-29 21:16:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:csi.volume.kubernetes.io/nodeid":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/instance-type":{},"f:failure-domain.beta.kubernetes.io/region":{},"f:failure-domain.beta.kubernetes.io/zone":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:node-role.kubernetes.io/worker":{},"f:node.kubernetes.io/instance-type":{},"f:node.openshift.io/os_id":{},"f:topology.ebs.csi.aws.com/zone":{},"f:topology.kubernetes.io/region":{},"f:topology.kubernetes.io/zone":{}}},"f:spec":{"f:providerID":{}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalDNS\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:attachable-volumes-aws-ebs":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:attachable-volumes-aws-ebs":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"DiskPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"MemoryPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PIDPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:images":{},"f:nodeInfo":{"f:architecture":{},"f:bootID":{},"f:containerRuntimeVersion":{},"f:kernelVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{},"f:machineID":{},"f:operatingSystem":{},"f:osImage":{},"f:systemUUID":{}}}}}]},Spec:NodeSpec{PodCIDR:10.128.3.0/24,DoNotUseExternalID:,ProviderID:aws:///us-east-2a/i-03781cb9a8092da57,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.128.3.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{321557344256 0} {<nil>} 314020844Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{16326778880 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},},Allocatable:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{3 0} {<nil>} 3 DecimalSI},ephemeral-storage: {{288327867528 0} {<nil>} 288327867528 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{15148179456 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2020-10-29 21:16:48 +0000 UTC,LastTransitionTime:2020-10-29 17:44:37 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2020-10-29 21:16:48 +0000 UTC,LastTransitionTime:2020-10-29 17:44:37 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2020-10-29 21:16:48 +0000 UTC,LastTransitionTime:2020-10-29 17:44:37 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2020-10-29 21:16:48 +0000 UTC,LastTransitionTime:2020-10-29 17:44:37 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.0.142.212,},NodeAddress{Type:Hostname,Address:ip-10-0-142-212.us-east-2.compute.internal,},NodeAddress{Type:InternalDNS,Address:ip-10-0-142-212.us-east-2.compute.internal,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:ec20bdb61ee45ac6e8e755af80323fd3,SystemUUID:ec20bdb6-1ee4-5ac6-e8e7-55af80323fd3,BootID:31e57073-a38a-44a7-8d88-3091338f4da5,KernelVersion:4.18.0-193.24.1.el8_2.dt1.x86_64,OSImage:Red Hat Enterprise Linux CoreOS 46.82.202010091720-0 (Ootpa),ContainerRuntimeVersion:cri-o://1.19.0-20.rhaos4.6.git97d715e.el8,KubeletVersion:v1.19.0+d59ce34,KubeProxyVersion:v1.19.0+d59ce34,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[quay.io/app-sre/managed-velero-operator-registry@sha256:7d152b07c12c961a67be142cc80597b0616f0db936c34af242e309e6b7be208a quay.io/app-sre/managed-velero-operator-registry:production-0a73057],SizeBytes:1036797635,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:79978a34d1ab3b0ed1ad2c93c09bcb2fcdd1806b35e48a53c99d106347e1a59d quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:769679082,},ContainerImage{Names:[registry.redhat.io/redhat/community-operator-index@sha256:02facc4e56ed383e6cbd19ba740f477f55068948963b18154f66738302f4dd03 registry.redhat.io/redhat/community-operator-index@sha256:467e5d70a7591e82cf325371297c9ee4938e18ab60f42903a01270c0ef8a6a1c registry.redhat.io/redhat/community-operator-index:latest],SizeBytes:509744014,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:82d1def7312de8ae5dee32d237ad59fe685923e78668fa3547e6bee445cd8842 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:499341399,},ContainerImage{Names:[registry.redhat.io/redhat/certified-operator-index@sha256:8e4d14186a6a7a377512dd952fd4b6e8306d669b4e82864c8535021f3e725bd8 registry.redhat.io/redhat/certified-operator-index@sha256:b674fbf172249ff5c27ab195ca05ad14015783e523b421617181a03f9275874f registry.redhat.io/redhat/certified-operator-index:v4.6],SizeBytes:497271693,},ContainerImage{Names:[registry.redhat.io/redhat/redhat-marketplace-index@sha256:1254d42b17fdf685608b3b2f58244a753ce46ce49ad3f5e849a372dfbb901001 registry.redhat.io/redhat/redhat-marketplace-index@sha256:cdd578d52e7fd59b341f35c84f2ccd5681726794bf091d84d06ed66730fd38db registry.redhat.io/redhat/redhat-marketplace-index:v4.6],SizeBytes:492995473,},ContainerImage{Names:[registry.redhat.io/redhat/redhat-operator-index@sha256:20b5fa8d4466c957ee23c97aca064f757f3d7ebb8a1b237d1b1a5e479853a0ca registry.redhat.io/redhat/redhat-operator-index@sha256:bd70797deab3e1e0487d93d488e91418840c6c891d1af2156f66c687638b1308 registry.redhat.io/redhat/redhat-operator-index:v4.6],SizeBytes:491000720,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4cfd55719faff41e96c2e4be69e3f2381a57b8b3445b80ae4acfe8ee33d7f99b quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:481479938,},ContainerImage{Names:[quay.io/jupierce/openshift-python-monitoring@sha256:98e3259170f7116837fa60b0515fe79cb9721060d34eb4931a14244b4658bb8d quay.io/jupierce/openshift-python-monitoring:stable],SizeBytes:474594250,},ContainerImage{Names:[quay.io/app-sre/managed-upgrade-operator-registry@sha256:5d4870d619f5dd35974f957d92d6939a6b56a03c705420ae8602e01e67a10bfc quay.io/app-sre/managed-upgrade-operator-registry:production-9b59534],SizeBytes:472464972,},ContainerImage{Names:[quay.io/app-sre/cloud-ingress-operator-registry@sha256:ce73d98128dc1584568c88d44ba893e924baa93ec8ce0e3b4036f62c01fbd52a quay.io/app-sre/cloud-ingress-operator-registry:production-a2e288b],SizeBytes:470975221,},ContainerImage{Names:[quay.io/app-sre/configure-alertmanager-operator-registry@sha256:11de3d989f814e60482c28ddd72a71d68650a6ae5787289e2eb159aa1af23f01 quay.io/app-sre/configure-alertmanager-operator-registry:production-0a8e5f3],SizeBytes:470905083,},ContainerImage{Names:[quay.io/app-sre/splunk-forwarder-operator-registry@sha256:15c212f82397caa807f5c1e58d47d16bb50d143a886e52cdf09616a50c9e3ef2 quay.io/app-sre/splunk-forwarder-operator-registry:production-7e8bc93],SizeBytes:470516300,},ContainerImage{Names:[quay.io/app-sre/rbac-permissions-operator-registry@sha256:7620af44252bbdbed32316748ccbd165cad0e9320f48ccd63f05a3afc20dc0c2 quay.io/app-sre/rbac-permissions-operator-registry:production-819ec06],SizeBytes:469646922,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2986a09ed686a571312bcb20d648baac46b422efa072f8b68eb41c7996e94610 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:463032763,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8923050603588c27d79b33b371afb651288470d5cdeb14f8e10249bca1a1c461 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:413031192,},ContainerImage{Names:[quay.io/openshift-sre/managed-prometheus-exporter-initcontainer@sha256:7fa36e4488191536ac4a570fdd3431d3c80db983db74b90bc2ce051ecef3035e quay.io/openshift-sre/managed-prometheus-exporter-initcontainer:v0.1.9-2019-03-28-4e558131],SizeBytes:388062834,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8f4882cff3c2f9521215eac681c5abda42876e3e955431c1387fb457940b8344 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:370109724,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:72323ce541f8a26fbad17ef65ff21b51498863bb851635a0faa8d5b1ac6ce0e4 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:366911472,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3d0361f380abf5252b1b640e3ceaaab8274e2af8cdb605b20b513a1a44b3a4dc quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:346080704,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f644e4b495071a2b9d0d6f5d48cb96dad9f7ea8298cc22c98824bc70229ea9dd quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:341116512,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6bada08687c20afe316c1f8cf76f001f31bedae317f256f3df3affaa5d0dc25e quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:336987966,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f42509c18cf5e41201d64cf3a9c1994ffa5318f8d7cee5de45fa2da914e68bbc quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:336588772,},ContainerImage{Names:[image-registry.openshift-image-registry.svc:5000/openshift/cli@sha256:6aa4bb97adf2142b0e74ccae7fd3661ada73cbaac803b86bb8712261e916d66d quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6aa4bb97adf2142b0e74ccae7fd3661ada73cbaac803b86bb8712261e916d66d image-registry.openshift-image-registry.svc:5000/openshift/cli:latest],SizeBytes:336576591,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:52424fd2af6fd7d7a5a1233032eb3f3c67f7691996b209e013e29f1524c5188c quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:326057200,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f86db3170270fc635dff0d7f1ba6e79a8f45de7e1dcfa5621474d1f6e07352ec quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:325370491,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:54fbeb744b82662fd38c0d301ebaad6ca8983707bc44db7235ead0fb7b95808f quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:321482743,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a37a568c63563257309cb0ffb6e185d98f662ff3201d2099cbe0df404b93f0c8 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:314357110,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e176509a104b2f96ee4f5c57275c6a712409aa80ac40071345d0a03bdde2b456 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:307311930,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0c42cd3a74176732e8c06105c47674c7d410c7167c8e3fbd80f9a76e9bfda5bd quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:302591889,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c75977f28becdf4f7065cfa37233464dd31208b1767e620c4f19658f53f8ff8c quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:300756060,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:cf565b2fab365e027962a25a8cffb41aa35cb5a00d001e081d53c7fed5a0c54b quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:300005108,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:78d3478e632c761c18e2dcb55d26e388ecfd126d4fde60317868133dc2fd57f7 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:292832919,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9e7f0468850aeb13585ef049f687cc42c05d82bc0e0200607d1a93d7f9740fe5 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:291621018,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a1aaf99f2ed745c5353d9fc715fa8e9111f42165e3012fad73640c438ba6aa6f quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:290507515,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6cdaecd5dd9df8fd74529be7fa5d8973daf6f4ea95be8acfb2f5ac97773ebe67 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:278303432,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e5bcf6d786fd218e1ef188eb40c39c31a98d03121fba3b7a1f16e87e45a7478b quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:276735847,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1dcc413b621958f97dfbb3fc998a9e225ef155a80ffb151eb4694bf8370b383a quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:274559501,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8c9e61400619c4613db5cc73097d287e3cd5d2125c85d1d84cc30cfdaa1093e7 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:271612142,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:12b11e2000b42ce1aaa228d9c1f4c9177395add2fa43835e667b7fc9007e40e6 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:270734898,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:88ddfbded8bc27b227ed7397ece050b756e522a9ffc34cbfba3c94c5ee58b740 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:269654227,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8bd90fcca7990c0edead15298dcec963968274d299428da95eae41aa23157b90 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:264244297,},ContainerImage{Names:[k8s.gcr.io/etcd@sha256:4ad90a11b55313b182afc186b9876c8e891531b8db4c9bf1541953021618d0e2 k8s.gcr.io/etcd@sha256:bd4d2c9a19be8a492bc79df53eee199fd04b415e9993eb69f7718052602a147a k8s.gcr.io/etcd:3.4.13-0],SizeBytes:254662613,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:73fdcef5de85e739831c5a8b76dce349a3c8832ff416a46263743d7e61655cbb quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:247448489,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b635694dc0663a1404d43a4a9ac8513a3087c7cfead50f6ab413f3c217c40b2a quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:240293385,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:71051bdf1b96c953fc1dfd48359915bf5c027613de6f5e2fa8adeea8d3dda311 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:236515650,},ContainerImage{Names:[quay.io/app-sre/cloud-ingress-operator@sha256:ceb064d013f51cdb15075f079eee2dfd5a0f42f4a09b489176abe92e91fb6a56 quay.io/app-sre/cloud-ingress-operator:v0.1.233-a2e288b],SizeBytes:203838503,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:9e842e2f77073ec30e2349f9082ec35357a3bf61e12488adc677144295e68c38 gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0],SizeBytes:203262845,},ContainerImage{Names:[quay.io/app-sre/managed-upgrade-operator@sha256:558a828a0b3c296cb2c0e52ef143a45177bdae40b1c84bfe8564486c940fd79a quay.io/app-sre/managed-upgrade-operator:9b59534],SizeBytes:198448288,},ContainerImage{Names:[quay.io/app-sre/splunk-forwarder-operator@sha256:bca474387af6112b08406697ef28977d428c1a884ebaf602a3e6ba1b45e6b31b quay.io/app-sre/splunk-forwarder-operator:7e8bc93],SizeBytes:191465195,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Oct 29 21:20:59.363: INFO: 
Logging kubelet events for node ip-10-0-142-212.us-east-2.compute.internal
Oct 29 21:20:59.385: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-142-212.us-east-2.compute.internal
Oct 29 21:20:59.461: INFO: dns-default-b89gt started at 2020-10-29 17:30:44 +0000 UTC (0+3 container statuses recorded)
Oct 29 21:20:59.461: INFO: 	Container dns ready: true, restart count 0
Oct 29 21:20:59.461: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 29 21:20:59.461: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:20:59.461: INFO: osd-patch-subscription-source-1604005200-ccz9j started at 2020-10-29 21:00:09 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:20:59.461: INFO: 	Container osd-patch-subscription-source ready: false, restart count 0
Oct 29 21:20:59.461: INFO: builds-pruner-1604005200-8wspq started at 2020-10-29 21:00:09 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:20:59.461: INFO: 	Container builds-pruner ready: false, restart count 0
Oct 29 21:20:59.461: INFO: node-exporter-p2phw started at 2020-10-29 17:30:44 +0000 UTC (1+2 container statuses recorded)
Oct 29 21:20:59.461: INFO: 	Init container init-textfile ready: true, restart count 0
Oct 29 21:20:59.461: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:20:59.461: INFO: 	Container node-exporter ready: true, restart count 0
Oct 29 21:20:59.461: INFO: node-ca-rq9vr started at 2020-10-29 17:30:44 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:20:59.461: INFO: 	Container node-ca ready: true, restart count 0
Oct 29 21:20:59.461: INFO: network-metrics-daemon-xrvxh started at 2020-10-29 17:30:44 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:20:59.461: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:20:59.461: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Oct 29 21:20:59.461: INFO: image-pruner-1604005200-bp5kw started at 2020-10-29 21:00:09 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:20:59.461: INFO: 	Container image-pruner ready: false, restart count 0
Oct 29 21:20:59.461: INFO: machine-config-daemon-l4r47 started at 2020-10-29 17:30:44 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:20:59.461: INFO: 	Container machine-config-daemon ready: true, restart count 0
Oct 29 21:20:59.461: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:20:59.461: INFO: ovs-66jns started at 2020-10-29 17:30:43 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:20:59.461: INFO: 	Container openvswitch ready: true, restart count 0
Oct 29 21:20:59.461: INFO: sdn-lbx27 started at 2020-10-29 17:30:44 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:20:59.461: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Oct 29 21:20:59.461: INFO: 	Container sdn ready: true, restart count 0
Oct 29 21:20:59.461: INFO: alert-pruner-1604005200-gtct6 started at 2020-10-29 21:00:09 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:20:59.461: INFO: 	Container alert-pruner ready: false, restart count 0
Oct 29 21:20:59.461: INFO: aws-ebs-csi-driver-node-5gmf4 started at 2020-10-29 17:30:44 +0000 UTC (0+3 container statuses recorded)
Oct 29 21:20:59.461: INFO: 	Container csi-driver ready: true, restart count 0
Oct 29 21:20:59.461: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Oct 29 21:20:59.461: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Oct 29 21:20:59.461: INFO: sre-dns-latency-exporter-29j4j started at 2020-10-29 20:32:47 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:20:59.461: INFO: 	Container main ready: true, restart count 0
Oct 29 21:20:59.461: INFO: crd-pruner-1604005200-w76df started at 2020-10-29 21:00:09 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:20:59.461: INFO: 	Container crd-pruner ready: false, restart count 0
Oct 29 21:20:59.461: INFO: multus-jst6k started at 2020-10-29 17:30:43 +0000 UTC (5+1 container statuses recorded)
Oct 29 21:20:59.461: INFO: 	Init container multus-binary-copy ready: true, restart count 0
Oct 29 21:20:59.461: INFO: 	Init container cni-plugins ready: true, restart count 0
Oct 29 21:20:59.461: INFO: 	Init container routeoverride-cni ready: true, restart count 0
Oct 29 21:20:59.461: INFO: 	Init container whereabouts-cni-bincopy ready: true, restart count 0
Oct 29 21:20:59.461: INFO: 	Init container whereabouts-cni ready: true, restart count 0
Oct 29 21:20:59.461: INFO: 	Container kube-multus ready: true, restart count 0
Oct 29 21:20:59.461: INFO: tuned-d6fwx started at 2020-10-29 17:30:44 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:20:59.461: INFO: 	Container tuned ready: true, restart count 0
W1029 21:20:59.483475   53132 metrics_grabber.go:83] Can't find any pods in namespace kube-system to grab metrics from
W1029 21:20:59.483494   53132 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1029 21:20:59.483501   53132 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1029 21:20:59.483507   53132 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Oct 29 21:20:59.557: INFO: 
Latency metrics for node ip-10-0-142-212.us-east-2.compute.internal
Oct 29 21:20:59.557: INFO: 
Logging node info for node ip-10-0-147-237.us-east-2.compute.internal
Oct 29 21:20:59.579: INFO: Node Info: &Node{ObjectMeta:{ip-10-0-147-237.us-east-2.compute.internal   /api/v1/nodes/ip-10-0-147-237.us-east-2.compute.internal aa194f55-2a68-47c3-afba-bc25619fed22 203621 0 2020-10-29 17:24:16 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:m5.xlarge beta.kubernetes.io/os:linux failure-domain.beta.kubernetes.io/region:us-east-2 failure-domain.beta.kubernetes.io/zone:us-east-2a kubernetes.io/arch:amd64 kubernetes.io/hostname:ip-10-0-147-237 kubernetes.io/os:linux node-role.kubernetes.io/master: node.kubernetes.io/instance-type:m5.xlarge node.openshift.io/os_id:rhcos topology.ebs.csi.aws.com/zone:us-east-2a topology.kubernetes.io/region:us-east-2 topology.kubernetes.io/zone:us-east-2a] map[csi.volume.kubernetes.io/nodeid:{"ebs.csi.aws.com":"i-0442e8b0a58ab50b7"} machine.openshift.io/machine:openshift-machine-api/jeder-461-cncf2-pvrph-master-0 machineconfiguration.openshift.io/currentConfig:rendered-master-b06f274cfa558dc0b5031aabca48f868 machineconfiguration.openshift.io/desiredConfig:rendered-master-b06f274cfa558dc0b5031aabca48f868 machineconfiguration.openshift.io/reason: machineconfiguration.openshift.io/state:Done volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{nodelink-controller Update v1 2020-10-29 17:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machine.openshift.io/machine":{}}}}} {machine-config-controller Update v1 2020-10-29 17:48:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/desiredConfig":{}}}}} {machine-config-daemon Update v1 2020-10-29 17:52:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/currentConfig":{},"f:machineconfiguration.openshift.io/reason":{},"f:machineconfiguration.openshift.io/state":{}}}}} {kube-controller-manager Update v1 2020-10-29 19:08:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{}}},"f:spec":{"f:podCIDR":{},"f:podCIDRs":{".":{},"v:\"10.128.2.0/24\"":{}},"f:taints":{}}}} {oc Update v1 2020-10-29 19:08:28 +0000 UTC FieldsV1 {"f:spec":{"f:unschedulable":{}}}} {kubelet Update v1 2020-10-29 21:17:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:csi.volume.kubernetes.io/nodeid":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/instance-type":{},"f:failure-domain.beta.kubernetes.io/region":{},"f:failure-domain.beta.kubernetes.io/zone":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:node-role.kubernetes.io/master":{},"f:node.kubernetes.io/instance-type":{},"f:node.openshift.io/os_id":{},"f:topology.ebs.csi.aws.com/zone":{},"f:topology.kubernetes.io/region":{},"f:topology.kubernetes.io/zone":{}}},"f:spec":{"f:providerID":{}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalDNS\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:attachable-volumes-aws-ebs":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:attachable-volumes-aws-ebs":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"DiskPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"MemoryPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PIDPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:images":{},"f:nodeInfo":{"f:architecture":{},"f:bootID":{},"f:containerRuntimeVersion":{},"f:kernelVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{},"f:machineID":{},"f:operatingSystem":{},"f:osImage":{},"f:systemUUID":{}}}}}]},Spec:NodeSpec{PodCIDR:10.128.2.0/24,DoNotUseExternalID:,ProviderID:aws:///us-east-2a/i-0442e8b0a58ab50b7,Unschedulable:true,Taints:[]Taint{Taint{Key:node-role.kubernetes.io/master,Value:,Effect:NoSchedule,TimeAdded:<nil>,},Taint{Key:node.kubernetes.io/unschedulable,Value:,Effect:NoSchedule,TimeAdded:2020-10-29 19:08:28 +0000 UTC,},},ConfigSource:nil,PodCIDRs:[10.128.2.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{375244435456 0} {<nil>} 366449644Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{16502939648 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},},Allocatable:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{3 0} {<nil>} 3 DecimalSI},ephemeral-storage: {{336646249528 0} {<nil>} 336646249528 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{15324340224 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2020-10-29 21:17:18 +0000 UTC,LastTransitionTime:2020-10-29 17:52:01 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2020-10-29 21:17:18 +0000 UTC,LastTransitionTime:2020-10-29 17:52:01 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2020-10-29 21:17:18 +0000 UTC,LastTransitionTime:2020-10-29 17:52:01 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2020-10-29 21:17:18 +0000 UTC,LastTransitionTime:2020-10-29 17:52:01 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.0.147.237,},NodeAddress{Type:Hostname,Address:ip-10-0-147-237.us-east-2.compute.internal,},NodeAddress{Type:InternalDNS,Address:ip-10-0-147-237.us-east-2.compute.internal,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:ec208baaeac2115a33465684b95070b7,SystemUUID:ec208baa-eac2-115a-3346-5684b95070b7,BootID:393f7083-012b-49c7-b353-7f3ef17ffba3,KernelVersion:4.18.0-193.24.1.el8_2.dt1.x86_64,OSImage:Red Hat Enterprise Linux CoreOS 46.82.202010091720-0 (Ootpa),ContainerRuntimeVersion:cri-o://1.19.0-20.rhaos4.6.git97d715e.el8,KubeletVersion:v1.19.0+d59ce34,KubeProxyVersion:v1.19.0+d59ce34,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:30094a2d586aa282d85e14f1be19abec1c30ce431673377b0e1c12d83e6bac8c quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:682301224,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c51d44f380ecff7d36b1de4bb3bdbd4ac66abc6669724f28d81bd4af5741a8ac quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:514079422,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:82d1def7312de8ae5dee32d237ad59fe685923e78668fa3547e6bee445cd8842 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:499341399,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4cfd55719faff41e96c2e4be69e3f2381a57b8b3445b80ae4acfe8ee33d7f99b quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:481479938,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ae6a5decd040a6b3adfa074d3211ab92a36b77b2d849962d9a678e1c2c5ef5c1 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:473291773,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8923050603588c27d79b33b371afb651288470d5cdeb14f8e10249bca1a1c461 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:413031192,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8f4882cff3c2f9521215eac681c5abda42876e3e955431c1387fb457940b8344 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:370109724,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:72323ce541f8a26fbad17ef65ff21b51498863bb851635a0faa8d5b1ac6ce0e4 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:366911472,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b7dc5f4101a8cb88c20d853908982258cab77bb0ac391e965b50b15648ddd854 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:359851126,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:93b3e1246884e357e1654e6c9578481aff9eef07eed1f9fdd0e9c8cc89a3770c quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:358141917,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e2b3f973bc5b9e55d2240a556c4648c921a3c8d3e12381757f1990a864208617 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:343841040,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a8214df42df962b965e3f4daad0b61932235e57241160861e503d84e38b775d5 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:337507384,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6bada08687c20afe316c1f8cf76f001f31bedae317f256f3df3affaa5d0dc25e quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:336987966,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6aa4bb97adf2142b0e74ccae7fd3661ada73cbaac803b86bb8712261e916d66d quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:336576591,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:aa4f37543b45bc248db8d9bd2dc45b6e159a8869b044c2310f541afba15b2694 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:336485164,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f52825e9905c926d399cd0b7afbb2b7d0370ae22da0416feac9131d555db0b98 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:336041278,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a2a167f59783ca402118fe35ea5fefbf457e01b64836f8be3be6695aefd76d76 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:335654632,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:059f0179c0528c6234dbdca7e70fe779cf37be5121f458dd045d2e9662192f06 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:335510757,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:5e591cab19b41c7ea26eab6056cd518f6d64b59e8051978de927b1b984abfb1d quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:334612215,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:7584014b0cb8cb2c5a09b909c79f2f8ad6e49854bcfabf72e96a22330bcf6f56 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:333011308,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:357e35286fd26fed015c03a9c451f6fdcf61cf0821d959025e7f800e7c533f29 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:332777114,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6ca671c810426b8c4f13dd0c7ac19639f9f265b952b8feb5a828e59fab785335 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:332487959,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f86db3170270fc635dff0d7f1ba6e79a8f45de7e1dcfa5621474d1f6e07352ec quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:325370491,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:54fbeb744b82662fd38c0d301ebaad6ca8983707bc44db7235ead0fb7b95808f quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:321482743,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:11c3a8bdddbbb2229bd68bb80b6009588873118881952c702dfebd1484046191 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:319944375,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9dbb31ac799b2c30270268714dcb3d11bafb329b98639a446657c8c7db41938c quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:318368538,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1ad85195e1a180698fe4b8df82e3d72075efb256b53f593d13e29faaf7f3e15a quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:316310728,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dc6a6a1d4a6b2af67421561e53d1af1d40c99ae72de69f4c3cc390d447f12269 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:315339691,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4691dc29704c9cb06d2345894f1a8f074b58a0d208318c5218241388b0916e1b quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:315327750,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f7b9278ef2fbe988f50e4bdeeea79d9373b55689d17b8c6d7c214429f5b3f9a0 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:315119766,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4289297f0b7ee7edf394348fd07e1fa1b3162655f2a2af2245e23af4b179e7f2 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:314343750,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:65206861218064576dc092040e9c24b0393b8a07502e351f513400f187f38cc7 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:313489720,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:45586fd7a5cfd43ff546dbfb282a70a91eaf0f069f604230af958dc802832f89 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:312615681,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c6b3aaaa38679b1d752ec09bd68c6d80a8911c74ec16d27c49de88ecb97823ee quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:312540521,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:01250de496444bb624ec7b472ac9b0f7023809c88306a71c6ac87bb302f7dbe3 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:312449767,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:25197b2709c0691c28424c9b07e505a71d13bf481e18bc42636cc84ee8fef033 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:312275620,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ee4abe53e80e561239e510a6f9999b4dc80b7b3fdc9848ab43d0bf8df24e815d quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:311984477,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:26f6c930942ee4dea7c1e22d220bba11561c37bdc47101c4490ce0ef77c9203a quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:311353697,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2689d3e66bbfdd7d493d969524b8a7da00142d1b4372e3e880bd825beb3da558 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:311351002,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9b564f882e31f497f57a0d99d406d5231eb15e9a97f0b450c21bec2bac7ff033 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:311323012,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a96e2e4a62bca22da0b6903c9e20d7c776bd241f13accf51ede88965b232aca8 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:310235384,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0531ff2ccf0ddea76e42cc9951470528bbd7def094884bc569f660376798f40a quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:308198752,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:12fe384de71c7621d9061f48afafeed3dc337679a66afd8d0a871e200295a1e5 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:307996829,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:82758fbc97d9da98f20eddcfb4a8bc279726b97da96263d4c165b404389cb553 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:307966006,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e176509a104b2f96ee4f5c57275c6a712409aa80ac40071345d0a03bdde2b456 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:307311930,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f18151bf70434e1841ed8182c42e819e92e3d1ad3bbd269c667be8b74ff78444 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:306621135,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:afdf0a3b426ac1c03df52e88a2b884f0714e54a1a03f33091954441a05a7f6b9 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:306116297,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:076b280e17c6bb4cc618db71403ccec75f8196c8849061a40c680a2808292bb6 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:304641618,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:01626d98c80e44e0cd3a522ad019eb236e39c30b0dfff0ac5a6fa98686159286 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:303742706,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:cfe62d81269929501517e75a7d337f7d8fc78ac9a17665adebfef52a2024584d quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:303388513,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Oct 29 21:20:59.579: INFO: 
Logging kubelet events for node ip-10-0-147-237.us-east-2.compute.internal
Oct 29 21:20:59.601: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-147-237.us-east-2.compute.internal
Oct 29 21:20:59.663: INFO: network-metrics-daemon-v66fq started at 2020-10-29 17:24:20 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:20:59.663: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:20:59.663: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Oct 29 21:20:59.663: INFO: machine-config-server-6gqlt started at 2020-10-29 17:26:37 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:20:59.663: INFO: 	Container machine-config-server ready: true, restart count 0
Oct 29 21:20:59.663: INFO: installer-11-ip-10-0-147-237.us-east-2.compute.internal started at 2020-10-29 19:53:13 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:20:59.663: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:20:59.663: INFO: machine-config-daemon-nkm27 started at 2020-10-29 17:24:37 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:20:59.663: INFO: 	Container machine-config-daemon ready: true, restart count 0
Oct 29 21:20:59.663: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:20:59.663: INFO: node-exporter-nsn4k started at 2020-10-29 17:24:51 +0000 UTC (1+2 container statuses recorded)
Oct 29 21:20:59.663: INFO: 	Init container init-textfile ready: true, restart count 0
Oct 29 21:20:59.663: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:20:59.663: INFO: 	Container node-exporter ready: true, restart count 0
Oct 29 21:20:59.663: INFO: installer-9-ip-10-0-147-237.us-east-2.compute.internal started at 2020-10-29 17:52:01 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:20:59.663: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:20:59.663: INFO: revision-pruner-9-ip-10-0-147-237.us-east-2.compute.internal started at 2020-10-29 19:45:50 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:20:59.663: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:20:59.663: INFO: multus-admission-controller-8gpns started at 2020-10-29 17:25:06 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:20:59.663: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:20:59.663: INFO: 	Container multus-admission-controller ready: true, restart count 0
Oct 29 21:20:59.663: INFO: node-ca-crbk4 started at 2020-10-29 17:25:50 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:20:59.663: INFO: 	Container node-ca ready: true, restart count 0
Oct 29 21:20:59.663: INFO: controller-manager-wm9j7 started at 2020-10-29 17:30:59 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:20:59.663: INFO: 	Container controller-manager ready: true, restart count 0
Oct 29 21:20:59.663: INFO: etcd-quorum-guard-644f5747b8-4fzdq started at 2020-10-29 17:52:23 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:20:59.663: INFO: 	Container guard ready: true, restart count 0
Oct 29 21:20:59.663: INFO: aws-ebs-csi-driver-node-dbdmq started at 2020-10-29 17:24:58 +0000 UTC (0+3 container statuses recorded)
Oct 29 21:20:59.663: INFO: 	Container csi-driver ready: true, restart count 0
Oct 29 21:20:59.663: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Oct 29 21:20:59.663: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Oct 29 21:20:59.663: INFO: revision-pruner-9-ip-10-0-147-237.us-east-2.compute.internal started at 2020-10-29 17:52:01 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:20:59.663: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:20:59.663: INFO: recyler-pod-ip-10-0-147-237.us-east-2.compute.internal started at 2020-10-29 17:52:01 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:20:59.663: INFO: 	Container recyler-container ready: false, restart count 0
Oct 29 21:20:59.663: INFO: installer-11-ip-10-0-147-237.us-east-2.compute.internal started at 2020-10-29 19:48:02 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:20:59.663: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:20:59.663: INFO: kube-apiserver-ip-10-0-147-237.us-east-2.compute.internal started at 2020-10-29 17:28:31 +0000 UTC (1+5 container statuses recorded)
Oct 29 21:20:59.663: INFO: 	Init container setup ready: true, restart count 1
Oct 29 21:20:59.663: INFO: 	Container kube-apiserver ready: true, restart count 0
Oct 29 21:20:59.663: INFO: 	Container kube-apiserver-cert-regeneration-controller ready: true, restart count 0
Oct 29 21:20:59.663: INFO: 	Container kube-apiserver-cert-syncer ready: true, restart count 0
Oct 29 21:20:59.663: INFO: 	Container kube-apiserver-check-endpoints ready: true, restart count 0
Oct 29 21:20:59.663: INFO: 	Container kube-apiserver-insecure-readyz ready: true, restart count 0
Oct 29 21:20:59.663: INFO: sdn-controller-h64cq started at 2020-10-29 17:24:20 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:20:59.663: INFO: 	Container sdn-controller ready: true, restart count 0
Oct 29 21:20:59.663: INFO: etcd-ip-10-0-147-237.us-east-2.compute.internal started at 2020-10-29 17:27:04 +0000 UTC (2+3 container statuses recorded)
Oct 29 21:20:59.663: INFO: 	Init container etcd-ensure-env-vars ready: true, restart count 0
Oct 29 21:20:59.663: INFO: 	Init container etcd-resources-copy ready: true, restart count 0
Oct 29 21:20:59.663: INFO: 	Container etcd ready: true, restart count 0
Oct 29 21:20:59.663: INFO: 	Container etcd-metrics ready: true, restart count 0
Oct 29 21:20:59.663: INFO: 	Container etcdctl ready: true, restart count 0
Oct 29 21:20:59.663: INFO: revision-pruner-9-ip-10-0-147-237.us-east-2.compute.internal started at 2020-10-29 17:52:46 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:20:59.663: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:20:59.663: INFO: dns-default-5hsz9 started at 2020-10-29 17:25:40 +0000 UTC (0+3 container statuses recorded)
Oct 29 21:20:59.663: INFO: 	Container dns ready: true, restart count 0
Oct 29 21:20:59.663: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 29 21:20:59.663: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:20:59.663: INFO: revision-pruner-8-ip-10-0-147-237.us-east-2.compute.internal started at 2020-10-29 17:52:01 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:20:59.663: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:20:59.663: INFO: revision-pruner-3-ip-10-0-147-237.us-east-2.compute.internal started at 2020-10-29 17:52:01 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:20:59.664: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:20:59.664: INFO: installer-10-ip-10-0-147-237.us-east-2.compute.internal started at 2020-10-29 19:47:16 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:20:59.664: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:20:59.664: INFO: kube-controller-manager-ip-10-0-147-237.us-east-2.compute.internal started at 2020-10-29 17:25:56 +0000 UTC (0+4 container statuses recorded)
Oct 29 21:20:59.664: INFO: 	Container cluster-policy-controller ready: true, restart count 0
Oct 29 21:20:59.664: INFO: 	Container kube-controller-manager ready: true, restart count 0
Oct 29 21:20:59.664: INFO: 	Container kube-controller-manager-cert-syncer ready: true, restart count 0
Oct 29 21:20:59.664: INFO: 	Container kube-controller-manager-recovery-controller ready: true, restart count 1
Oct 29 21:20:59.664: INFO: revision-pruner-10-ip-10-0-147-237.us-east-2.compute.internal started at 2020-10-29 19:47:46 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:20:59.664: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:20:59.664: INFO: tuned-ffzjj started at 2020-10-29 17:24:46 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:20:59.664: INFO: 	Container tuned ready: true, restart count 0
Oct 29 21:20:59.664: INFO: revision-pruner-7-ip-10-0-147-237.us-east-2.compute.internal started at 2020-10-29 17:52:01 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:20:59.664: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:20:59.664: INFO: apiserver-8b58dffb-5kptd started at 2020-10-29 17:52:23 +0000 UTC (1+2 container statuses recorded)
Oct 29 21:20:59.664: INFO: 	Init container fix-audit-permissions ready: true, restart count 0
Oct 29 21:20:59.664: INFO: 	Container openshift-apiserver ready: true, restart count 0
Oct 29 21:20:59.664: INFO: 	Container openshift-apiserver-check-endpoints ready: true, restart count 0
Oct 29 21:20:59.664: INFO: apiserver-566bbccb57-8vqrg started at 2020-10-29 17:52:23 +0000 UTC (1+1 container statuses recorded)
Oct 29 21:20:59.664: INFO: 	Init container fix-audit-permissions ready: true, restart count 0
Oct 29 21:20:59.664: INFO: 	Container oauth-apiserver ready: true, restart count 0
Oct 29 21:20:59.664: INFO: revision-pruner-10-ip-10-0-147-237.us-east-2.compute.internal started at 2020-10-29 18:09:04 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:20:59.664: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:20:59.664: INFO: openshift-kube-scheduler-ip-10-0-147-237.us-east-2.compute.internal started at 2020-10-29 17:26:22 +0000 UTC (1+2 container statuses recorded)
Oct 29 21:20:59.664: INFO: 	Init container wait-for-host-port ready: true, restart count 0
Oct 29 21:20:59.664: INFO: 	Container kube-scheduler ready: true, restart count 0
Oct 29 21:20:59.664: INFO: 	Container kube-scheduler-cert-syncer ready: true, restart count 0
Oct 29 21:20:59.664: INFO: revision-pruner-11-ip-10-0-147-237.us-east-2.compute.internal started at 2020-10-29 19:56:51 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:20:59.664: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:20:59.664: INFO: sdn-hpdmg started at 2020-10-29 17:24:20 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:20:59.664: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Oct 29 21:20:59.664: INFO: 	Container sdn ready: true, restart count 0
Oct 29 21:20:59.664: INFO: multus-75rx8 started at 2020-10-29 17:24:20 +0000 UTC (5+1 container statuses recorded)
Oct 29 21:20:59.664: INFO: 	Init container multus-binary-copy ready: true, restart count 0
Oct 29 21:20:59.664: INFO: 	Init container cni-plugins ready: true, restart count 0
Oct 29 21:20:59.664: INFO: 	Init container routeoverride-cni ready: true, restart count 0
Oct 29 21:20:59.664: INFO: 	Init container whereabouts-cni-bincopy ready: true, restart count 0
Oct 29 21:20:59.664: INFO: 	Init container whereabouts-cni ready: true, restart count 0
Oct 29 21:20:59.664: INFO: 	Container kube-multus ready: true, restart count 0
Oct 29 21:20:59.665: INFO: ovs-9xsrf started at 2020-10-29 17:24:20 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:20:59.665: INFO: 	Container openvswitch ready: true, restart count 0
Oct 29 21:20:59.665: INFO: validation-webhook-bd6dg started at 2020-10-29 17:39:26 +0000 UTC (1+1 container statuses recorded)
Oct 29 21:20:59.665: INFO: 	Init container inject-cert ready: true, restart count 0
Oct 29 21:20:59.665: INFO: 	Container webhooks ready: true, restart count 0
Oct 29 21:20:59.665: INFO: revision-pruner-11-ip-10-0-147-237.us-east-2.compute.internal started at 2020-10-29 19:49:13 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:20:59.665: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:20:59.665: INFO: installer-9-ip-10-0-147-237.us-east-2.compute.internal started at 2020-10-29 19:45:19 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:20:59.665: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:20:59.665: INFO: installer-10-ip-10-0-147-237.us-east-2.compute.internal started at 2020-10-29 18:07:12 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:20:59.665: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:20:59.665: INFO: oauth-openshift-6dc6d68fbf-d652r started at 2020-10-29 18:36:40 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:20:59.665: INFO: 	Container oauth-openshift ready: true, restart count 0
W1029 21:20:59.687417   53132 metrics_grabber.go:83] Can't find any pods in namespace kube-system to grab metrics from
W1029 21:20:59.687485   53132 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1029 21:20:59.687505   53132 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1029 21:20:59.687570   53132 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Oct 29 21:20:59.786: INFO: 
Latency metrics for node ip-10-0-147-237.us-east-2.compute.internal
Oct 29 21:20:59.786: INFO: 
Logging node info for node ip-10-0-157-225.us-east-2.compute.internal
Oct 29 21:20:59.807: INFO: Node Info: &Node{ObjectMeta:{ip-10-0-157-225.us-east-2.compute.internal   /api/v1/nodes/ip-10-0-157-225.us-east-2.compute.internal 4b6ca834-58eb-4f4d-aff2-e969afb1fe22 203437 0 2020-10-29 17:41:12 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:m5.xlarge beta.kubernetes.io/os:linux failure-domain.beta.kubernetes.io/region:us-east-2 failure-domain.beta.kubernetes.io/zone:us-east-2a kubernetes.io/arch:amd64 kubernetes.io/hostname:ip-10-0-157-225 kubernetes.io/os:linux node-role.kubernetes.io:infra node-role.kubernetes.io/infra: node-role.kubernetes.io/worker: node.kubernetes.io/instance-type:m5.xlarge node.openshift.io/os_id:rhcos topology.ebs.csi.aws.com/zone:us-east-2a topology.kubernetes.io/region:us-east-2 topology.kubernetes.io/zone:us-east-2a] map[csi.volume.kubernetes.io/nodeid:{"ebs.csi.aws.com":"i-0b87fa50e817e6ce3"} machine.openshift.io/machine:openshift-machine-api/jeder-461-cncf2-pvrph-infra-us-east-2a-m2hff machineconfiguration.openshift.io/currentConfig:rendered-worker-f26c105cafab97113f99101cc57ac048 machineconfiguration.openshift.io/desiredConfig:rendered-worker-f26c105cafab97113f99101cc57ac048 machineconfiguration.openshift.io/reason: machineconfiguration.openshift.io/state:Done volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{nodelink-controller Update v1 2020-10-29 17:41:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machine.openshift.io/machine":{}},"f:labels":{"f:node-role.kubernetes.io":{},"f:node-role.kubernetes.io/infra":{}}}}} {machine-config-controller Update v1 2020-10-29 17:46:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/desiredConfig":{}}}}} {machine-config-daemon Update v1 2020-10-29 17:48:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/currentConfig":{},"f:machineconfiguration.openshift.io/reason":{},"f:machineconfiguration.openshift.io/state":{}}}}} {kube-controller-manager Update v1 2020-10-29 19:47:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{}}},"f:spec":{"f:podCIDR":{},"f:podCIDRs":{".":{},"v:\"10.128.5.0/24\"":{}},"f:taints":{}},"f:status":{"f:volumesAttached":{}}}} {kubelet Update v1 2020-10-29 21:16:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:csi.volume.kubernetes.io/nodeid":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/instance-type":{},"f:failure-domain.beta.kubernetes.io/region":{},"f:failure-domain.beta.kubernetes.io/zone":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:node-role.kubernetes.io/worker":{},"f:node.kubernetes.io/instance-type":{},"f:node.openshift.io/os_id":{},"f:topology.ebs.csi.aws.com/zone":{},"f:topology.kubernetes.io/region":{},"f:topology.kubernetes.io/zone":{}}},"f:spec":{"f:providerID":{}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalDNS\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:attachable-volumes-aws-ebs":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:attachable-volumes-aws-ebs":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"DiskPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"MemoryPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PIDPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:images":{},"f:nodeInfo":{"f:architecture":{},"f:bootID":{},"f:containerRuntimeVersion":{},"f:kernelVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{},"f:machineID":{},"f:operatingSystem":{},"f:osImage":{},"f:systemUUID":{}},"f:volumesInUse":{}}}}]},Spec:NodeSpec{PodCIDR:10.128.5.0/24,DoNotUseExternalID:,ProviderID:aws:///us-east-2a/i-0b87fa50e817e6ce3,Unschedulable:false,Taints:[]Taint{Taint{Key:node-role.kubernetes.io/infra,Value:,Effect:PreferNoSchedule,TimeAdded:<nil>,},},ConfigSource:nil,PodCIDRs:[10.128.5.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{321557344256 0} {<nil>} 314020844Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{16502939648 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},},Allocatable:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{3 0} {<nil>} 3 DecimalSI},ephemeral-storage: {{288327867528 0} {<nil>} 288327867528 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{15324340224 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2020-10-29 21:16:49 +0000 UTC,LastTransitionTime:2020-10-29 17:48:32 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2020-10-29 21:16:49 +0000 UTC,LastTransitionTime:2020-10-29 17:48:32 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2020-10-29 21:16:49 +0000 UTC,LastTransitionTime:2020-10-29 17:48:32 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2020-10-29 21:16:49 +0000 UTC,LastTransitionTime:2020-10-29 17:48:32 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.0.157.225,},NodeAddress{Type:Hostname,Address:ip-10-0-157-225.us-east-2.compute.internal,},NodeAddress{Type:InternalDNS,Address:ip-10-0-157-225.us-east-2.compute.internal,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:ec2d145ac9adc52455b78a8130a251d9,SystemUUID:ec2d145a-c9ad-c524-55b7-8a8130a251d9,BootID:00446280-a220-46d9-9e85-83069f21255e,KernelVersion:4.18.0-193.24.1.el8_2.dt1.x86_64,OSImage:Red Hat Enterprise Linux CoreOS 46.82.202010091720-0 (Ootpa),ContainerRuntimeVersion:cri-o://1.19.0-20.rhaos4.6.git97d715e.el8,KubeletVersion:v1.19.0+d59ce34,KubeProxyVersion:v1.19.0+d59ce34,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:82d1def7312de8ae5dee32d237ad59fe685923e78668fa3547e6bee445cd8842 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:499341399,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4cfd55719faff41e96c2e4be69e3f2381a57b8b3445b80ae4acfe8ee33d7f99b quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:481479938,},ContainerImage{Names:[quay.io/jupierce/openshift-python-monitoring@sha256:98e3259170f7116837fa60b0515fe79cb9721060d34eb4931a14244b4658bb8d quay.io/jupierce/openshift-python-monitoring:stable],SizeBytes:474594250,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8248dec0d94b2928aa4d63a22973d9a8f8f173a1431b2ab4ad15fdfe80283d7c quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:419419150,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8923050603588c27d79b33b371afb651288470d5cdeb14f8e10249bca1a1c461 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:413031192,},ContainerImage{Names:[quay.io/openshift-sre/managed-prometheus-exporter-initcontainer@sha256:7fa36e4488191536ac4a570fdd3431d3c80db983db74b90bc2ce051ecef3035e quay.io/openshift-sre/managed-prometheus-exporter-initcontainer:v0.1.9-2019-03-28-4e558131],SizeBytes:388062834,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8f4882cff3c2f9521215eac681c5abda42876e3e955431c1387fb457940b8344 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:370109724,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:72323ce541f8a26fbad17ef65ff21b51498863bb851635a0faa8d5b1ac6ce0e4 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:366911472,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3d0361f380abf5252b1b640e3ceaaab8274e2af8cdb605b20b513a1a44b3a4dc quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:346080704,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f644e4b495071a2b9d0d6f5d48cb96dad9f7ea8298cc22c98824bc70229ea9dd quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:341116512,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6bada08687c20afe316c1f8cf76f001f31bedae317f256f3df3affaa5d0dc25e quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:336987966,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6aa4bb97adf2142b0e74ccae7fd3661ada73cbaac803b86bb8712261e916d66d quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:336576591,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:52424fd2af6fd7d7a5a1233032eb3f3c67f7691996b209e013e29f1524c5188c quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:326057200,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f86db3170270fc635dff0d7f1ba6e79a8f45de7e1dcfa5621474d1f6e07352ec quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:325370491,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:54fbeb744b82662fd38c0d301ebaad6ca8983707bc44db7235ead0fb7b95808f quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:321482743,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a37a568c63563257309cb0ffb6e185d98f662ff3201d2099cbe0df404b93f0c8 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:314357110,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:26f6c930942ee4dea7c1e22d220bba11561c37bdc47101c4490ce0ef77c9203a quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:311353697,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e176509a104b2f96ee4f5c57275c6a712409aa80ac40071345d0a03bdde2b456 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:307311930,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0c42cd3a74176732e8c06105c47674c7d410c7167c8e3fbd80f9a76e9bfda5bd quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:302591889,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c75977f28becdf4f7065cfa37233464dd31208b1767e620c4f19658f53f8ff8c quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:300756060,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:cf565b2fab365e027962a25a8cffb41aa35cb5a00d001e081d53c7fed5a0c54b quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:300005108,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9e7f0468850aeb13585ef049f687cc42c05d82bc0e0200607d1a93d7f9740fe5 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:291621018,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6cdaecd5dd9df8fd74529be7fa5d8973daf6f4ea95be8acfb2f5ac97773ebe67 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:278303432,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e5bcf6d786fd218e1ef188eb40c39c31a98d03121fba3b7a1f16e87e45a7478b quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:276735847,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1dcc413b621958f97dfbb3fc998a9e225ef155a80ffb151eb4694bf8370b383a quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:274559501,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8c9e61400619c4613db5cc73097d287e3cd5d2125c85d1d84cc30cfdaa1093e7 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:271612142,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:12b11e2000b42ce1aaa228d9c1f4c9177395add2fa43835e667b7fc9007e40e6 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:270734898,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:88ddfbded8bc27b227ed7397ece050b756e522a9ffc34cbfba3c94c5ee58b740 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:269654227,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8bd90fcca7990c0edead15298dcec963968274d299428da95eae41aa23157b90 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:264244297,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:73fdcef5de85e739831c5a8b76dce349a3c8832ff416a46263743d7e61655cbb quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:247448489,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b635694dc0663a1404d43a4a9ac8513a3087c7cfead50f6ab413f3c217c40b2a quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:240293385,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:71051bdf1b96c953fc1dfd48359915bf5c027613de6f5e2fa8adeea8d3dda311 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:236515650,},ContainerImage{Names:[docker.io/velero/velero@sha256:1e7b1f13e4e807952f069295f19686c4eb9380e59013771d8c1960fc4fa73634 docker.io/velero/velero@sha256:e248548c6787f5451e1ba42a48ad4890ce1fc10aa6f5efca9aeb3aa6cda396ff docker.io/velero/velero:v1.5.2],SizeBytes:165235519,},ContainerImage{Names:[docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4 docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:128894988,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0 k8s.gcr.io/e2e-test-images/agnhost@sha256:c243d6fc39291ac6ea9caf6b511719980d9c8d2e54dda8217ee019fbf9a124e1 k8s.gcr.io/e2e-test-images/agnhost:2.20],SizeBytes:116455133,},ContainerImage{Names:[docker.io/velero/velero-plugin-for-aws@sha256:e52d3545c3c52dbd061f0bf2ae8f7d6b21747d0a8bc64245fb58c8de54df9b33 docker.io/velero/velero-plugin-for-aws:v1.1.0],SizeBytes:112580300,},ContainerImage{Names:[k8s.gcr.io/pause@sha256:4a1c4b21597c1b4415bdbecb28a3296c6b5e23ca4f9feeb599860a1dac6a0108 k8s.gcr.io/pause@sha256:927d98197ec1141a368550822d18fa1c60bdae27b78b0c004f705f548c07814f k8s.gcr.io/pause:3.2],SizeBytes:688049,},},VolumesInUse:[kubernetes.io/aws-ebs/aws://us-east-2a/vol-043da107373ed8f17 kubernetes.io/aws-ebs/aws://us-east-2a/vol-06f1085fc292fe55d kubernetes.io/aws-ebs/aws://us-east-2a/vol-07e1e4fd424670210 kubernetes.io/aws-ebs/aws://us-east-2a/vol-08e61d2d126b801f3 kubernetes.io/aws-ebs/aws://us-east-2a/vol-0922ddac45c4a6428],VolumesAttached:[]AttachedVolume{AttachedVolume{Name:kubernetes.io/aws-ebs/aws://us-east-2a/vol-06f1085fc292fe55d,DevicePath:/dev/xvdch,},AttachedVolume{Name:kubernetes.io/aws-ebs/aws://us-east-2a/vol-08e61d2d126b801f3,DevicePath:/dev/xvdbn,},AttachedVolume{Name:kubernetes.io/aws-ebs/aws://us-east-2a/vol-07e1e4fd424670210,DevicePath:/dev/xvdbu,},AttachedVolume{Name:kubernetes.io/aws-ebs/aws://us-east-2a/vol-043da107373ed8f17,DevicePath:/dev/xvdbi,},AttachedVolume{Name:kubernetes.io/aws-ebs/aws://us-east-2a/vol-0922ddac45c4a6428,DevicePath:/dev/xvdcv,},},Config:nil,},}
Oct 29 21:20:59.808: INFO: 
Logging kubelet events for node ip-10-0-157-225.us-east-2.compute.internal
Oct 29 21:20:59.830: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-157-225.us-east-2.compute.internal
Oct 29 21:20:59.876: INFO: tuned-4tm5g started at 2020-10-29 17:41:11 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:20:59.877: INFO: 	Container tuned ready: true, restart count 0
Oct 29 21:20:59.877: INFO: dns-default-hr5xr started at 2020-10-29 17:41:12 +0000 UTC (0+3 container statuses recorded)
Oct 29 21:20:59.877: INFO: 	Container dns ready: true, restart count 0
Oct 29 21:20:59.877: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 29 21:20:59.877: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:20:59.877: INFO: image-registry-75b75c555d-cm6gf started at 2020-10-29 17:48:47 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:20:59.877: INFO: 	Container registry ready: true, restart count 0
Oct 29 21:20:59.877: INFO: aws-ebs-csi-driver-node-qvwrc started at 2020-10-29 17:41:11 +0000 UTC (0+3 container statuses recorded)
Oct 29 21:20:59.877: INFO: 	Container csi-driver ready: true, restart count 0
Oct 29 21:20:59.877: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Oct 29 21:20:59.877: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Oct 29 21:20:59.877: INFO: prometheus-adapter-6495ff5885-lw9vm started at 2020-10-29 17:48:48 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:20:59.877: INFO: 	Container prometheus-adapter ready: true, restart count 0
Oct 29 21:20:59.877: INFO: alertmanager-main-0 started at 2020-10-29 17:49:01 +0000 UTC (0+5 container statuses recorded)
Oct 29 21:20:59.877: INFO: 	Container alertmanager ready: true, restart count 0
Oct 29 21:20:59.877: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Oct 29 21:20:59.877: INFO: 	Container config-reloader ready: true, restart count 0
Oct 29 21:20:59.877: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:20:59.877: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:20:59.877: INFO: telemeter-client-577bcc4f8d-jv4m8 started at 2020-10-29 17:48:47 +0000 UTC (0+3 container statuses recorded)
Oct 29 21:20:59.877: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:20:59.877: INFO: 	Container reload ready: true, restart count 0
Oct 29 21:20:59.877: INFO: 	Container telemeter-client ready: true, restart count 0
Oct 29 21:20:59.877: INFO: alertmanager-main-2 started at 2020-10-29 17:49:10 +0000 UTC (0+5 container statuses recorded)
Oct 29 21:20:59.877: INFO: 	Container alertmanager ready: true, restart count 0
Oct 29 21:20:59.877: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Oct 29 21:20:59.877: INFO: 	Container config-reloader ready: true, restart count 0
Oct 29 21:20:59.877: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:20:59.877: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:20:59.877: INFO: multus-str5z started at 2020-10-29 17:41:12 +0000 UTC (5+1 container statuses recorded)
Oct 29 21:20:59.877: INFO: 	Init container multus-binary-copy ready: true, restart count 0
Oct 29 21:20:59.877: INFO: 	Init container cni-plugins ready: true, restart count 0
Oct 29 21:20:59.877: INFO: 	Init container routeoverride-cni ready: true, restart count 0
Oct 29 21:20:59.877: INFO: 	Init container whereabouts-cni-bincopy ready: true, restart count 0
Oct 29 21:20:59.877: INFO: 	Init container whereabouts-cni ready: true, restart count 0
Oct 29 21:20:59.877: INFO: 	Container kube-multus ready: true, restart count 0
Oct 29 21:20:59.877: INFO: node-ca-6tz25 started at 2020-10-29 17:41:11 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:20:59.877: INFO: 	Container node-ca ready: true, restart count 0
Oct 29 21:20:59.877: INFO: sre-dns-latency-exporter-l58wm started at 2020-10-29 17:41:56 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:20:59.877: INFO: 	Container main ready: true, restart count 0
Oct 29 21:20:59.877: INFO: sre-stuck-ebs-vols-1-98w59 started at 2020-10-29 17:48:47 +0000 UTC (1+1 container statuses recorded)
Oct 29 21:20:59.877: INFO: 	Init container setupcreds ready: true, restart count 0
Oct 29 21:20:59.877: INFO: 	Container main ready: true, restart count 0
Oct 29 21:20:59.877: INFO: prometheus-k8s-1 started at 2020-10-29 17:49:00 +0000 UTC (0+6 container statuses recorded)
Oct 29 21:20:59.877: INFO: 	Container config-reloader ready: true, restart count 0
Oct 29 21:20:59.877: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:20:59.877: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:20:59.877: INFO: 	Container prometheus ready: true, restart count 1
Oct 29 21:20:59.877: INFO: 	Container prometheus-proxy ready: true, restart count 0
Oct 29 21:20:59.877: INFO: 	Container thanos-sidecar ready: true, restart count 0
Oct 29 21:20:59.877: INFO: router-default-5f9d96945d-zxzg4 started at 2020-10-29 19:45:00 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:20:59.877: INFO: 	Container router ready: true, restart count 0
Oct 29 21:20:59.877: INFO: ovs-99tdx started at 2020-10-29 17:41:12 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:20:59.877: INFO: 	Container openvswitch ready: true, restart count 0
Oct 29 21:20:59.877: INFO: kube-state-metrics-596748788d-6mblj started at 2020-10-29 17:48:48 +0000 UTC (0+3 container statuses recorded)
Oct 29 21:20:59.877: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Oct 29 21:20:59.877: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Oct 29 21:20:59.877: INFO: 	Container kube-state-metrics ready: true, restart count 0
Oct 29 21:20:59.877: INFO: grafana-5cfb6b5fbb-gjr2l started at 2020-10-29 17:48:48 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:20:59.878: INFO: 	Container grafana ready: true, restart count 0
Oct 29 21:20:59.878: INFO: 	Container grafana-proxy ready: true, restart count 0
Oct 29 21:20:59.878: INFO: prometheus-operator-89f4b9855-wlkt2 started at 2020-10-29 17:48:47 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:20:59.878: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:20:59.878: INFO: 	Container prometheus-operator ready: true, restart count 0
Oct 29 21:20:59.878: INFO: sdn-cp5t4 started at 2020-10-29 17:41:12 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:20:59.878: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Oct 29 21:20:59.878: INFO: 	Container sdn ready: true, restart count 0
Oct 29 21:20:59.878: INFO: alertmanager-main-1 started at 2020-10-29 17:49:02 +0000 UTC (0+5 container statuses recorded)
Oct 29 21:20:59.878: INFO: 	Container alertmanager ready: true, restart count 0
Oct 29 21:20:59.878: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Oct 29 21:20:59.878: INFO: 	Container config-reloader ready: true, restart count 0
Oct 29 21:20:59.878: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:20:59.878: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:20:59.878: INFO: machine-config-daemon-mjcnt started at 2020-10-29 17:41:11 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:20:59.878: INFO: 	Container machine-config-daemon ready: true, restart count 0
Oct 29 21:20:59.878: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:20:59.878: INFO: image-registry-75b75c555d-ksztb started at 2020-10-29 17:48:47 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:20:59.878: INFO: 	Container registry ready: true, restart count 0
Oct 29 21:20:59.878: INFO: prometheus-adapter-6495ff5885-lrx64 started at 2020-10-29 17:48:47 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:20:59.878: INFO: 	Container prometheus-adapter ready: true, restart count 0
Oct 29 21:20:59.878: INFO: network-metrics-daemon-gcjdz started at 2020-10-29 17:41:13 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:20:59.878: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:20:59.878: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Oct 29 21:20:59.878: INFO: prometheus-k8s-0 started at 2020-10-29 17:48:53 +0000 UTC (0+6 container statuses recorded)
Oct 29 21:20:59.878: INFO: 	Container config-reloader ready: true, restart count 0
Oct 29 21:20:59.878: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:20:59.878: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:20:59.878: INFO: 	Container prometheus ready: true, restart count 1
Oct 29 21:20:59.878: INFO: 	Container prometheus-proxy ready: true, restart count 0
Oct 29 21:20:59.878: INFO: 	Container thanos-sidecar ready: true, restart count 0
Oct 29 21:20:59.878: INFO: node-exporter-d8llk started at 2020-10-29 17:41:13 +0000 UTC (1+2 container statuses recorded)
Oct 29 21:20:59.878: INFO: 	Init container init-textfile ready: true, restart count 0
Oct 29 21:20:59.878: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:20:59.878: INFO: 	Container node-exporter ready: true, restart count 0
W1029 21:20:59.899598   53132 metrics_grabber.go:83] Can't find any pods in namespace kube-system to grab metrics from
W1029 21:20:59.899620   53132 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1029 21:20:59.899626   53132 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1029 21:20:59.899632   53132 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Oct 29 21:20:59.998: INFO: 
Latency metrics for node ip-10-0-157-225.us-east-2.compute.internal
Oct 29 21:20:59.998: INFO: 
Logging node info for node ip-10-0-158-72.us-east-2.compute.internal
Oct 29 21:21:00.019: INFO: Node Info: &Node{ObjectMeta:{ip-10-0-158-72.us-east-2.compute.internal   /api/v1/nodes/ip-10-0-158-72.us-east-2.compute.internal ace26a9e-d457-4c97-8322-f7fa7da4df71 203410 0 2020-10-29 17:42:11 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:m5.xlarge beta.kubernetes.io/os:linux failure-domain.beta.kubernetes.io/region:us-east-2 failure-domain.beta.kubernetes.io/zone:us-east-2a kubernetes.io/arch:amd64 kubernetes.io/hostname:ip-10-0-158-72 kubernetes.io/os:linux node-role.kubernetes.io:infra node-role.kubernetes.io/infra: node-role.kubernetes.io/worker: node.kubernetes.io/instance-type:m5.xlarge node.openshift.io/os_id:rhcos topology.ebs.csi.aws.com/zone:us-east-2a topology.kubernetes.io/region:us-east-2 topology.kubernetes.io/zone:us-east-2a] map[csi.volume.kubernetes.io/nodeid:{"ebs.csi.aws.com":"i-07086da876de13ecd"} machine.openshift.io/machine:openshift-machine-api/jeder-461-cncf2-pvrph-infra-us-east-2a-l2r9l machineconfiguration.openshift.io/currentConfig:rendered-worker-f26c105cafab97113f99101cc57ac048 machineconfiguration.openshift.io/desiredConfig:rendered-worker-f26c105cafab97113f99101cc57ac048 machineconfiguration.openshift.io/reason: machineconfiguration.openshift.io/state:Done volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{nodelink-controller Update v1 2020-10-29 17:42:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machine.openshift.io/machine":{}},"f:labels":{"f:node-role.kubernetes.io":{},"f:node-role.kubernetes.io/infra":{}}}}} {machine-config-controller Update v1 2020-10-29 17:48:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/desiredConfig":{}}}}} {kube-controller-manager Update v1 2020-10-29 17:54:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{}}},"f:spec":{"f:podCIDR":{},"f:podCIDRs":{".":{},"v:\"10.128.6.0/24\"":{}},"f:taints":{}}}} {machine-config-daemon Update v1 2020-10-29 17:54:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/currentConfig":{},"f:machineconfiguration.openshift.io/reason":{},"f:machineconfiguration.openshift.io/state":{}}}}} {kubelet Update v1 2020-10-29 21:16:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:csi.volume.kubernetes.io/nodeid":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/instance-type":{},"f:failure-domain.beta.kubernetes.io/region":{},"f:failure-domain.beta.kubernetes.io/zone":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:node-role.kubernetes.io/worker":{},"f:node.kubernetes.io/instance-type":{},"f:node.openshift.io/os_id":{},"f:topology.ebs.csi.aws.com/zone":{},"f:topology.kubernetes.io/region":{},"f:topology.kubernetes.io/zone":{}}},"f:spec":{"f:providerID":{}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalDNS\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:attachable-volumes-aws-ebs":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:attachable-volumes-aws-ebs":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"DiskPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"MemoryPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PIDPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:images":{},"f:nodeInfo":{"f:architecture":{},"f:bootID":{},"f:containerRuntimeVersion":{},"f:kernelVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{},"f:machineID":{},"f:operatingSystem":{},"f:osImage":{},"f:systemUUID":{}}}}}]},Spec:NodeSpec{PodCIDR:10.128.6.0/24,DoNotUseExternalID:,ProviderID:aws:///us-east-2a/i-07086da876de13ecd,Unschedulable:false,Taints:[]Taint{Taint{Key:node-role.kubernetes.io/infra,Value:,Effect:PreferNoSchedule,TimeAdded:<nil>,},},ConfigSource:nil,PodCIDRs:[10.128.6.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{321557344256 0} {<nil>} 314020844Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{16502939648 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},},Allocatable:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{3 0} {<nil>} 3 DecimalSI},ephemeral-storage: {{288327867528 0} {<nil>} 288327867528 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{15324340224 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2020-10-29 21:16:45 +0000 UTC,LastTransitionTime:2020-10-29 17:54:47 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2020-10-29 21:16:45 +0000 UTC,LastTransitionTime:2020-10-29 17:54:47 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2020-10-29 21:16:45 +0000 UTC,LastTransitionTime:2020-10-29 17:54:47 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2020-10-29 21:16:45 +0000 UTC,LastTransitionTime:2020-10-29 17:54:47 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.0.158.72,},NodeAddress{Type:Hostname,Address:ip-10-0-158-72.us-east-2.compute.internal,},NodeAddress{Type:InternalDNS,Address:ip-10-0-158-72.us-east-2.compute.internal,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:ec269be649cbb435d7131f2acda04147,SystemUUID:ec269be6-49cb-b435-d713-1f2acda04147,BootID:f21654fd-8398-480a-9b38-cafec1d13c46,KernelVersion:4.18.0-193.24.1.el8_2.dt1.x86_64,OSImage:Red Hat Enterprise Linux CoreOS 46.82.202010091720-0 (Ootpa),ContainerRuntimeVersion:cri-o://1.19.0-20.rhaos4.6.git97d715e.el8,KubeletVersion:v1.19.0+d59ce34,KubeProxyVersion:v1.19.0+d59ce34,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:82d1def7312de8ae5dee32d237ad59fe685923e78668fa3547e6bee445cd8842 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:499341399,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4cfd55719faff41e96c2e4be69e3f2381a57b8b3445b80ae4acfe8ee33d7f99b quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:481479938,},ContainerImage{Names:[quay.io/jupierce/openshift-python-monitoring@sha256:98e3259170f7116837fa60b0515fe79cb9721060d34eb4931a14244b4658bb8d quay.io/jupierce/openshift-python-monitoring:stable],SizeBytes:474594250,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8248dec0d94b2928aa4d63a22973d9a8f8f173a1431b2ab4ad15fdfe80283d7c quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:419419150,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8923050603588c27d79b33b371afb651288470d5cdeb14f8e10249bca1a1c461 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:413031192,},ContainerImage{Names:[quay.io/openshift-sre/managed-prometheus-exporter-initcontainer@sha256:7fa36e4488191536ac4a570fdd3431d3c80db983db74b90bc2ce051ecef3035e quay.io/openshift-sre/managed-prometheus-exporter-initcontainer:v0.1.9-2019-03-28-4e558131],SizeBytes:388062834,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8f4882cff3c2f9521215eac681c5abda42876e3e955431c1387fb457940b8344 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:370109724,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:72323ce541f8a26fbad17ef65ff21b51498863bb851635a0faa8d5b1ac6ce0e4 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:366911472,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3d0361f380abf5252b1b640e3ceaaab8274e2af8cdb605b20b513a1a44b3a4dc quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:346080704,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f644e4b495071a2b9d0d6f5d48cb96dad9f7ea8298cc22c98824bc70229ea9dd quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:341116512,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6bada08687c20afe316c1f8cf76f001f31bedae317f256f3df3affaa5d0dc25e quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:336987966,},ContainerImage{Names:[image-registry.openshift-image-registry.svc:5000/openshift/cli@sha256:6aa4bb97adf2142b0e74ccae7fd3661ada73cbaac803b86bb8712261e916d66d quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6aa4bb97adf2142b0e74ccae7fd3661ada73cbaac803b86bb8712261e916d66d image-registry.openshift-image-registry.svc:5000/openshift/cli:latest],SizeBytes:336576591,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:52424fd2af6fd7d7a5a1233032eb3f3c67f7691996b209e013e29f1524c5188c quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:326057200,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f86db3170270fc635dff0d7f1ba6e79a8f45de7e1dcfa5621474d1f6e07352ec quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:325370491,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:54fbeb744b82662fd38c0d301ebaad6ca8983707bc44db7235ead0fb7b95808f quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:321482743,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a37a568c63563257309cb0ffb6e185d98f662ff3201d2099cbe0df404b93f0c8 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:314357110,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:26f6c930942ee4dea7c1e22d220bba11561c37bdc47101c4490ce0ef77c9203a quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:311353697,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e176509a104b2f96ee4f5c57275c6a712409aa80ac40071345d0a03bdde2b456 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:307311930,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0c42cd3a74176732e8c06105c47674c7d410c7167c8e3fbd80f9a76e9bfda5bd quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:302591889,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c75977f28becdf4f7065cfa37233464dd31208b1767e620c4f19658f53f8ff8c quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:300756060,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:cf565b2fab365e027962a25a8cffb41aa35cb5a00d001e081d53c7fed5a0c54b quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:300005108,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9e7f0468850aeb13585ef049f687cc42c05d82bc0e0200607d1a93d7f9740fe5 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:291621018,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6cdaecd5dd9df8fd74529be7fa5d8973daf6f4ea95be8acfb2f5ac97773ebe67 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:278303432,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e5bcf6d786fd218e1ef188eb40c39c31a98d03121fba3b7a1f16e87e45a7478b quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:276735847,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1dcc413b621958f97dfbb3fc998a9e225ef155a80ffb151eb4694bf8370b383a quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:274559501,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8c9e61400619c4613db5cc73097d287e3cd5d2125c85d1d84cc30cfdaa1093e7 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:271612142,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:12b11e2000b42ce1aaa228d9c1f4c9177395add2fa43835e667b7fc9007e40e6 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:270734898,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:88ddfbded8bc27b227ed7397ece050b756e522a9ffc34cbfba3c94c5ee58b740 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:269654227,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8bd90fcca7990c0edead15298dcec963968274d299428da95eae41aa23157b90 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:264244297,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:73fdcef5de85e739831c5a8b76dce349a3c8832ff416a46263743d7e61655cbb quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:247448489,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b635694dc0663a1404d43a4a9ac8513a3087c7cfead50f6ab413f3c217c40b2a quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:240293385,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:71051bdf1b96c953fc1dfd48359915bf5c027613de6f5e2fa8adeea8d3dda311 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:236515650,},ContainerImage{Names:[quay.io/app-sre/managed-velero-operator@sha256:a9eb4800dcafe3fe8879662c0413a7e5c53b5fe98ab0bbc81b9fc151044a4e07 quay.io/app-sre/managed-velero-operator:0a73057],SizeBytes:207938263,},ContainerImage{Names:[quay.io/app-sre/cloud-ingress-operator@sha256:ceb064d013f51cdb15075f079eee2dfd5a0f42f4a09b489176abe92e91fb6a56 quay.io/app-sre/cloud-ingress-operator:v0.1.233-a2e288b],SizeBytes:203838503,},ContainerImage{Names:[quay.io/app-sre/managed-upgrade-operator@sha256:558a828a0b3c296cb2c0e52ef143a45177bdae40b1c84bfe8564486c940fd79a quay.io/app-sre/managed-upgrade-operator:9b59534],SizeBytes:198448288,},ContainerImage{Names:[quay.io/app-sre/splunk-forwarder-operator@sha256:bca474387af6112b08406697ef28977d428c1a884ebaf602a3e6ba1b45e6b31b quay.io/app-sre/splunk-forwarder-operator:7e8bc93],SizeBytes:191465195,},ContainerImage{Names:[quay.io/app-sre/configure-alertmanager-operator@sha256:a2bed259f284358220d4e01390621a24f43c6f0d534474a5050cad0c6c6cb812 quay.io/app-sre/configure-alertmanager-operator:v0.1.237-0a8e5f3],SizeBytes:188443907,},ContainerImage{Names:[quay.io/app-sre/rbac-permissions-operator@sha256:8ccc461d19f30811dbf31f85048b5811e690d5f047c91831accd54eb5ab560b3 quay.io/app-sre/rbac-permissions-operator:819ec06],SizeBytes:187661365,},ContainerImage{Names:[docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4 docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:128894988,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0 k8s.gcr.io/e2e-test-images/agnhost@sha256:c243d6fc39291ac6ea9caf6b511719980d9c8d2e54dda8217ee019fbf9a124e1 k8s.gcr.io/e2e-test-images/agnhost:2.20],SizeBytes:116455133,},ContainerImage{Names:[k8s.gcr.io/pause@sha256:4a1c4b21597c1b4415bdbecb28a3296c6b5e23ca4f9feeb599860a1dac6a0108 k8s.gcr.io/pause@sha256:927d98197ec1141a368550822d18fa1c60bdae27b78b0c004f705f548c07814f k8s.gcr.io/pause:3.2],SizeBytes:688049,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Oct 29 21:21:00.020: INFO: 
Logging kubelet events for node ip-10-0-158-72.us-east-2.compute.internal
Oct 29 21:21:00.041: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-158-72.us-east-2.compute.internal
Oct 29 21:21:00.077: INFO: multus-l5sdf started at 2020-10-29 17:42:13 +0000 UTC (5+1 container statuses recorded)
Oct 29 21:21:00.077: INFO: 	Init container multus-binary-copy ready: true, restart count 0
Oct 29 21:21:00.077: INFO: 	Init container cni-plugins ready: true, restart count 0
Oct 29 21:21:00.077: INFO: 	Init container routeoverride-cni ready: true, restart count 0
Oct 29 21:21:00.077: INFO: 	Init container whereabouts-cni-bincopy ready: true, restart count 0
Oct 29 21:21:00.077: INFO: 	Init container whereabouts-cni ready: true, restart count 0
Oct 29 21:21:00.077: INFO: 	Container kube-multus ready: true, restart count 0
Oct 29 21:21:00.077: INFO: sre-ebs-iops-reporter-1-rrlxd started at 2020-10-29 19:28:48 +0000 UTC (1+1 container statuses recorded)
Oct 29 21:21:00.077: INFO: 	Init container setupcreds ready: true, restart count 0
Oct 29 21:21:00.077: INFO: 	Container main ready: true, restart count 0
Oct 29 21:21:00.077: INFO: cloud-ingress-operator-798549d69-9fh9n started at 2020-10-29 19:28:48 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.077: INFO: 	Container cloud-ingress-operator ready: true, restart count 0
Oct 29 21:21:00.077: INFO: splunk-forwarder-operator-6c55bb877-mw9lq started at 2020-10-29 19:28:48 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.077: INFO: 	Container splunk-forwarder-operator ready: true, restart count 0
Oct 29 21:21:00.077: INFO: sre-build-test-1604002260-284ls started at 2020-10-29 20:11:07 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.078: INFO: 	Container sre-build-test ready: false, restart count 0
Oct 29 21:21:00.078: INFO: sdn-8kf7x started at 2020-10-29 17:42:13 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:21:00.078: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Oct 29 21:21:00.078: INFO: 	Container sdn ready: true, restart count 0
Oct 29 21:21:00.078: INFO: tuned-jwndg started at 2020-10-29 17:42:13 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.078: INFO: 	Container tuned ready: true, restart count 0
Oct 29 21:21:00.078: INFO: sre-build-test-1604005860-qkzkn started at 2020-10-29 21:11:04 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.078: INFO: 	Container sre-build-test ready: false, restart count 0
Oct 29 21:21:00.078: INFO: ovs-9ct7r started at 2020-10-29 17:42:13 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.078: INFO: 	Container openvswitch ready: true, restart count 0
Oct 29 21:21:00.078: INFO: network-metrics-daemon-nb7sg started at 2020-10-29 17:42:13 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:21:00.078: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:00.078: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Oct 29 21:21:00.078: INFO: aws-ebs-csi-driver-node-zfr5l started at 2020-10-29 17:42:13 +0000 UTC (0+3 container statuses recorded)
Oct 29 21:21:00.078: INFO: 	Container csi-driver ready: true, restart count 0
Oct 29 21:21:00.078: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Oct 29 21:21:00.078: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Oct 29 21:21:00.078: INFO: dns-default-25bcd started at 2020-10-29 17:42:13 +0000 UTC (0+3 container statuses recorded)
Oct 29 21:21:00.078: INFO: 	Container dns ready: true, restart count 0
Oct 29 21:21:00.078: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 29 21:21:00.078: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:00.078: INFO: node-exporter-pdd9z started at 2020-10-29 17:42:13 +0000 UTC (1+2 container statuses recorded)
Oct 29 21:21:00.078: INFO: 	Init container init-textfile ready: true, restart count 0
Oct 29 21:21:00.078: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:00.078: INFO: 	Container node-exporter ready: true, restart count 0
Oct 29 21:21:00.078: INFO: node-ca-d275k started at 2020-10-29 17:42:13 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.078: INFO: 	Container node-ca ready: true, restart count 0
Oct 29 21:21:00.078: INFO: machine-config-daemon-6987z started at 2020-10-29 17:42:13 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:21:00.078: INFO: 	Container machine-config-daemon ready: true, restart count 0
Oct 29 21:21:00.078: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:21:00.078: INFO: sre-dns-latency-exporter-krp25 started at 2020-10-29 17:42:51 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.078: INFO: 	Container main ready: true, restart count 0
Oct 29 21:21:00.078: INFO: sre-build-test-1603998660-hmrqr started at 2020-10-29 19:11:03 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.078: INFO: 	Container sre-build-test ready: false, restart count 0
Oct 29 21:21:00.078: INFO: thanos-querier-7f688468b8-6jh8m started at 2020-10-29 19:28:48 +0000 UTC (0+5 container statuses recorded)
Oct 29 21:21:00.078: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:00.078: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Oct 29 21:21:00.078: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:21:00.078: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:21:00.078: INFO: 	Container thanos-query ready: true, restart count 0
Oct 29 21:21:00.078: INFO: router-default-5f9d96945d-ckql4 started at 2020-10-29 19:44:59 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.078: INFO: 	Container router ready: true, restart count 0
W1029 21:21:00.099893   53132 metrics_grabber.go:83] Can't find any pods in namespace kube-system to grab metrics from
W1029 21:21:00.099910   53132 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1029 21:21:00.099917   53132 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1029 21:21:00.099922   53132 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Oct 29 21:21:00.156: INFO: 
Latency metrics for node ip-10-0-158-72.us-east-2.compute.internal
Oct 29 21:21:00.156: INFO: 
Logging node info for node ip-10-0-231-174.us-east-2.compute.internal
Oct 29 21:21:00.177: INFO: Node Info: &Node{ObjectMeta:{ip-10-0-231-174.us-east-2.compute.internal   /api/v1/nodes/ip-10-0-231-174.us-east-2.compute.internal e5891fd2-1d97-465b-be6e-6039611f5983 204653 0 2020-10-29 17:23:44 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:m5.xlarge beta.kubernetes.io/os:linux failure-domain.beta.kubernetes.io/region:us-east-2 failure-domain.beta.kubernetes.io/zone:us-east-2a kubernetes.io/arch:amd64 kubernetes.io/hostname:ip-10-0-231-174 kubernetes.io/os:linux node-role.kubernetes.io/master: node.kubernetes.io/instance-type:m5.xlarge node.openshift.io/os_id:rhcos topology.ebs.csi.aws.com/zone:us-east-2a topology.kubernetes.io/region:us-east-2 topology.kubernetes.io/zone:us-east-2a] map[csi.volume.kubernetes.io/nodeid:{"ebs.csi.aws.com":"i-0a5b560458955c785"} machine.openshift.io/machine:openshift-machine-api/jeder-461-cncf2-pvrph-master-2 machineconfiguration.openshift.io/currentConfig:rendered-master-b06f274cfa558dc0b5031aabca48f868 machineconfiguration.openshift.io/desiredConfig:rendered-master-b06f274cfa558dc0b5031aabca48f868 machineconfiguration.openshift.io/reason: machineconfiguration.openshift.io/state:Done volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{nodelink-controller Update v1 2020-10-29 17:26:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machine.openshift.io/machine":{}}}}} {machine-config-controller Update v1 2020-10-29 17:40:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/desiredConfig":{}}}}} {machine-config-daemon Update v1 2020-10-29 17:42:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/currentConfig":{},"f:machineconfiguration.openshift.io/reason":{},"f:machineconfiguration.openshift.io/state":{}}}}} {kube-controller-manager Update v1 2020-10-29 19:08:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{}}},"f:spec":{"f:podCIDR":{},"f:podCIDRs":{".":{},"v:\"10.128.0.0/24\"":{}},"f:taints":{}}}} {oc Update v1 2020-10-29 19:08:29 +0000 UTC FieldsV1 {"f:spec":{"f:unschedulable":{}}}} {kubelet Update v1 2020-10-29 21:20:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:csi.volume.kubernetes.io/nodeid":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/instance-type":{},"f:failure-domain.beta.kubernetes.io/region":{},"f:failure-domain.beta.kubernetes.io/zone":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:node-role.kubernetes.io/master":{},"f:node.kubernetes.io/instance-type":{},"f:node.openshift.io/os_id":{},"f:topology.ebs.csi.aws.com/zone":{},"f:topology.kubernetes.io/region":{},"f:topology.kubernetes.io/zone":{}}},"f:spec":{"f:providerID":{}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalDNS\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:attachable-volumes-aws-ebs":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:attachable-volumes-aws-ebs":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"DiskPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"MemoryPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PIDPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:images":{},"f:nodeInfo":{"f:architecture":{},"f:bootID":{},"f:containerRuntimeVersion":{},"f:kernelVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{},"f:machineID":{},"f:operatingSystem":{},"f:osImage":{},"f:systemUUID":{}}}}}]},Spec:NodeSpec{PodCIDR:10.128.0.0/24,DoNotUseExternalID:,ProviderID:aws:///us-east-2a/i-0a5b560458955c785,Unschedulable:true,Taints:[]Taint{Taint{Key:node-role.kubernetes.io/master,Value:,Effect:NoSchedule,TimeAdded:<nil>,},Taint{Key:node.kubernetes.io/unschedulable,Value:,Effect:NoSchedule,TimeAdded:2020-10-29 19:08:29 +0000 UTC,},},ConfigSource:nil,PodCIDRs:[10.128.0.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{375244435456 0} {<nil>} 366449644Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{16502923264 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},},Allocatable:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{3 0} {<nil>} 3 DecimalSI},ephemeral-storage: {{336646249528 0} {<nil>} 336646249528 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{15324323840 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2020-10-29 21:20:06 +0000 UTC,LastTransitionTime:2020-10-29 17:42:41 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2020-10-29 21:20:06 +0000 UTC,LastTransitionTime:2020-10-29 17:42:41 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2020-10-29 21:20:06 +0000 UTC,LastTransitionTime:2020-10-29 17:42:41 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2020-10-29 21:20:06 +0000 UTC,LastTransitionTime:2020-10-29 17:42:41 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.0.231.174,},NodeAddress{Type:Hostname,Address:ip-10-0-231-174.us-east-2.compute.internal,},NodeAddress{Type:InternalDNS,Address:ip-10-0-231-174.us-east-2.compute.internal,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:ec2cb26928dd6ce9ad2bab1fbd8b1ea7,SystemUUID:ec2cb269-28dd-6ce9-ad2b-ab1fbd8b1ea7,BootID:74c7036a-ae1f-4944-b92b-7103f56935aa,KernelVersion:4.18.0-193.24.1.el8_2.dt1.x86_64,OSImage:Red Hat Enterprise Linux CoreOS 46.82.202010091720-0 (Ootpa),ContainerRuntimeVersion:cri-o://1.19.0-20.rhaos4.6.git97d715e.el8,KubeletVersion:v1.19.0+d59ce34,KubeProxyVersion:v1.19.0+d59ce34,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:30094a2d586aa282d85e14f1be19abec1c30ce431673377b0e1c12d83e6bac8c quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:682301224,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:82d1def7312de8ae5dee32d237ad59fe685923e78668fa3547e6bee445cd8842 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:499341399,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4cfd55719faff41e96c2e4be69e3f2381a57b8b3445b80ae4acfe8ee33d7f99b quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:481479938,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ae6a5decd040a6b3adfa074d3211ab92a36b77b2d849962d9a678e1c2c5ef5c1 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:473291773,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8923050603588c27d79b33b371afb651288470d5cdeb14f8e10249bca1a1c461 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:413031192,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8f4882cff3c2f9521215eac681c5abda42876e3e955431c1387fb457940b8344 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:370109724,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:72323ce541f8a26fbad17ef65ff21b51498863bb851635a0faa8d5b1ac6ce0e4 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:366911472,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:93b3e1246884e357e1654e6c9578481aff9eef07eed1f9fdd0e9c8cc89a3770c quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:358141917,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a8214df42df962b965e3f4daad0b61932235e57241160861e503d84e38b775d5 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:337507384,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6bada08687c20afe316c1f8cf76f001f31bedae317f256f3df3affaa5d0dc25e quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:336987966,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6aa4bb97adf2142b0e74ccae7fd3661ada73cbaac803b86bb8712261e916d66d quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:336576591,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:aa4f37543b45bc248db8d9bd2dc45b6e159a8869b044c2310f541afba15b2694 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:336485164,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f52825e9905c926d399cd0b7afbb2b7d0370ae22da0416feac9131d555db0b98 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:336041278,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:059f0179c0528c6234dbdca7e70fe779cf37be5121f458dd045d2e9662192f06 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:335510757,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:7584014b0cb8cb2c5a09b909c79f2f8ad6e49854bcfabf72e96a22330bcf6f56 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:333011308,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:357e35286fd26fed015c03a9c451f6fdcf61cf0821d959025e7f800e7c533f29 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:332777114,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6ca671c810426b8c4f13dd0c7ac19639f9f265b952b8feb5a828e59fab785335 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:332487959,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f86db3170270fc635dff0d7f1ba6e79a8f45de7e1dcfa5621474d1f6e07352ec quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:325370491,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:54fbeb744b82662fd38c0d301ebaad6ca8983707bc44db7235ead0fb7b95808f quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:321482743,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9dbb31ac799b2c30270268714dcb3d11bafb329b98639a446657c8c7db41938c quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:318368538,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1ad85195e1a180698fe4b8df82e3d72075efb256b53f593d13e29faaf7f3e15a quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:316310728,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dc6a6a1d4a6b2af67421561e53d1af1d40c99ae72de69f4c3cc390d447f12269 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:315339691,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4691dc29704c9cb06d2345894f1a8f074b58a0d208318c5218241388b0916e1b quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:315327750,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4289297f0b7ee7edf394348fd07e1fa1b3162655f2a2af2245e23af4b179e7f2 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:314343750,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:65206861218064576dc092040e9c24b0393b8a07502e351f513400f187f38cc7 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:313489720,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:45586fd7a5cfd43ff546dbfb282a70a91eaf0f069f604230af958dc802832f89 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:312615681,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c6b3aaaa38679b1d752ec09bd68c6d80a8911c74ec16d27c49de88ecb97823ee quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:312540521,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:01250de496444bb624ec7b472ac9b0f7023809c88306a71c6ac87bb302f7dbe3 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:312449767,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ee4abe53e80e561239e510a6f9999b4dc80b7b3fdc9848ab43d0bf8df24e815d quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:311984477,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:26f6c930942ee4dea7c1e22d220bba11561c37bdc47101c4490ce0ef77c9203a quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:311353697,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9b564f882e31f497f57a0d99d406d5231eb15e9a97f0b450c21bec2bac7ff033 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:311323012,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e176509a104b2f96ee4f5c57275c6a712409aa80ac40071345d0a03bdde2b456 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:307311930,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:afdf0a3b426ac1c03df52e88a2b884f0714e54a1a03f33091954441a05a7f6b9 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:306116297,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:076b280e17c6bb4cc618db71403ccec75f8196c8849061a40c680a2808292bb6 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:304641618,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c75977f28becdf4f7065cfa37233464dd31208b1767e620c4f19658f53f8ff8c quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:300756060,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3cdb4589ee683c85e3d8f3f239187bc089d30cac6c26847a54894f6c328817c3 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:300278177,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:cf565b2fab365e027962a25a8cffb41aa35cb5a00d001e081d53c7fed5a0c54b quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:300005108,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:92feaeb8763ece68147b522bfa8914bcd429e9825185b9b9c05247ad2857d03f quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:299900737,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:aef7c7802e877679d62d3d40ca4cac4fa8e84b2974673a8912f14d95eca08a08 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:296536411,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:7f93199dcc01838f017030e0e8dd32d1d23fa268d25472e338e6843c8830d364 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:287022906,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6cdaecd5dd9df8fd74529be7fa5d8973daf6f4ea95be8acfb2f5ac97773ebe67 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:278303432,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1dcc413b621958f97dfbb3fc998a9e225ef155a80ffb151eb4694bf8370b383a quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:274559501,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:12b11e2000b42ce1aaa228d9c1f4c9177395add2fa43835e667b7fc9007e40e6 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:270734898,},ContainerImage{Names:[quay.io/app-sre/sre-ssh-proxy@sha256:67ecbf78aa15cc991d92643675d0c947fe4efea1e7a30d743d34e21f84cc2ed8 quay.io/app-sre/sre-ssh-proxy:v49-1fde8e5],SizeBytes:266831444,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8bd90fcca7990c0edead15298dcec963968274d299428da95eae41aa23157b90 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:264244297,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:73fdcef5de85e739831c5a8b76dce349a3c8832ff416a46263743d7e61655cbb quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:247448489,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:71051bdf1b96c953fc1dfd48359915bf5c027613de6f5e2fa8adeea8d3dda311 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:236515650,},ContainerImage{Names:[quay.io/app-sre/managed-cluster-validating-webhooks@sha256:5bbcacd1704c9d6e32ded35587241768c016d5dac8a93c79389d65f649728961 quay.io/app-sre/managed-cluster-validating-webhooks:7fe1da6],SizeBytes:219127863,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Oct 29 21:21:00.178: INFO: 
Logging kubelet events for node ip-10-0-231-174.us-east-2.compute.internal
Oct 29 21:21:00.199: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-231-174.us-east-2.compute.internal
Oct 29 21:21:00.265: INFO: installer-8-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 17:41:15 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:21:00.265: INFO: revision-pruner-8-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 17:43:08 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:21:00.265: INFO: installer-10-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 19:47:53 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:21:00.265: INFO: dns-default-dc9s5 started at 2020-10-29 17:25:40 +0000 UTC (0+3 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container dns ready: true, restart count 0
Oct 29 21:21:00.265: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 29 21:21:00.265: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:00.265: INFO: kube-storage-version-migrator-operator-67f7f8c8ff-bzbvf started at 2020-10-29 17:48:59 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 0
Oct 29 21:21:00.265: INFO: sdn-controller-lxg9r started at 2020-10-29 17:24:02 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container sdn-controller ready: true, restart count 0
Oct 29 21:21:00.265: INFO: installer-7-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 17:40:41 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:21:00.265: INFO: revision-pruner-7-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 17:41:05 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:21:00.265: INFO: installer-9-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 17:43:50 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:21:00.265: INFO: installer-8-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 17:43:14 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:21:00.265: INFO: cluster-storage-operator-5bd675b77d-6grzf started at 2020-10-29 17:48:56 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Oct 29 21:21:00.265: INFO: catalog-operator-5b9bb86df6-27hkk started at 2020-10-29 17:49:02 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container catalog-operator ready: true, restart count 1
Oct 29 21:21:00.265: INFO: installer-10-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 18:01:07 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:21:00.265: INFO: installer-11-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 19:49:15 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:21:00.265: INFO: machine-config-daemon-qgz5x started at 2020-10-29 17:24:37 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container machine-config-daemon ready: true, restart count 0
Oct 29 21:21:00.265: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:21:00.265: INFO: controller-manager-bwmzk started at 2020-10-29 17:31:18 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container controller-manager ready: true, restart count 0
Oct 29 21:21:00.265: INFO: validation-webhook-lr64k started at 2020-10-29 17:39:26 +0000 UTC (1+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Init container inject-cert ready: true, restart count 0
Oct 29 21:21:00.265: INFO: 	Container webhooks ready: true, restart count 0
Oct 29 21:21:00.265: INFO: installer-7-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 17:42:41 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:21:00.265: INFO: apiserver-566bbccb57-tqc2f started at 2020-10-29 17:43:00 +0000 UTC (1+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Init container fix-audit-permissions ready: true, restart count 0
Oct 29 21:21:00.265: INFO: 	Container oauth-apiserver ready: true, restart count 0
Oct 29 21:21:00.265: INFO: packageserver-d4484c6fc-tflvz started at 2020-10-29 17:43:13 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container packageserver ready: true, restart count 0
Oct 29 21:21:00.265: INFO: revision-pruner-6-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 17:40:50 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:21:00.265: INFO: cluster-samples-operator-66b5c8685-vhd6w started at 2020-10-29 17:48:55 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Oct 29 21:21:00.265: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Oct 29 21:21:00.265: INFO: apiserver-8b58dffb-gffbl started at 2020-10-29 17:42:58 +0000 UTC (1+2 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Init container fix-audit-permissions ready: true, restart count 0
Oct 29 21:21:00.265: INFO: 	Container openshift-apiserver ready: true, restart count 0
Oct 29 21:21:00.265: INFO: 	Container openshift-apiserver-check-endpoints ready: true, restart count 0
Oct 29 21:21:00.265: INFO: revision-pruner-8-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 17:43:38 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:21:00.265: INFO: csi-snapshot-controller-operator-5b974dd5d8-t64z8 started at 2020-10-29 17:48:56 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Oct 29 21:21:00.265: INFO: machine-config-controller-856b795496-6zhcv started at 2020-10-29 17:48:58 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container machine-config-controller ready: true, restart count 0
Oct 29 21:21:00.265: INFO: revision-pruner-10-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 19:46:27 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:21:00.265: INFO: revision-pruner-9-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 17:44:26 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:21:00.265: INFO: tuned-7fcs5 started at 2020-10-29 17:24:46 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container tuned ready: true, restart count 0
Oct 29 21:21:00.265: INFO: revision-pruner-9-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 19:46:28 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:21:00.265: INFO: service-ca-544dc587b5-tvtzx started at 2020-10-29 17:49:03 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container service-ca-controller ready: true, restart count 0
Oct 29 21:21:00.265: INFO: network-operator-787d8b758c-hk6zw started at 2020-10-29 17:49:03 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container network-operator ready: true, restart count 0
Oct 29 21:21:00.265: INFO: installer-10-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 19:45:15 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:21:00.265: INFO: network-metrics-daemon-zd6mp started at 2020-10-29 17:23:59 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:00.265: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Oct 29 21:21:00.265: INFO: service-ca-operator-56d79f985d-77jgs started at 2020-10-29 17:43:13 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container service-ca-operator ready: true, restart count 0
Oct 29 21:21:00.265: INFO: openshift-kube-scheduler-operator-64d565dc97-55tk6 started at 2020-10-29 17:49:01 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container kube-scheduler-operator-container ready: true, restart count 0
Oct 29 21:21:00.265: INFO: ovs-ch957 started at 2020-10-29 17:24:02 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container openvswitch ready: true, restart count 0
Oct 29 21:21:00.265: INFO: multus-admission-controller-fvrtq started at 2020-10-29 17:24:24 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:00.265: INFO: 	Container multus-admission-controller ready: true, restart count 0
Oct 29 21:21:00.265: INFO: aws-ebs-csi-driver-node-rrvxx started at 2020-10-29 17:24:58 +0000 UTC (0+3 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container csi-driver ready: true, restart count 0
Oct 29 21:21:00.265: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Oct 29 21:21:00.265: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Oct 29 21:21:00.265: INFO: revision-pruner-7-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 17:40:53 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:21:00.265: INFO: etcd-quorum-guard-644f5747b8-429ns started at 2020-10-29 17:43:06 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container guard ready: true, restart count 0
Oct 29 21:21:00.265: INFO: revision-pruner-7-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 17:43:16 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:21:00.265: INFO: installer-9-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 19:46:02 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:21:00.265: INFO: kube-controller-manager-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 17:27:45 +0000 UTC (0+4 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container cluster-policy-controller ready: true, restart count 0
Oct 29 21:21:00.265: INFO: 	Container kube-controller-manager ready: true, restart count 0
Oct 29 21:21:00.265: INFO: 	Container kube-controller-manager-cert-syncer ready: true, restart count 0
Oct 29 21:21:00.265: INFO: 	Container kube-controller-manager-recovery-controller ready: true, restart count 0
Oct 29 21:21:00.265: INFO: openshift-kube-scheduler-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 17:25:14 +0000 UTC (1+2 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Init container wait-for-host-port ready: true, restart count 0
Oct 29 21:21:00.265: INFO: 	Container kube-scheduler ready: true, restart count 0
Oct 29 21:21:00.265: INFO: 	Container kube-scheduler-cert-syncer ready: true, restart count 0
Oct 29 21:21:00.265: INFO: cluster-node-tuning-operator-69d69f6546-m2drd started at 2020-10-29 17:48:55 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Oct 29 21:21:00.265: INFO: node-ca-hnmr8 started at 2020-10-29 17:25:50 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container node-ca ready: true, restart count 0
Oct 29 21:21:00.265: INFO: recyler-pod-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 17:42:41 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container recyler-container ready: false, restart count 0
Oct 29 21:21:00.265: INFO: cluster-monitoring-operator-7ffd9b8867-kft8z started at 2020-10-29 17:43:11 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Oct 29 21:21:00.265: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:00.265: INFO: rh-ssh-f45567f6b-hvlbt started at 2020-10-29 17:49:03 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container sshd ready: true, restart count 0
Oct 29 21:21:00.265: INFO: revision-pruner-10-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 18:02:59 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:21:00.265: INFO: revision-pruner-6-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 17:41:10 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:21:00.265: INFO: installer-9-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 17:43:16 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:21:00.265: INFO: revision-pruner-10-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 19:48:19 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:21:00.265: INFO: node-exporter-nbt47 started at 2020-10-29 17:24:50 +0000 UTC (1+2 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Init container init-textfile ready: true, restart count 0
Oct 29 21:21:00.265: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:00.265: INFO: 	Container node-exporter ready: true, restart count 0
Oct 29 21:21:00.265: INFO: revision-pruner-3-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 17:42:41 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:21:00.265: INFO: installer-11-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 19:46:56 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:21:00.265: INFO: revision-pruner-9-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 17:44:51 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:21:00.265: INFO: olm-operator-5f9964dfbf-prlh4 started at 2020-10-29 17:49:02 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container olm-operator ready: true, restart count 0
Oct 29 21:21:00.265: INFO: kube-apiserver-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 17:26:54 +0000 UTC (1+5 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Init container setup ready: true, restart count 1
Oct 29 21:21:00.265: INFO: 	Container kube-apiserver ready: true, restart count 0
Oct 29 21:21:00.265: INFO: 	Container kube-apiserver-cert-regeneration-controller ready: true, restart count 0
Oct 29 21:21:00.265: INFO: 	Container kube-apiserver-cert-syncer ready: true, restart count 0
Oct 29 21:21:00.265: INFO: 	Container kube-apiserver-check-endpoints ready: true, restart count 0
Oct 29 21:21:00.265: INFO: 	Container kube-apiserver-insecure-readyz ready: true, restart count 0
Oct 29 21:21:00.265: INFO: revision-pruner-11-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 19:50:15 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:21:00.265: INFO: multus-hbg4l started at 2020-10-29 17:23:58 +0000 UTC (5+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Init container multus-binary-copy ready: true, restart count 0
Oct 29 21:21:00.265: INFO: 	Init container cni-plugins ready: true, restart count 0
Oct 29 21:21:00.265: INFO: 	Init container routeoverride-cni ready: true, restart count 0
Oct 29 21:21:00.265: INFO: 	Init container whereabouts-cni-bincopy ready: true, restart count 0
Oct 29 21:21:00.265: INFO: 	Init container whereabouts-cni ready: true, restart count 0
Oct 29 21:21:00.265: INFO: 	Container kube-multus ready: true, restart count 0
Oct 29 21:21:00.265: INFO: sdn-pmb6l started at 2020-10-29 17:24:03 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Oct 29 21:21:00.265: INFO: 	Container sdn ready: true, restart count 0
Oct 29 21:21:00.265: INFO: installer-8-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 17:42:41 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:21:00.265: INFO: revision-pruner-8-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 17:43:42 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:21:00.265: INFO: authentication-operator-6cd54d4895-qgtlz started at 2020-10-29 17:48:54 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container authentication-operator ready: true, restart count 0
Oct 29 21:21:00.265: INFO: oauth-openshift-6dc6d68fbf-p4l57 started at 2020-10-29 18:36:48 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container oauth-openshift ready: true, restart count 0
Oct 29 21:21:00.265: INFO: machine-config-server-qww5z started at 2020-10-29 17:26:37 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container machine-config-server ready: true, restart count 0
Oct 29 21:21:00.265: INFO: etcd-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 17:25:10 +0000 UTC (2+3 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Init container etcd-ensure-env-vars ready: true, restart count 0
Oct 29 21:21:00.265: INFO: 	Init container etcd-resources-copy ready: true, restart count 0
Oct 29 21:21:00.265: INFO: 	Container etcd ready: true, restart count 0
Oct 29 21:21:00.265: INFO: 	Container etcd-metrics ready: true, restart count 0
Oct 29 21:21:00.265: INFO: 	Container etcdctl ready: true, restart count 0
Oct 29 21:21:00.265: INFO: pod-identity-webhook-75fc9d4d96-2hdfs started at 2020-10-29 17:48:55 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container pod-identity-webhook ready: true, restart count 0
Oct 29 21:21:00.265: INFO: console-575d66b587-jrlsg started at 2020-10-29 17:43:13 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container console ready: true, restart count 0
Oct 29 21:21:00.265: INFO: revision-pruner-11-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 19:50:38 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.265: INFO: 	Container pruner ready: false, restart count 0
W1029 21:21:00.286660   53132 metrics_grabber.go:83] Can't find any pods in namespace kube-system to grab metrics from
W1029 21:21:00.286678   53132 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1029 21:21:00.286684   53132 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1029 21:21:00.286690   53132 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Oct 29 21:21:00.356: INFO: 
Latency metrics for node ip-10-0-231-174.us-east-2.compute.internal
Oct 29 21:21:00.356: INFO: 
Logging node info for node ip-10-0-234-238.us-east-2.compute.internal
Oct 29 21:21:00.377: INFO: Node Info: &Node{ObjectMeta:{ip-10-0-234-238.us-east-2.compute.internal   /api/v1/nodes/ip-10-0-234-238.us-east-2.compute.internal f0844c0b-bcd8-46c5-9a7f-ece03266a2e0 203411 0 2020-10-29 17:30:47 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:m5.xlarge beta.kubernetes.io/os:linux failure-domain.beta.kubernetes.io/region:us-east-2 failure-domain.beta.kubernetes.io/zone:us-east-2a kubernetes.io/arch:amd64 kubernetes.io/hostname:ip-10-0-234-238 kubernetes.io/os:linux node-role.kubernetes.io/worker: node.kubernetes.io/instance-type:m5.xlarge node.openshift.io/os_id:rhcos topology.ebs.csi.aws.com/zone:us-east-2a topology.kubernetes.io/region:us-east-2 topology.kubernetes.io/zone:us-east-2a] map[csi.volume.kubernetes.io/nodeid:{"ebs.csi.aws.com":"i-0311b67a5abfd855e"} machine.openshift.io/machine:openshift-machine-api/jeder-461-cncf2-pvrph-worker-us-east-2a-bxbwb machineconfiguration.openshift.io/currentConfig:rendered-worker-f26c105cafab97113f99101cc57ac048 machineconfiguration.openshift.io/desiredConfig:rendered-worker-f26c105cafab97113f99101cc57ac048 machineconfiguration.openshift.io/reason: machineconfiguration.openshift.io/state:Done volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{nodelink-controller Update v1 2020-10-29 17:30:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machine.openshift.io/machine":{}}}}} {machine-config-controller Update v1 2020-10-29 17:44:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/desiredConfig":{}}}}} {kube-controller-manager Update v1 2020-10-29 17:46:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{}}},"f:spec":{"f:podCIDR":{},"f:podCIDRs":{".":{},"v:\"10.128.4.0/24\"":{}}}}} {machine-config-daemon Update v1 2020-10-29 17:46:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/currentConfig":{},"f:machineconfiguration.openshift.io/reason":{},"f:machineconfiguration.openshift.io/state":{}}}}} {kubelet Update v1 2020-10-29 21:16:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:csi.volume.kubernetes.io/nodeid":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/instance-type":{},"f:failure-domain.beta.kubernetes.io/region":{},"f:failure-domain.beta.kubernetes.io/zone":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:node-role.kubernetes.io/worker":{},"f:node.kubernetes.io/instance-type":{},"f:node.openshift.io/os_id":{},"f:topology.ebs.csi.aws.com/zone":{},"f:topology.kubernetes.io/region":{},"f:topology.kubernetes.io/zone":{}}},"f:spec":{"f:providerID":{}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalDNS\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:attachable-volumes-aws-ebs":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:attachable-volumes-aws-ebs":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"DiskPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"MemoryPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PIDPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:images":{},"f:nodeInfo":{"f:architecture":{},"f:bootID":{},"f:containerRuntimeVersion":{},"f:kernelVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{},"f:machineID":{},"f:operatingSystem":{},"f:osImage":{},"f:systemUUID":{}}}}}]},Spec:NodeSpec{PodCIDR:10.128.4.0/24,DoNotUseExternalID:,ProviderID:aws:///us-east-2a/i-0311b67a5abfd855e,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.128.4.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{321557344256 0} {<nil>} 314020844Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{16502939648 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},},Allocatable:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{3 0} {<nil>} 3 DecimalSI},ephemeral-storage: {{288327867528 0} {<nil>} 288327867528 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{15324340224 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2020-10-29 21:16:45 +0000 UTC,LastTransitionTime:2020-10-29 17:46:27 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2020-10-29 21:16:45 +0000 UTC,LastTransitionTime:2020-10-29 17:46:27 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2020-10-29 21:16:45 +0000 UTC,LastTransitionTime:2020-10-29 17:46:27 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2020-10-29 21:16:45 +0000 UTC,LastTransitionTime:2020-10-29 17:46:27 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.0.234.238,},NodeAddress{Type:Hostname,Address:ip-10-0-234-238.us-east-2.compute.internal,},NodeAddress{Type:InternalDNS,Address:ip-10-0-234-238.us-east-2.compute.internal,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:ec2ba4194beb01956828a9bcef15464f,SystemUUID:ec2ba419-4beb-0195-6828-a9bcef15464f,BootID:238b22fb-c838-4401-ba71-dbbf616e0421,KernelVersion:4.18.0-193.24.1.el8_2.dt1.x86_64,OSImage:Red Hat Enterprise Linux CoreOS 46.82.202010091720-0 (Ootpa),ContainerRuntimeVersion:cri-o://1.19.0-20.rhaos4.6.git97d715e.el8,KubeletVersion:v1.19.0+d59ce34,KubeProxyVersion:v1.19.0+d59ce34,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[quay.io/app-sre/managed-velero-operator-registry@sha256:7d152b07c12c961a67be142cc80597b0616f0db936c34af242e309e6b7be208a quay.io/app-sre/managed-velero-operator-registry:production-0a73057],SizeBytes:1036797635,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:79978a34d1ab3b0ed1ad2c93c09bcb2fcdd1806b35e48a53c99d106347e1a59d quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:769679082,},ContainerImage{Names:[registry.redhat.io/redhat/community-operator-index@sha256:02facc4e56ed383e6cbd19ba740f477f55068948963b18154f66738302f4dd03 registry.redhat.io/redhat/community-operator-index@sha256:467e5d70a7591e82cf325371297c9ee4938e18ab60f42903a01270c0ef8a6a1c registry.redhat.io/redhat/community-operator-index:latest],SizeBytes:509744014,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:82d1def7312de8ae5dee32d237ad59fe685923e78668fa3547e6bee445cd8842 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:499341399,},ContainerImage{Names:[registry.redhat.io/redhat/certified-operator-index@sha256:8e4d14186a6a7a377512dd952fd4b6e8306d669b4e82864c8535021f3e725bd8 registry.redhat.io/redhat/certified-operator-index@sha256:b674fbf172249ff5c27ab195ca05ad14015783e523b421617181a03f9275874f registry.redhat.io/redhat/certified-operator-index:v4.6],SizeBytes:497271693,},ContainerImage{Names:[registry.redhat.io/redhat/redhat-marketplace-index@sha256:1254d42b17fdf685608b3b2f58244a753ce46ce49ad3f5e849a372dfbb901001 registry.redhat.io/redhat/redhat-marketplace-index@sha256:cdd578d52e7fd59b341f35c84f2ccd5681726794bf091d84d06ed66730fd38db registry.redhat.io/redhat/redhat-marketplace-index:v4.6],SizeBytes:492995473,},ContainerImage{Names:[registry.redhat.io/redhat/redhat-operator-index@sha256:20b5fa8d4466c957ee23c97aca064f757f3d7ebb8a1b237d1b1a5e479853a0ca registry.redhat.io/redhat/redhat-operator-index@sha256:bd70797deab3e1e0487d93d488e91418840c6c891d1af2156f66c687638b1308 registry.redhat.io/redhat/redhat-operator-index:v4.6],SizeBytes:491000720,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4cfd55719faff41e96c2e4be69e3f2381a57b8b3445b80ae4acfe8ee33d7f99b quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:481479938,},ContainerImage{Names:[quay.io/jupierce/openshift-python-monitoring@sha256:98e3259170f7116837fa60b0515fe79cb9721060d34eb4931a14244b4658bb8d quay.io/jupierce/openshift-python-monitoring:stable],SizeBytes:474594250,},ContainerImage{Names:[quay.io/app-sre/managed-upgrade-operator-registry@sha256:5d4870d619f5dd35974f957d92d6939a6b56a03c705420ae8602e01e67a10bfc quay.io/app-sre/managed-upgrade-operator-registry:production-9b59534],SizeBytes:472464972,},ContainerImage{Names:[quay.io/app-sre/cloud-ingress-operator-registry@sha256:ce73d98128dc1584568c88d44ba893e924baa93ec8ce0e3b4036f62c01fbd52a quay.io/app-sre/cloud-ingress-operator-registry:production-a2e288b],SizeBytes:470975221,},ContainerImage{Names:[quay.io/app-sre/configure-alertmanager-operator-registry@sha256:11de3d989f814e60482c28ddd72a71d68650a6ae5787289e2eb159aa1af23f01 quay.io/app-sre/configure-alertmanager-operator-registry:production-0a8e5f3],SizeBytes:470905083,},ContainerImage{Names:[quay.io/app-sre/splunk-forwarder-operator-registry@sha256:15c212f82397caa807f5c1e58d47d16bb50d143a886e52cdf09616a50c9e3ef2 quay.io/app-sre/splunk-forwarder-operator-registry:production-7e8bc93],SizeBytes:470516300,},ContainerImage{Names:[quay.io/app-sre/rbac-permissions-operator-registry@sha256:7620af44252bbdbed32316748ccbd165cad0e9320f48ccd63f05a3afc20dc0c2 quay.io/app-sre/rbac-permissions-operator-registry:production-819ec06],SizeBytes:469646922,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8248dec0d94b2928aa4d63a22973d9a8f8f173a1431b2ab4ad15fdfe80283d7c quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:419419150,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8923050603588c27d79b33b371afb651288470d5cdeb14f8e10249bca1a1c461 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:413031192,},ContainerImage{Names:[quay.io/openshift-sre/managed-prometheus-exporter-initcontainer@sha256:7fa36e4488191536ac4a570fdd3431d3c80db983db74b90bc2ce051ecef3035e quay.io/openshift-sre/managed-prometheus-exporter-initcontainer:v0.1.9-2019-03-28-4e558131],SizeBytes:388062834,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8f4882cff3c2f9521215eac681c5abda42876e3e955431c1387fb457940b8344 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:370109724,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:72323ce541f8a26fbad17ef65ff21b51498863bb851635a0faa8d5b1ac6ce0e4 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:366911472,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3d0361f380abf5252b1b640e3ceaaab8274e2af8cdb605b20b513a1a44b3a4dc quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:346080704,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f644e4b495071a2b9d0d6f5d48cb96dad9f7ea8298cc22c98824bc70229ea9dd quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:341116512,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6bada08687c20afe316c1f8cf76f001f31bedae317f256f3df3affaa5d0dc25e quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:336987966,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f42509c18cf5e41201d64cf3a9c1994ffa5318f8d7cee5de45fa2da914e68bbc quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:336588772,},ContainerImage{Names:[image-registry.openshift-image-registry.svc:5000/openshift/cli@sha256:6aa4bb97adf2142b0e74ccae7fd3661ada73cbaac803b86bb8712261e916d66d quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6aa4bb97adf2142b0e74ccae7fd3661ada73cbaac803b86bb8712261e916d66d image-registry.openshift-image-registry.svc:5000/openshift/cli:latest],SizeBytes:336576591,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:52424fd2af6fd7d7a5a1233032eb3f3c67f7691996b209e013e29f1524c5188c quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:326057200,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f86db3170270fc635dff0d7f1ba6e79a8f45de7e1dcfa5621474d1f6e07352ec quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:325370491,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:54fbeb744b82662fd38c0d301ebaad6ca8983707bc44db7235ead0fb7b95808f quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:321482743,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a37a568c63563257309cb0ffb6e185d98f662ff3201d2099cbe0df404b93f0c8 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:314357110,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e176509a104b2f96ee4f5c57275c6a712409aa80ac40071345d0a03bdde2b456 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:307311930,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0c42cd3a74176732e8c06105c47674c7d410c7167c8e3fbd80f9a76e9bfda5bd quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:302591889,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c75977f28becdf4f7065cfa37233464dd31208b1767e620c4f19658f53f8ff8c quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:300756060,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:cf565b2fab365e027962a25a8cffb41aa35cb5a00d001e081d53c7fed5a0c54b quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:300005108,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:78d3478e632c761c18e2dcb55d26e388ecfd126d4fde60317868133dc2fd57f7 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:292832919,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9e7f0468850aeb13585ef049f687cc42c05d82bc0e0200607d1a93d7f9740fe5 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:291621018,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a1aaf99f2ed745c5353d9fc715fa8e9111f42165e3012fad73640c438ba6aa6f quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:290507515,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6cdaecd5dd9df8fd74529be7fa5d8973daf6f4ea95be8acfb2f5ac97773ebe67 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:278303432,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e5bcf6d786fd218e1ef188eb40c39c31a98d03121fba3b7a1f16e87e45a7478b quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:276735847,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1dcc413b621958f97dfbb3fc998a9e225ef155a80ffb151eb4694bf8370b383a quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:274559501,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8c9e61400619c4613db5cc73097d287e3cd5d2125c85d1d84cc30cfdaa1093e7 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:271612142,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:12b11e2000b42ce1aaa228d9c1f4c9177395add2fa43835e667b7fc9007e40e6 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:270734898,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:88ddfbded8bc27b227ed7397ece050b756e522a9ffc34cbfba3c94c5ee58b740 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:269654227,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8bd90fcca7990c0edead15298dcec963968274d299428da95eae41aa23157b90 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:264244297,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:73fdcef5de85e739831c5a8b76dce349a3c8832ff416a46263743d7e61655cbb quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:247448489,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b635694dc0663a1404d43a4a9ac8513a3087c7cfead50f6ab413f3c217c40b2a quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:240293385,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:71051bdf1b96c953fc1dfd48359915bf5c027613de6f5e2fa8adeea8d3dda311 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:236515650,},ContainerImage{Names:[quay.io/app-sre/managed-velero-operator@sha256:a9eb4800dcafe3fe8879662c0413a7e5c53b5fe98ab0bbc81b9fc151044a4e07 quay.io/app-sre/managed-velero-operator:0a73057],SizeBytes:207938263,},ContainerImage{Names:[quay.io/app-sre/cloud-ingress-operator@sha256:ceb064d013f51cdb15075f079eee2dfd5a0f42f4a09b489176abe92e91fb6a56 quay.io/app-sre/cloud-ingress-operator:v0.1.233-a2e288b],SizeBytes:203838503,},ContainerImage{Names:[quay.io/app-sre/managed-upgrade-operator@sha256:558a828a0b3c296cb2c0e52ef143a45177bdae40b1c84bfe8564486c940fd79a quay.io/app-sre/managed-upgrade-operator:9b59534],SizeBytes:198448288,},ContainerImage{Names:[quay.io/app-sre/splunk-forwarder-operator@sha256:bca474387af6112b08406697ef28977d428c1a884ebaf602a3e6ba1b45e6b31b quay.io/app-sre/splunk-forwarder-operator:7e8bc93],SizeBytes:191465195,},ContainerImage{Names:[quay.io/app-sre/configure-alertmanager-operator@sha256:a2bed259f284358220d4e01390621a24f43c6f0d534474a5050cad0c6c6cb812 quay.io/app-sre/configure-alertmanager-operator:v0.1.237-0a8e5f3],SizeBytes:188443907,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Oct 29 21:21:00.378: INFO: 
Logging kubelet events for node ip-10-0-234-238.us-east-2.compute.internal
Oct 29 21:21:00.401: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-234-238.us-east-2.compute.internal
Oct 29 21:21:00.440: INFO: deployments-pruner-1604005200-dfg2v started at 2020-10-29 21:00:09 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.440: INFO: 	Container deployments-pruner ready: false, restart count 0
Oct 29 21:21:00.440: INFO: multus-bcgtj started at 2020-10-29 17:30:47 +0000 UTC (5+1 container statuses recorded)
Oct 29 21:21:00.440: INFO: 	Init container multus-binary-copy ready: true, restart count 0
Oct 29 21:21:00.440: INFO: 	Init container cni-plugins ready: true, restart count 0
Oct 29 21:21:00.440: INFO: 	Init container routeoverride-cni ready: true, restart count 0
Oct 29 21:21:00.440: INFO: 	Init container whereabouts-cni-bincopy ready: true, restart count 0
Oct 29 21:21:00.440: INFO: 	Init container whereabouts-cni ready: true, restart count 0
Oct 29 21:21:00.440: INFO: 	Container kube-multus ready: true, restart count 0
Oct 29 21:21:00.440: INFO: sre-dns-latency-exporter-5wrqd started at 2020-10-29 17:39:27 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.440: INFO: 	Container main ready: true, restart count 0
Oct 29 21:21:00.440: INFO: migrator-b8d88f977-j4wqg started at 2020-10-29 19:28:48 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.440: INFO: 	Container migrator ready: true, restart count 0
Oct 29 21:21:00.440: INFO: certified-operators-pjgrs started at 2020-10-29 19:32:07 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.440: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:21:00.440: INFO: tuned-6rs49 started at 2020-10-29 17:30:47 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.440: INFO: 	Container tuned ready: true, restart count 0
Oct 29 21:21:00.440: INFO: velero-68656b49f9-zvplm started at 2020-10-29 17:46:41 +0000 UTC (1+1 container statuses recorded)
Oct 29 21:21:00.440: INFO: 	Init container velero-plugin-for-aws ready: true, restart count 0
Oct 29 21:21:00.440: INFO: 	Container velero ready: true, restart count 0
Oct 29 21:21:00.440: INFO: thanos-querier-7f688468b8-cfd7g started at 2020-10-29 17:48:47 +0000 UTC (0+5 container statuses recorded)
Oct 29 21:21:00.440: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:00.440: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Oct 29 21:21:00.440: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:21:00.440: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:21:00.440: INFO: 	Container thanos-query ready: true, restart count 0
Oct 29 21:21:00.440: INFO: downloads-85df645c7c-pxr2l started at 2020-10-29 19:28:48 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.440: INFO: 	Container download-server ready: true, restart count 0
Oct 29 21:21:00.441: INFO: cloud-ingress-operator-registry-zpdvx started at 2020-10-29 19:32:07 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.441: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:21:00.441: INFO: managed-upgrade-operator-7558857f4-cxqkg started at 2020-10-29 17:48:47 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.441: INFO: 	Container managed-upgrade-operator ready: true, restart count 0
Oct 29 21:21:00.441: INFO: managed-velero-operator-8685cc459-lsvfl started at 2020-10-29 17:48:48 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.441: INFO: 	Container managed-velero-operator ready: true, restart count 0
Oct 29 21:21:00.441: INFO: community-operators-rss65 started at 2020-10-29 19:32:06 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.441: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:21:00.441: INFO: crd-pruner-1603998000-ljqfl started at 2020-10-29 19:00:09 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.441: INFO: 	Container crd-pruner ready: false, restart count 0
Oct 29 21:21:00.441: INFO: downloads-85df645c7c-ft27h started at 2020-10-29 19:31:54 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.441: INFO: 	Container download-server ready: true, restart count 0
Oct 29 21:21:00.441: INFO: rbac-permissions-operator-registry-x892d started at 2020-10-29 19:32:08 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.441: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:21:00.441: INFO: dns-default-jxrwk started at 2020-10-29 17:30:47 +0000 UTC (0+3 container statuses recorded)
Oct 29 21:21:00.441: INFO: 	Container dns ready: true, restart count 0
Oct 29 21:21:00.441: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 29 21:21:00.441: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:00.441: INFO: machine-config-daemon-5rrfn started at 2020-10-29 17:30:47 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:21:00.441: INFO: 	Container machine-config-daemon ready: true, restart count 0
Oct 29 21:21:00.441: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:21:00.441: INFO: redhat-operators-pjjcr started at 2020-10-29 19:32:11 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.441: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:21:00.441: INFO: managed-upgrade-operator-catalog-g4m2g started at 2020-10-29 19:32:11 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.441: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:21:00.441: INFO: alert-pruner-1603998000-fgw8n started at 2020-10-29 19:00:09 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.441: INFO: 	Container alert-pruner ready: false, restart count 0
Oct 29 21:21:00.441: INFO: deployments-pruner-1603998000-vfdcp started at 2020-10-29 19:00:09 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.441: INFO: 	Container deployments-pruner ready: false, restart count 0
Oct 29 21:21:00.441: INFO: splunk-forwarder-operator-catalog-jg48j started at 2020-10-29 19:32:10 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.441: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:21:00.441: INFO: aws-ebs-csi-driver-node-r7m6f started at 2020-10-29 17:30:47 +0000 UTC (0+3 container statuses recorded)
Oct 29 21:21:00.441: INFO: 	Container csi-driver ready: true, restart count 0
Oct 29 21:21:00.441: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Oct 29 21:21:00.441: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Oct 29 21:21:00.441: INFO: network-metrics-daemon-5brxv started at 2020-10-29 17:30:48 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:21:00.441: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:00.441: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Oct 29 21:21:00.441: INFO: rbac-permissions-operator-78d47d8556-92kbv started at 2020-10-29 17:48:48 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.441: INFO: 	Container rbac-permissions-operator ready: true, restart count 0
Oct 29 21:21:00.441: INFO: managed-velero-operator-registry-9bghr started at 2020-10-29 19:32:07 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.441: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:21:00.441: INFO: redhat-marketplace-cwfzm started at 2020-10-29 19:32:08 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.441: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:21:00.441: INFO: node-exporter-lmwzp started at 2020-10-29 17:30:48 +0000 UTC (1+2 container statuses recorded)
Oct 29 21:21:00.442: INFO: 	Init container init-textfile ready: true, restart count 0
Oct 29 21:21:00.442: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:00.442: INFO: 	Container node-exporter ready: true, restart count 0
Oct 29 21:21:00.442: INFO: node-ca-wxm2h started at 2020-10-29 17:30:48 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.442: INFO: 	Container node-ca ready: true, restart count 0
Oct 29 21:21:00.442: INFO: ovs-9f9tm started at 2020-10-29 17:30:48 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.442: INFO: 	Container openvswitch ready: true, restart count 0
Oct 29 21:21:00.442: INFO: configure-alertmanager-operator-registry-zcvq5 started at 2020-10-29 19:32:07 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.442: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:21:00.442: INFO: sdn-9tj9v started at 2020-10-29 17:30:48 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:21:00.442: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Oct 29 21:21:00.442: INFO: 	Container sdn ready: true, restart count 0
Oct 29 21:21:00.442: INFO: configure-alertmanager-operator-579dfffff7-nmj5v started at 2020-10-29 17:48:47 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.442: INFO: 	Container configure-alertmanager-operator ready: true, restart count 0
Oct 29 21:21:00.442: INFO: openshift-state-metrics-74d99f5845-62ghm started at 2020-10-29 19:31:54 +0000 UTC (0+3 container statuses recorded)
Oct 29 21:21:00.442: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Oct 29 21:21:00.442: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Oct 29 21:21:00.442: INFO: 	Container openshift-state-metrics ready: true, restart count 0
W1029 21:21:00.463347   53132 metrics_grabber.go:83] Can't find any pods in namespace kube-system to grab metrics from
W1029 21:21:00.463365   53132 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1029 21:21:00.463374   53132 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1029 21:21:00.463379   53132 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Oct 29 21:21:00.525: INFO: 
Latency metrics for node ip-10-0-234-238.us-east-2.compute.internal
Oct 29 21:21:00.525: INFO: 
Logging node info for node ip-10-0-245-254.us-east-2.compute.internal
Oct 29 21:21:00.547: INFO: Node Info: &Node{ObjectMeta:{ip-10-0-245-254.us-east-2.compute.internal   /api/v1/nodes/ip-10-0-245-254.us-east-2.compute.internal be19adf8-253b-49a3-92e7-86a857679344 204752 0 2020-10-29 17:23:46 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:m5.xlarge beta.kubernetes.io/os:linux failure-domain.beta.kubernetes.io/region:us-east-2 failure-domain.beta.kubernetes.io/zone:us-east-2a kubernetes.io/arch:amd64 kubernetes.io/hostname:ip-10-0-245-254 kubernetes.io/os:linux node-role.kubernetes.io/master: node.kubernetes.io/instance-type:m5.xlarge node.openshift.io/os_id:rhcos topology.ebs.csi.aws.com/zone:us-east-2a topology.kubernetes.io/region:us-east-2 topology.kubernetes.io/zone:us-east-2a] map[csi.volume.kubernetes.io/nodeid:{"ebs.csi.aws.com":"i-0e7920427ef118c8d"} machine.openshift.io/machine:openshift-machine-api/jeder-461-cncf2-pvrph-master-1 machineconfiguration.openshift.io/currentConfig:rendered-master-b06f274cfa558dc0b5031aabca48f868 machineconfiguration.openshift.io/desiredConfig:rendered-master-b06f274cfa558dc0b5031aabca48f868 machineconfiguration.openshift.io/reason: machineconfiguration.openshift.io/state:Done volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{nodelink-controller Update v1 2020-10-29 17:26:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machine.openshift.io/machine":{}}}}} {machine-config-controller Update v1 2020-10-29 17:43:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/desiredConfig":{}}}}} {machine-config-daemon Update v1 2020-10-29 17:48:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/currentConfig":{},"f:machineconfiguration.openshift.io/reason":{},"f:machineconfiguration.openshift.io/state":{}}}}} {kube-controller-manager Update v1 2020-10-29 19:08:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{}}},"f:spec":{"f:podCIDR":{},"f:podCIDRs":{".":{},"v:\"10.128.1.0/24\"":{}},"f:taints":{}}}} {oc Update v1 2020-10-29 19:08:29 +0000 UTC FieldsV1 {"f:spec":{"f:unschedulable":{}}}} {kubelet Update v1 2020-10-29 21:20:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:csi.volume.kubernetes.io/nodeid":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/instance-type":{},"f:failure-domain.beta.kubernetes.io/region":{},"f:failure-domain.beta.kubernetes.io/zone":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:node-role.kubernetes.io/master":{},"f:node.kubernetes.io/instance-type":{},"f:node.openshift.io/os_id":{},"f:topology.ebs.csi.aws.com/zone":{},"f:topology.kubernetes.io/region":{},"f:topology.kubernetes.io/zone":{}}},"f:spec":{"f:providerID":{}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalDNS\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:attachable-volumes-aws-ebs":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:attachable-volumes-aws-ebs":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"DiskPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"MemoryPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PIDPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:images":{},"f:nodeInfo":{"f:architecture":{},"f:bootID":{},"f:containerRuntimeVersion":{},"f:kernelVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{},"f:machineID":{},"f:operatingSystem":{},"f:osImage":{},"f:systemUUID":{}}}}}]},Spec:NodeSpec{PodCIDR:10.128.1.0/24,DoNotUseExternalID:,ProviderID:aws:///us-east-2a/i-0e7920427ef118c8d,Unschedulable:true,Taints:[]Taint{Taint{Key:node-role.kubernetes.io/master,Value:,Effect:NoSchedule,TimeAdded:<nil>,},Taint{Key:node.kubernetes.io/unschedulable,Value:,Effect:NoSchedule,TimeAdded:2020-10-29 19:08:29 +0000 UTC,},},ConfigSource:nil,PodCIDRs:[10.128.1.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{375244435456 0} {<nil>} 366449644Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{16502939648 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},},Allocatable:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{3 0} {<nil>} 3 DecimalSI},ephemeral-storage: {{336646249528 0} {<nil>} 336646249528 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{15324340224 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2020-10-29 21:20:26 +0000 UTC,LastTransitionTime:2020-10-29 17:48:23 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2020-10-29 21:20:26 +0000 UTC,LastTransitionTime:2020-10-29 17:48:23 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2020-10-29 21:20:26 +0000 UTC,LastTransitionTime:2020-10-29 17:48:23 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2020-10-29 21:20:26 +0000 UTC,LastTransitionTime:2020-10-29 17:48:23 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.0.245.254,},NodeAddress{Type:Hostname,Address:ip-10-0-245-254.us-east-2.compute.internal,},NodeAddress{Type:InternalDNS,Address:ip-10-0-245-254.us-east-2.compute.internal,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:ec2b6d482b9145cdb130fb38cf5b5e4c,SystemUUID:ec2b6d48-2b91-45cd-b130-fb38cf5b5e4c,BootID:0fb27564-ff56-4d9f-8258-3c1907ece299,KernelVersion:4.18.0-193.24.1.el8_2.dt1.x86_64,OSImage:Red Hat Enterprise Linux CoreOS 46.82.202010091720-0 (Ootpa),ContainerRuntimeVersion:cri-o://1.19.0-20.rhaos4.6.git97d715e.el8,KubeletVersion:v1.19.0+d59ce34,KubeProxyVersion:v1.19.0+d59ce34,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:30094a2d586aa282d85e14f1be19abec1c30ce431673377b0e1c12d83e6bac8c quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:682301224,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c51d44f380ecff7d36b1de4bb3bdbd4ac66abc6669724f28d81bd4af5741a8ac quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:514079422,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:82d1def7312de8ae5dee32d237ad59fe685923e78668fa3547e6bee445cd8842 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:499341399,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4cfd55719faff41e96c2e4be69e3f2381a57b8b3445b80ae4acfe8ee33d7f99b quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:481479938,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ae6a5decd040a6b3adfa074d3211ab92a36b77b2d849962d9a678e1c2c5ef5c1 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:473291773,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8923050603588c27d79b33b371afb651288470d5cdeb14f8e10249bca1a1c461 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:413031192,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8f4882cff3c2f9521215eac681c5abda42876e3e955431c1387fb457940b8344 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:370109724,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:72323ce541f8a26fbad17ef65ff21b51498863bb851635a0faa8d5b1ac6ce0e4 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:366911472,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b7dc5f4101a8cb88c20d853908982258cab77bb0ac391e965b50b15648ddd854 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:359851126,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:93b3e1246884e357e1654e6c9578481aff9eef07eed1f9fdd0e9c8cc89a3770c quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:358141917,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e2b3f973bc5b9e55d2240a556c4648c921a3c8d3e12381757f1990a864208617 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:343841040,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a8214df42df962b965e3f4daad0b61932235e57241160861e503d84e38b775d5 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:337507384,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6bada08687c20afe316c1f8cf76f001f31bedae317f256f3df3affaa5d0dc25e quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:336987966,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6aa4bb97adf2142b0e74ccae7fd3661ada73cbaac803b86bb8712261e916d66d quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:336576591,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:aa4f37543b45bc248db8d9bd2dc45b6e159a8869b044c2310f541afba15b2694 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:336485164,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f52825e9905c926d399cd0b7afbb2b7d0370ae22da0416feac9131d555db0b98 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:336041278,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a2a167f59783ca402118fe35ea5fefbf457e01b64836f8be3be6695aefd76d76 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:335654632,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:5e591cab19b41c7ea26eab6056cd518f6d64b59e8051978de927b1b984abfb1d quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:334612215,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:7584014b0cb8cb2c5a09b909c79f2f8ad6e49854bcfabf72e96a22330bcf6f56 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:333011308,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:357e35286fd26fed015c03a9c451f6fdcf61cf0821d959025e7f800e7c533f29 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:332777114,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f86db3170270fc635dff0d7f1ba6e79a8f45de7e1dcfa5621474d1f6e07352ec quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:325370491,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:54fbeb744b82662fd38c0d301ebaad6ca8983707bc44db7235ead0fb7b95808f quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:321482743,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:11c3a8bdddbbb2229bd68bb80b6009588873118881952c702dfebd1484046191 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:319944375,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9dbb31ac799b2c30270268714dcb3d11bafb329b98639a446657c8c7db41938c quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:318368538,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-release@sha256:d78292e9730dd387ff6198197c8b0598da340be7678e8e1e4810b557a926c2b9 quay.io/openshift-release-dev/ocp-release@sha256:<none>],SizeBytes:317006092,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1ad85195e1a180698fe4b8df82e3d72075efb256b53f593d13e29faaf7f3e15a quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:316310728,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dc6a6a1d4a6b2af67421561e53d1af1d40c99ae72de69f4c3cc390d447f12269 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:315339691,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4691dc29704c9cb06d2345894f1a8f074b58a0d208318c5218241388b0916e1b quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:315327750,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f7b9278ef2fbe988f50e4bdeeea79d9373b55689d17b8c6d7c214429f5b3f9a0 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:315119766,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4289297f0b7ee7edf394348fd07e1fa1b3162655f2a2af2245e23af4b179e7f2 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:314343750,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:65206861218064576dc092040e9c24b0393b8a07502e351f513400f187f38cc7 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:313489720,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:45586fd7a5cfd43ff546dbfb282a70a91eaf0f069f604230af958dc802832f89 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:312615681,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c6b3aaaa38679b1d752ec09bd68c6d80a8911c74ec16d27c49de88ecb97823ee quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:312540521,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:25197b2709c0691c28424c9b07e505a71d13bf481e18bc42636cc84ee8fef033 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:312275620,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2689d3e66bbfdd7d493d969524b8a7da00142d1b4372e3e880bd825beb3da558 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:311351002,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9b564f882e31f497f57a0d99d406d5231eb15e9a97f0b450c21bec2bac7ff033 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:311323012,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a96e2e4a62bca22da0b6903c9e20d7c776bd241f13accf51ede88965b232aca8 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:310235384,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0531ff2ccf0ddea76e42cc9951470528bbd7def094884bc569f660376798f40a quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:308198752,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:12fe384de71c7621d9061f48afafeed3dc337679a66afd8d0a871e200295a1e5 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:307996829,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:82758fbc97d9da98f20eddcfb4a8bc279726b97da96263d4c165b404389cb553 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:307966006,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e176509a104b2f96ee4f5c57275c6a712409aa80ac40071345d0a03bdde2b456 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:307311930,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f18151bf70434e1841ed8182c42e819e92e3d1ad3bbd269c667be8b74ff78444 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:306621135,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:afdf0a3b426ac1c03df52e88a2b884f0714e54a1a03f33091954441a05a7f6b9 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:306116297,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:076b280e17c6bb4cc618db71403ccec75f8196c8849061a40c680a2808292bb6 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:304641618,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:01626d98c80e44e0cd3a522ad019eb236e39c30b0dfff0ac5a6fa98686159286 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:303742706,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:cfe62d81269929501517e75a7d337f7d8fc78ac9a17665adebfef52a2024584d quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:303388513,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c75977f28becdf4f7065cfa37233464dd31208b1767e620c4f19658f53f8ff8c quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:300756060,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3cdb4589ee683c85e3d8f3f239187bc089d30cac6c26847a54894f6c328817c3 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:300278177,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:cf565b2fab365e027962a25a8cffb41aa35cb5a00d001e081d53c7fed5a0c54b quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:300005108,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:92feaeb8763ece68147b522bfa8914bcd429e9825185b9b9c05247ad2857d03f quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:299900737,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Oct 29 21:21:00.547: INFO: 
Logging kubelet events for node ip-10-0-245-254.us-east-2.compute.internal
Oct 29 21:21:00.568: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-245-254.us-east-2.compute.internal
Oct 29 21:21:00.632: INFO: multus-admission-controller-j5cw4 started at 2020-10-29 17:24:26 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:21:00.632: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:00.632: INFO: 	Container multus-admission-controller ready: true, restart count 0
Oct 29 21:21:00.632: INFO: node-exporter-wptwd started at 2020-10-29 17:24:51 +0000 UTC (1+2 container statuses recorded)
Oct 29 21:21:00.632: INFO: 	Init container init-textfile ready: true, restart count 0
Oct 29 21:21:00.632: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:00.632: INFO: 	Container node-exporter ready: true, restart count 0
Oct 29 21:21:00.632: INFO: node-ca-cv467 started at 2020-10-29 17:25:50 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.632: INFO: 	Container node-ca ready: true, restart count 0
Oct 29 21:21:00.632: INFO: revision-pruner-9-ip-10-0-245-254.us-east-2.compute.internal started at 2020-10-29 17:46:56 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.632: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:21:00.632: INFO: insights-operator-5d7b76ffb6-sr4zx started at 2020-10-29 17:48:53 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.632: INFO: 	Container insights-operator ready: true, restart count 0
Oct 29 21:21:00.632: INFO: cloud-credential-operator-5b849fdff8-8shdg started at 2020-10-29 17:48:55 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:21:00.632: INFO: 	Container cloud-credential-operator ready: true, restart count 0
Oct 29 21:21:00.633: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:00.633: INFO: dns-default-5rqxh started at 2020-10-29 17:25:40 +0000 UTC (0+3 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container dns ready: true, restart count 0
Oct 29 21:21:00.633: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 29 21:21:00.633: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:00.633: INFO: revision-pruner-7-ip-10-0-245-254.us-east-2.compute.internal started at 2020-10-29 17:48:23 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:21:00.633: INFO: cluster-autoscaler-operator-75fff7d7dc-xql24 started at 2020-10-29 17:48:59 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container cluster-autoscaler-operator ready: true, restart count 0
Oct 29 21:21:00.633: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:00.633: INFO: revision-pruner-10-ip-10-0-245-254.us-east-2.compute.internal started at 2020-10-29 19:46:42 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:21:00.633: INFO: validation-webhook-zfwdv started at 2020-10-29 17:39:26 +0000 UTC (1+1 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Init container inject-cert ready: true, restart count 0
Oct 29 21:21:00.633: INFO: 	Container webhooks ready: true, restart count 0
Oct 29 21:21:00.633: INFO: kube-controller-manager-operator-86f4f896b9-qmxvg started at 2020-10-29 17:48:52 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container kube-controller-manager-operator ready: true, restart count 0
Oct 29 21:21:00.633: INFO: kube-controller-manager-ip-10-0-245-254.us-east-2.compute.internal started at 2020-10-29 17:28:24 +0000 UTC (0+4 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container cluster-policy-controller ready: true, restart count 0
Oct 29 21:21:00.633: INFO: 	Container kube-controller-manager ready: true, restart count 0
Oct 29 21:21:00.633: INFO: 	Container kube-controller-manager-cert-syncer ready: true, restart count 0
Oct 29 21:21:00.633: INFO: 	Container kube-controller-manager-recovery-controller ready: true, restart count 0
Oct 29 21:21:00.633: INFO: revision-pruner-3-ip-10-0-245-254.us-east-2.compute.internal started at 2020-10-29 17:48:23 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:21:00.633: INFO: kube-apiserver-operator-5c5fc96767-s6hck started at 2020-10-29 17:48:53 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container kube-apiserver-operator ready: true, restart count 0
Oct 29 21:21:00.633: INFO: aws-ebs-csi-driver-controller-6cbb586dfc-kbcwd started at 2020-10-29 17:48:55 +0000 UTC (0+6 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container csi-attacher ready: true, restart count 0
Oct 29 21:21:00.633: INFO: 	Container csi-driver ready: true, restart count 0
Oct 29 21:21:00.633: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Oct 29 21:21:00.633: INFO: 	Container csi-provisioner ready: true, restart count 0
Oct 29 21:21:00.633: INFO: 	Container csi-resizer ready: true, restart count 0
Oct 29 21:21:00.633: INFO: 	Container csi-snapshotter ready: true, restart count 0
Oct 29 21:21:00.633: INFO: machine-api-controllers-57968448cb-nxr9k started at 2020-10-29 17:48:59 +0000 UTC (0+7 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container kube-rbac-proxy-machine-mtrc ready: true, restart count 0
Oct 29 21:21:00.633: INFO: 	Container kube-rbac-proxy-machineset-mtrc ready: true, restart count 0
Oct 29 21:21:00.633: INFO: 	Container kube-rbac-proxy-mhc-mtrc ready: true, restart count 0
Oct 29 21:21:00.633: INFO: 	Container machine-controller ready: true, restart count 0
Oct 29 21:21:00.633: INFO: 	Container machine-healthcheck-controller ready: true, restart count 0
Oct 29 21:21:00.633: INFO: 	Container machineset-controller ready: true, restart count 0
Oct 29 21:21:00.633: INFO: 	Container nodelink-controller ready: true, restart count 0
Oct 29 21:21:00.633: INFO: aws-ebs-csi-driver-node-gzvtr started at 2020-10-29 17:24:58 +0000 UTC (0+3 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container csi-driver ready: true, restart count 0
Oct 29 21:21:00.633: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Oct 29 21:21:00.633: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Oct 29 21:21:00.633: INFO: dns-operator-59c89fbb97-gvh2x started at 2020-10-29 17:48:52 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container dns-operator ready: true, restart count 0
Oct 29 21:21:00.633: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:00.633: INFO: revision-pruner-10-ip-10-0-245-254.us-east-2.compute.internal started at 2020-10-29 18:06:49 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:21:00.633: INFO: revision-pruner-10-ip-10-0-245-254.us-east-2.compute.internal started at 2020-10-29 19:47:11 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:21:00.633: INFO: tuned-n8zzr started at 2020-10-29 17:24:46 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container tuned ready: true, restart count 0
Oct 29 21:21:00.633: INFO: machine-config-server-zcn4c started at 2020-10-29 17:26:37 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container machine-config-server ready: true, restart count 0
Oct 29 21:21:00.633: INFO: installer-10-ip-10-0-245-254.us-east-2.compute.internal started at 2020-10-29 18:03:07 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:21:00.633: INFO: kube-apiserver-ip-10-0-245-254.us-east-2.compute.internal started at 2020-10-29 17:27:30 +0000 UTC (1+5 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Init container setup ready: true, restart count 0
Oct 29 21:21:00.633: INFO: 	Container kube-apiserver ready: true, restart count 0
Oct 29 21:21:00.633: INFO: 	Container kube-apiserver-cert-regeneration-controller ready: true, restart count 0
Oct 29 21:21:00.633: INFO: 	Container kube-apiserver-cert-syncer ready: true, restart count 0
Oct 29 21:21:00.633: INFO: 	Container kube-apiserver-check-endpoints ready: true, restart count 0
Oct 29 21:21:00.633: INFO: 	Container kube-apiserver-insecure-readyz ready: true, restart count 0
Oct 29 21:21:00.633: INFO: sdn-cmt44 started at 2020-10-29 17:24:03 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Oct 29 21:21:00.633: INFO: 	Container sdn ready: true, restart count 0
Oct 29 21:21:00.633: INFO: revision-pruner-8-ip-10-0-245-254.us-east-2.compute.internal started at 2020-10-29 17:48:23 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:21:00.633: INFO: console-operator-b944ff564-n6dbf started at 2020-10-29 17:48:52 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container console-operator ready: true, restart count 0
Oct 29 21:21:00.633: INFO: multus-t45l2 started at 2020-10-29 17:23:58 +0000 UTC (5+1 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Init container multus-binary-copy ready: true, restart count 0
Oct 29 21:21:00.633: INFO: 	Init container cni-plugins ready: true, restart count 0
Oct 29 21:21:00.633: INFO: 	Init container routeoverride-cni ready: true, restart count 0
Oct 29 21:21:00.633: INFO: 	Init container whereabouts-cni-bincopy ready: true, restart count 0
Oct 29 21:21:00.633: INFO: 	Init container whereabouts-cni ready: true, restart count 0
Oct 29 21:21:00.633: INFO: 	Container kube-multus ready: true, restart count 0
Oct 29 21:21:00.633: INFO: cluster-image-registry-operator-8455b85cc6-4g5vr started at 2020-10-29 17:48:52 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Oct 29 21:21:00.633: INFO: installer-10-ip-10-0-245-254.us-east-2.compute.internal started at 2020-10-29 19:46:37 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:21:00.633: INFO: recyler-pod-ip-10-0-245-254.us-east-2.compute.internal started at 2020-10-29 17:48:23 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container recyler-container ready: false, restart count 0
Oct 29 21:21:00.633: INFO: ingress-operator-6d4c4d74f9-dplx6 started at 2020-10-29 17:48:53 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container ingress-operator ready: true, restart count 0
Oct 29 21:21:00.633: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:00.633: INFO: etcd-quorum-guard-644f5747b8-gc86j started at 2020-10-29 17:48:53 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container guard ready: true, restart count 0
Oct 29 21:21:00.633: INFO: packageserver-d4484c6fc-2ptvn started at 2020-10-29 17:49:02 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container packageserver ready: true, restart count 0
Oct 29 21:21:00.633: INFO: revision-pruner-9-ip-10-0-245-254.us-east-2.compute.internal started at 2020-10-29 17:49:33 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:21:00.633: INFO: revision-pruner-11-ip-10-0-245-254.us-east-2.compute.internal started at 2020-10-29 19:52:50 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:21:00.633: INFO: openshift-apiserver-operator-b8bcd86cf-vpp2h started at 2020-10-29 17:48:52 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container openshift-apiserver-operator ready: true, restart count 0
Oct 29 21:21:00.633: INFO: csi-snapshot-controller-8bb7f7589-9m9jc started at 2020-10-29 17:48:56 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container snapshot-controller ready: true, restart count 0
Oct 29 21:21:00.633: INFO: ovs-qwlht started at 2020-10-29 17:24:02 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container openvswitch ready: true, restart count 0
Oct 29 21:21:00.633: INFO: machine-config-daemon-l5m7d started at 2020-10-29 17:24:37 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container machine-config-daemon ready: true, restart count 0
Oct 29 21:21:00.633: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:21:00.633: INFO: etcd-ip-10-0-245-254.us-east-2.compute.internal started at 2020-10-29 17:26:35 +0000 UTC (2+3 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Init container etcd-ensure-env-vars ready: true, restart count 0
Oct 29 21:21:00.633: INFO: 	Init container etcd-resources-copy ready: true, restart count 0
Oct 29 21:21:00.633: INFO: 	Container etcd ready: true, restart count 0
Oct 29 21:21:00.633: INFO: 	Container etcd-metrics ready: true, restart count 0
Oct 29 21:21:00.633: INFO: 	Container etcdctl ready: true, restart count 0
Oct 29 21:21:00.633: INFO: installer-10-ip-10-0-245-254.us-east-2.compute.internal started at 2020-10-29 19:46:31 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:21:00.633: INFO: console-575d66b587-7h79z started at 2020-10-29 17:48:52 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container console ready: true, restart count 0
Oct 29 21:21:00.633: INFO: openshift-controller-manager-operator-8476766cf-srskg started at 2020-10-29 17:48:52 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container openshift-controller-manager-operator ready: true, restart count 0
Oct 29 21:21:00.633: INFO: openshift-kube-scheduler-ip-10-0-245-254.us-east-2.compute.internal started at 2020-10-29 17:25:50 +0000 UTC (1+2 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Init container wait-for-host-port ready: true, restart count 0
Oct 29 21:21:00.633: INFO: 	Container kube-scheduler ready: true, restart count 0
Oct 29 21:21:00.633: INFO: 	Container kube-scheduler-cert-syncer ready: true, restart count 0
Oct 29 21:21:00.633: INFO: sdn-controller-k68bz started at 2020-10-29 17:24:02 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container sdn-controller ready: true, restart count 0
Oct 29 21:21:00.633: INFO: installer-9-ip-10-0-245-254.us-east-2.compute.internal started at 2020-10-29 17:46:43 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:21:00.633: INFO: apiserver-8b58dffb-4vb54 started at 2020-10-29 17:48:53 +0000 UTC (1+2 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Init container fix-audit-permissions ready: true, restart count 0
Oct 29 21:21:00.633: INFO: 	Container openshift-apiserver ready: true, restart count 0
Oct 29 21:21:00.633: INFO: 	Container openshift-apiserver-check-endpoints ready: true, restart count 0
Oct 29 21:21:00.633: INFO: installer-11-ip-10-0-245-254.us-east-2.compute.internal started at 2020-10-29 19:50:52 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:21:00.633: INFO: cluster-version-operator-57b6bcb4df-g9px7 started at 2020-10-29 17:46:38 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container cluster-version-operator ready: true, restart count 0
Oct 29 21:21:00.633: INFO: etcd-operator-5996cd8d48-wg876 started at 2020-10-29 17:48:52 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container etcd-operator ready: true, restart count 0
Oct 29 21:21:00.633: INFO: apiserver-566bbccb57-tcdbq started at 2020-10-29 17:48:53 +0000 UTC (1+1 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Init container fix-audit-permissions ready: true, restart count 0
Oct 29 21:21:00.633: INFO: 	Container oauth-apiserver ready: true, restart count 0
Oct 29 21:21:00.633: INFO: machine-approver-5cd4c8656b-9hvtt started at 2020-10-29 17:48:55 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:00.633: INFO: 	Container machine-approver-controller ready: true, restart count 0
Oct 29 21:21:00.633: INFO: marketplace-operator-67d858bf7d-vpqvw started at 2020-10-29 17:49:02 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container marketplace-operator ready: true, restart count 0
Oct 29 21:21:00.633: INFO: installer-11-ip-10-0-245-254.us-east-2.compute.internal started at 2020-10-29 19:46:45 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:21:00.633: INFO: network-metrics-daemon-l8nn2 started at 2020-10-29 17:23:59 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:00.633: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Oct 29 21:21:00.633: INFO: controller-manager-ghvds started at 2020-10-29 17:30:59 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container controller-manager ready: true, restart count 0
Oct 29 21:21:00.633: INFO: openshift-config-operator-7c65574449-jccsp started at 2020-10-29 17:48:52 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container openshift-config-operator ready: true, restart count 0
Oct 29 21:21:00.633: INFO: aws-ebs-csi-driver-operator-579f547595-n74ks started at 2020-10-29 17:48:54 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container aws-ebs-csi-driver-operator ready: true, restart count 0
Oct 29 21:21:00.633: INFO: machine-api-operator-7654bc5777-6pzwq started at 2020-10-29 17:48:59 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:00.633: INFO: 	Container machine-api-operator ready: true, restart count 0
Oct 29 21:21:00.633: INFO: machine-config-operator-7c95b454cf-qk9cb started at 2020-10-29 17:49:01 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container machine-config-operator ready: true, restart count 0
Oct 29 21:21:00.633: INFO: revision-pruner-11-ip-10-0-245-254.us-east-2.compute.internal started at 2020-10-29 19:47:51 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:21:00.633: INFO: 	Container pruner ready: false, restart count 0
W1029 21:21:00.655398   53132 metrics_grabber.go:83] Can't find any pods in namespace kube-system to grab metrics from
W1029 21:21:00.655430   53132 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1029 21:21:00.655436   53132 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1029 21:21:00.655442   53132 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Oct 29 21:21:00.788: INFO: 
Latency metrics for node ip-10-0-245-254.us-east-2.compute.internal
Oct 29 21:21:00.788: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "sched-pred-1167" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

 Failure [603.221 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance] [It]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597

  Oct 29 21:20:59.218: Timed out after 10m0s waiting for stable cluster.

  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:55
------------------------------
{"msg":"FAILED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":19,"completed":1,"skipped":882,"failed":1,"failures":["[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:21:00.843: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should retry creating failed daemon pods [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Oct 29 21:21:01.200: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:21:01.201: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:21:01.201: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:21:01.222: INFO: Number of nodes with available pods: 0
Oct 29 21:21:01.222: INFO: Node ip-10-0-142-212.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:21:02.286: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:21:02.286: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:21:02.286: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:21:02.308: INFO: Number of nodes with available pods: 0
Oct 29 21:21:02.308: INFO: Node ip-10-0-142-212.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:21:03.304: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:21:03.304: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:21:03.304: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:21:03.326: INFO: Number of nodes with available pods: 0
Oct 29 21:21:03.326: INFO: Node ip-10-0-142-212.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:21:04.304: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:21:04.304: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:21:04.304: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:21:04.326: INFO: Number of nodes with available pods: 2
Oct 29 21:21:04.327: INFO: Node ip-10-0-142-212.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:21:05.303: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:21:05.303: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:21:05.304: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:21:05.325: INFO: Number of nodes with available pods: 4
Oct 29 21:21:05.325: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Oct 29 21:21:05.433: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:21:05.433: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:21:05.433: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:21:05.456: INFO: Number of nodes with available pods: 3
Oct 29 21:21:05.456: INFO: Node ip-10-0-158-72.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:21:06.577: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:21:06.577: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:21:06.577: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:21:06.602: INFO: Number of nodes with available pods: 3
Oct 29 21:21:06.602: INFO: Node ip-10-0-158-72.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:21:07.538: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:21:07.538: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:21:07.538: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:21:07.560: INFO: Number of nodes with available pods: 3
Oct 29 21:21:07.560: INFO: Node ip-10-0-158-72.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:21:08.538: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:21:08.538: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:21:08.538: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:21:08.559: INFO: Number of nodes with available pods: 4
Oct 29 21:21:08.559: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-311, will wait for the garbage collector to delete the pods
Oct 29 21:21:08.699: INFO: Deleting DaemonSet.extensions daemon-set took: 24.640483ms
Oct 29 21:21:08.799: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.422634ms
Oct 29 21:21:21.422: INFO: Number of nodes with available pods: 0
Oct 29 21:21:21.422: INFO: Number of running nodes: 0, number of available pods: 0
Oct 29 21:21:21.445: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-311/daemonsets","resourceVersion":"205082"},"items":null}

Oct 29 21:21:21.467: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-311/pods","resourceVersion":"205082"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:21:21.628: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "daemonsets-311" for this suite.

 [SLOW TEST:20.853 seconds]
[sig-apps] Daemon set [Serial]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":19,"completed":2,"skipped":1095,"failed":1,"failures":["[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:21:21.707: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should rollback without unnecessary restarts [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 21:21:22.064: INFO: Create a RollingUpdate DaemonSet
Oct 29 21:21:22.088: INFO: Check that daemon pods launch on every node of the cluster
Oct 29 21:21:22.117: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:21:22.117: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:21:22.117: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:21:22.163: INFO: Number of nodes with available pods: 0
Oct 29 21:21:22.163: INFO: Node ip-10-0-142-212.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:21:23.245: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:21:23.245: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:21:23.245: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:21:23.267: INFO: Number of nodes with available pods: 0
Oct 29 21:21:23.267: INFO: Node ip-10-0-142-212.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:21:24.246: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:21:24.246: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:21:24.246: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:21:24.269: INFO: Number of nodes with available pods: 0
Oct 29 21:21:24.269: INFO: Node ip-10-0-142-212.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:21:25.245: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:21:25.246: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:21:25.246: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:21:25.273: INFO: Number of nodes with available pods: 2
Oct 29 21:21:25.273: INFO: Node ip-10-0-142-212.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:21:26.245: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:21:26.245: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:21:26.245: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:21:26.267: INFO: Number of nodes with available pods: 4
Oct 29 21:21:26.267: INFO: Number of running nodes: 4, number of available pods: 4
Oct 29 21:21:26.267: INFO: Update the DaemonSet to trigger a rollout
Oct 29 21:21:26.332: INFO: Updating DaemonSet daemon-set
Oct 29 21:21:32.461: INFO: Roll back the DaemonSet before rollout is complete
Oct 29 21:21:32.507: INFO: Updating DaemonSet daemon-set
Oct 29 21:21:32.507: INFO: Make sure DaemonSet rollback is complete
Oct 29 21:21:32.548: INFO: Wrong image for pod: daemon-set-v7bcm. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Oct 29 21:21:32.548: INFO: Pod daemon-set-v7bcm is not available
Oct 29 21:21:32.590: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:21:32.590: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:21:32.590: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:21:33.631: INFO: Pod daemon-set-tdl7t is not available
Oct 29 21:21:33.693: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:21:33.693: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:21:33.693: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4677, will wait for the garbage collector to delete the pods
Oct 29 21:21:33.831: INFO: Deleting DaemonSet.extensions daemon-set took: 23.843402ms
Oct 29 21:21:33.931: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.211353ms
Oct 29 21:21:47.153: INFO: Number of nodes with available pods: 0
Oct 29 21:21:47.153: INFO: Number of running nodes: 0, number of available pods: 0
Oct 29 21:21:47.174: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4677/daemonsets","resourceVersion":"205431"},"items":null}

Oct 29 21:21:47.196: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4677/pods","resourceVersion":"205431"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:21:47.343: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "daemonsets-4677" for this suite.

 [SLOW TEST:25.701 seconds]
[sig-apps] Daemon set [Serial]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":19,"completed":3,"skipped":1586,"failed":1,"failures":["[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:21:47.438: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Oct 29 21:21:47.567: INFO: Waiting up to 1m0s for all (but 3) nodes to be ready
Oct 29 21:21:47.617: INFO: Waiting for terminating namespaces to be deleted...
Oct 29 21:21:47.642: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-142-212.us-east-2.compute.internal before test
Oct 29 21:21:47.694: INFO: aws-ebs-csi-driver-node-5gmf4 from openshift-cluster-csi-drivers started at 2020-10-29 17:30:44 +0000 UTC (3 container statuses recorded)
Oct 29 21:21:47.694: INFO: 	Container csi-driver ready: true, restart count 0
Oct 29 21:21:47.694: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Oct 29 21:21:47.694: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Oct 29 21:21:47.694: INFO: tuned-d6fwx from openshift-cluster-node-tuning-operator started at 2020-10-29 17:30:44 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.694: INFO: 	Container tuned ready: true, restart count 0
Oct 29 21:21:47.694: INFO: dns-default-b89gt from openshift-dns started at 2020-10-29 17:30:44 +0000 UTC (3 container statuses recorded)
Oct 29 21:21:47.694: INFO: 	Container dns ready: true, restart count 0
Oct 29 21:21:47.694: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 29 21:21:47.694: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:47.694: INFO: node-ca-rq9vr from openshift-image-registry started at 2020-10-29 17:30:44 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.694: INFO: 	Container node-ca ready: true, restart count 0
Oct 29 21:21:47.694: INFO: machine-config-daemon-l4r47 from openshift-machine-config-operator started at 2020-10-29 17:30:44 +0000 UTC (2 container statuses recorded)
Oct 29 21:21:47.694: INFO: 	Container machine-config-daemon ready: true, restart count 0
Oct 29 21:21:47.694: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:21:47.694: INFO: osd-patch-subscription-source-1604005200-ccz9j from openshift-marketplace started at 2020-10-29 21:00:09 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.694: INFO: 	Container osd-patch-subscription-source ready: false, restart count 0
Oct 29 21:21:47.694: INFO: node-exporter-p2phw from openshift-monitoring started at 2020-10-29 17:30:44 +0000 UTC (2 container statuses recorded)
Oct 29 21:21:47.694: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:47.694: INFO: 	Container node-exporter ready: true, restart count 0
Oct 29 21:21:47.694: INFO: sre-dns-latency-exporter-29j4j from openshift-monitoring started at 2020-10-29 20:32:47 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.694: INFO: 	Container main ready: true, restart count 0
Oct 29 21:21:47.694: INFO: multus-jst6k from openshift-multus started at 2020-10-29 17:30:43 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.694: INFO: 	Container kube-multus ready: true, restart count 0
Oct 29 21:21:47.694: INFO: network-metrics-daemon-xrvxh from openshift-multus started at 2020-10-29 17:30:44 +0000 UTC (2 container statuses recorded)
Oct 29 21:21:47.694: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:47.694: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Oct 29 21:21:47.694: INFO: ovs-66jns from openshift-sdn started at 2020-10-29 17:30:43 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.694: INFO: 	Container openvswitch ready: true, restart count 0
Oct 29 21:21:47.694: INFO: sdn-lbx27 from openshift-sdn started at 2020-10-29 17:30:44 +0000 UTC (2 container statuses recorded)
Oct 29 21:21:47.694: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Oct 29 21:21:47.694: INFO: 	Container sdn ready: true, restart count 0
Oct 29 21:21:47.694: INFO: alert-pruner-1604005200-gtct6 from openshift-sre-pruning started at 2020-10-29 21:00:09 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.694: INFO: 	Container alert-pruner ready: false, restart count 0
Oct 29 21:21:47.694: INFO: builds-pruner-1604005200-8wspq from openshift-sre-pruning started at 2020-10-29 21:00:09 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.694: INFO: 	Container builds-pruner ready: false, restart count 0
Oct 29 21:21:47.694: INFO: crd-pruner-1604005200-w76df from openshift-sre-pruning started at 2020-10-29 21:00:09 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.694: INFO: 	Container crd-pruner ready: false, restart count 0
Oct 29 21:21:47.694: INFO: image-pruner-1604005200-bp5kw from openshift-sre-pruning started at 2020-10-29 21:00:09 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.694: INFO: 	Container image-pruner ready: false, restart count 0
Oct 29 21:21:47.694: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-157-225.us-east-2.compute.internal before test
Oct 29 21:21:47.747: INFO: aws-ebs-csi-driver-node-qvwrc from openshift-cluster-csi-drivers started at 2020-10-29 17:41:11 +0000 UTC (3 container statuses recorded)
Oct 29 21:21:47.747: INFO: 	Container csi-driver ready: true, restart count 0
Oct 29 21:21:47.747: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Oct 29 21:21:47.747: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Oct 29 21:21:47.747: INFO: tuned-4tm5g from openshift-cluster-node-tuning-operator started at 2020-10-29 17:41:11 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.747: INFO: 	Container tuned ready: true, restart count 0
Oct 29 21:21:47.747: INFO: dns-default-hr5xr from openshift-dns started at 2020-10-29 17:41:12 +0000 UTC (3 container statuses recorded)
Oct 29 21:21:47.747: INFO: 	Container dns ready: true, restart count 0
Oct 29 21:21:47.747: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 29 21:21:47.747: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:47.747: INFO: image-registry-75b75c555d-cm6gf from openshift-image-registry started at 2020-10-29 17:48:47 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.747: INFO: 	Container registry ready: true, restart count 0
Oct 29 21:21:47.747: INFO: image-registry-75b75c555d-ksztb from openshift-image-registry started at 2020-10-29 17:48:47 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.747: INFO: 	Container registry ready: true, restart count 0
Oct 29 21:21:47.747: INFO: node-ca-6tz25 from openshift-image-registry started at 2020-10-29 17:41:11 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.747: INFO: 	Container node-ca ready: true, restart count 0
Oct 29 21:21:47.747: INFO: router-default-5f9d96945d-zxzg4 from openshift-ingress started at 2020-10-29 19:45:00 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.747: INFO: 	Container router ready: true, restart count 0
Oct 29 21:21:47.747: INFO: machine-config-daemon-mjcnt from openshift-machine-config-operator started at 2020-10-29 17:41:11 +0000 UTC (2 container statuses recorded)
Oct 29 21:21:47.747: INFO: 	Container machine-config-daemon ready: true, restart count 0
Oct 29 21:21:47.747: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:21:47.747: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-10-29 17:49:01 +0000 UTC (5 container statuses recorded)
Oct 29 21:21:47.747: INFO: 	Container alertmanager ready: true, restart count 0
Oct 29 21:21:47.747: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Oct 29 21:21:47.747: INFO: 	Container config-reloader ready: true, restart count 0
Oct 29 21:21:47.747: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:47.747: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:21:47.747: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-10-29 17:49:02 +0000 UTC (5 container statuses recorded)
Oct 29 21:21:47.747: INFO: 	Container alertmanager ready: true, restart count 0
Oct 29 21:21:47.747: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Oct 29 21:21:47.747: INFO: 	Container config-reloader ready: true, restart count 0
Oct 29 21:21:47.747: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:47.747: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:21:47.747: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-10-29 17:49:10 +0000 UTC (5 container statuses recorded)
Oct 29 21:21:47.747: INFO: 	Container alertmanager ready: true, restart count 0
Oct 29 21:21:47.747: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Oct 29 21:21:47.747: INFO: 	Container config-reloader ready: true, restart count 0
Oct 29 21:21:47.747: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:47.747: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:21:47.747: INFO: grafana-5cfb6b5fbb-gjr2l from openshift-monitoring started at 2020-10-29 17:48:48 +0000 UTC (2 container statuses recorded)
Oct 29 21:21:47.747: INFO: 	Container grafana ready: true, restart count 0
Oct 29 21:21:47.747: INFO: 	Container grafana-proxy ready: true, restart count 0
Oct 29 21:21:47.747: INFO: kube-state-metrics-596748788d-6mblj from openshift-monitoring started at 2020-10-29 17:48:48 +0000 UTC (3 container statuses recorded)
Oct 29 21:21:47.747: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Oct 29 21:21:47.747: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Oct 29 21:21:47.747: INFO: 	Container kube-state-metrics ready: true, restart count 0
Oct 29 21:21:47.747: INFO: node-exporter-d8llk from openshift-monitoring started at 2020-10-29 17:41:13 +0000 UTC (2 container statuses recorded)
Oct 29 21:21:47.747: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:47.747: INFO: 	Container node-exporter ready: true, restart count 0
Oct 29 21:21:47.747: INFO: prometheus-adapter-6495ff5885-lrx64 from openshift-monitoring started at 2020-10-29 17:48:47 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.747: INFO: 	Container prometheus-adapter ready: true, restart count 0
Oct 29 21:21:47.747: INFO: prometheus-adapter-6495ff5885-lw9vm from openshift-monitoring started at 2020-10-29 17:48:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.747: INFO: 	Container prometheus-adapter ready: true, restart count 0
Oct 29 21:21:47.747: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-10-29 17:48:53 +0000 UTC (6 container statuses recorded)
Oct 29 21:21:47.747: INFO: 	Container config-reloader ready: true, restart count 0
Oct 29 21:21:47.747: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:47.747: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:21:47.747: INFO: 	Container prometheus ready: true, restart count 1
Oct 29 21:21:47.747: INFO: 	Container prometheus-proxy ready: true, restart count 0
Oct 29 21:21:47.747: INFO: 	Container thanos-sidecar ready: true, restart count 0
Oct 29 21:21:47.747: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-10-29 17:49:00 +0000 UTC (6 container statuses recorded)
Oct 29 21:21:47.747: INFO: 	Container config-reloader ready: true, restart count 0
Oct 29 21:21:47.747: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:47.747: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:21:47.747: INFO: 	Container prometheus ready: true, restart count 1
Oct 29 21:21:47.747: INFO: 	Container prometheus-proxy ready: true, restart count 0
Oct 29 21:21:47.747: INFO: 	Container thanos-sidecar ready: true, restart count 0
Oct 29 21:21:47.747: INFO: prometheus-operator-89f4b9855-wlkt2 from openshift-monitoring started at 2020-10-29 17:48:47 +0000 UTC (2 container statuses recorded)
Oct 29 21:21:47.747: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:47.747: INFO: 	Container prometheus-operator ready: true, restart count 0
Oct 29 21:21:47.747: INFO: sre-dns-latency-exporter-l58wm from openshift-monitoring started at 2020-10-29 17:41:56 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.747: INFO: 	Container main ready: true, restart count 0
Oct 29 21:21:47.747: INFO: sre-stuck-ebs-vols-1-98w59 from openshift-monitoring started at 2020-10-29 17:48:47 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.747: INFO: 	Container main ready: true, restart count 0
Oct 29 21:21:47.747: INFO: telemeter-client-577bcc4f8d-jv4m8 from openshift-monitoring started at 2020-10-29 17:48:47 +0000 UTC (3 container statuses recorded)
Oct 29 21:21:47.747: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:47.747: INFO: 	Container reload ready: true, restart count 0
Oct 29 21:21:47.747: INFO: 	Container telemeter-client ready: true, restart count 0
Oct 29 21:21:47.747: INFO: multus-str5z from openshift-multus started at 2020-10-29 17:41:12 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.747: INFO: 	Container kube-multus ready: true, restart count 0
Oct 29 21:21:47.747: INFO: network-metrics-daemon-gcjdz from openshift-multus started at 2020-10-29 17:41:13 +0000 UTC (2 container statuses recorded)
Oct 29 21:21:47.747: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:47.747: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Oct 29 21:21:47.747: INFO: ovs-99tdx from openshift-sdn started at 2020-10-29 17:41:12 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.747: INFO: 	Container openvswitch ready: true, restart count 0
Oct 29 21:21:47.747: INFO: sdn-cp5t4 from openshift-sdn started at 2020-10-29 17:41:12 +0000 UTC (2 container statuses recorded)
Oct 29 21:21:47.747: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Oct 29 21:21:47.747: INFO: 	Container sdn ready: true, restart count 0
Oct 29 21:21:47.747: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-158-72.us-east-2.compute.internal before test
Oct 29 21:21:47.789: INFO: sre-build-test-1603998660-hmrqr from openshift-build-test started at 2020-10-29 19:11:03 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.789: INFO: 	Container sre-build-test ready: false, restart count 0
Oct 29 21:21:47.789: INFO: sre-build-test-1604002260-284ls from openshift-build-test started at 2020-10-29 20:11:07 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.789: INFO: 	Container sre-build-test ready: false, restart count 0
Oct 29 21:21:47.789: INFO: sre-build-test-1604005860-qkzkn from openshift-build-test started at 2020-10-29 21:11:04 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.789: INFO: 	Container sre-build-test ready: false, restart count 0
Oct 29 21:21:47.789: INFO: cloud-ingress-operator-798549d69-9fh9n from openshift-cloud-ingress-operator started at 2020-10-29 19:28:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.789: INFO: 	Container cloud-ingress-operator ready: true, restart count 0
Oct 29 21:21:47.789: INFO: aws-ebs-csi-driver-node-zfr5l from openshift-cluster-csi-drivers started at 2020-10-29 17:42:13 +0000 UTC (3 container statuses recorded)
Oct 29 21:21:47.789: INFO: 	Container csi-driver ready: true, restart count 0
Oct 29 21:21:47.789: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Oct 29 21:21:47.789: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Oct 29 21:21:47.789: INFO: tuned-jwndg from openshift-cluster-node-tuning-operator started at 2020-10-29 17:42:13 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.789: INFO: 	Container tuned ready: true, restart count 0
Oct 29 21:21:47.789: INFO: dns-default-25bcd from openshift-dns started at 2020-10-29 17:42:13 +0000 UTC (3 container statuses recorded)
Oct 29 21:21:47.789: INFO: 	Container dns ready: true, restart count 0
Oct 29 21:21:47.789: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 29 21:21:47.789: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:47.789: INFO: node-ca-d275k from openshift-image-registry started at 2020-10-29 17:42:13 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.789: INFO: 	Container node-ca ready: true, restart count 0
Oct 29 21:21:47.789: INFO: router-default-5f9d96945d-ckql4 from openshift-ingress started at 2020-10-29 19:44:59 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.789: INFO: 	Container router ready: true, restart count 0
Oct 29 21:21:47.789: INFO: machine-config-daemon-6987z from openshift-machine-config-operator started at 2020-10-29 17:42:13 +0000 UTC (2 container statuses recorded)
Oct 29 21:21:47.789: INFO: 	Container machine-config-daemon ready: true, restart count 0
Oct 29 21:21:47.789: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:21:47.789: INFO: node-exporter-pdd9z from openshift-monitoring started at 2020-10-29 17:42:13 +0000 UTC (2 container statuses recorded)
Oct 29 21:21:47.789: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:47.789: INFO: 	Container node-exporter ready: true, restart count 0
Oct 29 21:21:47.789: INFO: sre-dns-latency-exporter-krp25 from openshift-monitoring started at 2020-10-29 17:42:51 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.789: INFO: 	Container main ready: true, restart count 0
Oct 29 21:21:47.789: INFO: sre-ebs-iops-reporter-1-rrlxd from openshift-monitoring started at 2020-10-29 19:28:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.789: INFO: 	Container main ready: true, restart count 0
Oct 29 21:21:47.789: INFO: thanos-querier-7f688468b8-6jh8m from openshift-monitoring started at 2020-10-29 19:28:48 +0000 UTC (5 container statuses recorded)
Oct 29 21:21:47.789: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:47.789: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Oct 29 21:21:47.789: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:21:47.789: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:21:47.789: INFO: 	Container thanos-query ready: true, restart count 0
Oct 29 21:21:47.789: INFO: multus-l5sdf from openshift-multus started at 2020-10-29 17:42:13 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.789: INFO: 	Container kube-multus ready: true, restart count 0
Oct 29 21:21:47.789: INFO: network-metrics-daemon-nb7sg from openshift-multus started at 2020-10-29 17:42:13 +0000 UTC (2 container statuses recorded)
Oct 29 21:21:47.789: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:47.789: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Oct 29 21:21:47.789: INFO: ovs-9ct7r from openshift-sdn started at 2020-10-29 17:42:13 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.789: INFO: 	Container openvswitch ready: true, restart count 0
Oct 29 21:21:47.789: INFO: sdn-8kf7x from openshift-sdn started at 2020-10-29 17:42:13 +0000 UTC (2 container statuses recorded)
Oct 29 21:21:47.789: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Oct 29 21:21:47.789: INFO: 	Container sdn ready: true, restart count 0
Oct 29 21:21:47.789: INFO: splunk-forwarder-operator-6c55bb877-mw9lq from openshift-splunk-forwarder-operator started at 2020-10-29 19:28:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.789: INFO: 	Container splunk-forwarder-operator ready: true, restart count 0
Oct 29 21:21:47.789: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-234-238.us-east-2.compute.internal before test
Oct 29 21:21:47.834: INFO: cloud-ingress-operator-registry-zpdvx from openshift-cloud-ingress-operator started at 2020-10-29 19:32:07 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.834: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:21:47.834: INFO: aws-ebs-csi-driver-node-r7m6f from openshift-cluster-csi-drivers started at 2020-10-29 17:30:47 +0000 UTC (3 container statuses recorded)
Oct 29 21:21:47.834: INFO: 	Container csi-driver ready: true, restart count 0
Oct 29 21:21:47.834: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Oct 29 21:21:47.834: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Oct 29 21:21:47.834: INFO: tuned-6rs49 from openshift-cluster-node-tuning-operator started at 2020-10-29 17:30:47 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.834: INFO: 	Container tuned ready: true, restart count 0
Oct 29 21:21:47.834: INFO: downloads-85df645c7c-ft27h from openshift-console started at 2020-10-29 19:31:54 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.834: INFO: 	Container download-server ready: true, restart count 0
Oct 29 21:21:47.834: INFO: downloads-85df645c7c-pxr2l from openshift-console started at 2020-10-29 19:28:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.834: INFO: 	Container download-server ready: true, restart count 0
Oct 29 21:21:47.834: INFO: dns-default-jxrwk from openshift-dns started at 2020-10-29 17:30:47 +0000 UTC (3 container statuses recorded)
Oct 29 21:21:47.834: INFO: 	Container dns ready: true, restart count 0
Oct 29 21:21:47.834: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 29 21:21:47.834: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:47.834: INFO: node-ca-wxm2h from openshift-image-registry started at 2020-10-29 17:30:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.834: INFO: 	Container node-ca ready: true, restart count 0
Oct 29 21:21:47.834: INFO: migrator-b8d88f977-j4wqg from openshift-kube-storage-version-migrator started at 2020-10-29 19:28:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.834: INFO: 	Container migrator ready: true, restart count 0
Oct 29 21:21:47.834: INFO: machine-config-daemon-5rrfn from openshift-machine-config-operator started at 2020-10-29 17:30:47 +0000 UTC (2 container statuses recorded)
Oct 29 21:21:47.834: INFO: 	Container machine-config-daemon ready: true, restart count 0
Oct 29 21:21:47.834: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:21:47.834: INFO: managed-upgrade-operator-7558857f4-cxqkg from openshift-managed-upgrade-operator started at 2020-10-29 17:48:47 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.834: INFO: 	Container managed-upgrade-operator ready: true, restart count 0
Oct 29 21:21:47.834: INFO: managed-upgrade-operator-catalog-g4m2g from openshift-managed-upgrade-operator started at 2020-10-29 19:32:11 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.834: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:21:47.834: INFO: certified-operators-pjgrs from openshift-marketplace started at 2020-10-29 19:32:07 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.834: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:21:47.834: INFO: community-operators-rss65 from openshift-marketplace started at 2020-10-29 19:32:06 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.834: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:21:47.834: INFO: redhat-marketplace-cwfzm from openshift-marketplace started at 2020-10-29 19:32:08 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.834: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:21:47.834: INFO: redhat-operators-pjjcr from openshift-marketplace started at 2020-10-29 19:32:11 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.834: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:21:47.834: INFO: configure-alertmanager-operator-579dfffff7-nmj5v from openshift-monitoring started at 2020-10-29 17:48:47 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.834: INFO: 	Container configure-alertmanager-operator ready: true, restart count 0
Oct 29 21:21:47.834: INFO: configure-alertmanager-operator-registry-zcvq5 from openshift-monitoring started at 2020-10-29 19:32:07 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.834: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:21:47.834: INFO: node-exporter-lmwzp from openshift-monitoring started at 2020-10-29 17:30:48 +0000 UTC (2 container statuses recorded)
Oct 29 21:21:47.834: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:47.834: INFO: 	Container node-exporter ready: true, restart count 0
Oct 29 21:21:47.834: INFO: openshift-state-metrics-74d99f5845-62ghm from openshift-monitoring started at 2020-10-29 19:31:54 +0000 UTC (3 container statuses recorded)
Oct 29 21:21:47.834: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Oct 29 21:21:47.834: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Oct 29 21:21:47.834: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Oct 29 21:21:47.834: INFO: sre-dns-latency-exporter-5wrqd from openshift-monitoring started at 2020-10-29 17:39:27 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.834: INFO: 	Container main ready: true, restart count 0
Oct 29 21:21:47.834: INFO: thanos-querier-7f688468b8-cfd7g from openshift-monitoring started at 2020-10-29 17:48:47 +0000 UTC (5 container statuses recorded)
Oct 29 21:21:47.834: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:47.834: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Oct 29 21:21:47.834: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:21:47.834: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:21:47.834: INFO: 	Container thanos-query ready: true, restart count 0
Oct 29 21:21:47.834: INFO: multus-bcgtj from openshift-multus started at 2020-10-29 17:30:47 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.834: INFO: 	Container kube-multus ready: true, restart count 0
Oct 29 21:21:47.834: INFO: network-metrics-daemon-5brxv from openshift-multus started at 2020-10-29 17:30:48 +0000 UTC (2 container statuses recorded)
Oct 29 21:21:47.834: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:47.834: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Oct 29 21:21:47.834: INFO: rbac-permissions-operator-78d47d8556-92kbv from openshift-rbac-permissions started at 2020-10-29 17:48:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.834: INFO: 	Container rbac-permissions-operator ready: true, restart count 0
Oct 29 21:21:47.834: INFO: rbac-permissions-operator-registry-x892d from openshift-rbac-permissions started at 2020-10-29 19:32:08 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.834: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:21:47.834: INFO: ovs-9f9tm from openshift-sdn started at 2020-10-29 17:30:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.834: INFO: 	Container openvswitch ready: true, restart count 0
Oct 29 21:21:47.834: INFO: sdn-9tj9v from openshift-sdn started at 2020-10-29 17:30:48 +0000 UTC (2 container statuses recorded)
Oct 29 21:21:47.834: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Oct 29 21:21:47.834: INFO: 	Container sdn ready: true, restart count 0
Oct 29 21:21:47.834: INFO: splunk-forwarder-operator-catalog-jg48j from openshift-splunk-forwarder-operator started at 2020-10-29 19:32:10 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.834: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:21:47.834: INFO: alert-pruner-1603998000-fgw8n from openshift-sre-pruning started at 2020-10-29 19:00:09 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.834: INFO: 	Container alert-pruner ready: false, restart count 0
Oct 29 21:21:47.834: INFO: crd-pruner-1603998000-ljqfl from openshift-sre-pruning started at 2020-10-29 19:00:09 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.834: INFO: 	Container crd-pruner ready: false, restart count 0
Oct 29 21:21:47.834: INFO: deployments-pruner-1603998000-vfdcp from openshift-sre-pruning started at 2020-10-29 19:00:09 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.834: INFO: 	Container deployments-pruner ready: false, restart count 0
Oct 29 21:21:47.834: INFO: deployments-pruner-1604005200-dfg2v from openshift-sre-pruning started at 2020-10-29 21:00:09 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.834: INFO: 	Container deployments-pruner ready: false, restart count 0
Oct 29 21:21:47.834: INFO: managed-velero-operator-8685cc459-lsvfl from openshift-velero started at 2020-10-29 17:48:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.834: INFO: 	Container managed-velero-operator ready: true, restart count 0
Oct 29 21:21:47.834: INFO: managed-velero-operator-registry-9bghr from openshift-velero started at 2020-10-29 19:32:07 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.834: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:21:47.834: INFO: velero-68656b49f9-zvplm from openshift-velero started at 2020-10-29 17:46:41 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:47.834: INFO: 	Container velero ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-f094a593-3454-4e09-9259-9b29323797c6 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-f094a593-3454-4e09-9259-9b29323797c6 off the node ip-10-0-142-212.us-east-2.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-f094a593-3454-4e09-9259-9b29323797c6
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:21:56.236: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "sched-pred-866" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

 [SLOW TEST:8.865 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":19,"completed":4,"skipped":2262,"failed":1,"failures":["[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:21:56.317: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Oct 29 21:21:56.488: INFO: Waiting up to 1m0s for all (but 3) nodes to be ready
Oct 29 21:21:56.535: INFO: Waiting for terminating namespaces to be deleted...
Oct 29 21:21:56.564: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-142-212.us-east-2.compute.internal before test
Oct 29 21:21:56.616: INFO: aws-ebs-csi-driver-node-5gmf4 from openshift-cluster-csi-drivers started at 2020-10-29 17:30:44 +0000 UTC (3 container statuses recorded)
Oct 29 21:21:56.616: INFO: 	Container csi-driver ready: true, restart count 0
Oct 29 21:21:56.616: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Oct 29 21:21:56.616: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Oct 29 21:21:56.616: INFO: tuned-d6fwx from openshift-cluster-node-tuning-operator started at 2020-10-29 17:30:44 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.616: INFO: 	Container tuned ready: true, restart count 0
Oct 29 21:21:56.616: INFO: dns-default-b89gt from openshift-dns started at 2020-10-29 17:30:44 +0000 UTC (3 container statuses recorded)
Oct 29 21:21:56.616: INFO: 	Container dns ready: true, restart count 0
Oct 29 21:21:56.616: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 29 21:21:56.616: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:56.616: INFO: node-ca-rq9vr from openshift-image-registry started at 2020-10-29 17:30:44 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.616: INFO: 	Container node-ca ready: true, restart count 0
Oct 29 21:21:56.616: INFO: machine-config-daemon-l4r47 from openshift-machine-config-operator started at 2020-10-29 17:30:44 +0000 UTC (2 container statuses recorded)
Oct 29 21:21:56.616: INFO: 	Container machine-config-daemon ready: true, restart count 0
Oct 29 21:21:56.616: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:21:56.617: INFO: osd-patch-subscription-source-1604005200-ccz9j from openshift-marketplace started at 2020-10-29 21:00:09 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.617: INFO: 	Container osd-patch-subscription-source ready: false, restart count 0
Oct 29 21:21:56.617: INFO: node-exporter-p2phw from openshift-monitoring started at 2020-10-29 17:30:44 +0000 UTC (2 container statuses recorded)
Oct 29 21:21:56.617: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:56.617: INFO: 	Container node-exporter ready: true, restart count 0
Oct 29 21:21:56.617: INFO: sre-dns-latency-exporter-29j4j from openshift-monitoring started at 2020-10-29 20:32:47 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.617: INFO: 	Container main ready: true, restart count 0
Oct 29 21:21:56.617: INFO: multus-jst6k from openshift-multus started at 2020-10-29 17:30:43 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.617: INFO: 	Container kube-multus ready: true, restart count 0
Oct 29 21:21:56.617: INFO: network-metrics-daemon-xrvxh from openshift-multus started at 2020-10-29 17:30:44 +0000 UTC (2 container statuses recorded)
Oct 29 21:21:56.617: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:56.617: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Oct 29 21:21:56.617: INFO: ovs-66jns from openshift-sdn started at 2020-10-29 17:30:43 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.618: INFO: 	Container openvswitch ready: true, restart count 0
Oct 29 21:21:56.618: INFO: sdn-lbx27 from openshift-sdn started at 2020-10-29 17:30:44 +0000 UTC (2 container statuses recorded)
Oct 29 21:21:56.618: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Oct 29 21:21:56.618: INFO: 	Container sdn ready: true, restart count 0
Oct 29 21:21:56.618: INFO: alert-pruner-1604005200-gtct6 from openshift-sre-pruning started at 2020-10-29 21:00:09 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.618: INFO: 	Container alert-pruner ready: false, restart count 0
Oct 29 21:21:56.618: INFO: builds-pruner-1604005200-8wspq from openshift-sre-pruning started at 2020-10-29 21:00:09 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.618: INFO: 	Container builds-pruner ready: false, restart count 0
Oct 29 21:21:56.618: INFO: crd-pruner-1604005200-w76df from openshift-sre-pruning started at 2020-10-29 21:00:09 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.618: INFO: 	Container crd-pruner ready: false, restart count 0
Oct 29 21:21:56.618: INFO: image-pruner-1604005200-bp5kw from openshift-sre-pruning started at 2020-10-29 21:00:09 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.618: INFO: 	Container image-pruner ready: false, restart count 0
Oct 29 21:21:56.618: INFO: with-labels from sched-pred-866 started at 2020-10-29 21:21:32 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.618: INFO: 	Container with-labels ready: true, restart count 0
Oct 29 21:21:56.618: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-157-225.us-east-2.compute.internal before test
Oct 29 21:21:56.694: INFO: aws-ebs-csi-driver-node-qvwrc from openshift-cluster-csi-drivers started at 2020-10-29 17:41:11 +0000 UTC (3 container statuses recorded)
Oct 29 21:21:56.694: INFO: 	Container csi-driver ready: true, restart count 0
Oct 29 21:21:56.694: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Oct 29 21:21:56.694: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Oct 29 21:21:56.694: INFO: tuned-4tm5g from openshift-cluster-node-tuning-operator started at 2020-10-29 17:41:11 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.694: INFO: 	Container tuned ready: true, restart count 0
Oct 29 21:21:56.694: INFO: dns-default-hr5xr from openshift-dns started at 2020-10-29 17:41:12 +0000 UTC (3 container statuses recorded)
Oct 29 21:21:56.694: INFO: 	Container dns ready: true, restart count 0
Oct 29 21:21:56.694: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 29 21:21:56.694: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:56.694: INFO: image-registry-75b75c555d-cm6gf from openshift-image-registry started at 2020-10-29 17:48:47 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.694: INFO: 	Container registry ready: true, restart count 0
Oct 29 21:21:56.694: INFO: image-registry-75b75c555d-ksztb from openshift-image-registry started at 2020-10-29 17:48:47 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.694: INFO: 	Container registry ready: true, restart count 0
Oct 29 21:21:56.694: INFO: node-ca-6tz25 from openshift-image-registry started at 2020-10-29 17:41:11 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.694: INFO: 	Container node-ca ready: true, restart count 0
Oct 29 21:21:56.694: INFO: router-default-5f9d96945d-zxzg4 from openshift-ingress started at 2020-10-29 19:45:00 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.694: INFO: 	Container router ready: true, restart count 0
Oct 29 21:21:56.694: INFO: machine-config-daemon-mjcnt from openshift-machine-config-operator started at 2020-10-29 17:41:11 +0000 UTC (2 container statuses recorded)
Oct 29 21:21:56.694: INFO: 	Container machine-config-daemon ready: true, restart count 0
Oct 29 21:21:56.694: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:21:56.694: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-10-29 17:49:01 +0000 UTC (5 container statuses recorded)
Oct 29 21:21:56.694: INFO: 	Container alertmanager ready: true, restart count 0
Oct 29 21:21:56.694: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Oct 29 21:21:56.694: INFO: 	Container config-reloader ready: true, restart count 0
Oct 29 21:21:56.694: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:56.694: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:21:56.694: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-10-29 17:49:02 +0000 UTC (5 container statuses recorded)
Oct 29 21:21:56.694: INFO: 	Container alertmanager ready: true, restart count 0
Oct 29 21:21:56.694: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Oct 29 21:21:56.694: INFO: 	Container config-reloader ready: true, restart count 0
Oct 29 21:21:56.694: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:56.694: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:21:56.694: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-10-29 17:49:10 +0000 UTC (5 container statuses recorded)
Oct 29 21:21:56.694: INFO: 	Container alertmanager ready: true, restart count 0
Oct 29 21:21:56.694: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Oct 29 21:21:56.694: INFO: 	Container config-reloader ready: true, restart count 0
Oct 29 21:21:56.694: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:56.694: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:21:56.694: INFO: grafana-5cfb6b5fbb-gjr2l from openshift-monitoring started at 2020-10-29 17:48:48 +0000 UTC (2 container statuses recorded)
Oct 29 21:21:56.694: INFO: 	Container grafana ready: true, restart count 0
Oct 29 21:21:56.694: INFO: 	Container grafana-proxy ready: true, restart count 0
Oct 29 21:21:56.694: INFO: kube-state-metrics-596748788d-6mblj from openshift-monitoring started at 2020-10-29 17:48:48 +0000 UTC (3 container statuses recorded)
Oct 29 21:21:56.694: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Oct 29 21:21:56.694: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Oct 29 21:21:56.694: INFO: 	Container kube-state-metrics ready: true, restart count 0
Oct 29 21:21:56.694: INFO: node-exporter-d8llk from openshift-monitoring started at 2020-10-29 17:41:13 +0000 UTC (2 container statuses recorded)
Oct 29 21:21:56.694: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:56.694: INFO: 	Container node-exporter ready: true, restart count 0
Oct 29 21:21:56.694: INFO: prometheus-adapter-6495ff5885-lrx64 from openshift-monitoring started at 2020-10-29 17:48:47 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.694: INFO: 	Container prometheus-adapter ready: true, restart count 0
Oct 29 21:21:56.694: INFO: prometheus-adapter-6495ff5885-lw9vm from openshift-monitoring started at 2020-10-29 17:48:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.694: INFO: 	Container prometheus-adapter ready: true, restart count 0
Oct 29 21:21:56.694: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-10-29 17:48:53 +0000 UTC (6 container statuses recorded)
Oct 29 21:21:56.694: INFO: 	Container config-reloader ready: true, restart count 0
Oct 29 21:21:56.694: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:56.694: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:21:56.694: INFO: 	Container prometheus ready: true, restart count 1
Oct 29 21:21:56.694: INFO: 	Container prometheus-proxy ready: true, restart count 0
Oct 29 21:21:56.694: INFO: 	Container thanos-sidecar ready: true, restart count 0
Oct 29 21:21:56.694: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-10-29 17:49:00 +0000 UTC (6 container statuses recorded)
Oct 29 21:21:56.694: INFO: 	Container config-reloader ready: true, restart count 0
Oct 29 21:21:56.694: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:56.694: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:21:56.694: INFO: 	Container prometheus ready: true, restart count 1
Oct 29 21:21:56.694: INFO: 	Container prometheus-proxy ready: true, restart count 0
Oct 29 21:21:56.694: INFO: 	Container thanos-sidecar ready: true, restart count 0
Oct 29 21:21:56.694: INFO: prometheus-operator-89f4b9855-wlkt2 from openshift-monitoring started at 2020-10-29 17:48:47 +0000 UTC (2 container statuses recorded)
Oct 29 21:21:56.694: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:56.694: INFO: 	Container prometheus-operator ready: true, restart count 0
Oct 29 21:21:56.694: INFO: sre-dns-latency-exporter-l58wm from openshift-monitoring started at 2020-10-29 17:41:56 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.694: INFO: 	Container main ready: true, restart count 0
Oct 29 21:21:56.694: INFO: sre-stuck-ebs-vols-1-98w59 from openshift-monitoring started at 2020-10-29 17:48:47 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.694: INFO: 	Container main ready: true, restart count 0
Oct 29 21:21:56.694: INFO: telemeter-client-577bcc4f8d-jv4m8 from openshift-monitoring started at 2020-10-29 17:48:47 +0000 UTC (3 container statuses recorded)
Oct 29 21:21:56.694: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:56.694: INFO: 	Container reload ready: true, restart count 0
Oct 29 21:21:56.694: INFO: 	Container telemeter-client ready: true, restart count 0
Oct 29 21:21:56.694: INFO: multus-str5z from openshift-multus started at 2020-10-29 17:41:12 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.694: INFO: 	Container kube-multus ready: true, restart count 0
Oct 29 21:21:56.694: INFO: network-metrics-daemon-gcjdz from openshift-multus started at 2020-10-29 17:41:13 +0000 UTC (2 container statuses recorded)
Oct 29 21:21:56.694: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:56.694: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Oct 29 21:21:56.694: INFO: ovs-99tdx from openshift-sdn started at 2020-10-29 17:41:12 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.694: INFO: 	Container openvswitch ready: true, restart count 0
Oct 29 21:21:56.694: INFO: sdn-cp5t4 from openshift-sdn started at 2020-10-29 17:41:12 +0000 UTC (2 container statuses recorded)
Oct 29 21:21:56.694: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Oct 29 21:21:56.694: INFO: 	Container sdn ready: true, restart count 0
Oct 29 21:21:56.694: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-158-72.us-east-2.compute.internal before test
Oct 29 21:21:56.756: INFO: sre-build-test-1603998660-hmrqr from openshift-build-test started at 2020-10-29 19:11:03 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.756: INFO: 	Container sre-build-test ready: false, restart count 0
Oct 29 21:21:56.756: INFO: sre-build-test-1604002260-284ls from openshift-build-test started at 2020-10-29 20:11:07 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.756: INFO: 	Container sre-build-test ready: false, restart count 0
Oct 29 21:21:56.756: INFO: sre-build-test-1604005860-qkzkn from openshift-build-test started at 2020-10-29 21:11:04 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.756: INFO: 	Container sre-build-test ready: false, restart count 0
Oct 29 21:21:56.756: INFO: cloud-ingress-operator-798549d69-9fh9n from openshift-cloud-ingress-operator started at 2020-10-29 19:28:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.756: INFO: 	Container cloud-ingress-operator ready: true, restart count 0
Oct 29 21:21:56.756: INFO: aws-ebs-csi-driver-node-zfr5l from openshift-cluster-csi-drivers started at 2020-10-29 17:42:13 +0000 UTC (3 container statuses recorded)
Oct 29 21:21:56.756: INFO: 	Container csi-driver ready: true, restart count 0
Oct 29 21:21:56.756: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Oct 29 21:21:56.756: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Oct 29 21:21:56.756: INFO: tuned-jwndg from openshift-cluster-node-tuning-operator started at 2020-10-29 17:42:13 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.756: INFO: 	Container tuned ready: true, restart count 0
Oct 29 21:21:56.756: INFO: dns-default-25bcd from openshift-dns started at 2020-10-29 17:42:13 +0000 UTC (3 container statuses recorded)
Oct 29 21:21:56.756: INFO: 	Container dns ready: true, restart count 0
Oct 29 21:21:56.756: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 29 21:21:56.756: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:56.756: INFO: node-ca-d275k from openshift-image-registry started at 2020-10-29 17:42:13 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.756: INFO: 	Container node-ca ready: true, restart count 0
Oct 29 21:21:56.756: INFO: router-default-5f9d96945d-ckql4 from openshift-ingress started at 2020-10-29 19:44:59 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.756: INFO: 	Container router ready: true, restart count 0
Oct 29 21:21:56.756: INFO: machine-config-daemon-6987z from openshift-machine-config-operator started at 2020-10-29 17:42:13 +0000 UTC (2 container statuses recorded)
Oct 29 21:21:56.756: INFO: 	Container machine-config-daemon ready: true, restart count 0
Oct 29 21:21:56.756: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:21:56.756: INFO: node-exporter-pdd9z from openshift-monitoring started at 2020-10-29 17:42:13 +0000 UTC (2 container statuses recorded)
Oct 29 21:21:56.756: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:56.756: INFO: 	Container node-exporter ready: true, restart count 0
Oct 29 21:21:56.756: INFO: sre-dns-latency-exporter-krp25 from openshift-monitoring started at 2020-10-29 17:42:51 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.756: INFO: 	Container main ready: true, restart count 0
Oct 29 21:21:56.756: INFO: sre-ebs-iops-reporter-1-rrlxd from openshift-monitoring started at 2020-10-29 19:28:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.756: INFO: 	Container main ready: true, restart count 0
Oct 29 21:21:56.756: INFO: thanos-querier-7f688468b8-6jh8m from openshift-monitoring started at 2020-10-29 19:28:48 +0000 UTC (5 container statuses recorded)
Oct 29 21:21:56.756: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:56.756: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Oct 29 21:21:56.756: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:21:56.756: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:21:56.756: INFO: 	Container thanos-query ready: true, restart count 0
Oct 29 21:21:56.756: INFO: multus-l5sdf from openshift-multus started at 2020-10-29 17:42:13 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.756: INFO: 	Container kube-multus ready: true, restart count 0
Oct 29 21:21:56.756: INFO: network-metrics-daemon-nb7sg from openshift-multus started at 2020-10-29 17:42:13 +0000 UTC (2 container statuses recorded)
Oct 29 21:21:56.756: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:56.756: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Oct 29 21:21:56.756: INFO: ovs-9ct7r from openshift-sdn started at 2020-10-29 17:42:13 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.756: INFO: 	Container openvswitch ready: true, restart count 0
Oct 29 21:21:56.756: INFO: sdn-8kf7x from openshift-sdn started at 2020-10-29 17:42:13 +0000 UTC (2 container statuses recorded)
Oct 29 21:21:56.756: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Oct 29 21:21:56.756: INFO: 	Container sdn ready: true, restart count 0
Oct 29 21:21:56.756: INFO: splunk-forwarder-operator-6c55bb877-mw9lq from openshift-splunk-forwarder-operator started at 2020-10-29 19:28:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.756: INFO: 	Container splunk-forwarder-operator ready: true, restart count 0
Oct 29 21:21:56.756: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-234-238.us-east-2.compute.internal before test
Oct 29 21:21:56.807: INFO: cloud-ingress-operator-registry-zpdvx from openshift-cloud-ingress-operator started at 2020-10-29 19:32:07 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.807: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:21:56.807: INFO: aws-ebs-csi-driver-node-r7m6f from openshift-cluster-csi-drivers started at 2020-10-29 17:30:47 +0000 UTC (3 container statuses recorded)
Oct 29 21:21:56.807: INFO: 	Container csi-driver ready: true, restart count 0
Oct 29 21:21:56.807: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Oct 29 21:21:56.807: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Oct 29 21:21:56.807: INFO: tuned-6rs49 from openshift-cluster-node-tuning-operator started at 2020-10-29 17:30:47 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.807: INFO: 	Container tuned ready: true, restart count 0
Oct 29 21:21:56.807: INFO: downloads-85df645c7c-ft27h from openshift-console started at 2020-10-29 19:31:54 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.807: INFO: 	Container download-server ready: true, restart count 0
Oct 29 21:21:56.807: INFO: downloads-85df645c7c-pxr2l from openshift-console started at 2020-10-29 19:28:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.807: INFO: 	Container download-server ready: true, restart count 0
Oct 29 21:21:56.807: INFO: dns-default-jxrwk from openshift-dns started at 2020-10-29 17:30:47 +0000 UTC (3 container statuses recorded)
Oct 29 21:21:56.807: INFO: 	Container dns ready: true, restart count 0
Oct 29 21:21:56.807: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 29 21:21:56.807: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:56.807: INFO: node-ca-wxm2h from openshift-image-registry started at 2020-10-29 17:30:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.807: INFO: 	Container node-ca ready: true, restart count 0
Oct 29 21:21:56.807: INFO: migrator-b8d88f977-j4wqg from openshift-kube-storage-version-migrator started at 2020-10-29 19:28:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.807: INFO: 	Container migrator ready: true, restart count 0
Oct 29 21:21:56.807: INFO: machine-config-daemon-5rrfn from openshift-machine-config-operator started at 2020-10-29 17:30:47 +0000 UTC (2 container statuses recorded)
Oct 29 21:21:56.807: INFO: 	Container machine-config-daemon ready: true, restart count 0
Oct 29 21:21:56.807: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:21:56.807: INFO: managed-upgrade-operator-7558857f4-cxqkg from openshift-managed-upgrade-operator started at 2020-10-29 17:48:47 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.807: INFO: 	Container managed-upgrade-operator ready: true, restart count 0
Oct 29 21:21:56.807: INFO: managed-upgrade-operator-catalog-g4m2g from openshift-managed-upgrade-operator started at 2020-10-29 19:32:11 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.807: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:21:56.807: INFO: certified-operators-pjgrs from openshift-marketplace started at 2020-10-29 19:32:07 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.807: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:21:56.807: INFO: community-operators-rss65 from openshift-marketplace started at 2020-10-29 19:32:06 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.807: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:21:56.807: INFO: redhat-marketplace-cwfzm from openshift-marketplace started at 2020-10-29 19:32:08 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.807: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:21:56.807: INFO: redhat-operators-pjjcr from openshift-marketplace started at 2020-10-29 19:32:11 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.807: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:21:56.807: INFO: configure-alertmanager-operator-579dfffff7-nmj5v from openshift-monitoring started at 2020-10-29 17:48:47 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.807: INFO: 	Container configure-alertmanager-operator ready: true, restart count 0
Oct 29 21:21:56.807: INFO: configure-alertmanager-operator-registry-zcvq5 from openshift-monitoring started at 2020-10-29 19:32:07 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.807: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:21:56.807: INFO: node-exporter-lmwzp from openshift-monitoring started at 2020-10-29 17:30:48 +0000 UTC (2 container statuses recorded)
Oct 29 21:21:56.807: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:56.807: INFO: 	Container node-exporter ready: true, restart count 0
Oct 29 21:21:56.807: INFO: openshift-state-metrics-74d99f5845-62ghm from openshift-monitoring started at 2020-10-29 19:31:54 +0000 UTC (3 container statuses recorded)
Oct 29 21:21:56.807: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Oct 29 21:21:56.807: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Oct 29 21:21:56.807: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Oct 29 21:21:56.807: INFO: sre-dns-latency-exporter-5wrqd from openshift-monitoring started at 2020-10-29 17:39:27 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.807: INFO: 	Container main ready: true, restart count 0
Oct 29 21:21:56.807: INFO: thanos-querier-7f688468b8-cfd7g from openshift-monitoring started at 2020-10-29 17:48:47 +0000 UTC (5 container statuses recorded)
Oct 29 21:21:56.807: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:56.807: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Oct 29 21:21:56.807: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:21:56.807: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:21:56.807: INFO: 	Container thanos-query ready: true, restart count 0
Oct 29 21:21:56.807: INFO: multus-bcgtj from openshift-multus started at 2020-10-29 17:30:47 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.807: INFO: 	Container kube-multus ready: true, restart count 0
Oct 29 21:21:56.807: INFO: network-metrics-daemon-5brxv from openshift-multus started at 2020-10-29 17:30:48 +0000 UTC (2 container statuses recorded)
Oct 29 21:21:56.807: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:21:56.807: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Oct 29 21:21:56.807: INFO: rbac-permissions-operator-78d47d8556-92kbv from openshift-rbac-permissions started at 2020-10-29 17:48:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.807: INFO: 	Container rbac-permissions-operator ready: true, restart count 0
Oct 29 21:21:56.807: INFO: rbac-permissions-operator-registry-x892d from openshift-rbac-permissions started at 2020-10-29 19:32:08 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.807: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:21:56.807: INFO: ovs-9f9tm from openshift-sdn started at 2020-10-29 17:30:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.807: INFO: 	Container openvswitch ready: true, restart count 0
Oct 29 21:21:56.808: INFO: sdn-9tj9v from openshift-sdn started at 2020-10-29 17:30:48 +0000 UTC (2 container statuses recorded)
Oct 29 21:21:56.808: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Oct 29 21:21:56.808: INFO: 	Container sdn ready: true, restart count 0
Oct 29 21:21:56.808: INFO: splunk-forwarder-operator-catalog-jg48j from openshift-splunk-forwarder-operator started at 2020-10-29 19:32:10 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.808: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:21:56.808: INFO: alert-pruner-1603998000-fgw8n from openshift-sre-pruning started at 2020-10-29 19:00:09 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.808: INFO: 	Container alert-pruner ready: false, restart count 0
Oct 29 21:21:56.808: INFO: crd-pruner-1603998000-ljqfl from openshift-sre-pruning started at 2020-10-29 19:00:09 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.808: INFO: 	Container crd-pruner ready: false, restart count 0
Oct 29 21:21:56.808: INFO: deployments-pruner-1603998000-vfdcp from openshift-sre-pruning started at 2020-10-29 19:00:09 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.808: INFO: 	Container deployments-pruner ready: false, restart count 0
Oct 29 21:21:56.808: INFO: deployments-pruner-1604005200-dfg2v from openshift-sre-pruning started at 2020-10-29 21:00:09 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.808: INFO: 	Container deployments-pruner ready: false, restart count 0
Oct 29 21:21:56.808: INFO: managed-velero-operator-8685cc459-lsvfl from openshift-velero started at 2020-10-29 17:48:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.808: INFO: 	Container managed-velero-operator ready: true, restart count 0
Oct 29 21:21:56.808: INFO: managed-velero-operator-registry-9bghr from openshift-velero started at 2020-10-29 19:32:07 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.808: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:21:56.808: INFO: velero-68656b49f9-zvplm from openshift-velero started at 2020-10-29 17:46:41 +0000 UTC (1 container statuses recorded)
Oct 29 21:21:56.808: INFO: 	Container velero ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-553e41d3-4e40-4f26-939a-383948cd79ba 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-553e41d3-4e40-4f26-939a-383948cd79ba off the node ip-10-0-142-212.us-east-2.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-553e41d3-4e40-4f26-939a-383948cd79ba
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:22:15.364: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "sched-pred-8220" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

 [SLOW TEST:19.114 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":19,"completed":5,"skipped":2553,"failed":1,"failures":["[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:22:15.434: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Oct 29 21:22:15.646: INFO: Waiting up to 1m0s for all nodes to be ready
Oct 29 21:23:16.017: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:23:16.041: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:487
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Oct 29 21:23:20.384: INFO: found a healthy node: ip-10-0-142-212.us-east-2.compute.internal
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 21:23:40.754: INFO: pods created so far: [1 1 1]
Oct 29 21:23:40.754: INFO: length of pods created so far: 3
Oct 29 21:23:48.805: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:23:55.806: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-9947" for this suite.
[AfterEach] PreemptionExecutionPath
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:461
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:23:56.043: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "sched-preemption-4720" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

 [SLOW TEST:101.002 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:450
    runs ReplicaSets to verify preemption running path [Conformance]
    /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":19,"completed":6,"skipped":2694,"failed":1,"failures":["[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:23:56.438: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Oct 29 21:23:56.619: INFO: Waiting up to 1m0s for all (but 3) nodes to be ready
Oct 29 21:23:56.702: INFO: Waiting for terminating namespaces to be deleted...
Oct 29 21:23:56.733: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-142-212.us-east-2.compute.internal before test
Oct 29 21:23:56.813: INFO: aws-ebs-csi-driver-node-5gmf4 from openshift-cluster-csi-drivers started at 2020-10-29 17:30:44 +0000 UTC (3 container statuses recorded)
Oct 29 21:23:56.813: INFO: 	Container csi-driver ready: true, restart count 0
Oct 29 21:23:56.813: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Oct 29 21:23:56.813: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Oct 29 21:23:56.813: INFO: tuned-d6fwx from openshift-cluster-node-tuning-operator started at 2020-10-29 17:30:44 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:56.813: INFO: 	Container tuned ready: true, restart count 0
Oct 29 21:23:56.813: INFO: dns-default-b89gt from openshift-dns started at 2020-10-29 17:30:44 +0000 UTC (3 container statuses recorded)
Oct 29 21:23:56.813: INFO: 	Container dns ready: true, restart count 0
Oct 29 21:23:56.813: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 29 21:23:56.813: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:23:56.813: INFO: node-ca-rq9vr from openshift-image-registry started at 2020-10-29 17:30:44 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:56.813: INFO: 	Container node-ca ready: true, restart count 0
Oct 29 21:23:56.813: INFO: machine-config-daemon-l4r47 from openshift-machine-config-operator started at 2020-10-29 17:30:44 +0000 UTC (2 container statuses recorded)
Oct 29 21:23:56.813: INFO: 	Container machine-config-daemon ready: true, restart count 0
Oct 29 21:23:56.813: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:23:56.813: INFO: osd-patch-subscription-source-1604005200-ccz9j from openshift-marketplace started at 2020-10-29 21:00:09 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:56.813: INFO: 	Container osd-patch-subscription-source ready: false, restart count 0
Oct 29 21:23:56.813: INFO: node-exporter-p2phw from openshift-monitoring started at 2020-10-29 17:30:44 +0000 UTC (2 container statuses recorded)
Oct 29 21:23:56.813: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:23:56.813: INFO: 	Container node-exporter ready: true, restart count 0
Oct 29 21:23:56.813: INFO: sre-dns-latency-exporter-29j4j from openshift-monitoring started at 2020-10-29 20:32:47 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:56.813: INFO: 	Container main ready: true, restart count 0
Oct 29 21:23:56.813: INFO: multus-jst6k from openshift-multus started at 2020-10-29 17:30:43 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:56.813: INFO: 	Container kube-multus ready: true, restart count 0
Oct 29 21:23:56.813: INFO: network-metrics-daemon-xrvxh from openshift-multus started at 2020-10-29 17:30:44 +0000 UTC (2 container statuses recorded)
Oct 29 21:23:56.813: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:23:56.813: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Oct 29 21:23:56.813: INFO: ovs-66jns from openshift-sdn started at 2020-10-29 17:30:43 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:56.813: INFO: 	Container openvswitch ready: true, restart count 0
Oct 29 21:23:56.813: INFO: sdn-lbx27 from openshift-sdn started at 2020-10-29 17:30:44 +0000 UTC (2 container statuses recorded)
Oct 29 21:23:56.813: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Oct 29 21:23:56.813: INFO: 	Container sdn ready: true, restart count 0
Oct 29 21:23:56.813: INFO: alert-pruner-1604005200-gtct6 from openshift-sre-pruning started at 2020-10-29 21:00:09 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:56.813: INFO: 	Container alert-pruner ready: false, restart count 0
Oct 29 21:23:56.813: INFO: builds-pruner-1604005200-8wspq from openshift-sre-pruning started at 2020-10-29 21:00:09 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:56.813: INFO: 	Container builds-pruner ready: false, restart count 0
Oct 29 21:23:56.813: INFO: crd-pruner-1604005200-w76df from openshift-sre-pruning started at 2020-10-29 21:00:09 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:56.813: INFO: 	Container crd-pruner ready: false, restart count 0
Oct 29 21:23:56.813: INFO: image-pruner-1604005200-bp5kw from openshift-sre-pruning started at 2020-10-29 21:00:09 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:56.813: INFO: 	Container image-pruner ready: false, restart count 0
Oct 29 21:23:56.813: INFO: pod4 from sched-preemption-path-9947 started at 2020-10-29 21:23:28 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:56.813: INFO: 	Container pod4 ready: true, restart count 0
Oct 29 21:23:56.813: INFO: rs-pod3-xr9br from sched-preemption-path-9947 started at 2020-10-29 21:23:17 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:56.813: INFO: 	Container pod3 ready: true, restart count 0
Oct 29 21:23:56.813: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-157-225.us-east-2.compute.internal before test
Oct 29 21:23:56.925: INFO: aws-ebs-csi-driver-node-qvwrc from openshift-cluster-csi-drivers started at 2020-10-29 17:41:11 +0000 UTC (3 container statuses recorded)
Oct 29 21:23:56.925: INFO: 	Container csi-driver ready: true, restart count 0
Oct 29 21:23:56.925: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Oct 29 21:23:56.925: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Oct 29 21:23:56.925: INFO: tuned-4tm5g from openshift-cluster-node-tuning-operator started at 2020-10-29 17:41:11 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:56.925: INFO: 	Container tuned ready: true, restart count 0
Oct 29 21:23:56.925: INFO: dns-default-hr5xr from openshift-dns started at 2020-10-29 17:41:12 +0000 UTC (3 container statuses recorded)
Oct 29 21:23:56.925: INFO: 	Container dns ready: true, restart count 0
Oct 29 21:23:56.925: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 29 21:23:56.925: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:23:56.925: INFO: image-registry-75b75c555d-cm6gf from openshift-image-registry started at 2020-10-29 17:48:47 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:56.925: INFO: 	Container registry ready: true, restart count 0
Oct 29 21:23:56.925: INFO: image-registry-75b75c555d-ksztb from openshift-image-registry started at 2020-10-29 17:48:47 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:56.925: INFO: 	Container registry ready: true, restart count 0
Oct 29 21:23:56.925: INFO: node-ca-6tz25 from openshift-image-registry started at 2020-10-29 17:41:11 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:56.925: INFO: 	Container node-ca ready: true, restart count 0
Oct 29 21:23:56.925: INFO: router-default-5f9d96945d-zxzg4 from openshift-ingress started at 2020-10-29 19:45:00 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:56.925: INFO: 	Container router ready: true, restart count 0
Oct 29 21:23:56.925: INFO: machine-config-daemon-mjcnt from openshift-machine-config-operator started at 2020-10-29 17:41:11 +0000 UTC (2 container statuses recorded)
Oct 29 21:23:56.925: INFO: 	Container machine-config-daemon ready: true, restart count 0
Oct 29 21:23:56.925: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:23:56.925: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-10-29 17:49:01 +0000 UTC (5 container statuses recorded)
Oct 29 21:23:56.925: INFO: 	Container alertmanager ready: true, restart count 0
Oct 29 21:23:56.925: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Oct 29 21:23:56.925: INFO: 	Container config-reloader ready: true, restart count 0
Oct 29 21:23:56.925: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:23:56.926: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:23:56.926: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-10-29 17:49:02 +0000 UTC (5 container statuses recorded)
Oct 29 21:23:56.926: INFO: 	Container alertmanager ready: true, restart count 0
Oct 29 21:23:56.926: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Oct 29 21:23:56.926: INFO: 	Container config-reloader ready: true, restart count 0
Oct 29 21:23:56.926: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:23:56.926: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:23:56.926: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-10-29 17:49:10 +0000 UTC (5 container statuses recorded)
Oct 29 21:23:56.926: INFO: 	Container alertmanager ready: true, restart count 0
Oct 29 21:23:56.926: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Oct 29 21:23:56.926: INFO: 	Container config-reloader ready: true, restart count 0
Oct 29 21:23:56.926: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:23:56.926: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:23:56.926: INFO: grafana-5cfb6b5fbb-gjr2l from openshift-monitoring started at 2020-10-29 17:48:48 +0000 UTC (2 container statuses recorded)
Oct 29 21:23:56.926: INFO: 	Container grafana ready: true, restart count 0
Oct 29 21:23:56.926: INFO: 	Container grafana-proxy ready: true, restart count 0
Oct 29 21:23:56.926: INFO: kube-state-metrics-596748788d-6mblj from openshift-monitoring started at 2020-10-29 17:48:48 +0000 UTC (3 container statuses recorded)
Oct 29 21:23:56.926: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Oct 29 21:23:56.926: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Oct 29 21:23:56.926: INFO: 	Container kube-state-metrics ready: true, restart count 0
Oct 29 21:23:56.926: INFO: node-exporter-d8llk from openshift-monitoring started at 2020-10-29 17:41:13 +0000 UTC (2 container statuses recorded)
Oct 29 21:23:56.926: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:23:56.926: INFO: 	Container node-exporter ready: true, restart count 0
Oct 29 21:23:56.926: INFO: prometheus-adapter-6495ff5885-lrx64 from openshift-monitoring started at 2020-10-29 17:48:47 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:56.926: INFO: 	Container prometheus-adapter ready: true, restart count 0
Oct 29 21:23:56.926: INFO: prometheus-adapter-6495ff5885-lw9vm from openshift-monitoring started at 2020-10-29 17:48:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:56.926: INFO: 	Container prometheus-adapter ready: true, restart count 0
Oct 29 21:23:56.926: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-10-29 17:48:53 +0000 UTC (6 container statuses recorded)
Oct 29 21:23:56.926: INFO: 	Container config-reloader ready: true, restart count 0
Oct 29 21:23:56.926: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:23:56.926: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:23:56.926: INFO: 	Container prometheus ready: true, restart count 1
Oct 29 21:23:56.926: INFO: 	Container prometheus-proxy ready: true, restart count 0
Oct 29 21:23:56.926: INFO: 	Container thanos-sidecar ready: true, restart count 0
Oct 29 21:23:56.926: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-10-29 17:49:00 +0000 UTC (6 container statuses recorded)
Oct 29 21:23:56.926: INFO: 	Container config-reloader ready: true, restart count 0
Oct 29 21:23:56.926: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:23:56.926: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:23:56.926: INFO: 	Container prometheus ready: true, restart count 1
Oct 29 21:23:56.926: INFO: 	Container prometheus-proxy ready: true, restart count 0
Oct 29 21:23:56.926: INFO: 	Container thanos-sidecar ready: true, restart count 0
Oct 29 21:23:56.926: INFO: prometheus-operator-89f4b9855-wlkt2 from openshift-monitoring started at 2020-10-29 17:48:47 +0000 UTC (2 container statuses recorded)
Oct 29 21:23:56.926: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:23:56.926: INFO: 	Container prometheus-operator ready: true, restart count 0
Oct 29 21:23:56.926: INFO: sre-dns-latency-exporter-l58wm from openshift-monitoring started at 2020-10-29 17:41:56 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:56.926: INFO: 	Container main ready: true, restart count 0
Oct 29 21:23:56.926: INFO: sre-stuck-ebs-vols-1-98w59 from openshift-monitoring started at 2020-10-29 17:48:47 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:56.926: INFO: 	Container main ready: true, restart count 0
Oct 29 21:23:56.926: INFO: telemeter-client-577bcc4f8d-jv4m8 from openshift-monitoring started at 2020-10-29 17:48:47 +0000 UTC (3 container statuses recorded)
Oct 29 21:23:56.926: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:23:56.926: INFO: 	Container reload ready: true, restart count 0
Oct 29 21:23:56.926: INFO: 	Container telemeter-client ready: true, restart count 0
Oct 29 21:23:56.926: INFO: multus-str5z from openshift-multus started at 2020-10-29 17:41:12 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:56.926: INFO: 	Container kube-multus ready: true, restart count 0
Oct 29 21:23:56.926: INFO: network-metrics-daemon-gcjdz from openshift-multus started at 2020-10-29 17:41:13 +0000 UTC (2 container statuses recorded)
Oct 29 21:23:56.926: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:23:56.926: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Oct 29 21:23:56.926: INFO: ovs-99tdx from openshift-sdn started at 2020-10-29 17:41:12 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:56.926: INFO: 	Container openvswitch ready: true, restart count 0
Oct 29 21:23:56.926: INFO: sdn-cp5t4 from openshift-sdn started at 2020-10-29 17:41:12 +0000 UTC (2 container statuses recorded)
Oct 29 21:23:56.926: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Oct 29 21:23:56.926: INFO: 	Container sdn ready: true, restart count 0
Oct 29 21:23:56.926: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-158-72.us-east-2.compute.internal before test
Oct 29 21:23:57.052: INFO: sre-build-test-1603998660-hmrqr from openshift-build-test started at 2020-10-29 19:11:03 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:57.052: INFO: 	Container sre-build-test ready: false, restart count 0
Oct 29 21:23:57.052: INFO: sre-build-test-1604002260-284ls from openshift-build-test started at 2020-10-29 20:11:07 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:57.052: INFO: 	Container sre-build-test ready: false, restart count 0
Oct 29 21:23:57.052: INFO: sre-build-test-1604005860-qkzkn from openshift-build-test started at 2020-10-29 21:11:04 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:57.052: INFO: 	Container sre-build-test ready: false, restart count 0
Oct 29 21:23:57.052: INFO: cloud-ingress-operator-798549d69-9fh9n from openshift-cloud-ingress-operator started at 2020-10-29 19:28:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:57.052: INFO: 	Container cloud-ingress-operator ready: true, restart count 0
Oct 29 21:23:57.052: INFO: aws-ebs-csi-driver-node-zfr5l from openshift-cluster-csi-drivers started at 2020-10-29 17:42:13 +0000 UTC (3 container statuses recorded)
Oct 29 21:23:57.052: INFO: 	Container csi-driver ready: true, restart count 0
Oct 29 21:23:57.052: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Oct 29 21:23:57.052: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Oct 29 21:23:57.052: INFO: tuned-jwndg from openshift-cluster-node-tuning-operator started at 2020-10-29 17:42:13 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:57.052: INFO: 	Container tuned ready: true, restart count 0
Oct 29 21:23:57.052: INFO: dns-default-25bcd from openshift-dns started at 2020-10-29 17:42:13 +0000 UTC (3 container statuses recorded)
Oct 29 21:23:57.052: INFO: 	Container dns ready: true, restart count 0
Oct 29 21:23:57.052: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 29 21:23:57.052: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:23:57.052: INFO: node-ca-d275k from openshift-image-registry started at 2020-10-29 17:42:13 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:57.052: INFO: 	Container node-ca ready: true, restart count 0
Oct 29 21:23:57.052: INFO: router-default-5f9d96945d-ckql4 from openshift-ingress started at 2020-10-29 19:44:59 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:57.052: INFO: 	Container router ready: true, restart count 0
Oct 29 21:23:57.052: INFO: machine-config-daemon-6987z from openshift-machine-config-operator started at 2020-10-29 17:42:13 +0000 UTC (2 container statuses recorded)
Oct 29 21:23:57.052: INFO: 	Container machine-config-daemon ready: true, restart count 0
Oct 29 21:23:57.052: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:23:57.052: INFO: node-exporter-pdd9z from openshift-monitoring started at 2020-10-29 17:42:13 +0000 UTC (2 container statuses recorded)
Oct 29 21:23:57.052: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:23:57.052: INFO: 	Container node-exporter ready: true, restart count 0
Oct 29 21:23:57.052: INFO: sre-dns-latency-exporter-krp25 from openshift-monitoring started at 2020-10-29 17:42:51 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:57.052: INFO: 	Container main ready: true, restart count 0
Oct 29 21:23:57.052: INFO: sre-ebs-iops-reporter-1-rrlxd from openshift-monitoring started at 2020-10-29 19:28:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:57.052: INFO: 	Container main ready: true, restart count 0
Oct 29 21:23:57.052: INFO: thanos-querier-7f688468b8-6jh8m from openshift-monitoring started at 2020-10-29 19:28:48 +0000 UTC (5 container statuses recorded)
Oct 29 21:23:57.052: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:23:57.052: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Oct 29 21:23:57.052: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:23:57.052: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:23:57.052: INFO: 	Container thanos-query ready: true, restart count 0
Oct 29 21:23:57.052: INFO: multus-l5sdf from openshift-multus started at 2020-10-29 17:42:13 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:57.052: INFO: 	Container kube-multus ready: true, restart count 0
Oct 29 21:23:57.052: INFO: network-metrics-daemon-nb7sg from openshift-multus started at 2020-10-29 17:42:13 +0000 UTC (2 container statuses recorded)
Oct 29 21:23:57.052: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:23:57.052: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Oct 29 21:23:57.052: INFO: ovs-9ct7r from openshift-sdn started at 2020-10-29 17:42:13 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:57.052: INFO: 	Container openvswitch ready: true, restart count 0
Oct 29 21:23:57.052: INFO: sdn-8kf7x from openshift-sdn started at 2020-10-29 17:42:13 +0000 UTC (2 container statuses recorded)
Oct 29 21:23:57.052: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Oct 29 21:23:57.052: INFO: 	Container sdn ready: true, restart count 0
Oct 29 21:23:57.052: INFO: splunk-forwarder-operator-6c55bb877-mw9lq from openshift-splunk-forwarder-operator started at 2020-10-29 19:28:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:57.052: INFO: 	Container splunk-forwarder-operator ready: true, restart count 0
Oct 29 21:23:57.052: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-234-238.us-east-2.compute.internal before test
Oct 29 21:23:57.106: INFO: cloud-ingress-operator-registry-zpdvx from openshift-cloud-ingress-operator started at 2020-10-29 19:32:07 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:57.106: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:23:57.106: INFO: aws-ebs-csi-driver-node-r7m6f from openshift-cluster-csi-drivers started at 2020-10-29 17:30:47 +0000 UTC (3 container statuses recorded)
Oct 29 21:23:57.106: INFO: 	Container csi-driver ready: true, restart count 0
Oct 29 21:23:57.106: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Oct 29 21:23:57.106: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Oct 29 21:23:57.106: INFO: tuned-6rs49 from openshift-cluster-node-tuning-operator started at 2020-10-29 17:30:47 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:57.106: INFO: 	Container tuned ready: true, restart count 0
Oct 29 21:23:57.106: INFO: downloads-85df645c7c-ft27h from openshift-console started at 2020-10-29 19:31:54 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:57.106: INFO: 	Container download-server ready: true, restart count 0
Oct 29 21:23:57.106: INFO: downloads-85df645c7c-pxr2l from openshift-console started at 2020-10-29 19:28:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:57.106: INFO: 	Container download-server ready: true, restart count 0
Oct 29 21:23:57.106: INFO: dns-default-jxrwk from openshift-dns started at 2020-10-29 17:30:47 +0000 UTC (3 container statuses recorded)
Oct 29 21:23:57.106: INFO: 	Container dns ready: true, restart count 0
Oct 29 21:23:57.106: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 29 21:23:57.106: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:23:57.106: INFO: node-ca-wxm2h from openshift-image-registry started at 2020-10-29 17:30:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:57.106: INFO: 	Container node-ca ready: true, restart count 0
Oct 29 21:23:57.106: INFO: migrator-b8d88f977-j4wqg from openshift-kube-storage-version-migrator started at 2020-10-29 19:28:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:57.106: INFO: 	Container migrator ready: true, restart count 0
Oct 29 21:23:57.106: INFO: machine-config-daemon-5rrfn from openshift-machine-config-operator started at 2020-10-29 17:30:47 +0000 UTC (2 container statuses recorded)
Oct 29 21:23:57.106: INFO: 	Container machine-config-daemon ready: true, restart count 0
Oct 29 21:23:57.106: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:23:57.106: INFO: managed-upgrade-operator-7558857f4-cxqkg from openshift-managed-upgrade-operator started at 2020-10-29 17:48:47 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:57.106: INFO: 	Container managed-upgrade-operator ready: true, restart count 0
Oct 29 21:23:57.106: INFO: managed-upgrade-operator-catalog-g4m2g from openshift-managed-upgrade-operator started at 2020-10-29 19:32:11 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:57.106: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:23:57.106: INFO: certified-operators-pjgrs from openshift-marketplace started at 2020-10-29 19:32:07 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:57.106: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:23:57.106: INFO: community-operators-rss65 from openshift-marketplace started at 2020-10-29 19:32:06 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:57.106: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:23:57.106: INFO: redhat-marketplace-cwfzm from openshift-marketplace started at 2020-10-29 19:32:08 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:57.106: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:23:57.106: INFO: redhat-operators-pjjcr from openshift-marketplace started at 2020-10-29 19:32:11 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:57.106: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:23:57.106: INFO: configure-alertmanager-operator-579dfffff7-nmj5v from openshift-monitoring started at 2020-10-29 17:48:47 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:57.106: INFO: 	Container configure-alertmanager-operator ready: true, restart count 0
Oct 29 21:23:57.106: INFO: configure-alertmanager-operator-registry-zcvq5 from openshift-monitoring started at 2020-10-29 19:32:07 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:57.106: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:23:57.106: INFO: node-exporter-lmwzp from openshift-monitoring started at 2020-10-29 17:30:48 +0000 UTC (2 container statuses recorded)
Oct 29 21:23:57.106: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:23:57.106: INFO: 	Container node-exporter ready: true, restart count 0
Oct 29 21:23:57.106: INFO: openshift-state-metrics-74d99f5845-62ghm from openshift-monitoring started at 2020-10-29 19:31:54 +0000 UTC (3 container statuses recorded)
Oct 29 21:23:57.106: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Oct 29 21:23:57.106: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Oct 29 21:23:57.106: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Oct 29 21:23:57.106: INFO: sre-dns-latency-exporter-5wrqd from openshift-monitoring started at 2020-10-29 17:39:27 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:57.106: INFO: 	Container main ready: true, restart count 0
Oct 29 21:23:57.106: INFO: thanos-querier-7f688468b8-cfd7g from openshift-monitoring started at 2020-10-29 17:48:47 +0000 UTC (5 container statuses recorded)
Oct 29 21:23:57.106: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:23:57.106: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Oct 29 21:23:57.106: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:23:57.106: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:23:57.106: INFO: 	Container thanos-query ready: true, restart count 0
Oct 29 21:23:57.106: INFO: multus-bcgtj from openshift-multus started at 2020-10-29 17:30:47 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:57.106: INFO: 	Container kube-multus ready: true, restart count 0
Oct 29 21:23:57.106: INFO: network-metrics-daemon-5brxv from openshift-multus started at 2020-10-29 17:30:48 +0000 UTC (2 container statuses recorded)
Oct 29 21:23:57.106: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:23:57.106: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Oct 29 21:23:57.106: INFO: rbac-permissions-operator-78d47d8556-92kbv from openshift-rbac-permissions started at 2020-10-29 17:48:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:57.106: INFO: 	Container rbac-permissions-operator ready: true, restart count 0
Oct 29 21:23:57.106: INFO: rbac-permissions-operator-registry-x892d from openshift-rbac-permissions started at 2020-10-29 19:32:08 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:57.106: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:23:57.106: INFO: ovs-9f9tm from openshift-sdn started at 2020-10-29 17:30:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:57.106: INFO: 	Container openvswitch ready: true, restart count 0
Oct 29 21:23:57.106: INFO: sdn-9tj9v from openshift-sdn started at 2020-10-29 17:30:48 +0000 UTC (2 container statuses recorded)
Oct 29 21:23:57.107: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Oct 29 21:23:57.107: INFO: 	Container sdn ready: true, restart count 0
Oct 29 21:23:57.107: INFO: splunk-forwarder-operator-catalog-jg48j from openshift-splunk-forwarder-operator started at 2020-10-29 19:32:10 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:57.107: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:23:57.107: INFO: alert-pruner-1603998000-fgw8n from openshift-sre-pruning started at 2020-10-29 19:00:09 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:57.107: INFO: 	Container alert-pruner ready: false, restart count 0
Oct 29 21:23:57.107: INFO: crd-pruner-1603998000-ljqfl from openshift-sre-pruning started at 2020-10-29 19:00:09 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:57.107: INFO: 	Container crd-pruner ready: false, restart count 0
Oct 29 21:23:57.107: INFO: deployments-pruner-1603998000-vfdcp from openshift-sre-pruning started at 2020-10-29 19:00:09 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:57.107: INFO: 	Container deployments-pruner ready: false, restart count 0
Oct 29 21:23:57.107: INFO: deployments-pruner-1604005200-dfg2v from openshift-sre-pruning started at 2020-10-29 21:00:09 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:57.107: INFO: 	Container deployments-pruner ready: false, restart count 0
Oct 29 21:23:57.107: INFO: managed-velero-operator-8685cc459-lsvfl from openshift-velero started at 2020-10-29 17:48:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:57.107: INFO: 	Container managed-velero-operator ready: true, restart count 0
Oct 29 21:23:57.107: INFO: managed-velero-operator-registry-9bghr from openshift-velero started at 2020-10-29 19:32:07 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:57.107: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:23:57.107: INFO: velero-68656b49f9-zvplm from openshift-velero started at 2020-10-29 17:46:41 +0000 UTC (1 container statuses recorded)
Oct 29 21:23:57.107: INFO: 	Container velero ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-dfcfddc1-2a05-44d2-82e3-7037a242f2a5 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-dfcfddc1-2a05-44d2-82e3-7037a242f2a5 off the node ip-10-0-142-212.us-east-2.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-dfcfddc1-2a05-44d2-82e3-7037a242f2a5
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:29:03.611: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "sched-pred-4729" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

 [SLOW TEST:307.243 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":19,"completed":7,"skipped":2794,"failed":1,"failures":["[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:29:03.684: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Oct 29 21:29:03.921: INFO: Waiting up to 1m0s for all nodes to be ready
Oct 29 21:30:04.291: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
Oct 29 21:30:04.384: INFO: Created pod: pod0-sched-preemption-low-priority
Oct 29 21:30:04.442: INFO: Created pod: pod1-sched-preemption-medium-priority
Oct 29 21:30:04.501: INFO: Created pod: pod2-sched-preemption-medium-priority
Oct 29 21:30:04.561: INFO: Created pod: pod3-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:30:26.884: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "sched-preemption-3952" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

 [SLOW TEST:83.498 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":19,"completed":8,"skipped":2943,"failed":1,"failures":["[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:30:27.187: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Oct 29 21:30:27.337: INFO: Waiting up to 1m0s for all (but 3) nodes to be ready
Oct 29 21:30:27.409: INFO: Waiting for terminating namespaces to be deleted...
Oct 29 21:30:27.434: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-142-212.us-east-2.compute.internal before test
Oct 29 21:30:27.473: INFO: aws-ebs-csi-driver-node-5gmf4 from openshift-cluster-csi-drivers started at 2020-10-29 17:30:44 +0000 UTC (3 container statuses recorded)
Oct 29 21:30:27.473: INFO: 	Container csi-driver ready: true, restart count 0
Oct 29 21:30:27.473: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Oct 29 21:30:27.473: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Oct 29 21:30:27.473: INFO: tuned-d6fwx from openshift-cluster-node-tuning-operator started at 2020-10-29 17:30:44 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.473: INFO: 	Container tuned ready: true, restart count 0
Oct 29 21:30:27.473: INFO: dns-default-b89gt from openshift-dns started at 2020-10-29 17:30:44 +0000 UTC (3 container statuses recorded)
Oct 29 21:30:27.473: INFO: 	Container dns ready: true, restart count 0
Oct 29 21:30:27.473: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 29 21:30:27.473: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:30:27.473: INFO: node-ca-rq9vr from openshift-image-registry started at 2020-10-29 17:30:44 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.473: INFO: 	Container node-ca ready: true, restart count 0
Oct 29 21:30:27.473: INFO: machine-config-daemon-l4r47 from openshift-machine-config-operator started at 2020-10-29 17:30:44 +0000 UTC (2 container statuses recorded)
Oct 29 21:30:27.473: INFO: 	Container machine-config-daemon ready: true, restart count 0
Oct 29 21:30:27.473: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:30:27.473: INFO: osd-patch-subscription-source-1604005200-ccz9j from openshift-marketplace started at 2020-10-29 21:00:09 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.473: INFO: 	Container osd-patch-subscription-source ready: false, restart count 0
Oct 29 21:30:27.473: INFO: node-exporter-p2phw from openshift-monitoring started at 2020-10-29 17:30:44 +0000 UTC (2 container statuses recorded)
Oct 29 21:30:27.473: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:30:27.473: INFO: 	Container node-exporter ready: true, restart count 0
Oct 29 21:30:27.473: INFO: sre-dns-latency-exporter-29j4j from openshift-monitoring started at 2020-10-29 20:32:47 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.473: INFO: 	Container main ready: true, restart count 0
Oct 29 21:30:27.473: INFO: multus-jst6k from openshift-multus started at 2020-10-29 17:30:43 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.473: INFO: 	Container kube-multus ready: true, restart count 0
Oct 29 21:30:27.473: INFO: network-metrics-daemon-xrvxh from openshift-multus started at 2020-10-29 17:30:44 +0000 UTC (2 container statuses recorded)
Oct 29 21:30:27.473: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:30:27.473: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Oct 29 21:30:27.473: INFO: ovs-66jns from openshift-sdn started at 2020-10-29 17:30:43 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.473: INFO: 	Container openvswitch ready: true, restart count 0
Oct 29 21:30:27.473: INFO: sdn-lbx27 from openshift-sdn started at 2020-10-29 17:30:44 +0000 UTC (2 container statuses recorded)
Oct 29 21:30:27.473: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Oct 29 21:30:27.473: INFO: 	Container sdn ready: true, restart count 0
Oct 29 21:30:27.473: INFO: alert-pruner-1604005200-gtct6 from openshift-sre-pruning started at 2020-10-29 21:00:09 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.473: INFO: 	Container alert-pruner ready: false, restart count 0
Oct 29 21:30:27.473: INFO: builds-pruner-1604005200-8wspq from openshift-sre-pruning started at 2020-10-29 21:00:09 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.473: INFO: 	Container builds-pruner ready: false, restart count 0
Oct 29 21:30:27.473: INFO: crd-pruner-1604005200-w76df from openshift-sre-pruning started at 2020-10-29 21:00:09 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.473: INFO: 	Container crd-pruner ready: false, restart count 0
Oct 29 21:30:27.473: INFO: image-pruner-1604005200-bp5kw from openshift-sre-pruning started at 2020-10-29 21:00:09 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.473: INFO: 	Container image-pruner ready: false, restart count 0
Oct 29 21:30:27.473: INFO: preemptor-pod from sched-preemption-3952 started at 2020-10-29 21:30:03 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.473: INFO: 	Container preemptor-pod ready: true, restart count 0
Oct 29 21:30:27.473: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-157-225.us-east-2.compute.internal before test
Oct 29 21:30:27.517: INFO: aws-ebs-csi-driver-node-qvwrc from openshift-cluster-csi-drivers started at 2020-10-29 17:41:11 +0000 UTC (3 container statuses recorded)
Oct 29 21:30:27.517: INFO: 	Container csi-driver ready: true, restart count 0
Oct 29 21:30:27.517: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Oct 29 21:30:27.517: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Oct 29 21:30:27.517: INFO: tuned-4tm5g from openshift-cluster-node-tuning-operator started at 2020-10-29 17:41:11 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.517: INFO: 	Container tuned ready: true, restart count 0
Oct 29 21:30:27.517: INFO: dns-default-hr5xr from openshift-dns started at 2020-10-29 17:41:12 +0000 UTC (3 container statuses recorded)
Oct 29 21:30:27.517: INFO: 	Container dns ready: true, restart count 0
Oct 29 21:30:27.517: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 29 21:30:27.517: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:30:27.517: INFO: image-registry-75b75c555d-cm6gf from openshift-image-registry started at 2020-10-29 17:48:47 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.517: INFO: 	Container registry ready: true, restart count 0
Oct 29 21:30:27.517: INFO: image-registry-75b75c555d-ksztb from openshift-image-registry started at 2020-10-29 17:48:47 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.517: INFO: 	Container registry ready: true, restart count 0
Oct 29 21:30:27.517: INFO: node-ca-6tz25 from openshift-image-registry started at 2020-10-29 17:41:11 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.517: INFO: 	Container node-ca ready: true, restart count 0
Oct 29 21:30:27.517: INFO: router-default-5f9d96945d-zxzg4 from openshift-ingress started at 2020-10-29 19:45:00 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.517: INFO: 	Container router ready: true, restart count 0
Oct 29 21:30:27.517: INFO: machine-config-daemon-mjcnt from openshift-machine-config-operator started at 2020-10-29 17:41:11 +0000 UTC (2 container statuses recorded)
Oct 29 21:30:27.517: INFO: 	Container machine-config-daemon ready: true, restart count 0
Oct 29 21:30:27.517: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:30:27.517: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-10-29 17:49:01 +0000 UTC (5 container statuses recorded)
Oct 29 21:30:27.517: INFO: 	Container alertmanager ready: true, restart count 0
Oct 29 21:30:27.517: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Oct 29 21:30:27.517: INFO: 	Container config-reloader ready: true, restart count 0
Oct 29 21:30:27.517: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:30:27.517: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:30:27.517: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-10-29 17:49:02 +0000 UTC (5 container statuses recorded)
Oct 29 21:30:27.517: INFO: 	Container alertmanager ready: true, restart count 0
Oct 29 21:30:27.517: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Oct 29 21:30:27.517: INFO: 	Container config-reloader ready: true, restart count 0
Oct 29 21:30:27.517: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:30:27.517: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:30:27.517: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-10-29 17:49:10 +0000 UTC (5 container statuses recorded)
Oct 29 21:30:27.517: INFO: 	Container alertmanager ready: true, restart count 0
Oct 29 21:30:27.517: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Oct 29 21:30:27.517: INFO: 	Container config-reloader ready: true, restart count 0
Oct 29 21:30:27.517: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:30:27.517: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:30:27.517: INFO: grafana-5cfb6b5fbb-gjr2l from openshift-monitoring started at 2020-10-29 17:48:48 +0000 UTC (2 container statuses recorded)
Oct 29 21:30:27.517: INFO: 	Container grafana ready: true, restart count 0
Oct 29 21:30:27.517: INFO: 	Container grafana-proxy ready: true, restart count 0
Oct 29 21:30:27.517: INFO: kube-state-metrics-596748788d-6mblj from openshift-monitoring started at 2020-10-29 17:48:48 +0000 UTC (3 container statuses recorded)
Oct 29 21:30:27.517: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Oct 29 21:30:27.517: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Oct 29 21:30:27.517: INFO: 	Container kube-state-metrics ready: true, restart count 0
Oct 29 21:30:27.517: INFO: node-exporter-d8llk from openshift-monitoring started at 2020-10-29 17:41:13 +0000 UTC (2 container statuses recorded)
Oct 29 21:30:27.517: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:30:27.517: INFO: 	Container node-exporter ready: true, restart count 0
Oct 29 21:30:27.517: INFO: prometheus-adapter-6495ff5885-lrx64 from openshift-monitoring started at 2020-10-29 17:48:47 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.517: INFO: 	Container prometheus-adapter ready: true, restart count 0
Oct 29 21:30:27.517: INFO: prometheus-adapter-6495ff5885-lw9vm from openshift-monitoring started at 2020-10-29 17:48:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.517: INFO: 	Container prometheus-adapter ready: true, restart count 0
Oct 29 21:30:27.517: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-10-29 17:48:53 +0000 UTC (6 container statuses recorded)
Oct 29 21:30:27.517: INFO: 	Container config-reloader ready: true, restart count 0
Oct 29 21:30:27.517: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:30:27.517: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:30:27.517: INFO: 	Container prometheus ready: true, restart count 1
Oct 29 21:30:27.517: INFO: 	Container prometheus-proxy ready: true, restart count 0
Oct 29 21:30:27.517: INFO: 	Container thanos-sidecar ready: true, restart count 0
Oct 29 21:30:27.517: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-10-29 17:49:00 +0000 UTC (6 container statuses recorded)
Oct 29 21:30:27.517: INFO: 	Container config-reloader ready: true, restart count 0
Oct 29 21:30:27.517: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:30:27.517: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:30:27.517: INFO: 	Container prometheus ready: true, restart count 1
Oct 29 21:30:27.517: INFO: 	Container prometheus-proxy ready: true, restart count 0
Oct 29 21:30:27.517: INFO: 	Container thanos-sidecar ready: true, restart count 0
Oct 29 21:30:27.517: INFO: prometheus-operator-89f4b9855-wlkt2 from openshift-monitoring started at 2020-10-29 17:48:47 +0000 UTC (2 container statuses recorded)
Oct 29 21:30:27.517: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:30:27.517: INFO: 	Container prometheus-operator ready: true, restart count 0
Oct 29 21:30:27.517: INFO: sre-dns-latency-exporter-l58wm from openshift-monitoring started at 2020-10-29 17:41:56 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.517: INFO: 	Container main ready: true, restart count 0
Oct 29 21:30:27.517: INFO: sre-stuck-ebs-vols-1-98w59 from openshift-monitoring started at 2020-10-29 17:48:47 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.517: INFO: 	Container main ready: true, restart count 0
Oct 29 21:30:27.517: INFO: telemeter-client-577bcc4f8d-jv4m8 from openshift-monitoring started at 2020-10-29 17:48:47 +0000 UTC (3 container statuses recorded)
Oct 29 21:30:27.517: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:30:27.517: INFO: 	Container reload ready: true, restart count 0
Oct 29 21:30:27.517: INFO: 	Container telemeter-client ready: true, restart count 0
Oct 29 21:30:27.517: INFO: multus-str5z from openshift-multus started at 2020-10-29 17:41:12 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.517: INFO: 	Container kube-multus ready: true, restart count 0
Oct 29 21:30:27.517: INFO: network-metrics-daemon-gcjdz from openshift-multus started at 2020-10-29 17:41:13 +0000 UTC (2 container statuses recorded)
Oct 29 21:30:27.517: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:30:27.517: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Oct 29 21:30:27.517: INFO: ovs-99tdx from openshift-sdn started at 2020-10-29 17:41:12 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.517: INFO: 	Container openvswitch ready: true, restart count 0
Oct 29 21:30:27.517: INFO: sdn-cp5t4 from openshift-sdn started at 2020-10-29 17:41:12 +0000 UTC (2 container statuses recorded)
Oct 29 21:30:27.517: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Oct 29 21:30:27.517: INFO: 	Container sdn ready: true, restart count 0
Oct 29 21:30:27.517: INFO: pod1-sched-preemption-medium-priority from sched-preemption-3952 started at 2020-10-29 21:29:58 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.517: INFO: 	Container pod1-sched-preemption-medium-priority ready: true, restart count 0
Oct 29 21:30:27.517: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-158-72.us-east-2.compute.internal before test
Oct 29 21:30:27.558: INFO: sre-build-test-1603998660-hmrqr from openshift-build-test started at 2020-10-29 19:11:03 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.558: INFO: 	Container sre-build-test ready: false, restart count 0
Oct 29 21:30:27.558: INFO: sre-build-test-1604002260-284ls from openshift-build-test started at 2020-10-29 20:11:07 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.558: INFO: 	Container sre-build-test ready: false, restart count 0
Oct 29 21:30:27.558: INFO: sre-build-test-1604005860-qkzkn from openshift-build-test started at 2020-10-29 21:11:04 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.559: INFO: 	Container sre-build-test ready: false, restart count 0
Oct 29 21:30:27.559: INFO: cloud-ingress-operator-798549d69-9fh9n from openshift-cloud-ingress-operator started at 2020-10-29 19:28:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.559: INFO: 	Container cloud-ingress-operator ready: true, restart count 0
Oct 29 21:30:27.559: INFO: aws-ebs-csi-driver-node-zfr5l from openshift-cluster-csi-drivers started at 2020-10-29 17:42:13 +0000 UTC (3 container statuses recorded)
Oct 29 21:30:27.559: INFO: 	Container csi-driver ready: true, restart count 0
Oct 29 21:30:27.559: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Oct 29 21:30:27.559: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Oct 29 21:30:27.559: INFO: tuned-jwndg from openshift-cluster-node-tuning-operator started at 2020-10-29 17:42:13 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.559: INFO: 	Container tuned ready: true, restart count 0
Oct 29 21:30:27.559: INFO: dns-default-25bcd from openshift-dns started at 2020-10-29 17:42:13 +0000 UTC (3 container statuses recorded)
Oct 29 21:30:27.559: INFO: 	Container dns ready: true, restart count 0
Oct 29 21:30:27.559: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 29 21:30:27.559: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:30:27.559: INFO: node-ca-d275k from openshift-image-registry started at 2020-10-29 17:42:13 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.559: INFO: 	Container node-ca ready: true, restart count 0
Oct 29 21:30:27.559: INFO: router-default-5f9d96945d-ckql4 from openshift-ingress started at 2020-10-29 19:44:59 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.559: INFO: 	Container router ready: true, restart count 0
Oct 29 21:30:27.559: INFO: machine-config-daemon-6987z from openshift-machine-config-operator started at 2020-10-29 17:42:13 +0000 UTC (2 container statuses recorded)
Oct 29 21:30:27.559: INFO: 	Container machine-config-daemon ready: true, restart count 0
Oct 29 21:30:27.559: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:30:27.559: INFO: node-exporter-pdd9z from openshift-monitoring started at 2020-10-29 17:42:13 +0000 UTC (2 container statuses recorded)
Oct 29 21:30:27.559: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:30:27.559: INFO: 	Container node-exporter ready: true, restart count 0
Oct 29 21:30:27.559: INFO: sre-dns-latency-exporter-krp25 from openshift-monitoring started at 2020-10-29 17:42:51 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.559: INFO: 	Container main ready: true, restart count 0
Oct 29 21:30:27.559: INFO: sre-ebs-iops-reporter-1-rrlxd from openshift-monitoring started at 2020-10-29 19:28:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.559: INFO: 	Container main ready: true, restart count 0
Oct 29 21:30:27.559: INFO: thanos-querier-7f688468b8-6jh8m from openshift-monitoring started at 2020-10-29 19:28:48 +0000 UTC (5 container statuses recorded)
Oct 29 21:30:27.559: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:30:27.559: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Oct 29 21:30:27.559: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:30:27.559: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:30:27.559: INFO: 	Container thanos-query ready: true, restart count 0
Oct 29 21:30:27.559: INFO: multus-l5sdf from openshift-multus started at 2020-10-29 17:42:13 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.559: INFO: 	Container kube-multus ready: true, restart count 0
Oct 29 21:30:27.559: INFO: network-metrics-daemon-nb7sg from openshift-multus started at 2020-10-29 17:42:13 +0000 UTC (2 container statuses recorded)
Oct 29 21:30:27.559: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:30:27.559: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Oct 29 21:30:27.559: INFO: ovs-9ct7r from openshift-sdn started at 2020-10-29 17:42:13 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.559: INFO: 	Container openvswitch ready: true, restart count 0
Oct 29 21:30:27.559: INFO: sdn-8kf7x from openshift-sdn started at 2020-10-29 17:42:13 +0000 UTC (2 container statuses recorded)
Oct 29 21:30:27.559: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Oct 29 21:30:27.559: INFO: 	Container sdn ready: true, restart count 0
Oct 29 21:30:27.559: INFO: splunk-forwarder-operator-6c55bb877-mw9lq from openshift-splunk-forwarder-operator started at 2020-10-29 19:28:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.559: INFO: 	Container splunk-forwarder-operator ready: true, restart count 0
Oct 29 21:30:27.559: INFO: pod2-sched-preemption-medium-priority from sched-preemption-3952 started at 2020-10-29 21:29:47 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.559: INFO: 	Container pod2-sched-preemption-medium-priority ready: true, restart count 0
Oct 29 21:30:27.559: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-234-238.us-east-2.compute.internal before test
Oct 29 21:30:27.604: INFO: cloud-ingress-operator-registry-zpdvx from openshift-cloud-ingress-operator started at 2020-10-29 19:32:07 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.604: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:30:27.604: INFO: aws-ebs-csi-driver-node-r7m6f from openshift-cluster-csi-drivers started at 2020-10-29 17:30:47 +0000 UTC (3 container statuses recorded)
Oct 29 21:30:27.604: INFO: 	Container csi-driver ready: true, restart count 0
Oct 29 21:30:27.604: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Oct 29 21:30:27.604: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Oct 29 21:30:27.604: INFO: tuned-6rs49 from openshift-cluster-node-tuning-operator started at 2020-10-29 17:30:47 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.604: INFO: 	Container tuned ready: true, restart count 0
Oct 29 21:30:27.604: INFO: downloads-85df645c7c-ft27h from openshift-console started at 2020-10-29 19:31:54 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.604: INFO: 	Container download-server ready: true, restart count 0
Oct 29 21:30:27.604: INFO: downloads-85df645c7c-pxr2l from openshift-console started at 2020-10-29 19:28:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.604: INFO: 	Container download-server ready: true, restart count 0
Oct 29 21:30:27.604: INFO: dns-default-jxrwk from openshift-dns started at 2020-10-29 17:30:47 +0000 UTC (3 container statuses recorded)
Oct 29 21:30:27.604: INFO: 	Container dns ready: true, restart count 0
Oct 29 21:30:27.604: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 29 21:30:27.604: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:30:27.604: INFO: node-ca-wxm2h from openshift-image-registry started at 2020-10-29 17:30:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.604: INFO: 	Container node-ca ready: true, restart count 0
Oct 29 21:30:27.604: INFO: migrator-b8d88f977-j4wqg from openshift-kube-storage-version-migrator started at 2020-10-29 19:28:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.604: INFO: 	Container migrator ready: true, restart count 0
Oct 29 21:30:27.604: INFO: machine-config-daemon-5rrfn from openshift-machine-config-operator started at 2020-10-29 17:30:47 +0000 UTC (2 container statuses recorded)
Oct 29 21:30:27.604: INFO: 	Container machine-config-daemon ready: true, restart count 0
Oct 29 21:30:27.604: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:30:27.604: INFO: managed-upgrade-operator-7558857f4-cxqkg from openshift-managed-upgrade-operator started at 2020-10-29 17:48:47 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.604: INFO: 	Container managed-upgrade-operator ready: true, restart count 0
Oct 29 21:30:27.604: INFO: managed-upgrade-operator-catalog-g4m2g from openshift-managed-upgrade-operator started at 2020-10-29 19:32:11 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.604: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:30:27.604: INFO: certified-operators-pjgrs from openshift-marketplace started at 2020-10-29 19:32:07 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.604: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:30:27.604: INFO: community-operators-rss65 from openshift-marketplace started at 2020-10-29 19:32:06 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.604: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:30:27.604: INFO: redhat-marketplace-cwfzm from openshift-marketplace started at 2020-10-29 19:32:08 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.604: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:30:27.604: INFO: redhat-operators-pjjcr from openshift-marketplace started at 2020-10-29 19:32:11 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.604: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:30:27.604: INFO: configure-alertmanager-operator-579dfffff7-nmj5v from openshift-monitoring started at 2020-10-29 17:48:47 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.604: INFO: 	Container configure-alertmanager-operator ready: true, restart count 0
Oct 29 21:30:27.604: INFO: configure-alertmanager-operator-registry-zcvq5 from openshift-monitoring started at 2020-10-29 19:32:07 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.604: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:30:27.604: INFO: node-exporter-lmwzp from openshift-monitoring started at 2020-10-29 17:30:48 +0000 UTC (2 container statuses recorded)
Oct 29 21:30:27.604: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:30:27.604: INFO: 	Container node-exporter ready: true, restart count 0
Oct 29 21:30:27.604: INFO: openshift-state-metrics-74d99f5845-62ghm from openshift-monitoring started at 2020-10-29 19:31:54 +0000 UTC (3 container statuses recorded)
Oct 29 21:30:27.604: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Oct 29 21:30:27.604: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Oct 29 21:30:27.604: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Oct 29 21:30:27.604: INFO: sre-dns-latency-exporter-5wrqd from openshift-monitoring started at 2020-10-29 17:39:27 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.605: INFO: 	Container main ready: true, restart count 0
Oct 29 21:30:27.605: INFO: thanos-querier-7f688468b8-cfd7g from openshift-monitoring started at 2020-10-29 17:48:47 +0000 UTC (5 container statuses recorded)
Oct 29 21:30:27.605: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:30:27.605: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Oct 29 21:30:27.605: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:30:27.605: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:30:27.605: INFO: 	Container thanos-query ready: true, restart count 0
Oct 29 21:30:27.605: INFO: multus-bcgtj from openshift-multus started at 2020-10-29 17:30:47 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.605: INFO: 	Container kube-multus ready: true, restart count 0
Oct 29 21:30:27.605: INFO: network-metrics-daemon-5brxv from openshift-multus started at 2020-10-29 17:30:48 +0000 UTC (2 container statuses recorded)
Oct 29 21:30:27.605: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:30:27.605: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Oct 29 21:30:27.605: INFO: rbac-permissions-operator-78d47d8556-92kbv from openshift-rbac-permissions started at 2020-10-29 17:48:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.605: INFO: 	Container rbac-permissions-operator ready: true, restart count 0
Oct 29 21:30:27.605: INFO: rbac-permissions-operator-registry-x892d from openshift-rbac-permissions started at 2020-10-29 19:32:08 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.605: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:30:27.605: INFO: ovs-9f9tm from openshift-sdn started at 2020-10-29 17:30:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.605: INFO: 	Container openvswitch ready: true, restart count 0
Oct 29 21:30:27.605: INFO: sdn-9tj9v from openshift-sdn started at 2020-10-29 17:30:48 +0000 UTC (2 container statuses recorded)
Oct 29 21:30:27.605: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Oct 29 21:30:27.605: INFO: 	Container sdn ready: true, restart count 0
Oct 29 21:30:27.605: INFO: splunk-forwarder-operator-catalog-jg48j from openshift-splunk-forwarder-operator started at 2020-10-29 19:32:10 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.605: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:30:27.605: INFO: alert-pruner-1603998000-fgw8n from openshift-sre-pruning started at 2020-10-29 19:00:09 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.605: INFO: 	Container alert-pruner ready: false, restart count 0
Oct 29 21:30:27.605: INFO: crd-pruner-1603998000-ljqfl from openshift-sre-pruning started at 2020-10-29 19:00:09 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.605: INFO: 	Container crd-pruner ready: false, restart count 0
Oct 29 21:30:27.605: INFO: deployments-pruner-1603998000-vfdcp from openshift-sre-pruning started at 2020-10-29 19:00:09 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.605: INFO: 	Container deployments-pruner ready: false, restart count 0
Oct 29 21:30:27.605: INFO: deployments-pruner-1604005200-dfg2v from openshift-sre-pruning started at 2020-10-29 21:00:09 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.605: INFO: 	Container deployments-pruner ready: false, restart count 0
Oct 29 21:30:27.605: INFO: managed-velero-operator-8685cc459-lsvfl from openshift-velero started at 2020-10-29 17:48:48 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.605: INFO: 	Container managed-velero-operator ready: true, restart count 0
Oct 29 21:30:27.605: INFO: managed-velero-operator-registry-9bghr from openshift-velero started at 2020-10-29 19:32:07 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.605: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:30:27.605: INFO: velero-68656b49f9-zvplm from openshift-velero started at 2020-10-29 17:46:41 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.605: INFO: 	Container velero ready: true, restart count 0
Oct 29 21:30:27.605: INFO: pod3-sched-preemption-medium-priority from sched-preemption-3952 started at 2020-10-29 21:29:47 +0000 UTC (1 container statuses recorded)
Oct 29 21:30:27.605: INFO: 	Container pod3-sched-preemption-medium-priority ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to schedule Pod with nonempty NodeSelector.
Oct 29 21:40:29.045: INFO: Timed out waiting for the following pods to schedule
Oct 29 21:40:29.045: INFO: openshift-authentication/oauth-openshift-7779c8f48c-5p9nn
Oct 29 21:40:29.045: INFO: openshift-console/console-5bc554f4bf-nfqz6
Oct 29 21:40:29.045: FAIL: Timed out after 10m0s waiting for stable cluster.

Full Stack Trace
k8s.io/kubernetes/test/e2e/scheduling.WaitForStableCluster(0x53ec980, 0xc002fa09a0, 0xc000964600, 0x0)
	/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:55 +0x405
k8s.io/kubernetes/test/e2e/scheduling.glob..func4.6()
	/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:436 +0xae
k8s.io/kubernetes/test/e2e.RunE2ETests(0xc003648c00)
	_output/local/go/src/k8s.io/kubernetes/test/e2e/e2e.go:130 +0x345
k8s.io/kubernetes/test/e2e.TestE2E(0xc003648c00)
	_output/local/go/src/k8s.io/kubernetes/test/e2e/e2e_test.go:145 +0x2b
testing.tRunner(0xc003648c00, 0x4dca3c8)
	/usr/lib/golang/src/testing/testing.go:1127 +0xef
created by testing.(*T).Run
	/usr/lib/golang/src/testing/testing.go:1178 +0x386
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
STEP: Collecting events from namespace "sched-pred-2434".
STEP: Found 0 events.
Oct 29 21:40:29.089: INFO: POD  NODE  PHASE  GRACE  CONDITIONS
Oct 29 21:40:29.089: INFO: 
Oct 29 21:40:29.170: INFO: 
Logging node info for node ip-10-0-142-212.us-east-2.compute.internal
Oct 29 21:40:29.211: INFO: Node Info: &Node{ObjectMeta:{ip-10-0-142-212.us-east-2.compute.internal   /api/v1/nodes/ip-10-0-142-212.us-east-2.compute.internal 445eef2f-6888-42a8-af2e-f8f509d046fb 213085 0 2020-10-29 17:30:43 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:m5.xlarge beta.kubernetes.io/os:linux failure-domain.beta.kubernetes.io/region:us-east-2 failure-domain.beta.kubernetes.io/zone:us-east-2a kubernetes.io/arch:amd64 kubernetes.io/hostname:ip-10-0-142-212 kubernetes.io/os:linux node-role.kubernetes.io/worker: node.kubernetes.io/instance-type:m5.xlarge node.openshift.io/os_id:rhcos topology.ebs.csi.aws.com/zone:us-east-2a topology.kubernetes.io/region:us-east-2 topology.kubernetes.io/zone:us-east-2a] map[csi.volume.kubernetes.io/nodeid:{"ebs.csi.aws.com":"i-03781cb9a8092da57"} machine.openshift.io/machine:openshift-machine-api/jeder-461-cncf2-pvrph-worker-us-east-2a-jhsl2 machineconfiguration.openshift.io/currentConfig:rendered-worker-f26c105cafab97113f99101cc57ac048 machineconfiguration.openshift.io/desiredConfig:rendered-worker-f26c105cafab97113f99101cc57ac048 machineconfiguration.openshift.io/reason: machineconfiguration.openshift.io/state:Done volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{nodelink-controller Update v1 2020-10-29 17:30:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machine.openshift.io/machine":{}}}}} {machine-config-controller Update v1 2020-10-29 17:40:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/desiredConfig":{}}}}} {machine-config-daemon Update v1 2020-10-29 17:44:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/currentConfig":{},"f:machineconfiguration.openshift.io/reason":{},"f:machineconfiguration.openshift.io/state":{}}}}} {kube-controller-manager Update v1 2020-10-29 17:44:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{}}},"f:spec":{"f:podCIDR":{},"f:podCIDRs":{".":{},"v:\"10.128.3.0/24\"":{}}}}} {e2e.test Update v1 2020-10-29 21:29:44 +0000 UTC FieldsV1 {"f:status":{"f:capacity":{"f:example.com/fakecpu":{},"f:scheduling.k8s.io/foo":{}}}}} {kubelet Update v1 2020-10-29 21:39:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:csi.volume.kubernetes.io/nodeid":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/instance-type":{},"f:failure-domain.beta.kubernetes.io/region":{},"f:failure-domain.beta.kubernetes.io/zone":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:node-role.kubernetes.io/worker":{},"f:node.kubernetes.io/instance-type":{},"f:node.openshift.io/os_id":{},"f:topology.ebs.csi.aws.com/zone":{},"f:topology.kubernetes.io/region":{},"f:topology.kubernetes.io/zone":{}}},"f:spec":{"f:providerID":{}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalDNS\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:attachable-volumes-aws-ebs":{},"f:cpu":{},"f:ephemeral-storage":{},"f:example.com/fakecpu":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{},"f:scheduling.k8s.io/foo":{}},"f:capacity":{".":{},"f:attachable-volumes-aws-ebs":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"DiskPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"MemoryPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PIDPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:images":{},"f:nodeInfo":{"f:architecture":{},"f:bootID":{},"f:containerRuntimeVersion":{},"f:kernelVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{},"f:machineID":{},"f:operatingSystem":{},"f:osImage":{},"f:systemUUID":{}}}}}]},Spec:NodeSpec{PodCIDR:10.128.3.0/24,DoNotUseExternalID:,ProviderID:aws:///us-east-2a/i-03781cb9a8092da57,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.128.3.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{321557344256 0} {<nil>} 314020844Ki BinarySI},example.com/fakecpu: {{1 3} {<nil>} 1k DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{16326778880 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},scheduling.k8s.io/foo: {{3 0} {<nil>} 3 DecimalSI},},Allocatable:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{3 0} {<nil>} 3 DecimalSI},ephemeral-storage: {{288327867528 0} {<nil>} 288327867528 DecimalSI},example.com/fakecpu: {{1 3} {<nil>} 1k DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{15148179456 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},scheduling.k8s.io/foo: {{3 0} {<nil>} 3 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2020-10-29 21:39:50 +0000 UTC,LastTransitionTime:2020-10-29 17:44:37 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2020-10-29 21:39:50 +0000 UTC,LastTransitionTime:2020-10-29 17:44:37 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2020-10-29 21:39:50 +0000 UTC,LastTransitionTime:2020-10-29 17:44:37 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2020-10-29 21:39:50 +0000 UTC,LastTransitionTime:2020-10-29 17:44:37 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.0.142.212,},NodeAddress{Type:Hostname,Address:ip-10-0-142-212.us-east-2.compute.internal,},NodeAddress{Type:InternalDNS,Address:ip-10-0-142-212.us-east-2.compute.internal,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:ec20bdb61ee45ac6e8e755af80323fd3,SystemUUID:ec20bdb6-1ee4-5ac6-e8e7-55af80323fd3,BootID:31e57073-a38a-44a7-8d88-3091338f4da5,KernelVersion:4.18.0-193.24.1.el8_2.dt1.x86_64,OSImage:Red Hat Enterprise Linux CoreOS 46.82.202010091720-0 (Ootpa),ContainerRuntimeVersion:cri-o://1.19.0-20.rhaos4.6.git97d715e.el8,KubeletVersion:v1.19.0+d59ce34,KubeProxyVersion:v1.19.0+d59ce34,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[quay.io/app-sre/managed-velero-operator-registry@sha256:7d152b07c12c961a67be142cc80597b0616f0db936c34af242e309e6b7be208a quay.io/app-sre/managed-velero-operator-registry:production-0a73057],SizeBytes:1036797635,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:79978a34d1ab3b0ed1ad2c93c09bcb2fcdd1806b35e48a53c99d106347e1a59d quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:769679082,},ContainerImage{Names:[registry.redhat.io/redhat/community-operator-index@sha256:02facc4e56ed383e6cbd19ba740f477f55068948963b18154f66738302f4dd03 registry.redhat.io/redhat/community-operator-index@sha256:467e5d70a7591e82cf325371297c9ee4938e18ab60f42903a01270c0ef8a6a1c registry.redhat.io/redhat/community-operator-index:latest],SizeBytes:509744014,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:82d1def7312de8ae5dee32d237ad59fe685923e78668fa3547e6bee445cd8842 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:499341399,},ContainerImage{Names:[registry.redhat.io/redhat/certified-operator-index@sha256:8e4d14186a6a7a377512dd952fd4b6e8306d669b4e82864c8535021f3e725bd8 registry.redhat.io/redhat/certified-operator-index@sha256:b674fbf172249ff5c27ab195ca05ad14015783e523b421617181a03f9275874f registry.redhat.io/redhat/certified-operator-index:v4.6],SizeBytes:497271693,},ContainerImage{Names:[registry.redhat.io/redhat/redhat-marketplace-index@sha256:1254d42b17fdf685608b3b2f58244a753ce46ce49ad3f5e849a372dfbb901001 registry.redhat.io/redhat/redhat-marketplace-index@sha256:cdd578d52e7fd59b341f35c84f2ccd5681726794bf091d84d06ed66730fd38db registry.redhat.io/redhat/redhat-marketplace-index:v4.6],SizeBytes:492995473,},ContainerImage{Names:[registry.redhat.io/redhat/redhat-operator-index@sha256:20b5fa8d4466c957ee23c97aca064f757f3d7ebb8a1b237d1b1a5e479853a0ca registry.redhat.io/redhat/redhat-operator-index@sha256:bd70797deab3e1e0487d93d488e91418840c6c891d1af2156f66c687638b1308 registry.redhat.io/redhat/redhat-operator-index:v4.6],SizeBytes:491000720,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4cfd55719faff41e96c2e4be69e3f2381a57b8b3445b80ae4acfe8ee33d7f99b quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:481479938,},ContainerImage{Names:[quay.io/jupierce/openshift-python-monitoring@sha256:98e3259170f7116837fa60b0515fe79cb9721060d34eb4931a14244b4658bb8d quay.io/jupierce/openshift-python-monitoring:stable],SizeBytes:474594250,},ContainerImage{Names:[quay.io/app-sre/managed-upgrade-operator-registry@sha256:5d4870d619f5dd35974f957d92d6939a6b56a03c705420ae8602e01e67a10bfc quay.io/app-sre/managed-upgrade-operator-registry:production-9b59534],SizeBytes:472464972,},ContainerImage{Names:[quay.io/app-sre/cloud-ingress-operator-registry@sha256:ce73d98128dc1584568c88d44ba893e924baa93ec8ce0e3b4036f62c01fbd52a quay.io/app-sre/cloud-ingress-operator-registry:production-a2e288b],SizeBytes:470975221,},ContainerImage{Names:[quay.io/app-sre/configure-alertmanager-operator-registry@sha256:11de3d989f814e60482c28ddd72a71d68650a6ae5787289e2eb159aa1af23f01 quay.io/app-sre/configure-alertmanager-operator-registry:production-0a8e5f3],SizeBytes:470905083,},ContainerImage{Names:[quay.io/app-sre/splunk-forwarder-operator-registry@sha256:15c212f82397caa807f5c1e58d47d16bb50d143a886e52cdf09616a50c9e3ef2 quay.io/app-sre/splunk-forwarder-operator-registry:production-7e8bc93],SizeBytes:470516300,},ContainerImage{Names:[quay.io/app-sre/rbac-permissions-operator-registry@sha256:7620af44252bbdbed32316748ccbd165cad0e9320f48ccd63f05a3afc20dc0c2 quay.io/app-sre/rbac-permissions-operator-registry:production-819ec06],SizeBytes:469646922,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2986a09ed686a571312bcb20d648baac46b422efa072f8b68eb41c7996e94610 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:463032763,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8923050603588c27d79b33b371afb651288470d5cdeb14f8e10249bca1a1c461 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:413031192,},ContainerImage{Names:[quay.io/openshift-sre/managed-prometheus-exporter-initcontainer@sha256:7fa36e4488191536ac4a570fdd3431d3c80db983db74b90bc2ce051ecef3035e quay.io/openshift-sre/managed-prometheus-exporter-initcontainer:v0.1.9-2019-03-28-4e558131],SizeBytes:388062834,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8f4882cff3c2f9521215eac681c5abda42876e3e955431c1387fb457940b8344 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:370109724,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:72323ce541f8a26fbad17ef65ff21b51498863bb851635a0faa8d5b1ac6ce0e4 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:366911472,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3d0361f380abf5252b1b640e3ceaaab8274e2af8cdb605b20b513a1a44b3a4dc quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:346080704,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f644e4b495071a2b9d0d6f5d48cb96dad9f7ea8298cc22c98824bc70229ea9dd quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:341116512,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6bada08687c20afe316c1f8cf76f001f31bedae317f256f3df3affaa5d0dc25e quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:336987966,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f42509c18cf5e41201d64cf3a9c1994ffa5318f8d7cee5de45fa2da914e68bbc quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:336588772,},ContainerImage{Names:[image-registry.openshift-image-registry.svc:5000/openshift/cli@sha256:6aa4bb97adf2142b0e74ccae7fd3661ada73cbaac803b86bb8712261e916d66d quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6aa4bb97adf2142b0e74ccae7fd3661ada73cbaac803b86bb8712261e916d66d image-registry.openshift-image-registry.svc:5000/openshift/cli:latest],SizeBytes:336576591,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:52424fd2af6fd7d7a5a1233032eb3f3c67f7691996b209e013e29f1524c5188c quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:326057200,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f86db3170270fc635dff0d7f1ba6e79a8f45de7e1dcfa5621474d1f6e07352ec quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:325370491,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:54fbeb744b82662fd38c0d301ebaad6ca8983707bc44db7235ead0fb7b95808f quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:321482743,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a37a568c63563257309cb0ffb6e185d98f662ff3201d2099cbe0df404b93f0c8 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:314357110,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e176509a104b2f96ee4f5c57275c6a712409aa80ac40071345d0a03bdde2b456 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:307311930,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0c42cd3a74176732e8c06105c47674c7d410c7167c8e3fbd80f9a76e9bfda5bd quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:302591889,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c75977f28becdf4f7065cfa37233464dd31208b1767e620c4f19658f53f8ff8c quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:300756060,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:cf565b2fab365e027962a25a8cffb41aa35cb5a00d001e081d53c7fed5a0c54b quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:300005108,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:78d3478e632c761c18e2dcb55d26e388ecfd126d4fde60317868133dc2fd57f7 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:292832919,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9e7f0468850aeb13585ef049f687cc42c05d82bc0e0200607d1a93d7f9740fe5 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:291621018,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a1aaf99f2ed745c5353d9fc715fa8e9111f42165e3012fad73640c438ba6aa6f quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:290507515,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6cdaecd5dd9df8fd74529be7fa5d8973daf6f4ea95be8acfb2f5ac97773ebe67 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:278303432,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e5bcf6d786fd218e1ef188eb40c39c31a98d03121fba3b7a1f16e87e45a7478b quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:276735847,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1dcc413b621958f97dfbb3fc998a9e225ef155a80ffb151eb4694bf8370b383a quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:274559501,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8c9e61400619c4613db5cc73097d287e3cd5d2125c85d1d84cc30cfdaa1093e7 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:271612142,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:12b11e2000b42ce1aaa228d9c1f4c9177395add2fa43835e667b7fc9007e40e6 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:270734898,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:88ddfbded8bc27b227ed7397ece050b756e522a9ffc34cbfba3c94c5ee58b740 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:269654227,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8bd90fcca7990c0edead15298dcec963968274d299428da95eae41aa23157b90 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:264244297,},ContainerImage{Names:[k8s.gcr.io/etcd@sha256:4ad90a11b55313b182afc186b9876c8e891531b8db4c9bf1541953021618d0e2 k8s.gcr.io/etcd@sha256:bd4d2c9a19be8a492bc79df53eee199fd04b415e9993eb69f7718052602a147a k8s.gcr.io/etcd:3.4.13-0],SizeBytes:254662613,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:73fdcef5de85e739831c5a8b76dce349a3c8832ff416a46263743d7e61655cbb quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:247448489,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b635694dc0663a1404d43a4a9ac8513a3087c7cfead50f6ab413f3c217c40b2a quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:240293385,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:71051bdf1b96c953fc1dfd48359915bf5c027613de6f5e2fa8adeea8d3dda311 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:236515650,},ContainerImage{Names:[quay.io/app-sre/cloud-ingress-operator@sha256:ceb064d013f51cdb15075f079eee2dfd5a0f42f4a09b489176abe92e91fb6a56 quay.io/app-sre/cloud-ingress-operator:v0.1.233-a2e288b],SizeBytes:203838503,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:9e842e2f77073ec30e2349f9082ec35357a3bf61e12488adc677144295e68c38 gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0],SizeBytes:203262845,},ContainerImage{Names:[quay.io/app-sre/managed-upgrade-operator@sha256:558a828a0b3c296cb2c0e52ef143a45177bdae40b1c84bfe8564486c940fd79a quay.io/app-sre/managed-upgrade-operator:9b59534],SizeBytes:198448288,},ContainerImage{Names:[quay.io/app-sre/splunk-forwarder-operator@sha256:bca474387af6112b08406697ef28977d428c1a884ebaf602a3e6ba1b45e6b31b quay.io/app-sre/splunk-forwarder-operator:7e8bc93],SizeBytes:191465195,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Oct 29 21:40:29.212: INFO: 
Logging kubelet events for node ip-10-0-142-212.us-east-2.compute.internal
Oct 29 21:40:29.234: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-142-212.us-east-2.compute.internal
Oct 29 21:40:29.296: INFO: machine-config-daemon-l4r47 started at 2020-10-29 17:30:44 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:40:29.296: INFO: 	Container machine-config-daemon ready: true, restart count 0
Oct 29 21:40:29.296: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:40:29.296: INFO: ovs-66jns started at 2020-10-29 17:30:43 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.296: INFO: 	Container openvswitch ready: true, restart count 0
Oct 29 21:40:29.296: INFO: sdn-lbx27 started at 2020-10-29 17:30:44 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:40:29.296: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Oct 29 21:40:29.296: INFO: 	Container sdn ready: true, restart count 0
Oct 29 21:40:29.296: INFO: alert-pruner-1604005200-gtct6 started at 2020-10-29 21:00:09 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.296: INFO: 	Container alert-pruner ready: false, restart count 0
Oct 29 21:40:29.296: INFO: aws-ebs-csi-driver-node-5gmf4 started at 2020-10-29 17:30:44 +0000 UTC (0+3 container statuses recorded)
Oct 29 21:40:29.296: INFO: 	Container csi-driver ready: true, restart count 0
Oct 29 21:40:29.296: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Oct 29 21:40:29.296: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Oct 29 21:40:29.296: INFO: sre-dns-latency-exporter-29j4j started at 2020-10-29 20:32:47 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.296: INFO: 	Container main ready: true, restart count 0
Oct 29 21:40:29.296: INFO: crd-pruner-1604005200-w76df started at 2020-10-29 21:00:09 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.296: INFO: 	Container crd-pruner ready: false, restart count 0
Oct 29 21:40:29.296: INFO: multus-jst6k started at 2020-10-29 17:30:43 +0000 UTC (5+1 container statuses recorded)
Oct 29 21:40:29.296: INFO: 	Init container multus-binary-copy ready: true, restart count 0
Oct 29 21:40:29.296: INFO: 	Init container cni-plugins ready: true, restart count 0
Oct 29 21:40:29.296: INFO: 	Init container routeoverride-cni ready: true, restart count 0
Oct 29 21:40:29.296: INFO: 	Init container whereabouts-cni-bincopy ready: true, restart count 0
Oct 29 21:40:29.296: INFO: 	Init container whereabouts-cni ready: true, restart count 0
Oct 29 21:40:29.296: INFO: 	Container kube-multus ready: true, restart count 0
Oct 29 21:40:29.296: INFO: tuned-d6fwx started at 2020-10-29 17:30:44 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.296: INFO: 	Container tuned ready: true, restart count 0
Oct 29 21:40:29.296: INFO: dns-default-b89gt started at 2020-10-29 17:30:44 +0000 UTC (0+3 container statuses recorded)
Oct 29 21:40:29.296: INFO: 	Container dns ready: true, restart count 0
Oct 29 21:40:29.296: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 29 21:40:29.296: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:40:29.296: INFO: osd-patch-subscription-source-1604005200-ccz9j started at 2020-10-29 21:00:09 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.296: INFO: 	Container osd-patch-subscription-source ready: false, restart count 0
Oct 29 21:40:29.296: INFO: builds-pruner-1604005200-8wspq started at 2020-10-29 21:00:09 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.296: INFO: 	Container builds-pruner ready: false, restart count 0
Oct 29 21:40:29.296: INFO: node-exporter-p2phw started at 2020-10-29 17:30:44 +0000 UTC (1+2 container statuses recorded)
Oct 29 21:40:29.296: INFO: 	Init container init-textfile ready: true, restart count 0
Oct 29 21:40:29.296: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:40:29.296: INFO: 	Container node-exporter ready: true, restart count 0
Oct 29 21:40:29.296: INFO: node-ca-rq9vr started at 2020-10-29 17:30:44 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.296: INFO: 	Container node-ca ready: true, restart count 0
Oct 29 21:40:29.296: INFO: network-metrics-daemon-xrvxh started at 2020-10-29 17:30:44 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:40:29.296: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:40:29.296: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Oct 29 21:40:29.296: INFO: image-pruner-1604005200-bp5kw started at 2020-10-29 21:00:09 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.296: INFO: 	Container image-pruner ready: false, restart count 0
W1029 21:40:29.318488   53132 metrics_grabber.go:83] Can't find any pods in namespace kube-system to grab metrics from
W1029 21:40:29.318509   53132 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1029 21:40:29.318524   53132 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1029 21:40:29.318530   53132 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Oct 29 21:40:29.395: INFO: 
Latency metrics for node ip-10-0-142-212.us-east-2.compute.internal
Oct 29 21:40:29.395: INFO: 
Logging node info for node ip-10-0-147-237.us-east-2.compute.internal
Oct 29 21:40:29.417: INFO: Node Info: &Node{ObjectMeta:{ip-10-0-147-237.us-east-2.compute.internal   /api/v1/nodes/ip-10-0-147-237.us-east-2.compute.internal aa194f55-2a68-47c3-afba-bc25619fed22 212066 0 2020-10-29 17:24:16 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:m5.xlarge beta.kubernetes.io/os:linux failure-domain.beta.kubernetes.io/region:us-east-2 failure-domain.beta.kubernetes.io/zone:us-east-2a kubernetes.io/arch:amd64 kubernetes.io/hostname:ip-10-0-147-237 kubernetes.io/os:linux node-role.kubernetes.io/master: node.kubernetes.io/instance-type:m5.xlarge node.openshift.io/os_id:rhcos topology.ebs.csi.aws.com/zone:us-east-2a topology.kubernetes.io/region:us-east-2 topology.kubernetes.io/zone:us-east-2a] map[csi.volume.kubernetes.io/nodeid:{"ebs.csi.aws.com":"i-0442e8b0a58ab50b7"} machine.openshift.io/machine:openshift-machine-api/jeder-461-cncf2-pvrph-master-0 machineconfiguration.openshift.io/currentConfig:rendered-master-b06f274cfa558dc0b5031aabca48f868 machineconfiguration.openshift.io/desiredConfig:rendered-master-b06f274cfa558dc0b5031aabca48f868 machineconfiguration.openshift.io/reason: machineconfiguration.openshift.io/state:Done volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{nodelink-controller Update v1 2020-10-29 17:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machine.openshift.io/machine":{}}}}} {machine-config-controller Update v1 2020-10-29 17:48:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/desiredConfig":{}}}}} {machine-config-daemon Update v1 2020-10-29 17:52:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/currentConfig":{},"f:machineconfiguration.openshift.io/reason":{},"f:machineconfiguration.openshift.io/state":{}}}}} {kube-controller-manager Update v1 2020-10-29 19:08:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{}}},"f:spec":{"f:podCIDR":{},"f:podCIDRs":{".":{},"v:\"10.128.2.0/24\"":{}},"f:taints":{}}}} {oc Update v1 2020-10-29 19:08:28 +0000 UTC FieldsV1 {"f:spec":{"f:unschedulable":{}}}} {kubelet Update v1 2020-10-29 21:37:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:csi.volume.kubernetes.io/nodeid":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/instance-type":{},"f:failure-domain.beta.kubernetes.io/region":{},"f:failure-domain.beta.kubernetes.io/zone":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:node-role.kubernetes.io/master":{},"f:node.kubernetes.io/instance-type":{},"f:node.openshift.io/os_id":{},"f:topology.ebs.csi.aws.com/zone":{},"f:topology.kubernetes.io/region":{},"f:topology.kubernetes.io/zone":{}}},"f:spec":{"f:providerID":{}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalDNS\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:attachable-volumes-aws-ebs":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:attachable-volumes-aws-ebs":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"DiskPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"MemoryPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PIDPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:images":{},"f:nodeInfo":{"f:architecture":{},"f:bootID":{},"f:containerRuntimeVersion":{},"f:kernelVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{},"f:machineID":{},"f:operatingSystem":{},"f:osImage":{},"f:systemUUID":{}}}}}]},Spec:NodeSpec{PodCIDR:10.128.2.0/24,DoNotUseExternalID:,ProviderID:aws:///us-east-2a/i-0442e8b0a58ab50b7,Unschedulable:true,Taints:[]Taint{Taint{Key:node-role.kubernetes.io/master,Value:,Effect:NoSchedule,TimeAdded:<nil>,},Taint{Key:node.kubernetes.io/unschedulable,Value:,Effect:NoSchedule,TimeAdded:2020-10-29 19:08:28 +0000 UTC,},},ConfigSource:nil,PodCIDRs:[10.128.2.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{375244435456 0} {<nil>} 366449644Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{16502939648 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},},Allocatable:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{3 0} {<nil>} 3 DecimalSI},ephemeral-storage: {{336646249528 0} {<nil>} 336646249528 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{15324340224 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2020-10-29 21:37:19 +0000 UTC,LastTransitionTime:2020-10-29 17:52:01 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2020-10-29 21:37:19 +0000 UTC,LastTransitionTime:2020-10-29 17:52:01 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2020-10-29 21:37:19 +0000 UTC,LastTransitionTime:2020-10-29 17:52:01 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2020-10-29 21:37:19 +0000 UTC,LastTransitionTime:2020-10-29 17:52:01 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.0.147.237,},NodeAddress{Type:Hostname,Address:ip-10-0-147-237.us-east-2.compute.internal,},NodeAddress{Type:InternalDNS,Address:ip-10-0-147-237.us-east-2.compute.internal,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:ec208baaeac2115a33465684b95070b7,SystemUUID:ec208baa-eac2-115a-3346-5684b95070b7,BootID:393f7083-012b-49c7-b353-7f3ef17ffba3,KernelVersion:4.18.0-193.24.1.el8_2.dt1.x86_64,OSImage:Red Hat Enterprise Linux CoreOS 46.82.202010091720-0 (Ootpa),ContainerRuntimeVersion:cri-o://1.19.0-20.rhaos4.6.git97d715e.el8,KubeletVersion:v1.19.0+d59ce34,KubeProxyVersion:v1.19.0+d59ce34,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:30094a2d586aa282d85e14f1be19abec1c30ce431673377b0e1c12d83e6bac8c quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:682301224,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c51d44f380ecff7d36b1de4bb3bdbd4ac66abc6669724f28d81bd4af5741a8ac quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:514079422,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:82d1def7312de8ae5dee32d237ad59fe685923e78668fa3547e6bee445cd8842 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:499341399,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4cfd55719faff41e96c2e4be69e3f2381a57b8b3445b80ae4acfe8ee33d7f99b quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:481479938,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ae6a5decd040a6b3adfa074d3211ab92a36b77b2d849962d9a678e1c2c5ef5c1 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:473291773,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8923050603588c27d79b33b371afb651288470d5cdeb14f8e10249bca1a1c461 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:413031192,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8f4882cff3c2f9521215eac681c5abda42876e3e955431c1387fb457940b8344 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:370109724,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:72323ce541f8a26fbad17ef65ff21b51498863bb851635a0faa8d5b1ac6ce0e4 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:366911472,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b7dc5f4101a8cb88c20d853908982258cab77bb0ac391e965b50b15648ddd854 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:359851126,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:93b3e1246884e357e1654e6c9578481aff9eef07eed1f9fdd0e9c8cc89a3770c quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:358141917,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e2b3f973bc5b9e55d2240a556c4648c921a3c8d3e12381757f1990a864208617 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:343841040,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a8214df42df962b965e3f4daad0b61932235e57241160861e503d84e38b775d5 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:337507384,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6bada08687c20afe316c1f8cf76f001f31bedae317f256f3df3affaa5d0dc25e quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:336987966,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6aa4bb97adf2142b0e74ccae7fd3661ada73cbaac803b86bb8712261e916d66d quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:336576591,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:aa4f37543b45bc248db8d9bd2dc45b6e159a8869b044c2310f541afba15b2694 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:336485164,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f52825e9905c926d399cd0b7afbb2b7d0370ae22da0416feac9131d555db0b98 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:336041278,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a2a167f59783ca402118fe35ea5fefbf457e01b64836f8be3be6695aefd76d76 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:335654632,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:059f0179c0528c6234dbdca7e70fe779cf37be5121f458dd045d2e9662192f06 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:335510757,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:5e591cab19b41c7ea26eab6056cd518f6d64b59e8051978de927b1b984abfb1d quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:334612215,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:7584014b0cb8cb2c5a09b909c79f2f8ad6e49854bcfabf72e96a22330bcf6f56 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:333011308,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:357e35286fd26fed015c03a9c451f6fdcf61cf0821d959025e7f800e7c533f29 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:332777114,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6ca671c810426b8c4f13dd0c7ac19639f9f265b952b8feb5a828e59fab785335 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:332487959,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f86db3170270fc635dff0d7f1ba6e79a8f45de7e1dcfa5621474d1f6e07352ec quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:325370491,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:54fbeb744b82662fd38c0d301ebaad6ca8983707bc44db7235ead0fb7b95808f quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:321482743,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:11c3a8bdddbbb2229bd68bb80b6009588873118881952c702dfebd1484046191 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:319944375,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9dbb31ac799b2c30270268714dcb3d11bafb329b98639a446657c8c7db41938c quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:318368538,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1ad85195e1a180698fe4b8df82e3d72075efb256b53f593d13e29faaf7f3e15a quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:316310728,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dc6a6a1d4a6b2af67421561e53d1af1d40c99ae72de69f4c3cc390d447f12269 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:315339691,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4691dc29704c9cb06d2345894f1a8f074b58a0d208318c5218241388b0916e1b quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:315327750,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f7b9278ef2fbe988f50e4bdeeea79d9373b55689d17b8c6d7c214429f5b3f9a0 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:315119766,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4289297f0b7ee7edf394348fd07e1fa1b3162655f2a2af2245e23af4b179e7f2 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:314343750,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:65206861218064576dc092040e9c24b0393b8a07502e351f513400f187f38cc7 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:313489720,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:45586fd7a5cfd43ff546dbfb282a70a91eaf0f069f604230af958dc802832f89 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:312615681,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c6b3aaaa38679b1d752ec09bd68c6d80a8911c74ec16d27c49de88ecb97823ee quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:312540521,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:01250de496444bb624ec7b472ac9b0f7023809c88306a71c6ac87bb302f7dbe3 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:312449767,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:25197b2709c0691c28424c9b07e505a71d13bf481e18bc42636cc84ee8fef033 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:312275620,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ee4abe53e80e561239e510a6f9999b4dc80b7b3fdc9848ab43d0bf8df24e815d quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:311984477,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:26f6c930942ee4dea7c1e22d220bba11561c37bdc47101c4490ce0ef77c9203a quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:311353697,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2689d3e66bbfdd7d493d969524b8a7da00142d1b4372e3e880bd825beb3da558 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:311351002,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9b564f882e31f497f57a0d99d406d5231eb15e9a97f0b450c21bec2bac7ff033 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:311323012,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a96e2e4a62bca22da0b6903c9e20d7c776bd241f13accf51ede88965b232aca8 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:310235384,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0531ff2ccf0ddea76e42cc9951470528bbd7def094884bc569f660376798f40a quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:308198752,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:12fe384de71c7621d9061f48afafeed3dc337679a66afd8d0a871e200295a1e5 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:307996829,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:82758fbc97d9da98f20eddcfb4a8bc279726b97da96263d4c165b404389cb553 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:307966006,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e176509a104b2f96ee4f5c57275c6a712409aa80ac40071345d0a03bdde2b456 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:307311930,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f18151bf70434e1841ed8182c42e819e92e3d1ad3bbd269c667be8b74ff78444 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:306621135,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:afdf0a3b426ac1c03df52e88a2b884f0714e54a1a03f33091954441a05a7f6b9 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:306116297,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:076b280e17c6bb4cc618db71403ccec75f8196c8849061a40c680a2808292bb6 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:304641618,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:01626d98c80e44e0cd3a522ad019eb236e39c30b0dfff0ac5a6fa98686159286 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:303742706,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:cfe62d81269929501517e75a7d337f7d8fc78ac9a17665adebfef52a2024584d quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:303388513,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Oct 29 21:40:29.418: INFO: 
Logging kubelet events for node ip-10-0-147-237.us-east-2.compute.internal
Oct 29 21:40:29.440: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-147-237.us-east-2.compute.internal
Oct 29 21:40:29.507: INFO: tuned-ffzjj started at 2020-10-29 17:24:46 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.507: INFO: 	Container tuned ready: true, restart count 0
Oct 29 21:40:29.507: INFO: revision-pruner-7-ip-10-0-147-237.us-east-2.compute.internal started at 2020-10-29 17:52:01 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.507: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:40:29.507: INFO: apiserver-8b58dffb-5kptd started at 2020-10-29 17:52:23 +0000 UTC (1+2 container statuses recorded)
Oct 29 21:40:29.507: INFO: 	Init container fix-audit-permissions ready: true, restart count 0
Oct 29 21:40:29.507: INFO: 	Container openshift-apiserver ready: true, restart count 0
Oct 29 21:40:29.507: INFO: 	Container openshift-apiserver-check-endpoints ready: true, restart count 0
Oct 29 21:40:29.507: INFO: apiserver-566bbccb57-8vqrg started at 2020-10-29 17:52:23 +0000 UTC (1+1 container statuses recorded)
Oct 29 21:40:29.507: INFO: 	Init container fix-audit-permissions ready: true, restart count 0
Oct 29 21:40:29.507: INFO: 	Container oauth-apiserver ready: true, restart count 0
Oct 29 21:40:29.507: INFO: revision-pruner-10-ip-10-0-147-237.us-east-2.compute.internal started at 2020-10-29 18:09:04 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.507: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:40:29.507: INFO: openshift-kube-scheduler-ip-10-0-147-237.us-east-2.compute.internal started at 2020-10-29 17:26:22 +0000 UTC (1+2 container statuses recorded)
Oct 29 21:40:29.507: INFO: 	Init container wait-for-host-port ready: true, restart count 0
Oct 29 21:40:29.508: INFO: 	Container kube-scheduler ready: true, restart count 0
Oct 29 21:40:29.508: INFO: 	Container kube-scheduler-cert-syncer ready: true, restart count 0
Oct 29 21:40:29.508: INFO: revision-pruner-11-ip-10-0-147-237.us-east-2.compute.internal started at 2020-10-29 19:56:51 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.508: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:40:29.508: INFO: sdn-hpdmg started at 2020-10-29 17:24:20 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:40:29.508: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Oct 29 21:40:29.508: INFO: 	Container sdn ready: true, restart count 0
Oct 29 21:40:29.508: INFO: multus-75rx8 started at 2020-10-29 17:24:20 +0000 UTC (5+1 container statuses recorded)
Oct 29 21:40:29.508: INFO: 	Init container multus-binary-copy ready: true, restart count 0
Oct 29 21:40:29.508: INFO: 	Init container cni-plugins ready: true, restart count 0
Oct 29 21:40:29.508: INFO: 	Init container routeoverride-cni ready: true, restart count 0
Oct 29 21:40:29.508: INFO: 	Init container whereabouts-cni-bincopy ready: true, restart count 0
Oct 29 21:40:29.508: INFO: 	Init container whereabouts-cni ready: true, restart count 0
Oct 29 21:40:29.508: INFO: 	Container kube-multus ready: true, restart count 0
Oct 29 21:40:29.508: INFO: ovs-9xsrf started at 2020-10-29 17:24:20 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.508: INFO: 	Container openvswitch ready: true, restart count 0
Oct 29 21:40:29.509: INFO: validation-webhook-bd6dg started at 2020-10-29 17:39:26 +0000 UTC (1+1 container statuses recorded)
Oct 29 21:40:29.509: INFO: 	Init container inject-cert ready: true, restart count 0
Oct 29 21:40:29.509: INFO: 	Container webhooks ready: true, restart count 0
Oct 29 21:40:29.509: INFO: revision-pruner-11-ip-10-0-147-237.us-east-2.compute.internal started at 2020-10-29 19:49:13 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.509: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:40:29.509: INFO: installer-9-ip-10-0-147-237.us-east-2.compute.internal started at 2020-10-29 19:45:19 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.509: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:40:29.509: INFO: installer-10-ip-10-0-147-237.us-east-2.compute.internal started at 2020-10-29 18:07:12 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.509: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:40:29.509: INFO: oauth-openshift-6dc6d68fbf-d652r started at 2020-10-29 18:36:40 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.509: INFO: 	Container oauth-openshift ready: true, restart count 0
Oct 29 21:40:29.509: INFO: network-metrics-daemon-v66fq started at 2020-10-29 17:24:20 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:40:29.509: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:40:29.509: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Oct 29 21:40:29.509: INFO: machine-config-server-6gqlt started at 2020-10-29 17:26:37 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.509: INFO: 	Container machine-config-server ready: true, restart count 0
Oct 29 21:40:29.509: INFO: installer-11-ip-10-0-147-237.us-east-2.compute.internal started at 2020-10-29 19:53:13 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.509: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:40:29.510: INFO: machine-config-daemon-nkm27 started at 2020-10-29 17:24:37 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:40:29.510: INFO: 	Container machine-config-daemon ready: true, restart count 0
Oct 29 21:40:29.510: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:40:29.510: INFO: node-exporter-nsn4k started at 2020-10-29 17:24:51 +0000 UTC (1+2 container statuses recorded)
Oct 29 21:40:29.510: INFO: 	Init container init-textfile ready: true, restart count 0
Oct 29 21:40:29.510: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:40:29.510: INFO: 	Container node-exporter ready: true, restart count 0
Oct 29 21:40:29.510: INFO: installer-9-ip-10-0-147-237.us-east-2.compute.internal started at 2020-10-29 17:52:01 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.510: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:40:29.510: INFO: revision-pruner-9-ip-10-0-147-237.us-east-2.compute.internal started at 2020-10-29 19:45:50 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.510: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:40:29.510: INFO: multus-admission-controller-8gpns started at 2020-10-29 17:25:06 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:40:29.510: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:40:29.510: INFO: 	Container multus-admission-controller ready: true, restart count 0
Oct 29 21:40:29.510: INFO: node-ca-crbk4 started at 2020-10-29 17:25:50 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.510: INFO: 	Container node-ca ready: true, restart count 0
Oct 29 21:40:29.510: INFO: controller-manager-wm9j7 started at 2020-10-29 17:30:59 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.510: INFO: 	Container controller-manager ready: true, restart count 0
Oct 29 21:40:29.511: INFO: etcd-quorum-guard-644f5747b8-4fzdq started at 2020-10-29 17:52:23 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.511: INFO: 	Container guard ready: true, restart count 0
Oct 29 21:40:29.511: INFO: aws-ebs-csi-driver-node-dbdmq started at 2020-10-29 17:24:58 +0000 UTC (0+3 container statuses recorded)
Oct 29 21:40:29.511: INFO: 	Container csi-driver ready: true, restart count 0
Oct 29 21:40:29.511: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Oct 29 21:40:29.511: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Oct 29 21:40:29.511: INFO: revision-pruner-9-ip-10-0-147-237.us-east-2.compute.internal started at 2020-10-29 17:52:01 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.511: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:40:29.511: INFO: recyler-pod-ip-10-0-147-237.us-east-2.compute.internal started at 2020-10-29 17:52:01 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.511: INFO: 	Container recyler-container ready: false, restart count 0
Oct 29 21:40:29.511: INFO: installer-11-ip-10-0-147-237.us-east-2.compute.internal started at 2020-10-29 19:48:02 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.511: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:40:29.511: INFO: kube-apiserver-ip-10-0-147-237.us-east-2.compute.internal started at 2020-10-29 17:28:31 +0000 UTC (1+5 container statuses recorded)
Oct 29 21:40:29.511: INFO: 	Init container setup ready: true, restart count 1
Oct 29 21:40:29.511: INFO: 	Container kube-apiserver ready: true, restart count 0
Oct 29 21:40:29.511: INFO: 	Container kube-apiserver-cert-regeneration-controller ready: true, restart count 0
Oct 29 21:40:29.511: INFO: 	Container kube-apiserver-cert-syncer ready: true, restart count 0
Oct 29 21:40:29.511: INFO: 	Container kube-apiserver-check-endpoints ready: true, restart count 0
Oct 29 21:40:29.512: INFO: 	Container kube-apiserver-insecure-readyz ready: true, restart count 0
Oct 29 21:40:29.512: INFO: sdn-controller-h64cq started at 2020-10-29 17:24:20 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.512: INFO: 	Container sdn-controller ready: true, restart count 0
Oct 29 21:40:29.512: INFO: etcd-ip-10-0-147-237.us-east-2.compute.internal started at 2020-10-29 17:27:04 +0000 UTC (2+3 container statuses recorded)
Oct 29 21:40:29.512: INFO: 	Init container etcd-ensure-env-vars ready: true, restart count 0
Oct 29 21:40:29.512: INFO: 	Init container etcd-resources-copy ready: true, restart count 0
Oct 29 21:40:29.512: INFO: 	Container etcd ready: true, restart count 0
Oct 29 21:40:29.512: INFO: 	Container etcd-metrics ready: true, restart count 0
Oct 29 21:40:29.512: INFO: 	Container etcdctl ready: true, restart count 0
Oct 29 21:40:29.512: INFO: revision-pruner-9-ip-10-0-147-237.us-east-2.compute.internal started at 2020-10-29 17:52:46 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.512: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:40:29.512: INFO: dns-default-5hsz9 started at 2020-10-29 17:25:40 +0000 UTC (0+3 container statuses recorded)
Oct 29 21:40:29.512: INFO: 	Container dns ready: true, restart count 0
Oct 29 21:40:29.512: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 29 21:40:29.512: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:40:29.512: INFO: revision-pruner-8-ip-10-0-147-237.us-east-2.compute.internal started at 2020-10-29 17:52:01 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.512: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:40:29.512: INFO: revision-pruner-3-ip-10-0-147-237.us-east-2.compute.internal started at 2020-10-29 17:52:01 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.512: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:40:29.513: INFO: installer-10-ip-10-0-147-237.us-east-2.compute.internal started at 2020-10-29 19:47:16 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.513: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:40:29.513: INFO: kube-controller-manager-ip-10-0-147-237.us-east-2.compute.internal started at 2020-10-29 17:25:56 +0000 UTC (0+4 container statuses recorded)
Oct 29 21:40:29.513: INFO: 	Container cluster-policy-controller ready: true, restart count 0
Oct 29 21:40:29.513: INFO: 	Container kube-controller-manager ready: true, restart count 0
Oct 29 21:40:29.513: INFO: 	Container kube-controller-manager-cert-syncer ready: true, restart count 0
Oct 29 21:40:29.513: INFO: 	Container kube-controller-manager-recovery-controller ready: true, restart count 1
Oct 29 21:40:29.513: INFO: revision-pruner-10-ip-10-0-147-237.us-east-2.compute.internal started at 2020-10-29 19:47:46 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.513: INFO: 	Container pruner ready: false, restart count 0
W1029 21:40:29.535272   53132 metrics_grabber.go:83] Can't find any pods in namespace kube-system to grab metrics from
W1029 21:40:29.535291   53132 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1029 21:40:29.535298   53132 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1029 21:40:29.535303   53132 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Oct 29 21:40:29.622: INFO: 
Latency metrics for node ip-10-0-147-237.us-east-2.compute.internal
Oct 29 21:40:29.622: INFO: 
Logging node info for node ip-10-0-157-225.us-east-2.compute.internal
Oct 29 21:40:29.643: INFO: Node Info: &Node{ObjectMeta:{ip-10-0-157-225.us-east-2.compute.internal   /api/v1/nodes/ip-10-0-157-225.us-east-2.compute.internal 4b6ca834-58eb-4f4d-aff2-e969afb1fe22 213114 0 2020-10-29 17:41:12 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:m5.xlarge beta.kubernetes.io/os:linux failure-domain.beta.kubernetes.io/region:us-east-2 failure-domain.beta.kubernetes.io/zone:us-east-2a kubernetes.io/arch:amd64 kubernetes.io/hostname:ip-10-0-157-225 kubernetes.io/os:linux node-role.kubernetes.io:infra node-role.kubernetes.io/infra: node-role.kubernetes.io/worker: node.kubernetes.io/instance-type:m5.xlarge node.openshift.io/os_id:rhcos topology.ebs.csi.aws.com/zone:us-east-2a topology.kubernetes.io/region:us-east-2 topology.kubernetes.io/zone:us-east-2a] map[csi.volume.kubernetes.io/nodeid:{"ebs.csi.aws.com":"i-0b87fa50e817e6ce3"} machine.openshift.io/machine:openshift-machine-api/jeder-461-cncf2-pvrph-infra-us-east-2a-m2hff machineconfiguration.openshift.io/currentConfig:rendered-worker-f26c105cafab97113f99101cc57ac048 machineconfiguration.openshift.io/desiredConfig:rendered-worker-f26c105cafab97113f99101cc57ac048 machineconfiguration.openshift.io/reason: machineconfiguration.openshift.io/state:Done volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{nodelink-controller Update v1 2020-10-29 17:41:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machine.openshift.io/machine":{}},"f:labels":{"f:node-role.kubernetes.io":{},"f:node-role.kubernetes.io/infra":{}}}}} {machine-config-controller Update v1 2020-10-29 17:46:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/desiredConfig":{}}}}} {machine-config-daemon Update v1 2020-10-29 17:48:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/currentConfig":{},"f:machineconfiguration.openshift.io/reason":{},"f:machineconfiguration.openshift.io/state":{}}}}} {kube-controller-manager Update v1 2020-10-29 19:47:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{}}},"f:spec":{"f:podCIDR":{},"f:podCIDRs":{".":{},"v:\"10.128.5.0/24\"":{}},"f:taints":{}},"f:status":{"f:volumesAttached":{}}}} {e2e.test Update v1 2020-10-29 21:29:44 +0000 UTC FieldsV1 {"f:status":{"f:capacity":{"f:scheduling.k8s.io/foo":{}}}}} {kubelet Update v1 2020-10-29 21:39:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:csi.volume.kubernetes.io/nodeid":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/instance-type":{},"f:failure-domain.beta.kubernetes.io/region":{},"f:failure-domain.beta.kubernetes.io/zone":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:node-role.kubernetes.io/worker":{},"f:node.kubernetes.io/instance-type":{},"f:node.openshift.io/os_id":{},"f:topology.ebs.csi.aws.com/zone":{},"f:topology.kubernetes.io/region":{},"f:topology.kubernetes.io/zone":{}}},"f:spec":{"f:providerID":{}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalDNS\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:attachable-volumes-aws-ebs":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{},"f:scheduling.k8s.io/foo":{}},"f:capacity":{".":{},"f:attachable-volumes-aws-ebs":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"DiskPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"MemoryPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PIDPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:images":{},"f:nodeInfo":{"f:architecture":{},"f:bootID":{},"f:containerRuntimeVersion":{},"f:kernelVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{},"f:machineID":{},"f:operatingSystem":{},"f:osImage":{},"f:systemUUID":{}},"f:volumesInUse":{}}}}]},Spec:NodeSpec{PodCIDR:10.128.5.0/24,DoNotUseExternalID:,ProviderID:aws:///us-east-2a/i-0b87fa50e817e6ce3,Unschedulable:false,Taints:[]Taint{Taint{Key:node-role.kubernetes.io/infra,Value:,Effect:PreferNoSchedule,TimeAdded:<nil>,},},ConfigSource:nil,PodCIDRs:[10.128.5.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{321557344256 0} {<nil>} 314020844Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{16502939648 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},scheduling.k8s.io/foo: {{3 0} {<nil>} 3 DecimalSI},},Allocatable:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{3 0} {<nil>} 3 DecimalSI},ephemeral-storage: {{288327867528 0} {<nil>} 288327867528 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{15324340224 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},scheduling.k8s.io/foo: {{3 0} {<nil>} 3 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2020-10-29 21:39:51 +0000 UTC,LastTransitionTime:2020-10-29 17:48:32 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2020-10-29 21:39:51 +0000 UTC,LastTransitionTime:2020-10-29 17:48:32 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2020-10-29 21:39:51 +0000 UTC,LastTransitionTime:2020-10-29 17:48:32 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2020-10-29 21:39:51 +0000 UTC,LastTransitionTime:2020-10-29 17:48:32 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.0.157.225,},NodeAddress{Type:Hostname,Address:ip-10-0-157-225.us-east-2.compute.internal,},NodeAddress{Type:InternalDNS,Address:ip-10-0-157-225.us-east-2.compute.internal,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:ec2d145ac9adc52455b78a8130a251d9,SystemUUID:ec2d145a-c9ad-c524-55b7-8a8130a251d9,BootID:00446280-a220-46d9-9e85-83069f21255e,KernelVersion:4.18.0-193.24.1.el8_2.dt1.x86_64,OSImage:Red Hat Enterprise Linux CoreOS 46.82.202010091720-0 (Ootpa),ContainerRuntimeVersion:cri-o://1.19.0-20.rhaos4.6.git97d715e.el8,KubeletVersion:v1.19.0+d59ce34,KubeProxyVersion:v1.19.0+d59ce34,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:82d1def7312de8ae5dee32d237ad59fe685923e78668fa3547e6bee445cd8842 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:499341399,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4cfd55719faff41e96c2e4be69e3f2381a57b8b3445b80ae4acfe8ee33d7f99b quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:481479938,},ContainerImage{Names:[quay.io/jupierce/openshift-python-monitoring@sha256:98e3259170f7116837fa60b0515fe79cb9721060d34eb4931a14244b4658bb8d quay.io/jupierce/openshift-python-monitoring:stable],SizeBytes:474594250,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8248dec0d94b2928aa4d63a22973d9a8f8f173a1431b2ab4ad15fdfe80283d7c quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:419419150,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8923050603588c27d79b33b371afb651288470d5cdeb14f8e10249bca1a1c461 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:413031192,},ContainerImage{Names:[quay.io/openshift-sre/managed-prometheus-exporter-initcontainer@sha256:7fa36e4488191536ac4a570fdd3431d3c80db983db74b90bc2ce051ecef3035e quay.io/openshift-sre/managed-prometheus-exporter-initcontainer:v0.1.9-2019-03-28-4e558131],SizeBytes:388062834,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8f4882cff3c2f9521215eac681c5abda42876e3e955431c1387fb457940b8344 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:370109724,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:72323ce541f8a26fbad17ef65ff21b51498863bb851635a0faa8d5b1ac6ce0e4 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:366911472,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3d0361f380abf5252b1b640e3ceaaab8274e2af8cdb605b20b513a1a44b3a4dc quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:346080704,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f644e4b495071a2b9d0d6f5d48cb96dad9f7ea8298cc22c98824bc70229ea9dd quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:341116512,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6bada08687c20afe316c1f8cf76f001f31bedae317f256f3df3affaa5d0dc25e quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:336987966,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6aa4bb97adf2142b0e74ccae7fd3661ada73cbaac803b86bb8712261e916d66d quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:336576591,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:52424fd2af6fd7d7a5a1233032eb3f3c67f7691996b209e013e29f1524c5188c quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:326057200,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f86db3170270fc635dff0d7f1ba6e79a8f45de7e1dcfa5621474d1f6e07352ec quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:325370491,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:54fbeb744b82662fd38c0d301ebaad6ca8983707bc44db7235ead0fb7b95808f quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:321482743,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a37a568c63563257309cb0ffb6e185d98f662ff3201d2099cbe0df404b93f0c8 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:314357110,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:26f6c930942ee4dea7c1e22d220bba11561c37bdc47101c4490ce0ef77c9203a quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:311353697,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e176509a104b2f96ee4f5c57275c6a712409aa80ac40071345d0a03bdde2b456 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:307311930,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0c42cd3a74176732e8c06105c47674c7d410c7167c8e3fbd80f9a76e9bfda5bd quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:302591889,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c75977f28becdf4f7065cfa37233464dd31208b1767e620c4f19658f53f8ff8c quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:300756060,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:cf565b2fab365e027962a25a8cffb41aa35cb5a00d001e081d53c7fed5a0c54b quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:300005108,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9e7f0468850aeb13585ef049f687cc42c05d82bc0e0200607d1a93d7f9740fe5 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:291621018,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6cdaecd5dd9df8fd74529be7fa5d8973daf6f4ea95be8acfb2f5ac97773ebe67 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:278303432,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e5bcf6d786fd218e1ef188eb40c39c31a98d03121fba3b7a1f16e87e45a7478b quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:276735847,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1dcc413b621958f97dfbb3fc998a9e225ef155a80ffb151eb4694bf8370b383a quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:274559501,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8c9e61400619c4613db5cc73097d287e3cd5d2125c85d1d84cc30cfdaa1093e7 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:271612142,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:12b11e2000b42ce1aaa228d9c1f4c9177395add2fa43835e667b7fc9007e40e6 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:270734898,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:88ddfbded8bc27b227ed7397ece050b756e522a9ffc34cbfba3c94c5ee58b740 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:269654227,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8bd90fcca7990c0edead15298dcec963968274d299428da95eae41aa23157b90 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:264244297,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:73fdcef5de85e739831c5a8b76dce349a3c8832ff416a46263743d7e61655cbb quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:247448489,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b635694dc0663a1404d43a4a9ac8513a3087c7cfead50f6ab413f3c217c40b2a quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:240293385,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:71051bdf1b96c953fc1dfd48359915bf5c027613de6f5e2fa8adeea8d3dda311 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:236515650,},ContainerImage{Names:[docker.io/velero/velero@sha256:1e7b1f13e4e807952f069295f19686c4eb9380e59013771d8c1960fc4fa73634 docker.io/velero/velero@sha256:e248548c6787f5451e1ba42a48ad4890ce1fc10aa6f5efca9aeb3aa6cda396ff docker.io/velero/velero:v1.5.2],SizeBytes:165235519,},ContainerImage{Names:[docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4 docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:128894988,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0 k8s.gcr.io/e2e-test-images/agnhost@sha256:c243d6fc39291ac6ea9caf6b511719980d9c8d2e54dda8217ee019fbf9a124e1 k8s.gcr.io/e2e-test-images/agnhost:2.20],SizeBytes:116455133,},ContainerImage{Names:[docker.io/velero/velero-plugin-for-aws@sha256:e52d3545c3c52dbd061f0bf2ae8f7d6b21747d0a8bc64245fb58c8de54df9b33 docker.io/velero/velero-plugin-for-aws:v1.1.0],SizeBytes:112580300,},ContainerImage{Names:[k8s.gcr.io/pause@sha256:4a1c4b21597c1b4415bdbecb28a3296c6b5e23ca4f9feeb599860a1dac6a0108 k8s.gcr.io/pause@sha256:927d98197ec1141a368550822d18fa1c60bdae27b78b0c004f705f548c07814f k8s.gcr.io/pause:3.2],SizeBytes:688049,},},VolumesInUse:[kubernetes.io/aws-ebs/aws://us-east-2a/vol-043da107373ed8f17 kubernetes.io/aws-ebs/aws://us-east-2a/vol-06f1085fc292fe55d kubernetes.io/aws-ebs/aws://us-east-2a/vol-07e1e4fd424670210 kubernetes.io/aws-ebs/aws://us-east-2a/vol-08e61d2d126b801f3 kubernetes.io/aws-ebs/aws://us-east-2a/vol-0922ddac45c4a6428],VolumesAttached:[]AttachedVolume{AttachedVolume{Name:kubernetes.io/aws-ebs/aws://us-east-2a/vol-06f1085fc292fe55d,DevicePath:/dev/xvdch,},AttachedVolume{Name:kubernetes.io/aws-ebs/aws://us-east-2a/vol-08e61d2d126b801f3,DevicePath:/dev/xvdbn,},AttachedVolume{Name:kubernetes.io/aws-ebs/aws://us-east-2a/vol-07e1e4fd424670210,DevicePath:/dev/xvdbu,},AttachedVolume{Name:kubernetes.io/aws-ebs/aws://us-east-2a/vol-043da107373ed8f17,DevicePath:/dev/xvdbi,},AttachedVolume{Name:kubernetes.io/aws-ebs/aws://us-east-2a/vol-0922ddac45c4a6428,DevicePath:/dev/xvdcv,},},Config:nil,},}
Oct 29 21:40:29.644: INFO: 
Logging kubelet events for node ip-10-0-157-225.us-east-2.compute.internal
Oct 29 21:40:29.666: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-157-225.us-east-2.compute.internal
Oct 29 21:40:29.707: INFO: aws-ebs-csi-driver-node-qvwrc started at 2020-10-29 17:41:11 +0000 UTC (0+3 container statuses recorded)
Oct 29 21:40:29.707: INFO: 	Container csi-driver ready: true, restart count 0
Oct 29 21:40:29.707: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Oct 29 21:40:29.707: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Oct 29 21:40:29.707: INFO: tuned-4tm5g started at 2020-10-29 17:41:11 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.707: INFO: 	Container tuned ready: true, restart count 0
Oct 29 21:40:29.707: INFO: dns-default-hr5xr started at 2020-10-29 17:41:12 +0000 UTC (0+3 container statuses recorded)
Oct 29 21:40:29.707: INFO: 	Container dns ready: true, restart count 0
Oct 29 21:40:29.707: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 29 21:40:29.707: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:40:29.707: INFO: image-registry-75b75c555d-cm6gf started at 2020-10-29 17:48:47 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.707: INFO: 	Container registry ready: true, restart count 0
Oct 29 21:40:29.707: INFO: telemeter-client-577bcc4f8d-jv4m8 started at 2020-10-29 17:48:47 +0000 UTC (0+3 container statuses recorded)
Oct 29 21:40:29.707: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:40:29.707: INFO: 	Container reload ready: true, restart count 0
Oct 29 21:40:29.707: INFO: 	Container telemeter-client ready: true, restart count 0
Oct 29 21:40:29.707: INFO: prometheus-adapter-6495ff5885-lw9vm started at 2020-10-29 17:48:48 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.707: INFO: 	Container prometheus-adapter ready: true, restart count 0
Oct 29 21:40:29.707: INFO: alertmanager-main-0 started at 2020-10-29 17:49:01 +0000 UTC (0+5 container statuses recorded)
Oct 29 21:40:29.707: INFO: 	Container alertmanager ready: true, restart count 0
Oct 29 21:40:29.707: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Oct 29 21:40:29.707: INFO: 	Container config-reloader ready: true, restart count 0
Oct 29 21:40:29.707: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:40:29.707: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:40:29.707: INFO: alertmanager-main-2 started at 2020-10-29 17:49:10 +0000 UTC (0+5 container statuses recorded)
Oct 29 21:40:29.707: INFO: 	Container alertmanager ready: true, restart count 0
Oct 29 21:40:29.707: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Oct 29 21:40:29.707: INFO: 	Container config-reloader ready: true, restart count 0
Oct 29 21:40:29.707: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:40:29.707: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:40:29.707: INFO: ovs-99tdx started at 2020-10-29 17:41:12 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.707: INFO: 	Container openvswitch ready: true, restart count 0
Oct 29 21:40:29.707: INFO: multus-str5z started at 2020-10-29 17:41:12 +0000 UTC (5+1 container statuses recorded)
Oct 29 21:40:29.707: INFO: 	Init container multus-binary-copy ready: true, restart count 0
Oct 29 21:40:29.707: INFO: 	Init container cni-plugins ready: true, restart count 0
Oct 29 21:40:29.707: INFO: 	Init container routeoverride-cni ready: true, restart count 0
Oct 29 21:40:29.707: INFO: 	Init container whereabouts-cni-bincopy ready: true, restart count 0
Oct 29 21:40:29.707: INFO: 	Init container whereabouts-cni ready: true, restart count 0
Oct 29 21:40:29.707: INFO: 	Container kube-multus ready: true, restart count 0
Oct 29 21:40:29.707: INFO: node-ca-6tz25 started at 2020-10-29 17:41:11 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.707: INFO: 	Container node-ca ready: true, restart count 0
Oct 29 21:40:29.707: INFO: sre-dns-latency-exporter-l58wm started at 2020-10-29 17:41:56 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.707: INFO: 	Container main ready: true, restart count 0
Oct 29 21:40:29.707: INFO: sre-stuck-ebs-vols-1-98w59 started at 2020-10-29 17:48:47 +0000 UTC (1+1 container statuses recorded)
Oct 29 21:40:29.707: INFO: 	Init container setupcreds ready: true, restart count 0
Oct 29 21:40:29.707: INFO: 	Container main ready: true, restart count 0
Oct 29 21:40:29.707: INFO: prometheus-k8s-1 started at 2020-10-29 17:49:00 +0000 UTC (0+6 container statuses recorded)
Oct 29 21:40:29.707: INFO: 	Container config-reloader ready: true, restart count 0
Oct 29 21:40:29.707: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:40:29.707: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:40:29.707: INFO: 	Container prometheus ready: true, restart count 1
Oct 29 21:40:29.707: INFO: 	Container prometheus-proxy ready: true, restart count 0
Oct 29 21:40:29.707: INFO: 	Container thanos-sidecar ready: true, restart count 0
Oct 29 21:40:29.707: INFO: router-default-5f9d96945d-zxzg4 started at 2020-10-29 19:45:00 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.707: INFO: 	Container router ready: true, restart count 0
Oct 29 21:40:29.707: INFO: prometheus-operator-89f4b9855-wlkt2 started at 2020-10-29 17:48:47 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:40:29.707: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:40:29.707: INFO: 	Container prometheus-operator ready: true, restart count 0
Oct 29 21:40:29.707: INFO: kube-state-metrics-596748788d-6mblj started at 2020-10-29 17:48:48 +0000 UTC (0+3 container statuses recorded)
Oct 29 21:40:29.707: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Oct 29 21:40:29.707: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Oct 29 21:40:29.707: INFO: 	Container kube-state-metrics ready: true, restart count 0
Oct 29 21:40:29.707: INFO: grafana-5cfb6b5fbb-gjr2l started at 2020-10-29 17:48:48 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:40:29.707: INFO: 	Container grafana ready: true, restart count 0
Oct 29 21:40:29.707: INFO: 	Container grafana-proxy ready: true, restart count 0
Oct 29 21:40:29.707: INFO: machine-config-daemon-mjcnt started at 2020-10-29 17:41:11 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:40:29.707: INFO: 	Container machine-config-daemon ready: true, restart count 0
Oct 29 21:40:29.707: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:40:29.707: INFO: sdn-cp5t4 started at 2020-10-29 17:41:12 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:40:29.707: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Oct 29 21:40:29.707: INFO: 	Container sdn ready: true, restart count 0
Oct 29 21:40:29.707: INFO: alertmanager-main-1 started at 2020-10-29 17:49:02 +0000 UTC (0+5 container statuses recorded)
Oct 29 21:40:29.707: INFO: 	Container alertmanager ready: true, restart count 0
Oct 29 21:40:29.707: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Oct 29 21:40:29.707: INFO: 	Container config-reloader ready: true, restart count 0
Oct 29 21:40:29.707: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:40:29.707: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:40:29.707: INFO: network-metrics-daemon-gcjdz started at 2020-10-29 17:41:13 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:40:29.707: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:40:29.707: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Oct 29 21:40:29.707: INFO: image-registry-75b75c555d-ksztb started at 2020-10-29 17:48:47 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.707: INFO: 	Container registry ready: true, restart count 0
Oct 29 21:40:29.707: INFO: prometheus-adapter-6495ff5885-lrx64 started at 2020-10-29 17:48:47 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.707: INFO: 	Container prometheus-adapter ready: true, restart count 0
Oct 29 21:40:29.707: INFO: node-exporter-d8llk started at 2020-10-29 17:41:13 +0000 UTC (1+2 container statuses recorded)
Oct 29 21:40:29.707: INFO: 	Init container init-textfile ready: true, restart count 0
Oct 29 21:40:29.707: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:40:29.707: INFO: 	Container node-exporter ready: true, restart count 0
Oct 29 21:40:29.707: INFO: prometheus-k8s-0 started at 2020-10-29 17:48:53 +0000 UTC (0+6 container statuses recorded)
Oct 29 21:40:29.707: INFO: 	Container config-reloader ready: true, restart count 0
Oct 29 21:40:29.707: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:40:29.707: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:40:29.707: INFO: 	Container prometheus ready: true, restart count 1
Oct 29 21:40:29.707: INFO: 	Container prometheus-proxy ready: true, restart count 0
Oct 29 21:40:29.707: INFO: 	Container thanos-sidecar ready: true, restart count 0
W1029 21:40:29.729418   53132 metrics_grabber.go:83] Can't find any pods in namespace kube-system to grab metrics from
W1029 21:40:29.729437   53132 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1029 21:40:29.729443   53132 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1029 21:40:29.729449   53132 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Oct 29 21:40:29.827: INFO: 
Latency metrics for node ip-10-0-157-225.us-east-2.compute.internal
Oct 29 21:40:29.827: INFO: 
Logging node info for node ip-10-0-158-72.us-east-2.compute.internal
Oct 29 21:40:29.848: INFO: Node Info: &Node{ObjectMeta:{ip-10-0-158-72.us-east-2.compute.internal   /api/v1/nodes/ip-10-0-158-72.us-east-2.compute.internal ace26a9e-d457-4c97-8322-f7fa7da4df71 213062 0 2020-10-29 17:42:11 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:m5.xlarge beta.kubernetes.io/os:linux failure-domain.beta.kubernetes.io/region:us-east-2 failure-domain.beta.kubernetes.io/zone:us-east-2a kubernetes.io/arch:amd64 kubernetes.io/hostname:ip-10-0-158-72 kubernetes.io/os:linux node-role.kubernetes.io:infra node-role.kubernetes.io/infra: node-role.kubernetes.io/worker: node.kubernetes.io/instance-type:m5.xlarge node.openshift.io/os_id:rhcos topology.ebs.csi.aws.com/zone:us-east-2a topology.kubernetes.io/region:us-east-2 topology.kubernetes.io/zone:us-east-2a] map[csi.volume.kubernetes.io/nodeid:{"ebs.csi.aws.com":"i-07086da876de13ecd"} machine.openshift.io/machine:openshift-machine-api/jeder-461-cncf2-pvrph-infra-us-east-2a-l2r9l machineconfiguration.openshift.io/currentConfig:rendered-worker-f26c105cafab97113f99101cc57ac048 machineconfiguration.openshift.io/desiredConfig:rendered-worker-f26c105cafab97113f99101cc57ac048 machineconfiguration.openshift.io/reason: machineconfiguration.openshift.io/state:Done volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{nodelink-controller Update v1 2020-10-29 17:42:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machine.openshift.io/machine":{}},"f:labels":{"f:node-role.kubernetes.io":{},"f:node-role.kubernetes.io/infra":{}}}}} {machine-config-controller Update v1 2020-10-29 17:48:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/desiredConfig":{}}}}} {kube-controller-manager Update v1 2020-10-29 17:54:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{}}},"f:spec":{"f:podCIDR":{},"f:podCIDRs":{".":{},"v:\"10.128.6.0/24\"":{}},"f:taints":{}}}} {machine-config-daemon Update v1 2020-10-29 17:54:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/currentConfig":{},"f:machineconfiguration.openshift.io/reason":{},"f:machineconfiguration.openshift.io/state":{}}}}} {e2e.test Update v1 2020-10-29 21:29:44 +0000 UTC FieldsV1 {"f:status":{"f:capacity":{"f:scheduling.k8s.io/foo":{}}}}} {kubelet Update v1 2020-10-29 21:39:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:csi.volume.kubernetes.io/nodeid":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/instance-type":{},"f:failure-domain.beta.kubernetes.io/region":{},"f:failure-domain.beta.kubernetes.io/zone":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:node-role.kubernetes.io/worker":{},"f:node.kubernetes.io/instance-type":{},"f:node.openshift.io/os_id":{},"f:topology.ebs.csi.aws.com/zone":{},"f:topology.kubernetes.io/region":{},"f:topology.kubernetes.io/zone":{}}},"f:spec":{"f:providerID":{}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalDNS\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:attachable-volumes-aws-ebs":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{},"f:scheduling.k8s.io/foo":{}},"f:capacity":{".":{},"f:attachable-volumes-aws-ebs":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"DiskPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"MemoryPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PIDPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:images":{},"f:nodeInfo":{"f:architecture":{},"f:bootID":{},"f:containerRuntimeVersion":{},"f:kernelVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{},"f:machineID":{},"f:operatingSystem":{},"f:osImage":{},"f:systemUUID":{}}}}}]},Spec:NodeSpec{PodCIDR:10.128.6.0/24,DoNotUseExternalID:,ProviderID:aws:///us-east-2a/i-07086da876de13ecd,Unschedulable:false,Taints:[]Taint{Taint{Key:node-role.kubernetes.io/infra,Value:,Effect:PreferNoSchedule,TimeAdded:<nil>,},},ConfigSource:nil,PodCIDRs:[10.128.6.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{321557344256 0} {<nil>} 314020844Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{16502939648 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},scheduling.k8s.io/foo: {{3 0} {<nil>} 3 DecimalSI},},Allocatable:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{3 0} {<nil>} 3 DecimalSI},ephemeral-storage: {{288327867528 0} {<nil>} 288327867528 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{15324340224 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},scheduling.k8s.io/foo: {{3 0} {<nil>} 3 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2020-10-29 21:39:46 +0000 UTC,LastTransitionTime:2020-10-29 17:54:47 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2020-10-29 21:39:46 +0000 UTC,LastTransitionTime:2020-10-29 17:54:47 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2020-10-29 21:39:46 +0000 UTC,LastTransitionTime:2020-10-29 17:54:47 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2020-10-29 21:39:46 +0000 UTC,LastTransitionTime:2020-10-29 17:54:47 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.0.158.72,},NodeAddress{Type:Hostname,Address:ip-10-0-158-72.us-east-2.compute.internal,},NodeAddress{Type:InternalDNS,Address:ip-10-0-158-72.us-east-2.compute.internal,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:ec269be649cbb435d7131f2acda04147,SystemUUID:ec269be6-49cb-b435-d713-1f2acda04147,BootID:f21654fd-8398-480a-9b38-cafec1d13c46,KernelVersion:4.18.0-193.24.1.el8_2.dt1.x86_64,OSImage:Red Hat Enterprise Linux CoreOS 46.82.202010091720-0 (Ootpa),ContainerRuntimeVersion:cri-o://1.19.0-20.rhaos4.6.git97d715e.el8,KubeletVersion:v1.19.0+d59ce34,KubeProxyVersion:v1.19.0+d59ce34,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:82d1def7312de8ae5dee32d237ad59fe685923e78668fa3547e6bee445cd8842 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:499341399,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4cfd55719faff41e96c2e4be69e3f2381a57b8b3445b80ae4acfe8ee33d7f99b quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:481479938,},ContainerImage{Names:[quay.io/jupierce/openshift-python-monitoring@sha256:98e3259170f7116837fa60b0515fe79cb9721060d34eb4931a14244b4658bb8d quay.io/jupierce/openshift-python-monitoring:stable],SizeBytes:474594250,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8248dec0d94b2928aa4d63a22973d9a8f8f173a1431b2ab4ad15fdfe80283d7c quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:419419150,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8923050603588c27d79b33b371afb651288470d5cdeb14f8e10249bca1a1c461 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:413031192,},ContainerImage{Names:[quay.io/openshift-sre/managed-prometheus-exporter-initcontainer@sha256:7fa36e4488191536ac4a570fdd3431d3c80db983db74b90bc2ce051ecef3035e quay.io/openshift-sre/managed-prometheus-exporter-initcontainer:v0.1.9-2019-03-28-4e558131],SizeBytes:388062834,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8f4882cff3c2f9521215eac681c5abda42876e3e955431c1387fb457940b8344 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:370109724,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:72323ce541f8a26fbad17ef65ff21b51498863bb851635a0faa8d5b1ac6ce0e4 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:366911472,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3d0361f380abf5252b1b640e3ceaaab8274e2af8cdb605b20b513a1a44b3a4dc quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:346080704,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f644e4b495071a2b9d0d6f5d48cb96dad9f7ea8298cc22c98824bc70229ea9dd quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:341116512,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6bada08687c20afe316c1f8cf76f001f31bedae317f256f3df3affaa5d0dc25e quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:336987966,},ContainerImage{Names:[image-registry.openshift-image-registry.svc:5000/openshift/cli@sha256:6aa4bb97adf2142b0e74ccae7fd3661ada73cbaac803b86bb8712261e916d66d quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6aa4bb97adf2142b0e74ccae7fd3661ada73cbaac803b86bb8712261e916d66d image-registry.openshift-image-registry.svc:5000/openshift/cli:latest],SizeBytes:336576591,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:52424fd2af6fd7d7a5a1233032eb3f3c67f7691996b209e013e29f1524c5188c quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:326057200,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f86db3170270fc635dff0d7f1ba6e79a8f45de7e1dcfa5621474d1f6e07352ec quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:325370491,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:54fbeb744b82662fd38c0d301ebaad6ca8983707bc44db7235ead0fb7b95808f quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:321482743,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a37a568c63563257309cb0ffb6e185d98f662ff3201d2099cbe0df404b93f0c8 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:314357110,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:26f6c930942ee4dea7c1e22d220bba11561c37bdc47101c4490ce0ef77c9203a quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:311353697,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e176509a104b2f96ee4f5c57275c6a712409aa80ac40071345d0a03bdde2b456 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:307311930,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0c42cd3a74176732e8c06105c47674c7d410c7167c8e3fbd80f9a76e9bfda5bd quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:302591889,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c75977f28becdf4f7065cfa37233464dd31208b1767e620c4f19658f53f8ff8c quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:300756060,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:cf565b2fab365e027962a25a8cffb41aa35cb5a00d001e081d53c7fed5a0c54b quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:300005108,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9e7f0468850aeb13585ef049f687cc42c05d82bc0e0200607d1a93d7f9740fe5 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:291621018,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6cdaecd5dd9df8fd74529be7fa5d8973daf6f4ea95be8acfb2f5ac97773ebe67 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:278303432,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e5bcf6d786fd218e1ef188eb40c39c31a98d03121fba3b7a1f16e87e45a7478b quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:276735847,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1dcc413b621958f97dfbb3fc998a9e225ef155a80ffb151eb4694bf8370b383a quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:274559501,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8c9e61400619c4613db5cc73097d287e3cd5d2125c85d1d84cc30cfdaa1093e7 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:271612142,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:12b11e2000b42ce1aaa228d9c1f4c9177395add2fa43835e667b7fc9007e40e6 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:270734898,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:88ddfbded8bc27b227ed7397ece050b756e522a9ffc34cbfba3c94c5ee58b740 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:269654227,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8bd90fcca7990c0edead15298dcec963968274d299428da95eae41aa23157b90 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:264244297,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:73fdcef5de85e739831c5a8b76dce349a3c8832ff416a46263743d7e61655cbb quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:247448489,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b635694dc0663a1404d43a4a9ac8513a3087c7cfead50f6ab413f3c217c40b2a quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:240293385,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:71051bdf1b96c953fc1dfd48359915bf5c027613de6f5e2fa8adeea8d3dda311 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:236515650,},ContainerImage{Names:[quay.io/app-sre/managed-velero-operator@sha256:a9eb4800dcafe3fe8879662c0413a7e5c53b5fe98ab0bbc81b9fc151044a4e07 quay.io/app-sre/managed-velero-operator:0a73057],SizeBytes:207938263,},ContainerImage{Names:[quay.io/app-sre/cloud-ingress-operator@sha256:ceb064d013f51cdb15075f079eee2dfd5a0f42f4a09b489176abe92e91fb6a56 quay.io/app-sre/cloud-ingress-operator:v0.1.233-a2e288b],SizeBytes:203838503,},ContainerImage{Names:[quay.io/app-sre/managed-upgrade-operator@sha256:558a828a0b3c296cb2c0e52ef143a45177bdae40b1c84bfe8564486c940fd79a quay.io/app-sre/managed-upgrade-operator:9b59534],SizeBytes:198448288,},ContainerImage{Names:[quay.io/app-sre/splunk-forwarder-operator@sha256:bca474387af6112b08406697ef28977d428c1a884ebaf602a3e6ba1b45e6b31b quay.io/app-sre/splunk-forwarder-operator:7e8bc93],SizeBytes:191465195,},ContainerImage{Names:[quay.io/app-sre/configure-alertmanager-operator@sha256:a2bed259f284358220d4e01390621a24f43c6f0d534474a5050cad0c6c6cb812 quay.io/app-sre/configure-alertmanager-operator:v0.1.237-0a8e5f3],SizeBytes:188443907,},ContainerImage{Names:[quay.io/app-sre/rbac-permissions-operator@sha256:8ccc461d19f30811dbf31f85048b5811e690d5f047c91831accd54eb5ab560b3 quay.io/app-sre/rbac-permissions-operator:819ec06],SizeBytes:187661365,},ContainerImage{Names:[docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4 docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 docker.io/library/httpd:2.4.38-alpine],SizeBytes:128894988,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0 k8s.gcr.io/e2e-test-images/agnhost@sha256:c243d6fc39291ac6ea9caf6b511719980d9c8d2e54dda8217ee019fbf9a124e1 k8s.gcr.io/e2e-test-images/agnhost:2.20],SizeBytes:116455133,},ContainerImage{Names:[k8s.gcr.io/pause@sha256:4a1c4b21597c1b4415bdbecb28a3296c6b5e23ca4f9feeb599860a1dac6a0108 k8s.gcr.io/pause@sha256:927d98197ec1141a368550822d18fa1c60bdae27b78b0c004f705f548c07814f k8s.gcr.io/pause:3.2],SizeBytes:688049,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Oct 29 21:40:29.848: INFO: 
Logging kubelet events for node ip-10-0-158-72.us-east-2.compute.internal
Oct 29 21:40:29.870: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-158-72.us-east-2.compute.internal
Oct 29 21:40:29.905: INFO: sre-dns-latency-exporter-krp25 started at 2020-10-29 17:42:51 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.905: INFO: 	Container main ready: true, restart count 0
Oct 29 21:40:29.905: INFO: ovs-9ct7r started at 2020-10-29 17:42:13 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.905: INFO: 	Container openvswitch ready: true, restart count 0
Oct 29 21:40:29.905: INFO: network-metrics-daemon-nb7sg started at 2020-10-29 17:42:13 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:40:29.905: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:40:29.905: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Oct 29 21:40:29.905: INFO: aws-ebs-csi-driver-node-zfr5l started at 2020-10-29 17:42:13 +0000 UTC (0+3 container statuses recorded)
Oct 29 21:40:29.905: INFO: 	Container csi-driver ready: true, restart count 0
Oct 29 21:40:29.905: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Oct 29 21:40:29.905: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Oct 29 21:40:29.905: INFO: dns-default-25bcd started at 2020-10-29 17:42:13 +0000 UTC (0+3 container statuses recorded)
Oct 29 21:40:29.905: INFO: 	Container dns ready: true, restart count 0
Oct 29 21:40:29.905: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 29 21:40:29.905: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:40:29.905: INFO: node-exporter-pdd9z started at 2020-10-29 17:42:13 +0000 UTC (1+2 container statuses recorded)
Oct 29 21:40:29.905: INFO: 	Init container init-textfile ready: true, restart count 0
Oct 29 21:40:29.905: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:40:29.905: INFO: 	Container node-exporter ready: true, restart count 0
Oct 29 21:40:29.905: INFO: node-ca-d275k started at 2020-10-29 17:42:13 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.905: INFO: 	Container node-ca ready: true, restart count 0
Oct 29 21:40:29.905: INFO: machine-config-daemon-6987z started at 2020-10-29 17:42:13 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:40:29.905: INFO: 	Container machine-config-daemon ready: true, restart count 0
Oct 29 21:40:29.905: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:40:29.905: INFO: sre-build-test-1603998660-hmrqr started at 2020-10-29 19:11:03 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.905: INFO: 	Container sre-build-test ready: false, restart count 0
Oct 29 21:40:29.905: INFO: thanos-querier-7f688468b8-6jh8m started at 2020-10-29 19:28:48 +0000 UTC (0+5 container statuses recorded)
Oct 29 21:40:29.905: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:40:29.905: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Oct 29 21:40:29.905: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:40:29.905: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:40:29.905: INFO: 	Container thanos-query ready: true, restart count 0
Oct 29 21:40:29.905: INFO: router-default-5f9d96945d-ckql4 started at 2020-10-29 19:44:59 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.905: INFO: 	Container router ready: true, restart count 0
Oct 29 21:40:29.905: INFO: multus-l5sdf started at 2020-10-29 17:42:13 +0000 UTC (5+1 container statuses recorded)
Oct 29 21:40:29.905: INFO: 	Init container multus-binary-copy ready: true, restart count 0
Oct 29 21:40:29.905: INFO: 	Init container cni-plugins ready: true, restart count 0
Oct 29 21:40:29.905: INFO: 	Init container routeoverride-cni ready: true, restart count 0
Oct 29 21:40:29.905: INFO: 	Init container whereabouts-cni-bincopy ready: true, restart count 0
Oct 29 21:40:29.905: INFO: 	Init container whereabouts-cni ready: true, restart count 0
Oct 29 21:40:29.905: INFO: 	Container kube-multus ready: true, restart count 0
Oct 29 21:40:29.905: INFO: sre-ebs-iops-reporter-1-rrlxd started at 2020-10-29 19:28:48 +0000 UTC (1+1 container statuses recorded)
Oct 29 21:40:29.905: INFO: 	Init container setupcreds ready: true, restart count 0
Oct 29 21:40:29.905: INFO: 	Container main ready: true, restart count 0
Oct 29 21:40:29.905: INFO: cloud-ingress-operator-798549d69-9fh9n started at 2020-10-29 19:28:48 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.905: INFO: 	Container cloud-ingress-operator ready: true, restart count 0
Oct 29 21:40:29.905: INFO: splunk-forwarder-operator-6c55bb877-mw9lq started at 2020-10-29 19:28:48 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.905: INFO: 	Container splunk-forwarder-operator ready: true, restart count 0
Oct 29 21:40:29.905: INFO: sre-build-test-1604002260-284ls started at 2020-10-29 20:11:07 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.905: INFO: 	Container sre-build-test ready: false, restart count 0
Oct 29 21:40:29.905: INFO: sdn-8kf7x started at 2020-10-29 17:42:13 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:40:29.905: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Oct 29 21:40:29.905: INFO: 	Container sdn ready: true, restart count 0
Oct 29 21:40:29.905: INFO: tuned-jwndg started at 2020-10-29 17:42:13 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.905: INFO: 	Container tuned ready: true, restart count 0
Oct 29 21:40:29.905: INFO: sre-build-test-1604005860-qkzkn started at 2020-10-29 21:11:04 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:29.905: INFO: 	Container sre-build-test ready: false, restart count 0
W1029 21:40:29.926656   53132 metrics_grabber.go:83] Can't find any pods in namespace kube-system to grab metrics from
W1029 21:40:29.926693   53132 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1029 21:40:29.926700   53132 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1029 21:40:29.926707   53132 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Oct 29 21:40:29.981: INFO: 
Latency metrics for node ip-10-0-158-72.us-east-2.compute.internal
Oct 29 21:40:29.981: INFO: 
Logging node info for node ip-10-0-231-174.us-east-2.compute.internal
Oct 29 21:40:30.002: INFO: Node Info: &Node{ObjectMeta:{ip-10-0-231-174.us-east-2.compute.internal   /api/v1/nodes/ip-10-0-231-174.us-east-2.compute.internal e5891fd2-1d97-465b-be6e-6039611f5983 213211 0 2020-10-29 17:23:44 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:m5.xlarge beta.kubernetes.io/os:linux failure-domain.beta.kubernetes.io/region:us-east-2 failure-domain.beta.kubernetes.io/zone:us-east-2a kubernetes.io/arch:amd64 kubernetes.io/hostname:ip-10-0-231-174 kubernetes.io/os:linux node-role.kubernetes.io/master: node.kubernetes.io/instance-type:m5.xlarge node.openshift.io/os_id:rhcos topology.ebs.csi.aws.com/zone:us-east-2a topology.kubernetes.io/region:us-east-2 topology.kubernetes.io/zone:us-east-2a] map[csi.volume.kubernetes.io/nodeid:{"ebs.csi.aws.com":"i-0a5b560458955c785"} machine.openshift.io/machine:openshift-machine-api/jeder-461-cncf2-pvrph-master-2 machineconfiguration.openshift.io/currentConfig:rendered-master-b06f274cfa558dc0b5031aabca48f868 machineconfiguration.openshift.io/desiredConfig:rendered-master-b06f274cfa558dc0b5031aabca48f868 machineconfiguration.openshift.io/reason: machineconfiguration.openshift.io/state:Done volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{nodelink-controller Update v1 2020-10-29 17:26:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machine.openshift.io/machine":{}}}}} {machine-config-controller Update v1 2020-10-29 17:40:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/desiredConfig":{}}}}} {machine-config-daemon Update v1 2020-10-29 17:42:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/currentConfig":{},"f:machineconfiguration.openshift.io/reason":{},"f:machineconfiguration.openshift.io/state":{}}}}} {kube-controller-manager Update v1 2020-10-29 19:08:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{}}},"f:spec":{"f:podCIDR":{},"f:podCIDRs":{".":{},"v:\"10.128.0.0/24\"":{}},"f:taints":{}}}} {oc Update v1 2020-10-29 19:08:29 +0000 UTC FieldsV1 {"f:spec":{"f:unschedulable":{}}}} {kubelet Update v1 2020-10-29 21:40:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:csi.volume.kubernetes.io/nodeid":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/instance-type":{},"f:failure-domain.beta.kubernetes.io/region":{},"f:failure-domain.beta.kubernetes.io/zone":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:node-role.kubernetes.io/master":{},"f:node.kubernetes.io/instance-type":{},"f:node.openshift.io/os_id":{},"f:topology.ebs.csi.aws.com/zone":{},"f:topology.kubernetes.io/region":{},"f:topology.kubernetes.io/zone":{}}},"f:spec":{"f:providerID":{}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalDNS\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:attachable-volumes-aws-ebs":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:attachable-volumes-aws-ebs":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"DiskPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"MemoryPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PIDPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:images":{},"f:nodeInfo":{"f:architecture":{},"f:bootID":{},"f:containerRuntimeVersion":{},"f:kernelVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{},"f:machineID":{},"f:operatingSystem":{},"f:osImage":{},"f:systemUUID":{}}}}}]},Spec:NodeSpec{PodCIDR:10.128.0.0/24,DoNotUseExternalID:,ProviderID:aws:///us-east-2a/i-0a5b560458955c785,Unschedulable:true,Taints:[]Taint{Taint{Key:node-role.kubernetes.io/master,Value:,Effect:NoSchedule,TimeAdded:<nil>,},Taint{Key:node.kubernetes.io/unschedulable,Value:,Effect:NoSchedule,TimeAdded:2020-10-29 19:08:29 +0000 UTC,},},ConfigSource:nil,PodCIDRs:[10.128.0.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{375244435456 0} {<nil>} 366449644Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{16502923264 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},},Allocatable:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{3 0} {<nil>} 3 DecimalSI},ephemeral-storage: {{336646249528 0} {<nil>} 336646249528 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{15324323840 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2020-10-29 21:40:09 +0000 UTC,LastTransitionTime:2020-10-29 17:42:41 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2020-10-29 21:40:09 +0000 UTC,LastTransitionTime:2020-10-29 17:42:41 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2020-10-29 21:40:09 +0000 UTC,LastTransitionTime:2020-10-29 17:42:41 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2020-10-29 21:40:09 +0000 UTC,LastTransitionTime:2020-10-29 17:42:41 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.0.231.174,},NodeAddress{Type:Hostname,Address:ip-10-0-231-174.us-east-2.compute.internal,},NodeAddress{Type:InternalDNS,Address:ip-10-0-231-174.us-east-2.compute.internal,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:ec2cb26928dd6ce9ad2bab1fbd8b1ea7,SystemUUID:ec2cb269-28dd-6ce9-ad2b-ab1fbd8b1ea7,BootID:74c7036a-ae1f-4944-b92b-7103f56935aa,KernelVersion:4.18.0-193.24.1.el8_2.dt1.x86_64,OSImage:Red Hat Enterprise Linux CoreOS 46.82.202010091720-0 (Ootpa),ContainerRuntimeVersion:cri-o://1.19.0-20.rhaos4.6.git97d715e.el8,KubeletVersion:v1.19.0+d59ce34,KubeProxyVersion:v1.19.0+d59ce34,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:30094a2d586aa282d85e14f1be19abec1c30ce431673377b0e1c12d83e6bac8c quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:682301224,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:82d1def7312de8ae5dee32d237ad59fe685923e78668fa3547e6bee445cd8842 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:499341399,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4cfd55719faff41e96c2e4be69e3f2381a57b8b3445b80ae4acfe8ee33d7f99b quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:481479938,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ae6a5decd040a6b3adfa074d3211ab92a36b77b2d849962d9a678e1c2c5ef5c1 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:473291773,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8923050603588c27d79b33b371afb651288470d5cdeb14f8e10249bca1a1c461 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:413031192,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8f4882cff3c2f9521215eac681c5abda42876e3e955431c1387fb457940b8344 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:370109724,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:72323ce541f8a26fbad17ef65ff21b51498863bb851635a0faa8d5b1ac6ce0e4 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:366911472,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:93b3e1246884e357e1654e6c9578481aff9eef07eed1f9fdd0e9c8cc89a3770c quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:358141917,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a8214df42df962b965e3f4daad0b61932235e57241160861e503d84e38b775d5 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:337507384,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6bada08687c20afe316c1f8cf76f001f31bedae317f256f3df3affaa5d0dc25e quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:336987966,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6aa4bb97adf2142b0e74ccae7fd3661ada73cbaac803b86bb8712261e916d66d quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:336576591,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:aa4f37543b45bc248db8d9bd2dc45b6e159a8869b044c2310f541afba15b2694 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:336485164,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f52825e9905c926d399cd0b7afbb2b7d0370ae22da0416feac9131d555db0b98 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:336041278,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:059f0179c0528c6234dbdca7e70fe779cf37be5121f458dd045d2e9662192f06 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:335510757,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:7584014b0cb8cb2c5a09b909c79f2f8ad6e49854bcfabf72e96a22330bcf6f56 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:333011308,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:357e35286fd26fed015c03a9c451f6fdcf61cf0821d959025e7f800e7c533f29 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:332777114,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6ca671c810426b8c4f13dd0c7ac19639f9f265b952b8feb5a828e59fab785335 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:332487959,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f86db3170270fc635dff0d7f1ba6e79a8f45de7e1dcfa5621474d1f6e07352ec quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:325370491,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:54fbeb744b82662fd38c0d301ebaad6ca8983707bc44db7235ead0fb7b95808f quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:321482743,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9dbb31ac799b2c30270268714dcb3d11bafb329b98639a446657c8c7db41938c quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:318368538,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1ad85195e1a180698fe4b8df82e3d72075efb256b53f593d13e29faaf7f3e15a quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:316310728,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dc6a6a1d4a6b2af67421561e53d1af1d40c99ae72de69f4c3cc390d447f12269 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:315339691,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4691dc29704c9cb06d2345894f1a8f074b58a0d208318c5218241388b0916e1b quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:315327750,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4289297f0b7ee7edf394348fd07e1fa1b3162655f2a2af2245e23af4b179e7f2 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:314343750,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:65206861218064576dc092040e9c24b0393b8a07502e351f513400f187f38cc7 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:313489720,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:45586fd7a5cfd43ff546dbfb282a70a91eaf0f069f604230af958dc802832f89 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:312615681,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c6b3aaaa38679b1d752ec09bd68c6d80a8911c74ec16d27c49de88ecb97823ee quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:312540521,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:01250de496444bb624ec7b472ac9b0f7023809c88306a71c6ac87bb302f7dbe3 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:312449767,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ee4abe53e80e561239e510a6f9999b4dc80b7b3fdc9848ab43d0bf8df24e815d quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:311984477,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:26f6c930942ee4dea7c1e22d220bba11561c37bdc47101c4490ce0ef77c9203a quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:311353697,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9b564f882e31f497f57a0d99d406d5231eb15e9a97f0b450c21bec2bac7ff033 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:311323012,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e176509a104b2f96ee4f5c57275c6a712409aa80ac40071345d0a03bdde2b456 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:307311930,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:afdf0a3b426ac1c03df52e88a2b884f0714e54a1a03f33091954441a05a7f6b9 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:306116297,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:076b280e17c6bb4cc618db71403ccec75f8196c8849061a40c680a2808292bb6 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:304641618,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c75977f28becdf4f7065cfa37233464dd31208b1767e620c4f19658f53f8ff8c quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:300756060,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3cdb4589ee683c85e3d8f3f239187bc089d30cac6c26847a54894f6c328817c3 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:300278177,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:cf565b2fab365e027962a25a8cffb41aa35cb5a00d001e081d53c7fed5a0c54b quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:300005108,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:92feaeb8763ece68147b522bfa8914bcd429e9825185b9b9c05247ad2857d03f quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:299900737,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:aef7c7802e877679d62d3d40ca4cac4fa8e84b2974673a8912f14d95eca08a08 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:296536411,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:7f93199dcc01838f017030e0e8dd32d1d23fa268d25472e338e6843c8830d364 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:287022906,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6cdaecd5dd9df8fd74529be7fa5d8973daf6f4ea95be8acfb2f5ac97773ebe67 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:278303432,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1dcc413b621958f97dfbb3fc998a9e225ef155a80ffb151eb4694bf8370b383a quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:274559501,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:12b11e2000b42ce1aaa228d9c1f4c9177395add2fa43835e667b7fc9007e40e6 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:270734898,},ContainerImage{Names:[quay.io/app-sre/sre-ssh-proxy@sha256:67ecbf78aa15cc991d92643675d0c947fe4efea1e7a30d743d34e21f84cc2ed8 quay.io/app-sre/sre-ssh-proxy:v49-1fde8e5],SizeBytes:266831444,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8bd90fcca7990c0edead15298dcec963968274d299428da95eae41aa23157b90 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:264244297,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:73fdcef5de85e739831c5a8b76dce349a3c8832ff416a46263743d7e61655cbb quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:247448489,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:71051bdf1b96c953fc1dfd48359915bf5c027613de6f5e2fa8adeea8d3dda311 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:236515650,},ContainerImage{Names:[quay.io/app-sre/managed-cluster-validating-webhooks@sha256:5bbcacd1704c9d6e32ded35587241768c016d5dac8a93c79389d65f649728961 quay.io/app-sre/managed-cluster-validating-webhooks:7fe1da6],SizeBytes:219127863,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Oct 29 21:40:30.002: INFO: 
Logging kubelet events for node ip-10-0-231-174.us-east-2.compute.internal
Oct 29 21:40:30.023: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-231-174.us-east-2.compute.internal
Oct 29 21:40:30.087: INFO: cluster-node-tuning-operator-69d69f6546-m2drd started at 2020-10-29 17:48:55 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.087: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Oct 29 21:40:30.087: INFO: node-ca-hnmr8 started at 2020-10-29 17:25:50 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.087: INFO: 	Container node-ca ready: true, restart count 0
Oct 29 21:40:30.087: INFO: recyler-pod-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 17:42:41 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.087: INFO: 	Container recyler-container ready: false, restart count 0
Oct 29 21:40:30.087: INFO: cluster-monitoring-operator-7ffd9b8867-kft8z started at 2020-10-29 17:43:11 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:40:30.087: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Oct 29 21:40:30.087: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:40:30.087: INFO: rh-ssh-f45567f6b-hvlbt started at 2020-10-29 17:49:03 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.087: INFO: 	Container sshd ready: true, restart count 0
Oct 29 21:40:30.087: INFO: revision-pruner-10-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 18:02:59 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.087: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:40:30.087: INFO: revision-pruner-6-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 17:41:10 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.087: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:40:30.087: INFO: installer-9-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 17:43:16 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.087: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:40:30.087: INFO: revision-pruner-10-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 19:48:19 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.087: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:40:30.087: INFO: node-exporter-nbt47 started at 2020-10-29 17:24:50 +0000 UTC (1+2 container statuses recorded)
Oct 29 21:40:30.087: INFO: 	Init container init-textfile ready: true, restart count 0
Oct 29 21:40:30.087: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:40:30.087: INFO: 	Container node-exporter ready: true, restart count 0
Oct 29 21:40:30.087: INFO: revision-pruner-3-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 17:42:41 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.087: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:40:30.087: INFO: installer-11-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 19:46:56 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.087: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:40:30.087: INFO: revision-pruner-9-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 17:44:51 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.087: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:40:30.087: INFO: olm-operator-5f9964dfbf-prlh4 started at 2020-10-29 17:49:02 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.087: INFO: 	Container olm-operator ready: true, restart count 0
Oct 29 21:40:30.087: INFO: kube-apiserver-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 17:26:54 +0000 UTC (1+5 container statuses recorded)
Oct 29 21:40:30.087: INFO: 	Init container setup ready: true, restart count 1
Oct 29 21:40:30.087: INFO: 	Container kube-apiserver ready: true, restart count 0
Oct 29 21:40:30.087: INFO: 	Container kube-apiserver-cert-regeneration-controller ready: true, restart count 0
Oct 29 21:40:30.087: INFO: 	Container kube-apiserver-cert-syncer ready: true, restart count 0
Oct 29 21:40:30.087: INFO: 	Container kube-apiserver-check-endpoints ready: true, restart count 0
Oct 29 21:40:30.087: INFO: 	Container kube-apiserver-insecure-readyz ready: true, restart count 0
Oct 29 21:40:30.087: INFO: revision-pruner-11-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 19:50:15 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.087: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:40:30.087: INFO: multus-hbg4l started at 2020-10-29 17:23:58 +0000 UTC (5+1 container statuses recorded)
Oct 29 21:40:30.087: INFO: 	Init container multus-binary-copy ready: true, restart count 0
Oct 29 21:40:30.087: INFO: 	Init container cni-plugins ready: true, restart count 0
Oct 29 21:40:30.087: INFO: 	Init container routeoverride-cni ready: true, restart count 0
Oct 29 21:40:30.087: INFO: 	Init container whereabouts-cni-bincopy ready: true, restart count 0
Oct 29 21:40:30.087: INFO: 	Init container whereabouts-cni ready: true, restart count 0
Oct 29 21:40:30.087: INFO: 	Container kube-multus ready: true, restart count 0
Oct 29 21:40:30.087: INFO: sdn-pmb6l started at 2020-10-29 17:24:03 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:40:30.087: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Oct 29 21:40:30.087: INFO: 	Container sdn ready: true, restart count 0
Oct 29 21:40:30.087: INFO: installer-8-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 17:42:41 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.087: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:40:30.087: INFO: revision-pruner-8-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 17:43:42 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.087: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:40:30.087: INFO: authentication-operator-6cd54d4895-qgtlz started at 2020-10-29 17:48:54 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.087: INFO: 	Container authentication-operator ready: true, restart count 0
Oct 29 21:40:30.087: INFO: oauth-openshift-6dc6d68fbf-p4l57 started at 2020-10-29 18:36:48 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.087: INFO: 	Container oauth-openshift ready: true, restart count 0
Oct 29 21:40:30.087: INFO: machine-config-server-qww5z started at 2020-10-29 17:26:37 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.087: INFO: 	Container machine-config-server ready: true, restart count 0
Oct 29 21:40:30.087: INFO: etcd-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 17:25:10 +0000 UTC (2+3 container statuses recorded)
Oct 29 21:40:30.087: INFO: 	Init container etcd-ensure-env-vars ready: true, restart count 0
Oct 29 21:40:30.087: INFO: 	Init container etcd-resources-copy ready: true, restart count 0
Oct 29 21:40:30.087: INFO: 	Container etcd ready: true, restart count 0
Oct 29 21:40:30.087: INFO: 	Container etcd-metrics ready: true, restart count 0
Oct 29 21:40:30.087: INFO: 	Container etcdctl ready: true, restart count 0
Oct 29 21:40:30.087: INFO: pod-identity-webhook-75fc9d4d96-2hdfs started at 2020-10-29 17:48:55 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.087: INFO: 	Container pod-identity-webhook ready: true, restart count 0
Oct 29 21:40:30.087: INFO: console-575d66b587-jrlsg started at 2020-10-29 17:43:13 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.087: INFO: 	Container console ready: true, restart count 0
Oct 29 21:40:30.087: INFO: revision-pruner-11-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 19:50:38 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.087: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:40:30.087: INFO: installer-8-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 17:41:15 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.087: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:40:30.087: INFO: revision-pruner-8-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 17:43:08 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.087: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:40:30.087: INFO: installer-10-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 19:47:53 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.087: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:40:30.087: INFO: dns-default-dc9s5 started at 2020-10-29 17:25:40 +0000 UTC (0+3 container statuses recorded)
Oct 29 21:40:30.087: INFO: 	Container dns ready: true, restart count 0
Oct 29 21:40:30.087: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 29 21:40:30.087: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:40:30.087: INFO: kube-storage-version-migrator-operator-67f7f8c8ff-bzbvf started at 2020-10-29 17:48:59 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.087: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 0
Oct 29 21:40:30.087: INFO: installer-10-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 18:01:07 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.087: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:40:30.087: INFO: sdn-controller-lxg9r started at 2020-10-29 17:24:02 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.087: INFO: 	Container sdn-controller ready: true, restart count 0
Oct 29 21:40:30.087: INFO: installer-7-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 17:40:41 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.087: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:40:30.087: INFO: revision-pruner-7-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 17:41:05 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.087: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:40:30.087: INFO: installer-9-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 17:43:50 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.087: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:40:30.087: INFO: installer-8-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 17:43:14 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.087: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:40:30.087: INFO: cluster-storage-operator-5bd675b77d-6grzf started at 2020-10-29 17:48:56 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.087: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Oct 29 21:40:30.087: INFO: catalog-operator-5b9bb86df6-27hkk started at 2020-10-29 17:49:02 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.087: INFO: 	Container catalog-operator ready: true, restart count 1
Oct 29 21:40:30.087: INFO: installer-11-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 19:49:15 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.087: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:40:30.087: INFO: machine-config-daemon-qgz5x started at 2020-10-29 17:24:37 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:40:30.087: INFO: 	Container machine-config-daemon ready: true, restart count 0
Oct 29 21:40:30.087: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:40:30.087: INFO: controller-manager-bwmzk started at 2020-10-29 17:31:18 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.087: INFO: 	Container controller-manager ready: true, restart count 0
Oct 29 21:40:30.087: INFO: validation-webhook-lr64k started at 2020-10-29 17:39:26 +0000 UTC (1+1 container statuses recorded)
Oct 29 21:40:30.088: INFO: 	Init container inject-cert ready: true, restart count 0
Oct 29 21:40:30.088: INFO: 	Container webhooks ready: true, restart count 0
Oct 29 21:40:30.088: INFO: installer-7-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 17:42:41 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.088: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:40:30.088: INFO: apiserver-566bbccb57-tqc2f started at 2020-10-29 17:43:00 +0000 UTC (1+1 container statuses recorded)
Oct 29 21:40:30.088: INFO: 	Init container fix-audit-permissions ready: true, restart count 0
Oct 29 21:40:30.088: INFO: 	Container oauth-apiserver ready: true, restart count 0
Oct 29 21:40:30.088: INFO: packageserver-d4484c6fc-tflvz started at 2020-10-29 17:43:13 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.088: INFO: 	Container packageserver ready: true, restart count 0
Oct 29 21:40:30.088: INFO: revision-pruner-6-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 17:40:50 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.088: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:40:30.088: INFO: cluster-samples-operator-66b5c8685-vhd6w started at 2020-10-29 17:48:55 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:40:30.088: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Oct 29 21:40:30.088: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Oct 29 21:40:30.088: INFO: apiserver-8b58dffb-gffbl started at 2020-10-29 17:42:58 +0000 UTC (1+2 container statuses recorded)
Oct 29 21:40:30.088: INFO: 	Init container fix-audit-permissions ready: true, restart count 0
Oct 29 21:40:30.088: INFO: 	Container openshift-apiserver ready: true, restart count 0
Oct 29 21:40:30.088: INFO: 	Container openshift-apiserver-check-endpoints ready: true, restart count 0
Oct 29 21:40:30.088: INFO: revision-pruner-8-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 17:43:38 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.088: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:40:30.088: INFO: csi-snapshot-controller-operator-5b974dd5d8-t64z8 started at 2020-10-29 17:48:56 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.088: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Oct 29 21:40:30.088: INFO: machine-config-controller-856b795496-6zhcv started at 2020-10-29 17:48:58 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.088: INFO: 	Container machine-config-controller ready: true, restart count 0
Oct 29 21:40:30.088: INFO: revision-pruner-10-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 19:46:27 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.088: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:40:30.088: INFO: revision-pruner-9-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 17:44:26 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.088: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:40:30.088: INFO: tuned-7fcs5 started at 2020-10-29 17:24:46 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.088: INFO: 	Container tuned ready: true, restart count 0
Oct 29 21:40:30.088: INFO: revision-pruner-9-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 19:46:28 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.088: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:40:30.088: INFO: service-ca-544dc587b5-tvtzx started at 2020-10-29 17:49:03 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.088: INFO: 	Container service-ca-controller ready: true, restart count 0
Oct 29 21:40:30.088: INFO: network-operator-787d8b758c-hk6zw started at 2020-10-29 17:49:03 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.088: INFO: 	Container network-operator ready: true, restart count 0
Oct 29 21:40:30.088: INFO: installer-10-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 19:45:15 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.088: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:40:30.088: INFO: network-metrics-daemon-zd6mp started at 2020-10-29 17:23:59 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:40:30.088: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:40:30.088: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Oct 29 21:40:30.088: INFO: service-ca-operator-56d79f985d-77jgs started at 2020-10-29 17:43:13 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.088: INFO: 	Container service-ca-operator ready: true, restart count 0
Oct 29 21:40:30.088: INFO: openshift-kube-scheduler-operator-64d565dc97-55tk6 started at 2020-10-29 17:49:01 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.088: INFO: 	Container kube-scheduler-operator-container ready: true, restart count 0
Oct 29 21:40:30.088: INFO: kube-controller-manager-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 17:27:45 +0000 UTC (0+4 container statuses recorded)
Oct 29 21:40:30.088: INFO: 	Container cluster-policy-controller ready: true, restart count 0
Oct 29 21:40:30.088: INFO: 	Container kube-controller-manager ready: true, restart count 0
Oct 29 21:40:30.088: INFO: 	Container kube-controller-manager-cert-syncer ready: true, restart count 0
Oct 29 21:40:30.088: INFO: 	Container kube-controller-manager-recovery-controller ready: true, restart count 0
Oct 29 21:40:30.088: INFO: ovs-ch957 started at 2020-10-29 17:24:02 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.088: INFO: 	Container openvswitch ready: true, restart count 0
Oct 29 21:40:30.088: INFO: multus-admission-controller-fvrtq started at 2020-10-29 17:24:24 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:40:30.088: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:40:30.088: INFO: 	Container multus-admission-controller ready: true, restart count 0
Oct 29 21:40:30.088: INFO: aws-ebs-csi-driver-node-rrvxx started at 2020-10-29 17:24:58 +0000 UTC (0+3 container statuses recorded)
Oct 29 21:40:30.088: INFO: 	Container csi-driver ready: true, restart count 0
Oct 29 21:40:30.088: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Oct 29 21:40:30.088: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Oct 29 21:40:30.088: INFO: revision-pruner-7-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 17:40:53 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.088: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:40:30.088: INFO: etcd-quorum-guard-644f5747b8-429ns started at 2020-10-29 17:43:06 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.088: INFO: 	Container guard ready: true, restart count 0
Oct 29 21:40:30.088: INFO: revision-pruner-7-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 17:43:16 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.088: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:40:30.088: INFO: installer-9-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 19:46:02 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.088: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:40:30.088: INFO: openshift-kube-scheduler-ip-10-0-231-174.us-east-2.compute.internal started at 2020-10-29 17:25:14 +0000 UTC (1+2 container statuses recorded)
Oct 29 21:40:30.088: INFO: 	Init container wait-for-host-port ready: true, restart count 0
Oct 29 21:40:30.088: INFO: 	Container kube-scheduler ready: true, restart count 0
Oct 29 21:40:30.088: INFO: 	Container kube-scheduler-cert-syncer ready: true, restart count 0
W1029 21:40:30.109978   53132 metrics_grabber.go:83] Can't find any pods in namespace kube-system to grab metrics from
W1029 21:40:30.110007   53132 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1029 21:40:30.110031   53132 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1029 21:40:30.110038   53132 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Oct 29 21:40:30.235: INFO: 
Latency metrics for node ip-10-0-231-174.us-east-2.compute.internal
Oct 29 21:40:30.235: INFO: 
Logging node info for node ip-10-0-234-238.us-east-2.compute.internal
Oct 29 21:40:30.256: INFO: Node Info: &Node{ObjectMeta:{ip-10-0-234-238.us-east-2.compute.internal   /api/v1/nodes/ip-10-0-234-238.us-east-2.compute.internal f0844c0b-bcd8-46c5-9a7f-ece03266a2e0 213063 0 2020-10-29 17:30:47 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:m5.xlarge beta.kubernetes.io/os:linux failure-domain.beta.kubernetes.io/region:us-east-2 failure-domain.beta.kubernetes.io/zone:us-east-2a kubernetes.io/arch:amd64 kubernetes.io/hostname:ip-10-0-234-238 kubernetes.io/os:linux node-role.kubernetes.io/worker: node.kubernetes.io/instance-type:m5.xlarge node.openshift.io/os_id:rhcos topology.ebs.csi.aws.com/zone:us-east-2a topology.kubernetes.io/region:us-east-2 topology.kubernetes.io/zone:us-east-2a] map[csi.volume.kubernetes.io/nodeid:{"ebs.csi.aws.com":"i-0311b67a5abfd855e"} machine.openshift.io/machine:openshift-machine-api/jeder-461-cncf2-pvrph-worker-us-east-2a-bxbwb machineconfiguration.openshift.io/currentConfig:rendered-worker-f26c105cafab97113f99101cc57ac048 machineconfiguration.openshift.io/desiredConfig:rendered-worker-f26c105cafab97113f99101cc57ac048 machineconfiguration.openshift.io/reason: machineconfiguration.openshift.io/state:Done volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{nodelink-controller Update v1 2020-10-29 17:30:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machine.openshift.io/machine":{}}}}} {machine-config-controller Update v1 2020-10-29 17:44:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/desiredConfig":{}}}}} {kube-controller-manager Update v1 2020-10-29 17:46:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{}}},"f:spec":{"f:podCIDR":{},"f:podCIDRs":{".":{},"v:\"10.128.4.0/24\"":{}}}}} {machine-config-daemon Update v1 2020-10-29 17:46:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/currentConfig":{},"f:machineconfiguration.openshift.io/reason":{},"f:machineconfiguration.openshift.io/state":{}}}}} {e2e.test Update v1 2020-10-29 21:29:44 +0000 UTC FieldsV1 {"f:status":{"f:capacity":{"f:scheduling.k8s.io/foo":{}}}}} {kubelet Update v1 2020-10-29 21:39:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:csi.volume.kubernetes.io/nodeid":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/instance-type":{},"f:failure-domain.beta.kubernetes.io/region":{},"f:failure-domain.beta.kubernetes.io/zone":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:node-role.kubernetes.io/worker":{},"f:node.kubernetes.io/instance-type":{},"f:node.openshift.io/os_id":{},"f:topology.ebs.csi.aws.com/zone":{},"f:topology.kubernetes.io/region":{},"f:topology.kubernetes.io/zone":{}}},"f:spec":{"f:providerID":{}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalDNS\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:attachable-volumes-aws-ebs":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{},"f:scheduling.k8s.io/foo":{}},"f:capacity":{".":{},"f:attachable-volumes-aws-ebs":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"DiskPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"MemoryPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PIDPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:images":{},"f:nodeInfo":{"f:architecture":{},"f:bootID":{},"f:containerRuntimeVersion":{},"f:kernelVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{},"f:machineID":{},"f:operatingSystem":{},"f:osImage":{},"f:systemUUID":{}}}}}]},Spec:NodeSpec{PodCIDR:10.128.4.0/24,DoNotUseExternalID:,ProviderID:aws:///us-east-2a/i-0311b67a5abfd855e,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.128.4.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{321557344256 0} {<nil>} 314020844Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{16502939648 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},scheduling.k8s.io/foo: {{3 0} {<nil>} 3 DecimalSI},},Allocatable:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{3 0} {<nil>} 3 DecimalSI},ephemeral-storage: {{288327867528 0} {<nil>} 288327867528 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{15324340224 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},scheduling.k8s.io/foo: {{3 0} {<nil>} 3 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2020-10-29 21:39:46 +0000 UTC,LastTransitionTime:2020-10-29 17:46:27 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2020-10-29 21:39:46 +0000 UTC,LastTransitionTime:2020-10-29 17:46:27 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2020-10-29 21:39:46 +0000 UTC,LastTransitionTime:2020-10-29 17:46:27 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2020-10-29 21:39:46 +0000 UTC,LastTransitionTime:2020-10-29 17:46:27 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.0.234.238,},NodeAddress{Type:Hostname,Address:ip-10-0-234-238.us-east-2.compute.internal,},NodeAddress{Type:InternalDNS,Address:ip-10-0-234-238.us-east-2.compute.internal,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:ec2ba4194beb01956828a9bcef15464f,SystemUUID:ec2ba419-4beb-0195-6828-a9bcef15464f,BootID:238b22fb-c838-4401-ba71-dbbf616e0421,KernelVersion:4.18.0-193.24.1.el8_2.dt1.x86_64,OSImage:Red Hat Enterprise Linux CoreOS 46.82.202010091720-0 (Ootpa),ContainerRuntimeVersion:cri-o://1.19.0-20.rhaos4.6.git97d715e.el8,KubeletVersion:v1.19.0+d59ce34,KubeProxyVersion:v1.19.0+d59ce34,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[quay.io/app-sre/managed-velero-operator-registry@sha256:7d152b07c12c961a67be142cc80597b0616f0db936c34af242e309e6b7be208a quay.io/app-sre/managed-velero-operator-registry:production-0a73057],SizeBytes:1036797635,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:79978a34d1ab3b0ed1ad2c93c09bcb2fcdd1806b35e48a53c99d106347e1a59d quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:769679082,},ContainerImage{Names:[registry.redhat.io/redhat/community-operator-index@sha256:02facc4e56ed383e6cbd19ba740f477f55068948963b18154f66738302f4dd03 registry.redhat.io/redhat/community-operator-index@sha256:467e5d70a7591e82cf325371297c9ee4938e18ab60f42903a01270c0ef8a6a1c registry.redhat.io/redhat/community-operator-index:latest],SizeBytes:509744014,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:82d1def7312de8ae5dee32d237ad59fe685923e78668fa3547e6bee445cd8842 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:499341399,},ContainerImage{Names:[registry.redhat.io/redhat/certified-operator-index@sha256:8e4d14186a6a7a377512dd952fd4b6e8306d669b4e82864c8535021f3e725bd8 registry.redhat.io/redhat/certified-operator-index@sha256:b674fbf172249ff5c27ab195ca05ad14015783e523b421617181a03f9275874f registry.redhat.io/redhat/certified-operator-index:v4.6],SizeBytes:497271693,},ContainerImage{Names:[registry.redhat.io/redhat/redhat-marketplace-index@sha256:1254d42b17fdf685608b3b2f58244a753ce46ce49ad3f5e849a372dfbb901001 registry.redhat.io/redhat/redhat-marketplace-index@sha256:cdd578d52e7fd59b341f35c84f2ccd5681726794bf091d84d06ed66730fd38db registry.redhat.io/redhat/redhat-marketplace-index:v4.6],SizeBytes:492995473,},ContainerImage{Names:[registry.redhat.io/redhat/redhat-operator-index@sha256:20b5fa8d4466c957ee23c97aca064f757f3d7ebb8a1b237d1b1a5e479853a0ca registry.redhat.io/redhat/redhat-operator-index@sha256:bd70797deab3e1e0487d93d488e91418840c6c891d1af2156f66c687638b1308 registry.redhat.io/redhat/redhat-operator-index:v4.6],SizeBytes:491000720,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4cfd55719faff41e96c2e4be69e3f2381a57b8b3445b80ae4acfe8ee33d7f99b quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:481479938,},ContainerImage{Names:[quay.io/jupierce/openshift-python-monitoring@sha256:98e3259170f7116837fa60b0515fe79cb9721060d34eb4931a14244b4658bb8d quay.io/jupierce/openshift-python-monitoring:stable],SizeBytes:474594250,},ContainerImage{Names:[quay.io/app-sre/managed-upgrade-operator-registry@sha256:5d4870d619f5dd35974f957d92d6939a6b56a03c705420ae8602e01e67a10bfc quay.io/app-sre/managed-upgrade-operator-registry:production-9b59534],SizeBytes:472464972,},ContainerImage{Names:[quay.io/app-sre/cloud-ingress-operator-registry@sha256:ce73d98128dc1584568c88d44ba893e924baa93ec8ce0e3b4036f62c01fbd52a quay.io/app-sre/cloud-ingress-operator-registry:production-a2e288b],SizeBytes:470975221,},ContainerImage{Names:[quay.io/app-sre/configure-alertmanager-operator-registry@sha256:11de3d989f814e60482c28ddd72a71d68650a6ae5787289e2eb159aa1af23f01 quay.io/app-sre/configure-alertmanager-operator-registry:production-0a8e5f3],SizeBytes:470905083,},ContainerImage{Names:[quay.io/app-sre/splunk-forwarder-operator-registry@sha256:15c212f82397caa807f5c1e58d47d16bb50d143a886e52cdf09616a50c9e3ef2 quay.io/app-sre/splunk-forwarder-operator-registry:production-7e8bc93],SizeBytes:470516300,},ContainerImage{Names:[quay.io/app-sre/rbac-permissions-operator-registry@sha256:7620af44252bbdbed32316748ccbd165cad0e9320f48ccd63f05a3afc20dc0c2 quay.io/app-sre/rbac-permissions-operator-registry:production-819ec06],SizeBytes:469646922,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8248dec0d94b2928aa4d63a22973d9a8f8f173a1431b2ab4ad15fdfe80283d7c quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:419419150,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8923050603588c27d79b33b371afb651288470d5cdeb14f8e10249bca1a1c461 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:413031192,},ContainerImage{Names:[quay.io/openshift-sre/managed-prometheus-exporter-initcontainer@sha256:7fa36e4488191536ac4a570fdd3431d3c80db983db74b90bc2ce051ecef3035e quay.io/openshift-sre/managed-prometheus-exporter-initcontainer:v0.1.9-2019-03-28-4e558131],SizeBytes:388062834,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8f4882cff3c2f9521215eac681c5abda42876e3e955431c1387fb457940b8344 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:370109724,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:72323ce541f8a26fbad17ef65ff21b51498863bb851635a0faa8d5b1ac6ce0e4 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:366911472,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3d0361f380abf5252b1b640e3ceaaab8274e2af8cdb605b20b513a1a44b3a4dc quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:346080704,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f644e4b495071a2b9d0d6f5d48cb96dad9f7ea8298cc22c98824bc70229ea9dd quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:341116512,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6bada08687c20afe316c1f8cf76f001f31bedae317f256f3df3affaa5d0dc25e quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:336987966,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f42509c18cf5e41201d64cf3a9c1994ffa5318f8d7cee5de45fa2da914e68bbc quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:336588772,},ContainerImage{Names:[image-registry.openshift-image-registry.svc:5000/openshift/cli@sha256:6aa4bb97adf2142b0e74ccae7fd3661ada73cbaac803b86bb8712261e916d66d quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6aa4bb97adf2142b0e74ccae7fd3661ada73cbaac803b86bb8712261e916d66d image-registry.openshift-image-registry.svc:5000/openshift/cli:latest],SizeBytes:336576591,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:52424fd2af6fd7d7a5a1233032eb3f3c67f7691996b209e013e29f1524c5188c quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:326057200,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f86db3170270fc635dff0d7f1ba6e79a8f45de7e1dcfa5621474d1f6e07352ec quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:325370491,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:54fbeb744b82662fd38c0d301ebaad6ca8983707bc44db7235ead0fb7b95808f quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:321482743,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a37a568c63563257309cb0ffb6e185d98f662ff3201d2099cbe0df404b93f0c8 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:314357110,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e176509a104b2f96ee4f5c57275c6a712409aa80ac40071345d0a03bdde2b456 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:307311930,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0c42cd3a74176732e8c06105c47674c7d410c7167c8e3fbd80f9a76e9bfda5bd quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:302591889,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c75977f28becdf4f7065cfa37233464dd31208b1767e620c4f19658f53f8ff8c quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:300756060,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:cf565b2fab365e027962a25a8cffb41aa35cb5a00d001e081d53c7fed5a0c54b quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:300005108,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:78d3478e632c761c18e2dcb55d26e388ecfd126d4fde60317868133dc2fd57f7 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:292832919,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9e7f0468850aeb13585ef049f687cc42c05d82bc0e0200607d1a93d7f9740fe5 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:291621018,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a1aaf99f2ed745c5353d9fc715fa8e9111f42165e3012fad73640c438ba6aa6f quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:290507515,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6cdaecd5dd9df8fd74529be7fa5d8973daf6f4ea95be8acfb2f5ac97773ebe67 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:278303432,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e5bcf6d786fd218e1ef188eb40c39c31a98d03121fba3b7a1f16e87e45a7478b quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:276735847,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1dcc413b621958f97dfbb3fc998a9e225ef155a80ffb151eb4694bf8370b383a quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:274559501,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8c9e61400619c4613db5cc73097d287e3cd5d2125c85d1d84cc30cfdaa1093e7 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:271612142,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:12b11e2000b42ce1aaa228d9c1f4c9177395add2fa43835e667b7fc9007e40e6 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:270734898,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:88ddfbded8bc27b227ed7397ece050b756e522a9ffc34cbfba3c94c5ee58b740 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:269654227,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8bd90fcca7990c0edead15298dcec963968274d299428da95eae41aa23157b90 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:264244297,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:73fdcef5de85e739831c5a8b76dce349a3c8832ff416a46263743d7e61655cbb quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:247448489,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b635694dc0663a1404d43a4a9ac8513a3087c7cfead50f6ab413f3c217c40b2a quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:240293385,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:71051bdf1b96c953fc1dfd48359915bf5c027613de6f5e2fa8adeea8d3dda311 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:236515650,},ContainerImage{Names:[quay.io/app-sre/managed-velero-operator@sha256:a9eb4800dcafe3fe8879662c0413a7e5c53b5fe98ab0bbc81b9fc151044a4e07 quay.io/app-sre/managed-velero-operator:0a73057],SizeBytes:207938263,},ContainerImage{Names:[quay.io/app-sre/cloud-ingress-operator@sha256:ceb064d013f51cdb15075f079eee2dfd5a0f42f4a09b489176abe92e91fb6a56 quay.io/app-sre/cloud-ingress-operator:v0.1.233-a2e288b],SizeBytes:203838503,},ContainerImage{Names:[quay.io/app-sre/managed-upgrade-operator@sha256:558a828a0b3c296cb2c0e52ef143a45177bdae40b1c84bfe8564486c940fd79a quay.io/app-sre/managed-upgrade-operator:9b59534],SizeBytes:198448288,},ContainerImage{Names:[quay.io/app-sre/splunk-forwarder-operator@sha256:bca474387af6112b08406697ef28977d428c1a884ebaf602a3e6ba1b45e6b31b quay.io/app-sre/splunk-forwarder-operator:7e8bc93],SizeBytes:191465195,},ContainerImage{Names:[quay.io/app-sre/configure-alertmanager-operator@sha256:a2bed259f284358220d4e01390621a24f43c6f0d534474a5050cad0c6c6cb812 quay.io/app-sre/configure-alertmanager-operator:v0.1.237-0a8e5f3],SizeBytes:188443907,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Oct 29 21:40:30.257: INFO: 
Logging kubelet events for node ip-10-0-234-238.us-east-2.compute.internal
Oct 29 21:40:30.278: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-234-238.us-east-2.compute.internal
Oct 29 21:40:30.317: INFO: ovs-9f9tm started at 2020-10-29 17:30:48 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.317: INFO: 	Container openvswitch ready: true, restart count 0
Oct 29 21:40:30.317: INFO: managed-velero-operator-registry-9bghr started at 2020-10-29 19:32:07 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.317: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:40:30.317: INFO: redhat-marketplace-cwfzm started at 2020-10-29 19:32:08 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.317: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:40:30.317: INFO: node-exporter-lmwzp started at 2020-10-29 17:30:48 +0000 UTC (1+2 container statuses recorded)
Oct 29 21:40:30.317: INFO: 	Init container init-textfile ready: true, restart count 0
Oct 29 21:40:30.317: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:40:30.317: INFO: 	Container node-exporter ready: true, restart count 0
Oct 29 21:40:30.317: INFO: node-ca-wxm2h started at 2020-10-29 17:30:48 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.317: INFO: 	Container node-ca ready: true, restart count 0
Oct 29 21:40:30.317: INFO: openshift-state-metrics-74d99f5845-62ghm started at 2020-10-29 19:31:54 +0000 UTC (0+3 container statuses recorded)
Oct 29 21:40:30.317: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Oct 29 21:40:30.317: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Oct 29 21:40:30.317: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Oct 29 21:40:30.317: INFO: configure-alertmanager-operator-registry-zcvq5 started at 2020-10-29 19:32:07 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.317: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:40:30.317: INFO: sdn-9tj9v started at 2020-10-29 17:30:48 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:40:30.317: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Oct 29 21:40:30.317: INFO: 	Container sdn ready: true, restart count 0
Oct 29 21:40:30.317: INFO: configure-alertmanager-operator-579dfffff7-nmj5v started at 2020-10-29 17:48:47 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.317: INFO: 	Container configure-alertmanager-operator ready: true, restart count 0
Oct 29 21:40:30.317: INFO: migrator-b8d88f977-j4wqg started at 2020-10-29 19:28:48 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.317: INFO: 	Container migrator ready: true, restart count 0
Oct 29 21:40:30.317: INFO: deployments-pruner-1604005200-dfg2v started at 2020-10-29 21:00:09 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.317: INFO: 	Container deployments-pruner ready: false, restart count 0
Oct 29 21:40:30.317: INFO: multus-bcgtj started at 2020-10-29 17:30:47 +0000 UTC (5+1 container statuses recorded)
Oct 29 21:40:30.317: INFO: 	Init container multus-binary-copy ready: true, restart count 0
Oct 29 21:40:30.317: INFO: 	Init container cni-plugins ready: true, restart count 0
Oct 29 21:40:30.317: INFO: 	Init container routeoverride-cni ready: true, restart count 0
Oct 29 21:40:30.317: INFO: 	Init container whereabouts-cni-bincopy ready: true, restart count 0
Oct 29 21:40:30.317: INFO: 	Init container whereabouts-cni ready: true, restart count 0
Oct 29 21:40:30.317: INFO: 	Container kube-multus ready: true, restart count 0
Oct 29 21:40:30.317: INFO: sre-dns-latency-exporter-5wrqd started at 2020-10-29 17:39:27 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.317: INFO: 	Container main ready: true, restart count 0
Oct 29 21:40:30.317: INFO: thanos-querier-7f688468b8-cfd7g started at 2020-10-29 17:48:47 +0000 UTC (0+5 container statuses recorded)
Oct 29 21:40:30.317: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:40:30.317: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Oct 29 21:40:30.317: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:40:30.317: INFO: 	Container prom-label-proxy ready: true, restart count 0
Oct 29 21:40:30.317: INFO: 	Container thanos-query ready: true, restart count 0
Oct 29 21:40:30.317: INFO: certified-operators-pjgrs started at 2020-10-29 19:32:07 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.317: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:40:30.317: INFO: tuned-6rs49 started at 2020-10-29 17:30:47 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.317: INFO: 	Container tuned ready: true, restart count 0
Oct 29 21:40:30.317: INFO: velero-68656b49f9-zvplm started at 2020-10-29 17:46:41 +0000 UTC (1+1 container statuses recorded)
Oct 29 21:40:30.317: INFO: 	Init container velero-plugin-for-aws ready: true, restart count 0
Oct 29 21:40:30.317: INFO: 	Container velero ready: true, restart count 0
Oct 29 21:40:30.317: INFO: cloud-ingress-operator-registry-zpdvx started at 2020-10-29 19:32:07 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.317: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:40:30.317: INFO: downloads-85df645c7c-pxr2l started at 2020-10-29 19:28:48 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.317: INFO: 	Container download-server ready: true, restart count 0
Oct 29 21:40:30.317: INFO: community-operators-rss65 started at 2020-10-29 19:32:06 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.317: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:40:30.317: INFO: managed-upgrade-operator-7558857f4-cxqkg started at 2020-10-29 17:48:47 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.317: INFO: 	Container managed-upgrade-operator ready: true, restart count 0
Oct 29 21:40:30.317: INFO: managed-velero-operator-8685cc459-lsvfl started at 2020-10-29 17:48:48 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.317: INFO: 	Container managed-velero-operator ready: true, restart count 0
Oct 29 21:40:30.317: INFO: crd-pruner-1603998000-ljqfl started at 2020-10-29 19:00:09 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.317: INFO: 	Container crd-pruner ready: false, restart count 0
Oct 29 21:40:30.317: INFO: downloads-85df645c7c-ft27h started at 2020-10-29 19:31:54 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.317: INFO: 	Container download-server ready: true, restart count 0
Oct 29 21:40:30.317: INFO: rbac-permissions-operator-registry-x892d started at 2020-10-29 19:32:08 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.317: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:40:30.317: INFO: dns-default-jxrwk started at 2020-10-29 17:30:47 +0000 UTC (0+3 container statuses recorded)
Oct 29 21:40:30.317: INFO: 	Container dns ready: true, restart count 0
Oct 29 21:40:30.317: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 29 21:40:30.317: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:40:30.317: INFO: machine-config-daemon-5rrfn started at 2020-10-29 17:30:47 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:40:30.317: INFO: 	Container machine-config-daemon ready: true, restart count 0
Oct 29 21:40:30.317: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:40:30.317: INFO: redhat-operators-pjjcr started at 2020-10-29 19:32:11 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.317: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:40:30.317: INFO: managed-upgrade-operator-catalog-g4m2g started at 2020-10-29 19:32:11 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.317: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:40:30.317: INFO: rbac-permissions-operator-78d47d8556-92kbv started at 2020-10-29 17:48:48 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.317: INFO: 	Container rbac-permissions-operator ready: true, restart count 0
Oct 29 21:40:30.317: INFO: alert-pruner-1603998000-fgw8n started at 2020-10-29 19:00:09 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.317: INFO: 	Container alert-pruner ready: false, restart count 0
Oct 29 21:40:30.317: INFO: deployments-pruner-1603998000-vfdcp started at 2020-10-29 19:00:09 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.317: INFO: 	Container deployments-pruner ready: false, restart count 0
Oct 29 21:40:30.317: INFO: splunk-forwarder-operator-catalog-jg48j started at 2020-10-29 19:32:10 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.317: INFO: 	Container registry-server ready: true, restart count 0
Oct 29 21:40:30.317: INFO: aws-ebs-csi-driver-node-r7m6f started at 2020-10-29 17:30:47 +0000 UTC (0+3 container statuses recorded)
Oct 29 21:40:30.317: INFO: 	Container csi-driver ready: true, restart count 0
Oct 29 21:40:30.317: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Oct 29 21:40:30.317: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Oct 29 21:40:30.317: INFO: network-metrics-daemon-5brxv started at 2020-10-29 17:30:48 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:40:30.317: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:40:30.317: INFO: 	Container network-metrics-daemon ready: true, restart count 0
W1029 21:40:30.344963   53132 metrics_grabber.go:83] Can't find any pods in namespace kube-system to grab metrics from
W1029 21:40:30.344982   53132 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1029 21:40:30.344989   53132 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1029 21:40:30.344995   53132 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Oct 29 21:40:30.403: INFO: 
Latency metrics for node ip-10-0-234-238.us-east-2.compute.internal
Oct 29 21:40:30.403: INFO: 
Logging node info for node ip-10-0-245-254.us-east-2.compute.internal
Oct 29 21:40:30.425: INFO: Node Info: &Node{ObjectMeta:{ip-10-0-245-254.us-east-2.compute.internal   /api/v1/nodes/ip-10-0-245-254.us-east-2.compute.internal be19adf8-253b-49a3-92e7-86a857679344 211458 0 2020-10-29 17:23:46 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:m5.xlarge beta.kubernetes.io/os:linux failure-domain.beta.kubernetes.io/region:us-east-2 failure-domain.beta.kubernetes.io/zone:us-east-2a kubernetes.io/arch:amd64 kubernetes.io/hostname:ip-10-0-245-254 kubernetes.io/os:linux node-role.kubernetes.io/master: node.kubernetes.io/instance-type:m5.xlarge node.openshift.io/os_id:rhcos topology.ebs.csi.aws.com/zone:us-east-2a topology.kubernetes.io/region:us-east-2 topology.kubernetes.io/zone:us-east-2a] map[csi.volume.kubernetes.io/nodeid:{"ebs.csi.aws.com":"i-0e7920427ef118c8d"} machine.openshift.io/machine:openshift-machine-api/jeder-461-cncf2-pvrph-master-1 machineconfiguration.openshift.io/currentConfig:rendered-master-b06f274cfa558dc0b5031aabca48f868 machineconfiguration.openshift.io/desiredConfig:rendered-master-b06f274cfa558dc0b5031aabca48f868 machineconfiguration.openshift.io/reason: machineconfiguration.openshift.io/state:Done volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{nodelink-controller Update v1 2020-10-29 17:26:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machine.openshift.io/machine":{}}}}} {machine-config-controller Update v1 2020-10-29 17:43:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/desiredConfig":{}}}}} {machine-config-daemon Update v1 2020-10-29 17:48:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:machineconfiguration.openshift.io/currentConfig":{},"f:machineconfiguration.openshift.io/reason":{},"f:machineconfiguration.openshift.io/state":{}}}}} {kube-controller-manager Update v1 2020-10-29 19:08:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{}}},"f:spec":{"f:podCIDR":{},"f:podCIDRs":{".":{},"v:\"10.128.1.0/24\"":{}},"f:taints":{}}}} {oc Update v1 2020-10-29 19:08:29 +0000 UTC FieldsV1 {"f:spec":{"f:unschedulable":{}}}} {kubelet Update v1 2020-10-29 21:35:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:csi.volume.kubernetes.io/nodeid":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/instance-type":{},"f:failure-domain.beta.kubernetes.io/region":{},"f:failure-domain.beta.kubernetes.io/zone":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:node-role.kubernetes.io/master":{},"f:node.kubernetes.io/instance-type":{},"f:node.openshift.io/os_id":{},"f:topology.ebs.csi.aws.com/zone":{},"f:topology.kubernetes.io/region":{},"f:topology.kubernetes.io/zone":{}}},"f:spec":{"f:providerID":{}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalDNS\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:attachable-volumes-aws-ebs":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:attachable-volumes-aws-ebs":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"DiskPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"MemoryPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PIDPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:images":{},"f:nodeInfo":{"f:architecture":{},"f:bootID":{},"f:containerRuntimeVersion":{},"f:kernelVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{},"f:machineID":{},"f:operatingSystem":{},"f:osImage":{},"f:systemUUID":{}}}}}]},Spec:NodeSpec{PodCIDR:10.128.1.0/24,DoNotUseExternalID:,ProviderID:aws:///us-east-2a/i-0e7920427ef118c8d,Unschedulable:true,Taints:[]Taint{Taint{Key:node-role.kubernetes.io/master,Value:,Effect:NoSchedule,TimeAdded:<nil>,},Taint{Key:node.kubernetes.io/unschedulable,Value:,Effect:NoSchedule,TimeAdded:2020-10-29 19:08:29 +0000 UTC,},},ConfigSource:nil,PodCIDRs:[10.128.1.0/24],},Status:NodeStatus{Capacity:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{375244435456 0} {<nil>} 366449644Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{16502939648 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},},Allocatable:ResourceList{attachable-volumes-aws-ebs: {{25 0} {<nil>} 25 DecimalSI},cpu: {{3 0} {<nil>} 3 DecimalSI},ephemeral-storage: {{336646249528 0} {<nil>} 336646249528 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{15324340224 0} {<nil>}  BinarySI},pods: {{250 0} {<nil>} 250 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2020-10-29 21:35:28 +0000 UTC,LastTransitionTime:2020-10-29 17:48:23 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2020-10-29 21:35:28 +0000 UTC,LastTransitionTime:2020-10-29 17:48:23 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2020-10-29 21:35:28 +0000 UTC,LastTransitionTime:2020-10-29 17:48:23 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2020-10-29 21:35:28 +0000 UTC,LastTransitionTime:2020-10-29 17:48:23 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.0.245.254,},NodeAddress{Type:Hostname,Address:ip-10-0-245-254.us-east-2.compute.internal,},NodeAddress{Type:InternalDNS,Address:ip-10-0-245-254.us-east-2.compute.internal,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:ec2b6d482b9145cdb130fb38cf5b5e4c,SystemUUID:ec2b6d48-2b91-45cd-b130-fb38cf5b5e4c,BootID:0fb27564-ff56-4d9f-8258-3c1907ece299,KernelVersion:4.18.0-193.24.1.el8_2.dt1.x86_64,OSImage:Red Hat Enterprise Linux CoreOS 46.82.202010091720-0 (Ootpa),ContainerRuntimeVersion:cri-o://1.19.0-20.rhaos4.6.git97d715e.el8,KubeletVersion:v1.19.0+d59ce34,KubeProxyVersion:v1.19.0+d59ce34,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:30094a2d586aa282d85e14f1be19abec1c30ce431673377b0e1c12d83e6bac8c quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:682301224,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c51d44f380ecff7d36b1de4bb3bdbd4ac66abc6669724f28d81bd4af5741a8ac quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:514079422,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:82d1def7312de8ae5dee32d237ad59fe685923e78668fa3547e6bee445cd8842 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:499341399,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4cfd55719faff41e96c2e4be69e3f2381a57b8b3445b80ae4acfe8ee33d7f99b quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:481479938,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ae6a5decd040a6b3adfa074d3211ab92a36b77b2d849962d9a678e1c2c5ef5c1 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:473291773,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8923050603588c27d79b33b371afb651288470d5cdeb14f8e10249bca1a1c461 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:413031192,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8f4882cff3c2f9521215eac681c5abda42876e3e955431c1387fb457940b8344 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:370109724,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:72323ce541f8a26fbad17ef65ff21b51498863bb851635a0faa8d5b1ac6ce0e4 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:366911472,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b7dc5f4101a8cb88c20d853908982258cab77bb0ac391e965b50b15648ddd854 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:359851126,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:93b3e1246884e357e1654e6c9578481aff9eef07eed1f9fdd0e9c8cc89a3770c quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:358141917,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e2b3f973bc5b9e55d2240a556c4648c921a3c8d3e12381757f1990a864208617 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:343841040,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a8214df42df962b965e3f4daad0b61932235e57241160861e503d84e38b775d5 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:337507384,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6bada08687c20afe316c1f8cf76f001f31bedae317f256f3df3affaa5d0dc25e quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:336987966,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6aa4bb97adf2142b0e74ccae7fd3661ada73cbaac803b86bb8712261e916d66d quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:336576591,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:aa4f37543b45bc248db8d9bd2dc45b6e159a8869b044c2310f541afba15b2694 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:336485164,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f52825e9905c926d399cd0b7afbb2b7d0370ae22da0416feac9131d555db0b98 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:336041278,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a2a167f59783ca402118fe35ea5fefbf457e01b64836f8be3be6695aefd76d76 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:335654632,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:5e591cab19b41c7ea26eab6056cd518f6d64b59e8051978de927b1b984abfb1d quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:334612215,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:7584014b0cb8cb2c5a09b909c79f2f8ad6e49854bcfabf72e96a22330bcf6f56 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:333011308,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:357e35286fd26fed015c03a9c451f6fdcf61cf0821d959025e7f800e7c533f29 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:332777114,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f86db3170270fc635dff0d7f1ba6e79a8f45de7e1dcfa5621474d1f6e07352ec quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:325370491,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:54fbeb744b82662fd38c0d301ebaad6ca8983707bc44db7235ead0fb7b95808f quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:321482743,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:11c3a8bdddbbb2229bd68bb80b6009588873118881952c702dfebd1484046191 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:319944375,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9dbb31ac799b2c30270268714dcb3d11bafb329b98639a446657c8c7db41938c quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:318368538,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-release@sha256:d78292e9730dd387ff6198197c8b0598da340be7678e8e1e4810b557a926c2b9 quay.io/openshift-release-dev/ocp-release@sha256:<none>],SizeBytes:317006092,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1ad85195e1a180698fe4b8df82e3d72075efb256b53f593d13e29faaf7f3e15a quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:316310728,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dc6a6a1d4a6b2af67421561e53d1af1d40c99ae72de69f4c3cc390d447f12269 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:315339691,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4691dc29704c9cb06d2345894f1a8f074b58a0d208318c5218241388b0916e1b quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:315327750,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f7b9278ef2fbe988f50e4bdeeea79d9373b55689d17b8c6d7c214429f5b3f9a0 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:315119766,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4289297f0b7ee7edf394348fd07e1fa1b3162655f2a2af2245e23af4b179e7f2 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:314343750,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:65206861218064576dc092040e9c24b0393b8a07502e351f513400f187f38cc7 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:313489720,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:45586fd7a5cfd43ff546dbfb282a70a91eaf0f069f604230af958dc802832f89 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:312615681,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c6b3aaaa38679b1d752ec09bd68c6d80a8911c74ec16d27c49de88ecb97823ee quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:312540521,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:25197b2709c0691c28424c9b07e505a71d13bf481e18bc42636cc84ee8fef033 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:312275620,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2689d3e66bbfdd7d493d969524b8a7da00142d1b4372e3e880bd825beb3da558 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:311351002,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9b564f882e31f497f57a0d99d406d5231eb15e9a97f0b450c21bec2bac7ff033 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:311323012,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a96e2e4a62bca22da0b6903c9e20d7c776bd241f13accf51ede88965b232aca8 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:310235384,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0531ff2ccf0ddea76e42cc9951470528bbd7def094884bc569f660376798f40a quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:308198752,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:12fe384de71c7621d9061f48afafeed3dc337679a66afd8d0a871e200295a1e5 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:307996829,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:82758fbc97d9da98f20eddcfb4a8bc279726b97da96263d4c165b404389cb553 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:307966006,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e176509a104b2f96ee4f5c57275c6a712409aa80ac40071345d0a03bdde2b456 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:307311930,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f18151bf70434e1841ed8182c42e819e92e3d1ad3bbd269c667be8b74ff78444 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:306621135,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:afdf0a3b426ac1c03df52e88a2b884f0714e54a1a03f33091954441a05a7f6b9 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:306116297,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:076b280e17c6bb4cc618db71403ccec75f8196c8849061a40c680a2808292bb6 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:304641618,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:01626d98c80e44e0cd3a522ad019eb236e39c30b0dfff0ac5a6fa98686159286 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:303742706,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:cfe62d81269929501517e75a7d337f7d8fc78ac9a17665adebfef52a2024584d quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:303388513,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c75977f28becdf4f7065cfa37233464dd31208b1767e620c4f19658f53f8ff8c quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:300756060,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3cdb4589ee683c85e3d8f3f239187bc089d30cac6c26847a54894f6c328817c3 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:300278177,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:cf565b2fab365e027962a25a8cffb41aa35cb5a00d001e081d53c7fed5a0c54b quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:300005108,},ContainerImage{Names:[quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:92feaeb8763ece68147b522bfa8914bcd429e9825185b9b9c05247ad2857d03f quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:<none>],SizeBytes:299900737,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Oct 29 21:40:30.425: INFO: 
Logging kubelet events for node ip-10-0-245-254.us-east-2.compute.internal
Oct 29 21:40:30.447: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-245-254.us-east-2.compute.internal
Oct 29 21:40:30.511: INFO: console-575d66b587-7h79z started at 2020-10-29 17:48:52 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.511: INFO: 	Container console ready: true, restart count 0
Oct 29 21:40:30.511: INFO: openshift-controller-manager-operator-8476766cf-srskg started at 2020-10-29 17:48:52 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.511: INFO: 	Container openshift-controller-manager-operator ready: true, restart count 0
Oct 29 21:40:30.511: INFO: openshift-kube-scheduler-ip-10-0-245-254.us-east-2.compute.internal started at 2020-10-29 17:25:50 +0000 UTC (1+2 container statuses recorded)
Oct 29 21:40:30.511: INFO: 	Init container wait-for-host-port ready: true, restart count 0
Oct 29 21:40:30.511: INFO: 	Container kube-scheduler ready: true, restart count 0
Oct 29 21:40:30.511: INFO: 	Container kube-scheduler-cert-syncer ready: true, restart count 0
Oct 29 21:40:30.511: INFO: ovs-qwlht started at 2020-10-29 17:24:02 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.511: INFO: 	Container openvswitch ready: true, restart count 0
Oct 29 21:40:30.511: INFO: machine-config-daemon-l5m7d started at 2020-10-29 17:24:37 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:40:30.511: INFO: 	Container machine-config-daemon ready: true, restart count 0
Oct 29 21:40:30.511: INFO: 	Container oauth-proxy ready: true, restart count 0
Oct 29 21:40:30.511: INFO: etcd-ip-10-0-245-254.us-east-2.compute.internal started at 2020-10-29 17:26:35 +0000 UTC (2+3 container statuses recorded)
Oct 29 21:40:30.511: INFO: 	Init container etcd-ensure-env-vars ready: true, restart count 0
Oct 29 21:40:30.511: INFO: 	Init container etcd-resources-copy ready: true, restart count 0
Oct 29 21:40:30.511: INFO: 	Container etcd ready: true, restart count 0
Oct 29 21:40:30.511: INFO: 	Container etcd-metrics ready: true, restart count 0
Oct 29 21:40:30.511: INFO: 	Container etcdctl ready: true, restart count 0
Oct 29 21:40:30.511: INFO: installer-10-ip-10-0-245-254.us-east-2.compute.internal started at 2020-10-29 19:46:31 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.511: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:40:30.511: INFO: sdn-controller-k68bz started at 2020-10-29 17:24:02 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.511: INFO: 	Container sdn-controller ready: true, restart count 0
Oct 29 21:40:30.511: INFO: installer-9-ip-10-0-245-254.us-east-2.compute.internal started at 2020-10-29 17:46:43 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.511: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:40:30.511: INFO: apiserver-8b58dffb-4vb54 started at 2020-10-29 17:48:53 +0000 UTC (1+2 container statuses recorded)
Oct 29 21:40:30.511: INFO: 	Init container fix-audit-permissions ready: true, restart count 0
Oct 29 21:40:30.511: INFO: 	Container openshift-apiserver ready: true, restart count 0
Oct 29 21:40:30.511: INFO: 	Container openshift-apiserver-check-endpoints ready: true, restart count 0
Oct 29 21:40:30.511: INFO: installer-11-ip-10-0-245-254.us-east-2.compute.internal started at 2020-10-29 19:50:52 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.511: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:40:30.511: INFO: marketplace-operator-67d858bf7d-vpqvw started at 2020-10-29 17:49:02 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.511: INFO: 	Container marketplace-operator ready: true, restart count 0
Oct 29 21:40:30.511: INFO: installer-11-ip-10-0-245-254.us-east-2.compute.internal started at 2020-10-29 19:46:45 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.511: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:40:30.511: INFO: cluster-version-operator-57b6bcb4df-g9px7 started at 2020-10-29 17:46:38 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.511: INFO: 	Container cluster-version-operator ready: true, restart count 0
Oct 29 21:40:30.511: INFO: etcd-operator-5996cd8d48-wg876 started at 2020-10-29 17:48:52 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.511: INFO: 	Container etcd-operator ready: true, restart count 0
Oct 29 21:40:30.511: INFO: apiserver-566bbccb57-tcdbq started at 2020-10-29 17:48:53 +0000 UTC (1+1 container statuses recorded)
Oct 29 21:40:30.511: INFO: 	Init container fix-audit-permissions ready: true, restart count 0
Oct 29 21:40:30.511: INFO: 	Container oauth-apiserver ready: true, restart count 0
Oct 29 21:40:30.511: INFO: machine-approver-5cd4c8656b-9hvtt started at 2020-10-29 17:48:55 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:40:30.511: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:40:30.511: INFO: 	Container machine-approver-controller ready: true, restart count 0
Oct 29 21:40:30.511: INFO: machine-api-operator-7654bc5777-6pzwq started at 2020-10-29 17:48:59 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:40:30.511: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:40:30.511: INFO: 	Container machine-api-operator ready: true, restart count 0
Oct 29 21:40:30.511: INFO: machine-config-operator-7c95b454cf-qk9cb started at 2020-10-29 17:49:01 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.511: INFO: 	Container machine-config-operator ready: true, restart count 0
Oct 29 21:40:30.512: INFO: revision-pruner-11-ip-10-0-245-254.us-east-2.compute.internal started at 2020-10-29 19:47:51 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.512: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:40:30.512: INFO: network-metrics-daemon-l8nn2 started at 2020-10-29 17:23:59 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:40:30.512: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:40:30.512: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Oct 29 21:40:30.512: INFO: controller-manager-ghvds started at 2020-10-29 17:30:59 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.512: INFO: 	Container controller-manager ready: true, restart count 0
Oct 29 21:40:30.512: INFO: openshift-config-operator-7c65574449-jccsp started at 2020-10-29 17:48:52 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.512: INFO: 	Container openshift-config-operator ready: true, restart count 0
Oct 29 21:40:30.512: INFO: aws-ebs-csi-driver-operator-579f547595-n74ks started at 2020-10-29 17:48:54 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.512: INFO: 	Container aws-ebs-csi-driver-operator ready: true, restart count 0
Oct 29 21:40:30.512: INFO: insights-operator-5d7b76ffb6-sr4zx started at 2020-10-29 17:48:53 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.512: INFO: 	Container insights-operator ready: true, restart count 0
Oct 29 21:40:30.512: INFO: cloud-credential-operator-5b849fdff8-8shdg started at 2020-10-29 17:48:55 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:40:30.512: INFO: 	Container cloud-credential-operator ready: true, restart count 0
Oct 29 21:40:30.512: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:40:30.512: INFO: multus-admission-controller-j5cw4 started at 2020-10-29 17:24:26 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:40:30.512: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:40:30.512: INFO: 	Container multus-admission-controller ready: true, restart count 0
Oct 29 21:40:30.512: INFO: node-exporter-wptwd started at 2020-10-29 17:24:51 +0000 UTC (1+2 container statuses recorded)
Oct 29 21:40:30.512: INFO: 	Init container init-textfile ready: true, restart count 0
Oct 29 21:40:30.512: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:40:30.512: INFO: 	Container node-exporter ready: true, restart count 0
Oct 29 21:40:30.512: INFO: node-ca-cv467 started at 2020-10-29 17:25:50 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.512: INFO: 	Container node-ca ready: true, restart count 0
Oct 29 21:40:30.512: INFO: revision-pruner-9-ip-10-0-245-254.us-east-2.compute.internal started at 2020-10-29 17:46:56 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.512: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:40:30.512: INFO: dns-default-5rqxh started at 2020-10-29 17:25:40 +0000 UTC (0+3 container statuses recorded)
Oct 29 21:40:30.512: INFO: 	Container dns ready: true, restart count 0
Oct 29 21:40:30.512: INFO: 	Container dns-node-resolver ready: true, restart count 0
Oct 29 21:40:30.512: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:40:30.512: INFO: revision-pruner-7-ip-10-0-245-254.us-east-2.compute.internal started at 2020-10-29 17:48:23 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.512: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:40:30.512: INFO: cluster-autoscaler-operator-75fff7d7dc-xql24 started at 2020-10-29 17:48:59 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:40:30.512: INFO: 	Container cluster-autoscaler-operator ready: true, restart count 0
Oct 29 21:40:30.512: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:40:30.512: INFO: revision-pruner-10-ip-10-0-245-254.us-east-2.compute.internal started at 2020-10-29 19:46:42 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.512: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:40:30.512: INFO: validation-webhook-zfwdv started at 2020-10-29 17:39:26 +0000 UTC (1+1 container statuses recorded)
Oct 29 21:40:30.512: INFO: 	Init container inject-cert ready: true, restart count 0
Oct 29 21:40:30.512: INFO: 	Container webhooks ready: true, restart count 0
Oct 29 21:40:30.512: INFO: kube-controller-manager-operator-86f4f896b9-qmxvg started at 2020-10-29 17:48:52 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.512: INFO: 	Container kube-controller-manager-operator ready: true, restart count 0
Oct 29 21:40:30.512: INFO: kube-controller-manager-ip-10-0-245-254.us-east-2.compute.internal started at 2020-10-29 17:28:24 +0000 UTC (0+4 container statuses recorded)
Oct 29 21:40:30.512: INFO: 	Container cluster-policy-controller ready: true, restart count 0
Oct 29 21:40:30.512: INFO: 	Container kube-controller-manager ready: true, restart count 0
Oct 29 21:40:30.512: INFO: 	Container kube-controller-manager-cert-syncer ready: true, restart count 0
Oct 29 21:40:30.512: INFO: 	Container kube-controller-manager-recovery-controller ready: true, restart count 0
Oct 29 21:40:30.512: INFO: machine-api-controllers-57968448cb-nxr9k started at 2020-10-29 17:48:59 +0000 UTC (0+7 container statuses recorded)
Oct 29 21:40:30.512: INFO: 	Container kube-rbac-proxy-machine-mtrc ready: true, restart count 0
Oct 29 21:40:30.512: INFO: 	Container kube-rbac-proxy-machineset-mtrc ready: true, restart count 0
Oct 29 21:40:30.512: INFO: 	Container kube-rbac-proxy-mhc-mtrc ready: true, restart count 0
Oct 29 21:40:30.512: INFO: 	Container machine-controller ready: true, restart count 0
Oct 29 21:40:30.512: INFO: 	Container machine-healthcheck-controller ready: true, restart count 0
Oct 29 21:40:30.512: INFO: 	Container machineset-controller ready: true, restart count 0
Oct 29 21:40:30.512: INFO: 	Container nodelink-controller ready: true, restart count 0
Oct 29 21:40:30.512: INFO: revision-pruner-3-ip-10-0-245-254.us-east-2.compute.internal started at 2020-10-29 17:48:23 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.512: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:40:30.512: INFO: kube-apiserver-operator-5c5fc96767-s6hck started at 2020-10-29 17:48:53 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.512: INFO: 	Container kube-apiserver-operator ready: true, restart count 0
Oct 29 21:40:30.512: INFO: aws-ebs-csi-driver-controller-6cbb586dfc-kbcwd started at 2020-10-29 17:48:55 +0000 UTC (0+6 container statuses recorded)
Oct 29 21:40:30.512: INFO: 	Container csi-attacher ready: true, restart count 0
Oct 29 21:40:30.512: INFO: 	Container csi-driver ready: true, restart count 0
Oct 29 21:40:30.512: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Oct 29 21:40:30.512: INFO: 	Container csi-provisioner ready: true, restart count 0
Oct 29 21:40:30.512: INFO: 	Container csi-resizer ready: true, restart count 0
Oct 29 21:40:30.512: INFO: 	Container csi-snapshotter ready: true, restart count 0
Oct 29 21:40:30.512: INFO: revision-pruner-10-ip-10-0-245-254.us-east-2.compute.internal started at 2020-10-29 19:47:11 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.512: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:40:30.512: INFO: aws-ebs-csi-driver-node-gzvtr started at 2020-10-29 17:24:58 +0000 UTC (0+3 container statuses recorded)
Oct 29 21:40:30.512: INFO: 	Container csi-driver ready: true, restart count 0
Oct 29 21:40:30.512: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Oct 29 21:40:30.512: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Oct 29 21:40:30.512: INFO: dns-operator-59c89fbb97-gvh2x started at 2020-10-29 17:48:52 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:40:30.512: INFO: 	Container dns-operator ready: true, restart count 0
Oct 29 21:40:30.512: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:40:30.512: INFO: revision-pruner-10-ip-10-0-245-254.us-east-2.compute.internal started at 2020-10-29 18:06:49 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.512: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:40:30.512: INFO: tuned-n8zzr started at 2020-10-29 17:24:46 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.512: INFO: 	Container tuned ready: true, restart count 0
Oct 29 21:40:30.512: INFO: machine-config-server-zcn4c started at 2020-10-29 17:26:37 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.512: INFO: 	Container machine-config-server ready: true, restart count 0
Oct 29 21:40:30.512: INFO: installer-10-ip-10-0-245-254.us-east-2.compute.internal started at 2020-10-29 18:03:07 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.512: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:40:30.512: INFO: kube-apiserver-ip-10-0-245-254.us-east-2.compute.internal started at 2020-10-29 17:27:30 +0000 UTC (1+5 container statuses recorded)
Oct 29 21:40:30.512: INFO: 	Init container setup ready: true, restart count 0
Oct 29 21:40:30.512: INFO: 	Container kube-apiserver ready: true, restart count 0
Oct 29 21:40:30.512: INFO: 	Container kube-apiserver-cert-regeneration-controller ready: true, restart count 0
Oct 29 21:40:30.512: INFO: 	Container kube-apiserver-cert-syncer ready: true, restart count 0
Oct 29 21:40:30.512: INFO: 	Container kube-apiserver-check-endpoints ready: true, restart count 0
Oct 29 21:40:30.512: INFO: 	Container kube-apiserver-insecure-readyz ready: true, restart count 0
Oct 29 21:40:30.512: INFO: sdn-cmt44 started at 2020-10-29 17:24:03 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:40:30.512: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Oct 29 21:40:30.512: INFO: 	Container sdn ready: true, restart count 0
Oct 29 21:40:30.512: INFO: revision-pruner-8-ip-10-0-245-254.us-east-2.compute.internal started at 2020-10-29 17:48:23 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.512: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:40:30.512: INFO: console-operator-b944ff564-n6dbf started at 2020-10-29 17:48:52 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.512: INFO: 	Container console-operator ready: true, restart count 0
Oct 29 21:40:30.512: INFO: multus-t45l2 started at 2020-10-29 17:23:58 +0000 UTC (5+1 container statuses recorded)
Oct 29 21:40:30.512: INFO: 	Init container multus-binary-copy ready: true, restart count 0
Oct 29 21:40:30.512: INFO: 	Init container cni-plugins ready: true, restart count 0
Oct 29 21:40:30.512: INFO: 	Init container routeoverride-cni ready: true, restart count 0
Oct 29 21:40:30.512: INFO: 	Init container whereabouts-cni-bincopy ready: true, restart count 0
Oct 29 21:40:30.512: INFO: 	Init container whereabouts-cni ready: true, restart count 0
Oct 29 21:40:30.512: INFO: 	Container kube-multus ready: true, restart count 0
Oct 29 21:40:30.512: INFO: cluster-image-registry-operator-8455b85cc6-4g5vr started at 2020-10-29 17:48:52 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.512: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Oct 29 21:40:30.512: INFO: installer-10-ip-10-0-245-254.us-east-2.compute.internal started at 2020-10-29 19:46:37 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.512: INFO: 	Container installer ready: false, restart count 0
Oct 29 21:40:30.512: INFO: recyler-pod-ip-10-0-245-254.us-east-2.compute.internal started at 2020-10-29 17:48:23 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.512: INFO: 	Container recyler-container ready: false, restart count 0
Oct 29 21:40:30.512: INFO: ingress-operator-6d4c4d74f9-dplx6 started at 2020-10-29 17:48:53 +0000 UTC (0+2 container statuses recorded)
Oct 29 21:40:30.512: INFO: 	Container ingress-operator ready: true, restart count 0
Oct 29 21:40:30.512: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Oct 29 21:40:30.512: INFO: etcd-quorum-guard-644f5747b8-gc86j started at 2020-10-29 17:48:53 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.512: INFO: 	Container guard ready: true, restart count 0
Oct 29 21:40:30.512: INFO: packageserver-d4484c6fc-2ptvn started at 2020-10-29 17:49:02 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.512: INFO: 	Container packageserver ready: true, restart count 0
Oct 29 21:40:30.512: INFO: revision-pruner-9-ip-10-0-245-254.us-east-2.compute.internal started at 2020-10-29 17:49:33 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.512: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:40:30.512: INFO: revision-pruner-11-ip-10-0-245-254.us-east-2.compute.internal started at 2020-10-29 19:52:50 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.512: INFO: 	Container pruner ready: false, restart count 0
Oct 29 21:40:30.512: INFO: openshift-apiserver-operator-b8bcd86cf-vpp2h started at 2020-10-29 17:48:52 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.512: INFO: 	Container openshift-apiserver-operator ready: true, restart count 0
Oct 29 21:40:30.512: INFO: csi-snapshot-controller-8bb7f7589-9m9jc started at 2020-10-29 17:48:56 +0000 UTC (0+1 container statuses recorded)
Oct 29 21:40:30.512: INFO: 	Container snapshot-controller ready: true, restart count 0
W1029 21:40:30.534004   53132 metrics_grabber.go:83] Can't find any pods in namespace kube-system to grab metrics from
W1029 21:40:30.534021   53132 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1029 21:40:30.534027   53132 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1029 21:40:30.534033   53132 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Oct 29 21:40:30.676: INFO: 
Latency metrics for node ip-10-0-245-254.us-east-2.compute.internal
Oct 29 21:40:30.676: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "sched-pred-2434" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

 Failure [603.535 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance] [It]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597

  Oct 29 21:40:29.045: Timed out after 10m0s waiting for stable cluster.

  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:55
------------------------------
{"msg":"FAILED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":19,"completed":8,"skipped":3182,"failed":2,"failures":["[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:40:30.724: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:41:03.318: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "namespaces-383" for this suite.
STEP: Destroying namespace "nsdeletetest-1942" for this suite.
Oct 29 21:41:03.444: INFO: Namespace nsdeletetest-1942 was already deleted
STEP: Destroying namespace "nsdeletetest-6817" for this suite.

 [SLOW TEST:32.743 seconds]
[sig-api-machinery] Namespaces [Serial]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":19,"completed":9,"skipped":3313,"failed":2,"failures":["[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:41:03.469: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Oct 29 21:41:03.603: INFO: Waiting up to 1m0s for all nodes to be ready
Oct 29 21:42:03.955: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 21:42:03.978: INFO: Starting informer...
STEP: Starting pod...
Oct 29 21:42:04.033: INFO: Pod is running on ip-10-0-142-212.us-east-2.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Oct 29 21:42:04.126: INFO: Pod wasn't evicted. Proceeding
Oct 29 21:42:04.126: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Oct 29 21:43:19.222: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:43:19.222: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "taint-single-pod-7308" for this suite.

 [SLOW TEST:135.860 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  removing taint cancels eviction [Disruptive] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":19,"completed":10,"skipped":3359,"failed":2,"failures":["[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:43:19.339: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Oct 29 21:43:20.805: INFO: Pod name wrapped-volume-race-252a69fc-4759-4f87-94b8-9ee79e003e93: Found 0 pods out of 5
Oct 29 21:43:25.887: INFO: Pod name wrapped-volume-race-252a69fc-4759-4f87-94b8-9ee79e003e93: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-252a69fc-4759-4f87-94b8-9ee79e003e93 in namespace emptydir-wrapper-6151, will wait for the garbage collector to delete the pods
Oct 29 21:43:26.091: INFO: Deleting ReplicationController wrapped-volume-race-252a69fc-4759-4f87-94b8-9ee79e003e93 took: 24.723496ms
Oct 29 21:43:26.191: INFO: Terminating ReplicationController wrapped-volume-race-252a69fc-4759-4f87-94b8-9ee79e003e93 pods took: 100.237715ms
STEP: Creating RC which spawns configmap-volume pods
Oct 29 21:43:37.467: INFO: Pod name wrapped-volume-race-7c0c19ca-eed5-4070-a470-712326259ec1: Found 1 pods out of 5
Oct 29 21:43:42.556: INFO: Pod name wrapped-volume-race-7c0c19ca-eed5-4070-a470-712326259ec1: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-7c0c19ca-eed5-4070-a470-712326259ec1 in namespace emptydir-wrapper-6151, will wait for the garbage collector to delete the pods
Oct 29 21:43:44.808: INFO: Deleting ReplicationController wrapped-volume-race-7c0c19ca-eed5-4070-a470-712326259ec1 took: 25.185455ms
Oct 29 21:43:44.908: INFO: Terminating ReplicationController wrapped-volume-race-7c0c19ca-eed5-4070-a470-712326259ec1 pods took: 100.136098ms
STEP: Creating RC which spawns configmap-volume pods
Oct 29 21:43:47.487: INFO: Pod name wrapped-volume-race-9e721543-582e-4b5e-a8cb-14601971d34b: Found 0 pods out of 5
Oct 29 21:43:52.570: INFO: Pod name wrapped-volume-race-9e721543-582e-4b5e-a8cb-14601971d34b: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-9e721543-582e-4b5e-a8cb-14601971d34b in namespace emptydir-wrapper-6151, will wait for the garbage collector to delete the pods
Oct 29 21:43:52.775: INFO: Deleting ReplicationController wrapped-volume-race-9e721543-582e-4b5e-a8cb-14601971d34b took: 25.474271ms
Oct 29 21:43:52.875: INFO: Terminating ReplicationController wrapped-volume-race-9e721543-582e-4b5e-a8cb-14601971d34b pods took: 100.351967ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:44:08.679: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6151" for this suite.

 [SLOW TEST:49.406 seconds]
[sig-storage] EmptyDir wrapper volumes
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":19,"completed":11,"skipped":3868,"failed":2,"failures":["[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:44:08.754: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop complex daemon [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 21:44:09.034: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Oct 29 21:44:09.079: INFO: Number of nodes with available pods: 0
Oct 29 21:44:09.079: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Oct 29 21:44:09.211: INFO: Number of nodes with available pods: 0
Oct 29 21:44:09.211: INFO: Node ip-10-0-142-212.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:44:10.233: INFO: Number of nodes with available pods: 0
Oct 29 21:44:10.233: INFO: Node ip-10-0-142-212.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:44:11.233: INFO: Number of nodes with available pods: 0
Oct 29 21:44:11.233: INFO: Node ip-10-0-142-212.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:44:12.233: INFO: Number of nodes with available pods: 0
Oct 29 21:44:12.233: INFO: Node ip-10-0-142-212.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:44:13.236: INFO: Number of nodes with available pods: 1
Oct 29 21:44:13.236: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Oct 29 21:44:13.374: INFO: Number of nodes with available pods: 1
Oct 29 21:44:13.374: INFO: Number of running nodes: 0, number of available pods: 1
Oct 29 21:44:14.397: INFO: Number of nodes with available pods: 0
Oct 29 21:44:14.397: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Oct 29 21:44:14.445: INFO: Number of nodes with available pods: 0
Oct 29 21:44:14.445: INFO: Node ip-10-0-142-212.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:44:15.467: INFO: Number of nodes with available pods: 0
Oct 29 21:44:15.467: INFO: Node ip-10-0-142-212.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:44:16.467: INFO: Number of nodes with available pods: 0
Oct 29 21:44:16.467: INFO: Node ip-10-0-142-212.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:44:17.467: INFO: Number of nodes with available pods: 0
Oct 29 21:44:17.467: INFO: Node ip-10-0-142-212.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:44:18.467: INFO: Number of nodes with available pods: 0
Oct 29 21:44:18.467: INFO: Node ip-10-0-142-212.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:44:19.467: INFO: Number of nodes with available pods: 0
Oct 29 21:44:19.467: INFO: Node ip-10-0-142-212.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:44:20.468: INFO: Number of nodes with available pods: 0
Oct 29 21:44:20.468: INFO: Node ip-10-0-142-212.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:44:21.467: INFO: Number of nodes with available pods: 0
Oct 29 21:44:21.467: INFO: Node ip-10-0-142-212.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:44:22.467: INFO: Number of nodes with available pods: 0
Oct 29 21:44:22.467: INFO: Node ip-10-0-142-212.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:44:23.467: INFO: Number of nodes with available pods: 0
Oct 29 21:44:23.467: INFO: Node ip-10-0-142-212.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:44:24.467: INFO: Number of nodes with available pods: 0
Oct 29 21:44:24.467: INFO: Node ip-10-0-142-212.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:44:25.467: INFO: Number of nodes with available pods: 0
Oct 29 21:44:25.467: INFO: Node ip-10-0-142-212.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:44:26.467: INFO: Number of nodes with available pods: 0
Oct 29 21:44:26.467: INFO: Node ip-10-0-142-212.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:44:27.467: INFO: Number of nodes with available pods: 0
Oct 29 21:44:27.467: INFO: Node ip-10-0-142-212.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:44:28.467: INFO: Number of nodes with available pods: 0
Oct 29 21:44:28.467: INFO: Node ip-10-0-142-212.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:44:29.467: INFO: Number of nodes with available pods: 1
Oct 29 21:44:29.467: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1259, will wait for the garbage collector to delete the pods
Oct 29 21:44:29.608: INFO: Deleting DaemonSet.extensions daemon-set took: 25.435778ms
Oct 29 21:44:29.708: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.26533ms
Oct 29 21:44:33.330: INFO: Number of nodes with available pods: 0
Oct 29 21:44:33.330: INFO: Number of running nodes: 0, number of available pods: 0
Oct 29 21:44:33.351: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1259/daemonsets","resourceVersion":"215691"},"items":null}

Oct 29 21:44:33.372: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1259/pods","resourceVersion":"215691"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:44:33.545: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "daemonsets-1259" for this suite.

 [SLOW TEST:24.856 seconds]
[sig-apps] Daemon set [Serial]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":19,"completed":12,"skipped":4087,"failed":2,"failures":["[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:44:33.617: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop simple daemon [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Oct 29 21:44:33.986: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:44:33.986: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:44:33.986: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:44:34.010: INFO: Number of nodes with available pods: 0
Oct 29 21:44:34.010: INFO: Node ip-10-0-142-212.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:44:35.092: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:44:35.092: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:44:35.092: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:44:35.113: INFO: Number of nodes with available pods: 0
Oct 29 21:44:35.113: INFO: Node ip-10-0-142-212.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:44:36.091: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:44:36.091: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:44:36.091: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:44:36.113: INFO: Number of nodes with available pods: 0
Oct 29 21:44:36.113: INFO: Node ip-10-0-142-212.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:44:37.092: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:44:37.092: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:44:37.092: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:44:37.114: INFO: Number of nodes with available pods: 1
Oct 29 21:44:37.114: INFO: Node ip-10-0-142-212.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:44:38.091: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:44:38.091: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:44:38.091: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:44:38.113: INFO: Number of nodes with available pods: 4
Oct 29 21:44:38.113: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Stop a daemon pod, check that the daemon pod is revived.
Oct 29 21:44:38.204: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:44:38.204: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:44:38.204: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:44:38.226: INFO: Number of nodes with available pods: 3
Oct 29 21:44:38.226: INFO: Node ip-10-0-158-72.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:44:39.308: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:44:39.308: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:44:39.308: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:44:39.333: INFO: Number of nodes with available pods: 3
Oct 29 21:44:39.333: INFO: Node ip-10-0-158-72.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:44:40.307: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:44:40.307: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:44:40.307: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:44:40.329: INFO: Number of nodes with available pods: 3
Oct 29 21:44:40.329: INFO: Node ip-10-0-158-72.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:44:41.308: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:44:41.308: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:44:41.308: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:44:41.339: INFO: Number of nodes with available pods: 3
Oct 29 21:44:41.339: INFO: Node ip-10-0-158-72.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:44:42.308: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:44:42.308: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:44:42.308: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:44:42.330: INFO: Number of nodes with available pods: 3
Oct 29 21:44:42.330: INFO: Node ip-10-0-158-72.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:44:43.308: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:44:43.308: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:44:43.308: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:44:43.331: INFO: Number of nodes with available pods: 3
Oct 29 21:44:43.331: INFO: Node ip-10-0-158-72.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:44:44.308: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:44:44.308: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:44:44.308: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:44:44.330: INFO: Number of nodes with available pods: 3
Oct 29 21:44:44.330: INFO: Node ip-10-0-158-72.us-east-2.compute.internal is running more than one daemon pod
Oct 29 21:44:45.307: INFO: DaemonSet pods can't tolerate node ip-10-0-147-237.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:28 +0000 UTC}], skip checking this node
Oct 29 21:44:45.307: INFO: DaemonSet pods can't tolerate node ip-10-0-231-174.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:44:45.307: INFO: DaemonSet pods can't tolerate node ip-10-0-245-254.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2020-10-29 19:08:29 +0000 UTC}], skip checking this node
Oct 29 21:44:45.329: INFO: Number of nodes with available pods: 4
Oct 29 21:44:45.329: INFO: Number of running nodes: 4, number of available pods: 4
[AfterEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3159, will wait for the garbage collector to delete the pods
Oct 29 21:44:45.448: INFO: Deleting DaemonSet.extensions daemon-set took: 24.858174ms
Oct 29 21:44:45.548: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.391366ms
Oct 29 21:44:57.376: INFO: Number of nodes with available pods: 0
Oct 29 21:44:57.376: INFO: Number of running nodes: 0, number of available pods: 0
Oct 29 21:44:57.413: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3159/daemonsets","resourceVersion":"215986"},"items":null}

Oct 29 21:44:57.436: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3159/pods","resourceVersion":"215986"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:44:57.567: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "daemonsets-3159" for this suite.

 [SLOW TEST:24.017 seconds]
[sig-apps] Daemon set [Serial]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":19,"completed":13,"skipped":4206,"failed":2,"failures":["[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:44:57.646: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:45:04.153: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "namespaces-5242" for this suite.
STEP: Destroying namespace "nsdeletetest-6639" for this suite.
Oct 29 21:45:04.295: INFO: Namespace nsdeletetest-6639 was already deleted
STEP: Destroying namespace "nsdeletetest-2890" for this suite.

 [SLOW TEST:6.672 seconds]
[sig-api-machinery] Namespaces [Serial]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":19,"completed":14,"skipped":4651,"failed":2,"failures":["[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:45:04.321: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Oct 29 21:45:04.542: INFO: Waiting up to 1m0s for all nodes to be ready
Oct 29 21:46:04.895: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
Oct 29 21:46:04.978: INFO: Created pod: pod0-sched-preemption-low-priority
Oct 29 21:46:05.034: INFO: Created pod: pod1-sched-preemption-medium-priority
Oct 29 21:46:05.088: INFO: Created pod: pod2-sched-preemption-medium-priority
Oct 29 21:46:05.143: INFO: Created pod: pod3-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:46:21.507: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "sched-preemption-862" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

 [SLOW TEST:77.499 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":19,"completed":15,"skipped":4788,"failed":2,"failures":["[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:46:21.827: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Oct 29 21:46:21.990: INFO: Waiting up to 1m0s for all nodes to be ready
Oct 29 21:47:22.329: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 21:47:22.352: INFO: Starting informer...
STEP: Starting pods...
Oct 29 21:47:22.426: INFO: Pod1 is running on ip-10-0-142-212.us-east-2.compute.internal. Tainting Node
Oct 29 21:47:26.538: INFO: Pod2 is running on ip-10-0-142-212.us-east-2.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Oct 29 21:47:36.558: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Oct 29 21:47:56.554: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:47:56.631: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-7216" for this suite.

 [SLOW TEST:94.872 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":19,"completed":16,"skipped":4928,"failed":2,"failures":["[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:47:56.704: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:47:57.041: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "namespaces-8781" for this suite.
STEP: Destroying namespace "nspatchtest-c2f7405c-6106-4674-be50-64ec89cc432a-7413" for this suite.
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":19,"completed":17,"skipped":5051,"failed":2,"failures":["[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSOct 29 21:47:57.119: INFO: Running AfterSuite actions on all nodes
Oct 29 21:47:57.119: INFO: Running AfterSuite actions on node 1
Oct 29 21:47:57.119: INFO: Dumping logs locally to: /home/jeder/osd_46_conformance/origin/_output/scripts/conformance-k8s/artifacts
Oct 29 21:47:57.120: INFO: Error running cluster/log-dump/log-dump.sh: fork/exec ../../cluster/log-dump/log-dump.sh: no such file or directory

JUnit report was created: /home/jeder/osd_46_conformance/origin/_output/scripts/conformance-k8s/artifacts/junit_01.xml
{"msg":"Test Suite completed","total":19,"completed":17,"skipped":5215,"failed":2,"failures":["[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]"]}


Summarizing 2 Failures:

[Fail] [sig-scheduling] SchedulerPredicates [Serial] [It] validates resource limits of pods that are allowed to run  [Conformance] 
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:55

[Fail] [sig-scheduling] SchedulerPredicates [Serial] [It] validates that NodeSelector is respected if not matching  [Conformance] 
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:55

Ran 19 of 5234 Specs in 2269.506 seconds
FAIL! -- 17 Passed | 2 Failed | 0 Pending | 5215 Skipped
--- FAIL: TestE2E (2269.56s)
FAIL

Ginkgo ran 1 suite in 37m50.837347752s
Test Suite Failed
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1604008077 - Will randomize all specs
Will run 5234 specs

Running in parallel across 4 nodes

Oct 29 21:47:58.871: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
Oct 29 21:47:58.875: INFO: Waiting up to 30m0s for all (but 3) nodes to be schedulable
Oct 29 21:47:59.031: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Oct 29 21:47:59.108: INFO: 0 / 0 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Oct 29 21:47:59.108: INFO: expected 0 pod replicas in namespace 'kube-system', 0 are Running and Ready.
Oct 29 21:47:59.108: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Oct 29 21:47:59.133: INFO: e2e test version: v1.19.4-rc.0.27+9dfb4c876bfca7
Oct 29 21:47:59.153: INFO: kube-apiserver version: v1.19.0+d59ce34
Oct 29 21:47:59.153: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
Oct 29 21:47:59.177: INFO: Cluster IP family: ipv4

SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
Oct 29 21:47:59.179: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
Oct 29 21:47:59.252: INFO: Cluster IP family: ipv4

S
------------------------------
Oct 29 21:47:59.183: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
Oct 29 21:47:59.255: INFO: Cluster IP family: ipv4

S
------------------------------
Oct 29 21:47:59.180: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
Oct 29 21:47:59.256: INFO: Cluster IP family: ipv4

SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected configMap
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:47:59.204: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename projected
Oct 29 21:47:59.306: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-ad3e2c5a-84d4-49fa-84bf-f9d36f7894ba
STEP: Creating a pod to test consume configMaps
Oct 29 21:47:59.580: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e06a4530-8e25-4ae7-8a27-f8da55f36375" in namespace "projected-4762" to be "Succeeded or Failed"
Oct 29 21:47:59.606: INFO: Pod "pod-projected-configmaps-e06a4530-8e25-4ae7-8a27-f8da55f36375": Phase="Pending", Reason="", readiness=false. Elapsed: 25.331634ms
Oct 29 21:48:01.628: INFO: Pod "pod-projected-configmaps-e06a4530-8e25-4ae7-8a27-f8da55f36375": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047935769s
Oct 29 21:48:03.650: INFO: Pod "pod-projected-configmaps-e06a4530-8e25-4ae7-8a27-f8da55f36375": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.069691031s
STEP: Saw pod success
Oct 29 21:48:03.650: INFO: Pod "pod-projected-configmaps-e06a4530-8e25-4ae7-8a27-f8da55f36375" satisfied condition "Succeeded or Failed"
Oct 29 21:48:03.674: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-projected-configmaps-e06a4530-8e25-4ae7-8a27-f8da55f36375 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Oct 29 21:48:03.754: INFO: Waiting for pod pod-projected-configmaps-e06a4530-8e25-4ae7-8a27-f8da55f36375 to disappear
Oct 29 21:48:03.798: INFO: Pod pod-projected-configmaps-e06a4530-8e25-4ae7-8a27-f8da55f36375 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:48:03.798: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-4762" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":1,"skipped":25,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected secret
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:47:59.258: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename projected
Oct 29 21:47:59.498: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-06198698-044f-4e24-890c-4ed9bf9d1a8d
STEP: Creating a pod to test consume secrets
Oct 29 21:47:59.640: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bad889d7-25fa-4afb-a0ae-a32249c30213" in namespace "projected-6346" to be "Succeeded or Failed"
Oct 29 21:47:59.679: INFO: Pod "pod-projected-secrets-bad889d7-25fa-4afb-a0ae-a32249c30213": Phase="Pending", Reason="", readiness=false. Elapsed: 38.453272ms
Oct 29 21:48:01.700: INFO: Pod "pod-projected-secrets-bad889d7-25fa-4afb-a0ae-a32249c30213": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06018333s
Oct 29 21:48:03.727: INFO: Pod "pod-projected-secrets-bad889d7-25fa-4afb-a0ae-a32249c30213": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.08705163s
STEP: Saw pod success
Oct 29 21:48:03.727: INFO: Pod "pod-projected-secrets-bad889d7-25fa-4afb-a0ae-a32249c30213" satisfied condition "Succeeded or Failed"
Oct 29 21:48:03.751: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-projected-secrets-bad889d7-25fa-4afb-a0ae-a32249c30213 container projected-secret-volume-test: <nil>
STEP: delete the pod
Oct 29 21:48:03.866: INFO: Waiting for pod pod-projected-secrets-bad889d7-25fa-4afb-a0ae-a32249c30213 to disappear
Oct 29 21:48:03.888: INFO: Pod pod-projected-secrets-bad889d7-25fa-4afb-a0ae-a32249c30213 no longer exists
[AfterEach] [sig-storage] Projected secret
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:48:03.888: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-6346" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":1,"skipped":2,"failed":0}

SSSS
------------------------------
[BeforeEach] [sig-api-machinery] Secrets
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:48:04.007: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name secret-emptykey-test-bc112949-75c5-4f21-a8e0-f285e7ff2ded
[AfterEach] [sig-api-machinery] Secrets
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:48:04.912: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-5202" for this suite.


------------------------------
{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":-1,"completed":2,"skipped":6,"failed":0}

S
------------------------------
[BeforeEach] [sig-network] Service endpoints latency
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:47:59.274: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename svc-latency
Oct 29 21:47:59.498: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 21:47:59.538: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: creating replication controller svc-latency-rc in namespace svc-latency-3538
I1029 21:47:59.589569   53211 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3538, replica count: 1
I1029 21:48:00.640111   53211 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1029 21:48:01.640406   53211 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1029 21:48:02.640607   53211 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 29 21:48:02.773: INFO: Created: latency-svc-stwcp
Oct 29 21:48:02.781: INFO: Got endpoints: latency-svc-stwcp [41.036793ms]
Oct 29 21:48:02.819: INFO: Created: latency-svc-pxffh
Oct 29 21:48:02.833: INFO: Created: latency-svc-kjfr8
Oct 29 21:48:02.838: INFO: Got endpoints: latency-svc-pxffh [56.70529ms]
Oct 29 21:48:02.861: INFO: Created: latency-svc-mshhx
Oct 29 21:48:02.861: INFO: Got endpoints: latency-svc-kjfr8 [79.934772ms]
Oct 29 21:48:02.866: INFO: Got endpoints: latency-svc-mshhx [84.527336ms]
Oct 29 21:48:02.881: INFO: Created: latency-svc-lpgzw
Oct 29 21:48:02.901: INFO: Created: latency-svc-mtp59
Oct 29 21:48:02.904: INFO: Got endpoints: latency-svc-lpgzw [122.585706ms]
Oct 29 21:48:02.917: INFO: Got endpoints: latency-svc-mtp59 [135.218999ms]
Oct 29 21:48:02.956: INFO: Created: latency-svc-6c84c
Oct 29 21:48:02.968: INFO: Created: latency-svc-vl785
Oct 29 21:48:02.971: INFO: Created: latency-svc-rdvfj
Oct 29 21:48:02.981: INFO: Got endpoints: latency-svc-6c84c [199.061011ms]
Oct 29 21:48:02.988: INFO: Got endpoints: latency-svc-rdvfj [205.878449ms]
Oct 29 21:48:02.988: INFO: Got endpoints: latency-svc-vl785 [206.176321ms]
Oct 29 21:48:02.990: INFO: Created: latency-svc-rqbbv
Oct 29 21:48:03.002: INFO: Created: latency-svc-tbs4k
Oct 29 21:48:03.016: INFO: Created: latency-svc-84bt9
Oct 29 21:48:03.016: INFO: Got endpoints: latency-svc-rqbbv [234.312148ms]
Oct 29 21:48:03.016: INFO: Got endpoints: latency-svc-tbs4k [234.381273ms]
Oct 29 21:48:03.026: INFO: Created: latency-svc-2mpvf
Oct 29 21:48:03.028: INFO: Got endpoints: latency-svc-84bt9 [246.226874ms]
Oct 29 21:48:03.044: INFO: Created: latency-svc-mmnqf
Oct 29 21:48:03.078: INFO: Got endpoints: latency-svc-mmnqf [296.226493ms]
Oct 29 21:48:03.078: INFO: Got endpoints: latency-svc-2mpvf [296.321238ms]
Oct 29 21:48:03.086: INFO: Created: latency-svc-v7tzm
Oct 29 21:48:03.091: INFO: Created: latency-svc-kptbl
Oct 29 21:48:03.095: INFO: Got endpoints: latency-svc-v7tzm [313.319606ms]
Oct 29 21:48:03.101: INFO: Got endpoints: latency-svc-kptbl [318.857488ms]
Oct 29 21:48:03.108: INFO: Created: latency-svc-jf54n
Oct 29 21:48:03.117: INFO: Got endpoints: latency-svc-jf54n [278.74228ms]
Oct 29 21:48:03.133: INFO: Created: latency-svc-krl72
Oct 29 21:48:03.151: INFO: Got endpoints: latency-svc-krl72 [289.944843ms]
Oct 29 21:48:03.161: INFO: Created: latency-svc-6j46z
Oct 29 21:48:03.168: INFO: Got endpoints: latency-svc-6j46z [302.036319ms]
Oct 29 21:48:03.176: INFO: Created: latency-svc-lzwdf
Oct 29 21:48:03.189: INFO: Created: latency-svc-j7pzj
Oct 29 21:48:03.197: INFO: Got endpoints: latency-svc-lzwdf [293.097938ms]
Oct 29 21:48:03.215: INFO: Created: latency-svc-kjhjw
Oct 29 21:48:03.216: INFO: Got endpoints: latency-svc-j7pzj [298.624224ms]
Oct 29 21:48:03.225: INFO: Created: latency-svc-68rqb
Oct 29 21:48:03.231: INFO: Got endpoints: latency-svc-kjhjw [249.863954ms]
Oct 29 21:48:03.234: INFO: Got endpoints: latency-svc-68rqb [246.490405ms]
Oct 29 21:48:03.235: INFO: Created: latency-svc-qlx76
Oct 29 21:48:03.256: INFO: Got endpoints: latency-svc-qlx76 [268.463658ms]
Oct 29 21:48:03.272: INFO: Created: latency-svc-zqt6r
Oct 29 21:48:03.284: INFO: Created: latency-svc-z9fzp
Oct 29 21:48:03.286: INFO: Got endpoints: latency-svc-zqt6r [270.085169ms]
Oct 29 21:48:03.290: INFO: Created: latency-svc-fj9st
Oct 29 21:48:03.296: INFO: Created: latency-svc-68nlb
Oct 29 21:48:03.312: INFO: Got endpoints: latency-svc-z9fzp [296.048857ms]
Oct 29 21:48:03.313: INFO: Got endpoints: latency-svc-fj9st [284.700218ms]
Oct 29 21:48:03.313: INFO: Got endpoints: latency-svc-68nlb [235.141627ms]
Oct 29 21:48:03.341: INFO: Created: latency-svc-4xskr
Oct 29 21:48:03.351: INFO: Got endpoints: latency-svc-4xskr [272.809939ms]
Oct 29 21:48:03.353: INFO: Created: latency-svc-h7wpd
Oct 29 21:48:03.366: INFO: Created: latency-svc-94jbw
Oct 29 21:48:03.366: INFO: Got endpoints: latency-svc-h7wpd [270.616431ms]
Oct 29 21:48:03.374: INFO: Got endpoints: latency-svc-94jbw [273.154639ms]
Oct 29 21:48:03.379: INFO: Created: latency-svc-k8v7b
Oct 29 21:48:03.403: INFO: Created: latency-svc-6mtft
Oct 29 21:48:03.403: INFO: Got endpoints: latency-svc-k8v7b [285.867334ms]
Oct 29 21:48:03.409: INFO: Created: latency-svc-qc697
Oct 29 21:48:03.438: INFO: Got endpoints: latency-svc-qc697 [269.125169ms]
Oct 29 21:48:03.438: INFO: Got endpoints: latency-svc-6mtft [286.145107ms]
Oct 29 21:48:03.445: INFO: Created: latency-svc-ctb9q
Oct 29 21:48:03.453: INFO: Got endpoints: latency-svc-ctb9q [255.686974ms]
Oct 29 21:48:03.455: INFO: Created: latency-svc-5w2ck
Oct 29 21:48:03.464: INFO: Got endpoints: latency-svc-5w2ck [248.243049ms]
Oct 29 21:48:03.466: INFO: Created: latency-svc-hsghp
Oct 29 21:48:03.473: INFO: Got endpoints: latency-svc-hsghp [242.20316ms]
Oct 29 21:48:03.480: INFO: Created: latency-svc-kkphs
Oct 29 21:48:03.481: INFO: Created: latency-svc-z9zzd
Oct 29 21:48:03.492: INFO: Got endpoints: latency-svc-kkphs [258.003504ms]
Oct 29 21:48:03.493: INFO: Got endpoints: latency-svc-z9zzd [236.597778ms]
Oct 29 21:48:03.497: INFO: Created: latency-svc-d6mjh
Oct 29 21:48:03.503: INFO: Got endpoints: latency-svc-d6mjh [216.594779ms]
Oct 29 21:48:03.505: INFO: Created: latency-svc-5kh78
Oct 29 21:48:03.513: INFO: Got endpoints: latency-svc-5kh78 [200.857796ms]
Oct 29 21:48:03.515: INFO: Created: latency-svc-cb4cw
Oct 29 21:48:03.525: INFO: Created: latency-svc-s6lkc
Oct 29 21:48:03.526: INFO: Got endpoints: latency-svc-cb4cw [212.161687ms]
Oct 29 21:48:03.532: INFO: Created: latency-svc-mwkh7
Oct 29 21:48:03.533: INFO: Got endpoints: latency-svc-s6lkc [219.737448ms]
Oct 29 21:48:03.541: INFO: Created: latency-svc-fv642
Oct 29 21:48:03.548: INFO: Created: latency-svc-h4bk4
Oct 29 21:48:03.574: INFO: Got endpoints: latency-svc-mwkh7 [222.994517ms]
Oct 29 21:48:03.575: INFO: Got endpoints: latency-svc-fv642 [208.601226ms]
Oct 29 21:48:03.575: INFO: Got endpoints: latency-svc-h4bk4 [200.940309ms]
Oct 29 21:48:03.582: INFO: Created: latency-svc-q4ltg
Oct 29 21:48:03.589: INFO: Created: latency-svc-wxm55
Oct 29 21:48:03.611: INFO: Got endpoints: latency-svc-q4ltg [207.554075ms]
Oct 29 21:48:03.613: INFO: Got endpoints: latency-svc-wxm55 [175.22333ms]
Oct 29 21:48:03.615: INFO: Created: latency-svc-k6p48
Oct 29 21:48:03.627: INFO: Got endpoints: latency-svc-k6p48 [189.048106ms]
Oct 29 21:48:03.628: INFO: Created: latency-svc-xdnbh
Oct 29 21:48:03.637: INFO: Got endpoints: latency-svc-xdnbh [183.4825ms]
Oct 29 21:48:03.639: INFO: Created: latency-svc-wf498
Oct 29 21:48:03.647: INFO: Got endpoints: latency-svc-wf498 [182.780214ms]
Oct 29 21:48:03.649: INFO: Created: latency-svc-zs5bx
Oct 29 21:48:03.655: INFO: Got endpoints: latency-svc-zs5bx [182.225321ms]
Oct 29 21:48:03.657: INFO: Created: latency-svc-x9p48
Oct 29 21:48:03.663: INFO: Got endpoints: latency-svc-x9p48 [36.09944ms]
Oct 29 21:48:03.666: INFO: Created: latency-svc-x9kfg
Oct 29 21:48:03.687: INFO: Got endpoints: latency-svc-x9kfg [194.591707ms]
Oct 29 21:48:03.688: INFO: Created: latency-svc-82tdh
Oct 29 21:48:03.710: INFO: Created: latency-svc-gzfxb
Oct 29 21:48:03.710: INFO: Got endpoints: latency-svc-82tdh [217.209803ms]
Oct 29 21:48:03.717: INFO: Got endpoints: latency-svc-gzfxb [214.360537ms]
Oct 29 21:48:03.720: INFO: Created: latency-svc-rlw2j
Oct 29 21:48:03.729: INFO: Got endpoints: latency-svc-rlw2j [216.041101ms]
Oct 29 21:48:03.733: INFO: Created: latency-svc-l47q4
Oct 29 21:48:03.742: INFO: Created: latency-svc-q5w5g
Oct 29 21:48:03.748: INFO: Got endpoints: latency-svc-l47q4 [222.072264ms]
Oct 29 21:48:03.751: INFO: Got endpoints: latency-svc-q5w5g [218.13492ms]
Oct 29 21:48:03.752: INFO: Created: latency-svc-4gnkp
Oct 29 21:48:03.759: INFO: Got endpoints: latency-svc-4gnkp [185.28872ms]
Oct 29 21:48:03.762: INFO: Created: latency-svc-5vwqf
Oct 29 21:48:03.767: INFO: Got endpoints: latency-svc-5vwqf [192.703413ms]
Oct 29 21:48:03.771: INFO: Created: latency-svc-46npj
Oct 29 21:48:03.800: INFO: Got endpoints: latency-svc-46npj [225.393236ms]
Oct 29 21:48:03.821: INFO: Created: latency-svc-mhbwd
Oct 29 21:48:03.826: INFO: Created: latency-svc-vbmvm
Oct 29 21:48:03.826: INFO: Got endpoints: latency-svc-vbmvm [215.193762ms]
Oct 29 21:48:03.830: INFO: Got endpoints: latency-svc-mhbwd [217.008725ms]
Oct 29 21:48:03.832: INFO: Created: latency-svc-76smg
Oct 29 21:48:03.839: INFO: Created: latency-svc-pdpwl
Oct 29 21:48:03.839: INFO: Got endpoints: latency-svc-76smg [202.471219ms]
Oct 29 21:48:03.848: INFO: Created: latency-svc-bmsdq
Oct 29 21:48:03.851: INFO: Got endpoints: latency-svc-pdpwl [204.404495ms]
Oct 29 21:48:03.862: INFO: Created: latency-svc-bnrmd
Oct 29 21:48:03.862: INFO: Got endpoints: latency-svc-bmsdq [206.631515ms]
Oct 29 21:48:03.872: INFO: Created: latency-svc-mc28h
Oct 29 21:48:03.872: INFO: Got endpoints: latency-svc-bnrmd [209.590548ms]
Oct 29 21:48:03.880: INFO: Got endpoints: latency-svc-mc28h [192.894346ms]
Oct 29 21:48:03.888: INFO: Created: latency-svc-8nnr2
Oct 29 21:48:03.891: INFO: Created: latency-svc-8kzl6
Oct 29 21:48:03.896: INFO: Got endpoints: latency-svc-8nnr2 [185.235825ms]
Oct 29 21:48:03.899: INFO: Created: latency-svc-bmwpk
Oct 29 21:48:03.931: INFO: Got endpoints: latency-svc-bmwpk [201.265599ms]
Oct 29 21:48:03.931: INFO: Got endpoints: latency-svc-8kzl6 [213.224597ms]
Oct 29 21:48:03.943: INFO: Created: latency-svc-ptwbb
Oct 29 21:48:03.951: INFO: Got endpoints: latency-svc-ptwbb [203.389249ms]
Oct 29 21:48:03.953: INFO: Created: latency-svc-msbkb
Oct 29 21:48:03.962: INFO: Got endpoints: latency-svc-msbkb [210.83185ms]
Oct 29 21:48:03.962: INFO: Created: latency-svc-gs7k8
Oct 29 21:48:03.969: INFO: Got endpoints: latency-svc-gs7k8 [209.234141ms]
Oct 29 21:48:03.974: INFO: Created: latency-svc-82zhd
Oct 29 21:48:03.979: INFO: Created: latency-svc-78cpx
Oct 29 21:48:03.986: INFO: Got endpoints: latency-svc-82zhd [218.480357ms]
Oct 29 21:48:03.986: INFO: Got endpoints: latency-svc-78cpx [185.493066ms]
Oct 29 21:48:03.992: INFO: Created: latency-svc-rlwxn
Oct 29 21:48:04.003: INFO: Created: latency-svc-kdlhf
Oct 29 21:48:04.028: INFO: Created: latency-svc-s27qg
Oct 29 21:48:04.034: INFO: Created: latency-svc-6rddx
Oct 29 21:48:04.044: INFO: Got endpoints: latency-svc-rlwxn [218.072117ms]
Oct 29 21:48:04.074: INFO: Created: latency-svc-7xtln
Oct 29 21:48:04.079: INFO: Created: latency-svc-jlhg8
Oct 29 21:48:04.098: INFO: Created: latency-svc-qcv7l
Oct 29 21:48:04.111: INFO: Created: latency-svc-mwrzv
Oct 29 21:48:04.114: INFO: Got endpoints: latency-svc-kdlhf [284.113078ms]
Oct 29 21:48:04.123: INFO: Created: latency-svc-x5v4r
Oct 29 21:48:04.130: INFO: Created: latency-svc-9j2k9
Oct 29 21:48:04.145: INFO: Created: latency-svc-m76dl
Oct 29 21:48:04.152: INFO: Created: latency-svc-q9jcx
Oct 29 21:48:04.175: INFO: Got endpoints: latency-svc-6rddx [323.67374ms]
Oct 29 21:48:04.176: INFO: Created: latency-svc-7r9dh
Oct 29 21:48:04.193: INFO: Created: latency-svc-b5k2n
Oct 29 21:48:04.193: INFO: Got endpoints: latency-svc-s27qg [354.26268ms]
Oct 29 21:48:04.204: INFO: Created: latency-svc-bjzxb
Oct 29 21:48:04.208: INFO: Created: latency-svc-ct9qs
Oct 29 21:48:04.222: INFO: Created: latency-svc-69p8g
Oct 29 21:48:04.239: INFO: Created: latency-svc-jvjtf
Oct 29 21:48:04.258: INFO: Created: latency-svc-jwh7b
Oct 29 21:48:04.296: INFO: Got endpoints: latency-svc-7xtln [433.60189ms]
Oct 29 21:48:04.328: INFO: Got endpoints: latency-svc-jlhg8 [455.820326ms]
Oct 29 21:48:04.331: INFO: Created: latency-svc-5f9f5
Oct 29 21:48:04.351: INFO: Got endpoints: latency-svc-qcv7l [471.484511ms]
Oct 29 21:48:04.365: INFO: Created: latency-svc-vhgkz
Oct 29 21:48:04.379: INFO: Created: latency-svc-zxl6g
Oct 29 21:48:04.414: INFO: Got endpoints: latency-svc-mwrzv [518.600021ms]
Oct 29 21:48:04.454: INFO: Got endpoints: latency-svc-x5v4r [523.615924ms]
Oct 29 21:48:04.477: INFO: Got endpoints: latency-svc-9j2k9 [546.750538ms]
Oct 29 21:48:04.479: INFO: Created: latency-svc-v4ptq
Oct 29 21:48:04.479: INFO: Got endpoints: latency-svc-m76dl [527.865524ms]
Oct 29 21:48:04.486: INFO: Got endpoints: latency-svc-q9jcx [524.486994ms]
Oct 29 21:48:04.486: INFO: Got endpoints: latency-svc-7r9dh [517.420318ms]
Oct 29 21:48:04.514: INFO: Created: latency-svc-bhrrg
Oct 29 21:48:04.517: INFO: Got endpoints: latency-svc-b5k2n [530.928054ms]
Oct 29 21:48:04.531: INFO: Got endpoints: latency-svc-ct9qs [487.246042ms]
Oct 29 21:48:04.531: INFO: Got endpoints: latency-svc-bjzxb [545.395722ms]
Oct 29 21:48:04.537: INFO: Created: latency-svc-mszz7
Oct 29 21:48:04.555: INFO: Created: latency-svc-wfxhr
Oct 29 21:48:04.562: INFO: Created: latency-svc-kjjsp
Oct 29 21:48:04.570: INFO: Got endpoints: latency-svc-jvjtf [394.642022ms]
Oct 29 21:48:04.573: INFO: Got endpoints: latency-svc-69p8g [458.772135ms]
Oct 29 21:48:04.573: INFO: Created: latency-svc-dv2mm
Oct 29 21:48:04.578: INFO: Got endpoints: latency-svc-jwh7b [384.53272ms]
Oct 29 21:48:04.587: INFO: Created: latency-svc-p74lx
Oct 29 21:48:04.615: INFO: Created: latency-svc-4bqwb
Oct 29 21:48:04.625: INFO: Got endpoints: latency-svc-5f9f5 [329.27042ms]
Oct 29 21:48:04.625: INFO: Got endpoints: latency-svc-vhgkz [296.66802ms]
Oct 29 21:48:04.640: INFO: Created: latency-svc-rx927
Oct 29 21:48:04.688: INFO: Created: latency-svc-n9sfq
Oct 29 21:48:04.689: INFO: Got endpoints: latency-svc-zxl6g [337.86279ms]
Oct 29 21:48:04.689: INFO: Got endpoints: latency-svc-v4ptq [275.076539ms]
Oct 29 21:48:04.689: INFO: Got endpoints: latency-svc-bhrrg [235.022943ms]
Oct 29 21:48:04.702: INFO: Created: latency-svc-rwkrk
Oct 29 21:48:04.738: INFO: Created: latency-svc-n45xb
Oct 29 21:48:04.740: INFO: Got endpoints: latency-svc-mszz7 [262.249215ms]
Oct 29 21:48:04.749: INFO: Got endpoints: latency-svc-kjjsp [263.224002ms]
Oct 29 21:48:04.772: INFO: Got endpoints: latency-svc-wfxhr [292.508621ms]
Oct 29 21:48:04.772: INFO: Got endpoints: latency-svc-p74lx [254.732919ms]
Oct 29 21:48:04.776: INFO: Created: latency-svc-2mzcx
Oct 29 21:48:04.777: INFO: Got endpoints: latency-svc-dv2mm [290.364856ms]
Oct 29 21:48:04.802: INFO: Got endpoints: latency-svc-4bqwb [270.489023ms]
Oct 29 21:48:04.802: INFO: Created: latency-svc-hw89n
Oct 29 21:48:04.813: INFO: Created: latency-svc-pr5hj
Oct 29 21:48:04.828: INFO: Got endpoints: latency-svc-rx927 [296.516534ms]
Oct 29 21:48:04.828: INFO: Got endpoints: latency-svc-rwkrk [255.358256ms]
Oct 29 21:48:04.833: INFO: Got endpoints: latency-svc-2mzcx [207.751119ms]
Oct 29 21:48:04.833: INFO: Got endpoints: latency-svc-n45xb [254.674495ms]
Oct 29 21:48:04.833: INFO: Got endpoints: latency-svc-n9sfq [263.436602ms]
Oct 29 21:48:04.844: INFO: Created: latency-svc-mcctb
Oct 29 21:48:04.864: INFO: Created: latency-svc-g7v4l
Oct 29 21:48:04.869: INFO: Got endpoints: latency-svc-hw89n [243.94218ms]
Oct 29 21:48:04.869: INFO: Got endpoints: latency-svc-pr5hj [179.599997ms]
Oct 29 21:48:04.879: INFO: Got endpoints: latency-svc-mcctb [189.77199ms]
Oct 29 21:48:04.879: INFO: Got endpoints: latency-svc-g7v4l [190.150338ms]
Oct 29 21:48:04.888: INFO: Created: latency-svc-lfdn7
Oct 29 21:48:04.931: INFO: Created: latency-svc-b75hl
Oct 29 21:48:04.937: INFO: Got endpoints: latency-svc-lfdn7 [196.971366ms]
Oct 29 21:48:04.953: INFO: Got endpoints: latency-svc-b75hl [203.24373ms]
Oct 29 21:48:04.953: INFO: Created: latency-svc-fh8hb
Oct 29 21:48:04.965: INFO: Created: latency-svc-p62bx
Oct 29 21:48:04.978: INFO: Got endpoints: latency-svc-fh8hb [206.269303ms]
Oct 29 21:48:04.992: INFO: Created: latency-svc-49d87
Oct 29 21:48:04.993: INFO: Got endpoints: latency-svc-p62bx [221.389496ms]
Oct 29 21:48:05.009: INFO: Created: latency-svc-zfzh2
Oct 29 21:48:05.011: INFO: Got endpoints: latency-svc-49d87 [74.479482ms]
Oct 29 21:48:05.025: INFO: Created: latency-svc-bn5xg
Oct 29 21:48:05.039: INFO: Created: latency-svc-4ndth
Oct 29 21:48:05.040: INFO: Got endpoints: latency-svc-zfzh2 [263.864195ms]
Oct 29 21:48:05.046: INFO: Got endpoints: latency-svc-bn5xg [244.147067ms]
Oct 29 21:48:05.048: INFO: Got endpoints: latency-svc-4ndth [220.725719ms]
Oct 29 21:48:05.054: INFO: Created: latency-svc-dc5wd
Oct 29 21:48:05.067: INFO: Created: latency-svc-2rvz4
Oct 29 21:48:05.072: INFO: Got endpoints: latency-svc-dc5wd [243.480113ms]
Oct 29 21:48:05.082: INFO: Created: latency-svc-jbpm8
Oct 29 21:48:05.088: INFO: Got endpoints: latency-svc-2rvz4 [255.580329ms]
Oct 29 21:48:05.091: INFO: Created: latency-svc-xht2d
Oct 29 21:48:05.093: INFO: Got endpoints: latency-svc-jbpm8 [260.229911ms]
Oct 29 21:48:05.105: INFO: Got endpoints: latency-svc-xht2d [271.644652ms]
Oct 29 21:48:05.111: INFO: Created: latency-svc-4mr97
Oct 29 21:48:05.120: INFO: Got endpoints: latency-svc-4mr97 [251.307216ms]
Oct 29 21:48:05.138: INFO: Created: latency-svc-7scbc
Oct 29 21:48:05.195: INFO: Got endpoints: latency-svc-7scbc [326.147436ms]
Oct 29 21:48:05.212: INFO: Created: latency-svc-xsc4s
Oct 29 21:48:05.225: INFO: Created: latency-svc-v5s65
Oct 29 21:48:05.234: INFO: Created: latency-svc-zsrxx
Oct 29 21:48:05.244: INFO: Got endpoints: latency-svc-v5s65 [364.550855ms]
Oct 29 21:48:05.248: INFO: Got endpoints: latency-svc-xsc4s [369.102535ms]
Oct 29 21:48:05.252: INFO: Got endpoints: latency-svc-zsrxx [299.40718ms]
Oct 29 21:48:05.264: INFO: Created: latency-svc-c6frn
Oct 29 21:48:05.278: INFO: Got endpoints: latency-svc-c6frn [300.565079ms]
Oct 29 21:48:05.284: INFO: Created: latency-svc-48r92
Oct 29 21:48:05.291: INFO: Got endpoints: latency-svc-48r92 [297.768448ms]
Oct 29 21:48:05.292: INFO: Created: latency-svc-2xqw8
Oct 29 21:48:05.301: INFO: Got endpoints: latency-svc-2xqw8 [290.36348ms]
Oct 29 21:48:05.306: INFO: Created: latency-svc-t98fc
Oct 29 21:48:05.321: INFO: Created: latency-svc-zqk7x
Oct 29 21:48:05.327: INFO: Got endpoints: latency-svc-t98fc [286.696417ms]
Oct 29 21:48:05.330: INFO: Created: latency-svc-p5sql
Oct 29 21:48:05.336: INFO: Got endpoints: latency-svc-zqk7x [289.781899ms]
Oct 29 21:48:05.348: INFO: Got endpoints: latency-svc-p5sql [299.192691ms]
Oct 29 21:48:05.374: INFO: Created: latency-svc-2gqk9
Oct 29 21:48:05.381: INFO: Created: latency-svc-2ctsh
Oct 29 21:48:05.382: INFO: Got endpoints: latency-svc-2gqk9 [310.260173ms]
Oct 29 21:48:05.403: INFO: Got endpoints: latency-svc-2ctsh [314.790287ms]
Oct 29 21:48:05.408: INFO: Created: latency-svc-g9lw9
Oct 29 21:48:05.419: INFO: Got endpoints: latency-svc-g9lw9 [325.585578ms]
Oct 29 21:48:05.433: INFO: Created: latency-svc-5fd52
Oct 29 21:48:05.445: INFO: Got endpoints: latency-svc-5fd52 [339.792172ms]
Oct 29 21:48:05.457: INFO: Created: latency-svc-8ljds
Oct 29 21:48:05.466: INFO: Created: latency-svc-cf8gc
Oct 29 21:48:05.466: INFO: Got endpoints: latency-svc-8ljds [346.148506ms]
Oct 29 21:48:05.503: INFO: Got endpoints: latency-svc-cf8gc [307.584285ms]
Oct 29 21:48:05.508: INFO: Created: latency-svc-5kwcp
Oct 29 21:48:05.521: INFO: Got endpoints: latency-svc-5kwcp [277.155418ms]
Oct 29 21:48:05.541: INFO: Created: latency-svc-rxndk
Oct 29 21:48:05.551: INFO: Created: latency-svc-49fdt
Oct 29 21:48:05.563: INFO: Got endpoints: latency-svc-rxndk [315.028945ms]
Oct 29 21:48:05.569: INFO: Got endpoints: latency-svc-49fdt [317.021929ms]
Oct 29 21:48:05.573: INFO: Created: latency-svc-jfcgs
Oct 29 21:48:05.579: INFO: Got endpoints: latency-svc-jfcgs [301.016325ms]
Oct 29 21:48:05.597: INFO: Created: latency-svc-p6cz9
Oct 29 21:48:05.611: INFO: Created: latency-svc-kxlk7
Oct 29 21:48:05.611: INFO: Got endpoints: latency-svc-p6cz9 [320.035766ms]
Oct 29 21:48:05.619: INFO: Got endpoints: latency-svc-kxlk7 [317.272938ms]
Oct 29 21:48:05.623: INFO: Created: latency-svc-qhms4
Oct 29 21:48:05.629: INFO: Got endpoints: latency-svc-qhms4 [301.388647ms]
Oct 29 21:48:05.631: INFO: Created: latency-svc-52bnt
Oct 29 21:48:05.639: INFO: Got endpoints: latency-svc-52bnt [303.614146ms]
Oct 29 21:48:05.643: INFO: Created: latency-svc-qt9m4
Oct 29 21:48:05.653: INFO: Got endpoints: latency-svc-qt9m4 [305.123587ms]
Oct 29 21:48:05.655: INFO: Created: latency-svc-d6x52
Oct 29 21:48:05.663: INFO: Created: latency-svc-zplvs
Oct 29 21:48:05.666: INFO: Got endpoints: latency-svc-d6x52 [283.596859ms]
Oct 29 21:48:05.672: INFO: Created: latency-svc-j57j4
Oct 29 21:48:05.674: INFO: Got endpoints: latency-svc-zplvs [271.227597ms]
Oct 29 21:48:05.685: INFO: Got endpoints: latency-svc-j57j4 [266.154392ms]
Oct 29 21:48:05.687: INFO: Created: latency-svc-q6qwv
Oct 29 21:48:05.706: INFO: Got endpoints: latency-svc-q6qwv [261.290836ms]
Oct 29 21:48:05.708: INFO: Created: latency-svc-7gfzc
Oct 29 21:48:05.718: INFO: Created: latency-svc-qbmlv
Oct 29 21:48:05.726: INFO: Got endpoints: latency-svc-7gfzc [259.838859ms]
Oct 29 21:48:05.727: INFO: Got endpoints: latency-svc-qbmlv [224.400919ms]
Oct 29 21:48:05.735: INFO: Created: latency-svc-226ct
Oct 29 21:48:05.740: INFO: Got endpoints: latency-svc-226ct [218.620673ms]
Oct 29 21:48:05.756: INFO: Created: latency-svc-sp2j9
Oct 29 21:48:05.764: INFO: Got endpoints: latency-svc-sp2j9 [200.263148ms]
Oct 29 21:48:05.771: INFO: Created: latency-svc-bhv2q
Oct 29 21:48:05.779: INFO: Got endpoints: latency-svc-bhv2q [209.763851ms]
Oct 29 21:48:05.808: INFO: Created: latency-svc-jtg2d
Oct 29 21:48:05.817: INFO: Created: latency-svc-d8trz
Oct 29 21:48:05.818: INFO: Got endpoints: latency-svc-jtg2d [238.661407ms]
Oct 29 21:48:05.828: INFO: Got endpoints: latency-svc-d8trz [217.236682ms]
Oct 29 21:48:05.828: INFO: Created: latency-svc-976qw
Oct 29 21:48:05.833: INFO: Got endpoints: latency-svc-976qw [213.775496ms]
Oct 29 21:48:05.833: INFO: Created: latency-svc-hlrmg
Oct 29 21:48:05.840: INFO: Got endpoints: latency-svc-hlrmg [211.894543ms]
Oct 29 21:48:05.843: INFO: Created: latency-svc-cb2w6
Oct 29 21:48:05.848: INFO: Created: latency-svc-kxdzq
Oct 29 21:48:05.851: INFO: Got endpoints: latency-svc-cb2w6 [211.574545ms]
Oct 29 21:48:05.856: INFO: Got endpoints: latency-svc-kxdzq [202.955241ms]
Oct 29 21:48:05.871: INFO: Created: latency-svc-bdxqf
Oct 29 21:48:05.880: INFO: Got endpoints: latency-svc-bdxqf [214.271649ms]
Oct 29 21:48:05.896: INFO: Created: latency-svc-qv64q
Oct 29 21:48:05.923: INFO: Created: latency-svc-pvc9x
Oct 29 21:48:05.925: INFO: Got endpoints: latency-svc-qv64q [250.330309ms]
Oct 29 21:48:05.930: INFO: Got endpoints: latency-svc-pvc9x [245.618214ms]
Oct 29 21:48:05.934: INFO: Created: latency-svc-6knmb
Oct 29 21:48:05.947: INFO: Got endpoints: latency-svc-6knmb [241.300324ms]
Oct 29 21:48:05.961: INFO: Created: latency-svc-6f6td
Oct 29 21:48:05.966: INFO: Got endpoints: latency-svc-6f6td [239.735444ms]
Oct 29 21:48:05.973: INFO: Created: latency-svc-g7gmd
Oct 29 21:48:05.980: INFO: Got endpoints: latency-svc-g7gmd [253.373178ms]
Oct 29 21:48:05.984: INFO: Created: latency-svc-2tqvx
Oct 29 21:48:05.991: INFO: Created: latency-svc-jth24
Oct 29 21:48:05.997: INFO: Got endpoints: latency-svc-2tqvx [257.148734ms]
Oct 29 21:48:06.016: INFO: Created: latency-svc-vvcm7
Oct 29 21:48:06.018: INFO: Got endpoints: latency-svc-jth24 [254.844088ms]
Oct 29 21:48:06.023: INFO: Created: latency-svc-kwcr5
Oct 29 21:48:06.030: INFO: Got endpoints: latency-svc-vvcm7 [250.747623ms]
Oct 29 21:48:06.033: INFO: Created: latency-svc-dgphr
Oct 29 21:48:06.035: INFO: Got endpoints: latency-svc-kwcr5 [217.111454ms]
Oct 29 21:48:06.040: INFO: Got endpoints: latency-svc-dgphr [211.897237ms]
Oct 29 21:48:06.043: INFO: Created: latency-svc-f6wtf
Oct 29 21:48:06.043: INFO: Created: latency-svc-6rp77
Oct 29 21:48:06.052: INFO: Created: latency-svc-54h6k
Oct 29 21:48:06.054: INFO: Got endpoints: latency-svc-6rp77 [221.054727ms]
Oct 29 21:48:06.055: INFO: Got endpoints: latency-svc-f6wtf [214.437365ms]
Oct 29 21:48:06.062: INFO: Created: latency-svc-2n66w
Oct 29 21:48:06.062: INFO: Got endpoints: latency-svc-54h6k [211.579442ms]
Oct 29 21:48:06.068: INFO: Got endpoints: latency-svc-2n66w [212.124475ms]
Oct 29 21:48:06.077: INFO: Created: latency-svc-kfhhp
Oct 29 21:48:06.089: INFO: Created: latency-svc-rfvdj
Oct 29 21:48:06.095: INFO: Got endpoints: latency-svc-kfhhp [214.63875ms]
Oct 29 21:48:06.102: INFO: Created: latency-svc-c5hvv
Oct 29 21:48:06.102: INFO: Got endpoints: latency-svc-rfvdj [177.322227ms]
Oct 29 21:48:06.112: INFO: Got endpoints: latency-svc-c5hvv [181.814174ms]
Oct 29 21:48:06.112: INFO: Created: latency-svc-t55p5
Oct 29 21:48:06.120: INFO: Got endpoints: latency-svc-t55p5 [172.960217ms]
Oct 29 21:48:06.120: INFO: Created: latency-svc-rppcf
Oct 29 21:48:06.129: INFO: Got endpoints: latency-svc-rppcf [163.317018ms]
Oct 29 21:48:06.151: INFO: Created: latency-svc-p7r2d
Oct 29 21:48:06.171: INFO: Got endpoints: latency-svc-p7r2d [190.901648ms]
Oct 29 21:48:06.173: INFO: Created: latency-svc-76265
Oct 29 21:48:06.183: INFO: Created: latency-svc-kpkgz
Oct 29 21:48:06.184: INFO: Got endpoints: latency-svc-76265 [186.728732ms]
Oct 29 21:48:06.191: INFO: Got endpoints: latency-svc-kpkgz [172.400471ms]
Oct 29 21:48:06.198: INFO: Created: latency-svc-lgcsc
Oct 29 21:48:06.207: INFO: Created: latency-svc-p9tgv
Oct 29 21:48:06.211: INFO: Got endpoints: latency-svc-lgcsc [181.133516ms]
Oct 29 21:48:06.212: INFO: Got endpoints: latency-svc-p9tgv [176.987358ms]
Oct 29 21:48:06.224: INFO: Created: latency-svc-jtkrn
Oct 29 21:48:06.241: INFO: Got endpoints: latency-svc-jtkrn [200.713249ms]
Oct 29 21:48:06.241: INFO: Created: latency-svc-62rhz
Oct 29 21:48:06.253: INFO: Created: latency-svc-4rlhj
Oct 29 21:48:06.254: INFO: Got endpoints: latency-svc-62rhz [200.229156ms]
Oct 29 21:48:06.264: INFO: Got endpoints: latency-svc-4rlhj [209.533074ms]
Oct 29 21:48:06.267: INFO: Created: latency-svc-tcpcx
Oct 29 21:48:06.277: INFO: Created: latency-svc-46wxk
Oct 29 21:48:06.281: INFO: Got endpoints: latency-svc-tcpcx [218.448584ms]
Oct 29 21:48:06.296: INFO: Created: latency-svc-79jcq
Oct 29 21:48:06.304: INFO: Got endpoints: latency-svc-46wxk [236.320437ms]
Oct 29 21:48:06.308: INFO: Got endpoints: latency-svc-79jcq [213.201569ms]
Oct 29 21:48:06.332: INFO: Created: latency-svc-lwnvn
Oct 29 21:48:06.353: INFO: Got endpoints: latency-svc-lwnvn [250.946215ms]
Oct 29 21:48:06.353: INFO: Latencies: [36.09944ms 56.70529ms 74.479482ms 79.934772ms 84.527336ms 122.585706ms 135.218999ms 163.317018ms 172.400471ms 172.960217ms 175.22333ms 176.987358ms 177.322227ms 179.599997ms 181.133516ms 181.814174ms 182.225321ms 182.780214ms 183.4825ms 185.235825ms 185.28872ms 185.493066ms 186.728732ms 189.048106ms 189.77199ms 190.150338ms 190.901648ms 192.703413ms 192.894346ms 194.591707ms 196.971366ms 199.061011ms 200.229156ms 200.263148ms 200.713249ms 200.857796ms 200.940309ms 201.265599ms 202.471219ms 202.955241ms 203.24373ms 203.389249ms 204.404495ms 205.878449ms 206.176321ms 206.269303ms 206.631515ms 207.554075ms 207.751119ms 208.601226ms 209.234141ms 209.533074ms 209.590548ms 209.763851ms 210.83185ms 211.574545ms 211.579442ms 211.894543ms 211.897237ms 212.124475ms 212.161687ms 213.201569ms 213.224597ms 213.775496ms 214.271649ms 214.360537ms 214.437365ms 214.63875ms 215.193762ms 216.041101ms 216.594779ms 217.008725ms 217.111454ms 217.209803ms 217.236682ms 218.072117ms 218.13492ms 218.448584ms 218.480357ms 218.620673ms 219.737448ms 220.725719ms 221.054727ms 221.389496ms 222.072264ms 222.994517ms 224.400919ms 225.393236ms 234.312148ms 234.381273ms 235.022943ms 235.141627ms 236.320437ms 236.597778ms 238.661407ms 239.735444ms 241.300324ms 242.20316ms 243.480113ms 243.94218ms 244.147067ms 245.618214ms 246.226874ms 246.490405ms 248.243049ms 249.863954ms 250.330309ms 250.747623ms 250.946215ms 251.307216ms 253.373178ms 254.674495ms 254.732919ms 254.844088ms 255.358256ms 255.580329ms 255.686974ms 257.148734ms 258.003504ms 259.838859ms 260.229911ms 261.290836ms 262.249215ms 263.224002ms 263.436602ms 263.864195ms 266.154392ms 268.463658ms 269.125169ms 270.085169ms 270.489023ms 270.616431ms 271.227597ms 271.644652ms 272.809939ms 273.154639ms 275.076539ms 277.155418ms 278.74228ms 283.596859ms 284.113078ms 284.700218ms 285.867334ms 286.145107ms 286.696417ms 289.781899ms 289.944843ms 290.36348ms 290.364856ms 292.508621ms 293.097938ms 296.048857ms 296.226493ms 296.321238ms 296.516534ms 296.66802ms 297.768448ms 298.624224ms 299.192691ms 299.40718ms 300.565079ms 301.016325ms 301.388647ms 302.036319ms 303.614146ms 305.123587ms 307.584285ms 310.260173ms 313.319606ms 314.790287ms 315.028945ms 317.021929ms 317.272938ms 318.857488ms 320.035766ms 323.67374ms 325.585578ms 326.147436ms 329.27042ms 337.86279ms 339.792172ms 346.148506ms 354.26268ms 364.550855ms 369.102535ms 384.53272ms 394.642022ms 433.60189ms 455.820326ms 458.772135ms 471.484511ms 487.246042ms 517.420318ms 518.600021ms 523.615924ms 524.486994ms 527.865524ms 530.928054ms 545.395722ms 546.750538ms]
Oct 29 21:48:06.353: INFO: 50 %ile: 244.147067ms
Oct 29 21:48:06.353: INFO: 90 %ile: 339.792172ms
Oct 29 21:48:06.353: INFO: 99 %ile: 545.395722ms
Oct 29 21:48:06.353: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:48:06.353: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "svc-latency-3538" for this suite.


 [SLOW TEST:7.170 seconds]
[sig-network] Service endpoints latency
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":-1,"completed":1,"skipped":14,"failed":0}

SSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected secret
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:48:05.042: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-33da3854-bf2d-45c1-be15-ce15aa97c28a
STEP: Creating a pod to test consume secrets
Oct 29 21:48:05.374: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-83b11c59-9ac1-4096-a8f6-8caa20a8b4e5" in namespace "projected-1186" to be "Succeeded or Failed"
Oct 29 21:48:05.405: INFO: Pod "pod-projected-secrets-83b11c59-9ac1-4096-a8f6-8caa20a8b4e5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.441133ms
Oct 29 21:48:07.426: INFO: Pod "pod-projected-secrets-83b11c59-9ac1-4096-a8f6-8caa20a8b4e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051352561s
Oct 29 21:48:09.464: INFO: Pod "pod-projected-secrets-83b11c59-9ac1-4096-a8f6-8caa20a8b4e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.089851384s
STEP: Saw pod success
Oct 29 21:48:09.464: INFO: Pod "pod-projected-secrets-83b11c59-9ac1-4096-a8f6-8caa20a8b4e5" satisfied condition "Succeeded or Failed"
Oct 29 21:48:09.554: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-projected-secrets-83b11c59-9ac1-4096-a8f6-8caa20a8b4e5 container projected-secret-volume-test: <nil>
STEP: delete the pod
Oct 29 21:48:09.610: INFO: Waiting for pod pod-projected-secrets-83b11c59-9ac1-4096-a8f6-8caa20a8b4e5 to disappear
Oct 29 21:48:09.631: INFO: Pod pod-projected-secrets-83b11c59-9ac1-4096-a8f6-8caa20a8b4e5 no longer exists
[AfterEach] [sig-storage] Projected secret
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:48:09.631: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-1186" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":3,"skipped":7,"failed":0}

SSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Container Runtime
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:48:09.753: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Oct 29 21:48:14.132: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:48:14.194: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-runtime-3991" for this suite.


------------------------------
[BeforeEach] [sig-api-machinery] Garbage collector
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:48:03.957: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W1029 21:48:14.962836   53213 metrics_grabber.go:83] Can't find any pods in namespace kube-system to grab metrics from
W1029 21:48:14.962852   53213 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1029 21:48:14.962858   53213 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1029 21:48:14.962864   53213 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Oct 29 21:48:14.962: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:48:14.962: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "gc-6084" for this suite.


 [SLOW TEST:11.117 seconds]
[sig-api-machinery] Garbage collector
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":-1,"completed":2,"skipped":49,"failed":0}

SSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:48:06.458: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:48:17.852: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "resourcequota-2119" for this suite.


 [SLOW TEST:11.506 seconds]
[sig-api-machinery] ResourceQuota
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":-1,"completed":2,"skipped":27,"failed":0}

SSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:48:15.085: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on tmpfs
Oct 29 21:48:15.370: INFO: Waiting up to 5m0s for pod "pod-e9b5e1ac-ac21-48bd-8993-d9ed620f106d" in namespace "emptydir-6851" to be "Succeeded or Failed"
Oct 29 21:48:15.412: INFO: Pod "pod-e9b5e1ac-ac21-48bd-8993-d9ed620f106d": Phase="Pending", Reason="", readiness=false. Elapsed: 41.663845ms
Oct 29 21:48:17.434: INFO: Pod "pod-e9b5e1ac-ac21-48bd-8993-d9ed620f106d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063622172s
Oct 29 21:48:19.463: INFO: Pod "pod-e9b5e1ac-ac21-48bd-8993-d9ed620f106d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.093166391s
STEP: Saw pod success
Oct 29 21:48:19.463: INFO: Pod "pod-e9b5e1ac-ac21-48bd-8993-d9ed620f106d" satisfied condition "Succeeded or Failed"
Oct 29 21:48:19.491: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-e9b5e1ac-ac21-48bd-8993-d9ed620f106d container test-container: <nil>
STEP: delete the pod
Oct 29 21:48:19.558: INFO: Waiting for pod pod-e9b5e1ac-ac21-48bd-8993-d9ed620f106d to disappear
Oct 29 21:48:19.642: INFO: Pod pod-e9b5e1ac-ac21-48bd-8993-d9ed620f106d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:48:19.642: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-6851" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":3,"skipped":63,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":-1,"completed":4,"skipped":16,"failed":0}
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:48:14.316: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl replace
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1581
[It] should update a single-container pod's image  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Oct 29 21:48:14.475: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-6443'
Oct 29 21:48:19.177: INFO: stderr: ""
Oct 29 21:48:19.177: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Oct 29 21:48:24.228: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig get pod e2e-test-httpd-pod --namespace=kubectl-6443 -o json'
Oct 29 21:48:24.367: INFO: stderr: ""
Oct 29 21:48:24.367: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"k8s.v1.cni.cncf.io/network-status\": \"[{\\n    \\\"name\\\": \\\"\\\",\\n    \\\"interface\\\": \\\"eth0\\\",\\n    \\\"ips\\\": [\\n        \\\"10.131.1.106\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"\\\",\\n    \\\"interface\\\": \\\"eth0\\\",\\n    \\\"ips\\\": [\\n        \\\"10.131.1.106\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2020-10-29T21:47:59Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-10-29T21:47:59Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:annotations\": {\n                            \"f:k8s.v1.cni.cncf.io/network-status\": {},\n                            \"f:k8s.v1.cni.cncf.io/networks-status\": {}\n                        }\n                    }\n                },\n                \"manager\": \"multus\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-10-29T21:48:01Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"10.131.1.106\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-10-29T21:48:03Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6443\",\n        \"resourceVersion\": \"221058\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-6443/pods/e2e-test-httpd-pod\",\n        \"uid\": \"c7bf30d3-995e-4f2d-99a4-413eaf9ee7ff\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-9lhkr\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"imagePullSecrets\": [\n            {\n                \"name\": \"default-dockercfg-lpng7\"\n            }\n        ],\n        \"nodeName\": \"ip-10-0-142-212.us-east-2.compute.internal\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c84,c34\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-9lhkr\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-9lhkr\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-10-29T21:47:59Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-10-29T21:48:03Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-10-29T21:48:03Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-10-29T21:47:59Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://75b176edb14ed7306c0b3233971b4ad0661e36391dd1433fd5c1946ecfc1cf28\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-10-29T21:48:02Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.142.212\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.131.1.106\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.131.1.106\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-10-29T21:47:59Z\"\n    }\n}\n"
STEP: replace the image in the pod
Oct 29 21:48:24.367: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig replace -f - --namespace=kubectl-6443'
Oct 29 21:48:25.037: INFO: stderr: ""
Oct 29 21:48:25.037: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1586
Oct 29 21:48:25.059: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig delete pods e2e-test-httpd-pod --namespace=kubectl-6443'
Oct 29 21:48:26.671: INFO: stderr: ""
Oct 29 21:48:26.671: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:48:26.671: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-6443" for this suite.


 [SLOW TEST:12.455 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1577
    should update a single-container pod's image  [Conformance]
    /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":-1,"completed":5,"skipped":16,"failed":0}

SSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:48:26.788: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should test the lifecycle of an Endpoint [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:48:27.235: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-7920" for this suite.
[AfterEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786


------------------------------
{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":-1,"completed":6,"skipped":33,"failed":0}

SSSSSS
------------------------------
[BeforeEach] [k8s.io] [sig-node] Events
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:48:19.776: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Oct 29 21:48:24.111: INFO: &Pod{ObjectMeta:{send-events-d7facf51-84ee-4200-bc98-1c1d91165143  events-9300 /api/v1/namespaces/events-9300/pods/send-events-d7facf51-84ee-4200-bc98-1c1d91165143 41436e64-11c8-4e61-ab75-54334f41df1a 221146 0 2020-10-29 21:48:00 +0000 UTC <nil> <nil> map[name:foo time:961654625] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.131.1.108"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.131.1.108"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [] []  [{e2e.test Update v1 2020-10-29 21:48:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {multus Update v1 2020-10-29 21:48:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2020-10-29 21:48:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.1.108\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qljdr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qljdr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qljdr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-142-212.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c84,c54,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-d5mzs,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:48:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:48:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:48:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:48:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.142.212,PodIP:10.131.1.108,StartTime:2020-10-29 21:48:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-10-29 21:48:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:cri-o://a3408ac4058ef52af56aebb9a7dc5ac005e017569a50b714bfbc0d33dc6b6720,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.1.108,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Oct 29 21:48:26.134: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Oct 29 21:48:28.156: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:48:28.181: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "events-9300" for this suite.


 [SLOW TEST:8.509 seconds]
[k8s.io] [sig-node] Events
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":-1,"completed":4,"skipped":94,"failed":0}

SS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:48:28.289: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Oct 29 21:48:28.511: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d5bfc573-a738-47cc-8f57-c3246764bfea" in namespace "projected-3561" to be "Succeeded or Failed"
Oct 29 21:48:28.533: INFO: Pod "downwardapi-volume-d5bfc573-a738-47cc-8f57-c3246764bfea": Phase="Pending", Reason="", readiness=false. Elapsed: 21.624887ms
Oct 29 21:48:30.555: INFO: Pod "downwardapi-volume-d5bfc573-a738-47cc-8f57-c3246764bfea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043895147s
Oct 29 21:48:32.580: INFO: Pod "downwardapi-volume-d5bfc573-a738-47cc-8f57-c3246764bfea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.068415152s
STEP: Saw pod success
Oct 29 21:48:32.580: INFO: Pod "downwardapi-volume-d5bfc573-a738-47cc-8f57-c3246764bfea" satisfied condition "Succeeded or Failed"
Oct 29 21:48:32.602: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod downwardapi-volume-d5bfc573-a738-47cc-8f57-c3246764bfea container client-container: <nil>
STEP: delete the pod
Oct 29 21:48:32.659: INFO: Waiting for pod downwardapi-volume-d5bfc573-a738-47cc-8f57-c3246764bfea to disappear
Oct 29 21:48:32.681: INFO: Pod downwardapi-volume-d5bfc573-a738-47cc-8f57-c3246764bfea no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:48:32.681: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-3561" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":-1,"completed":5,"skipped":96,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:48:27.291: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Update Demo
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:308
[It] should create and stop a replication controller  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Oct 29 21:48:27.428: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig create -f - --namespace=kubectl-9457'
Oct 29 21:48:27.926: INFO: stderr: ""
Oct 29 21:48:27.926: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Oct 29 21:48:27.926: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9457'
Oct 29 21:48:28.066: INFO: stderr: ""
Oct 29 21:48:28.066: INFO: stdout: "update-demo-nautilus-pnxmh update-demo-nautilus-vc5pb "
Oct 29 21:48:28.066: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig get pods update-demo-nautilus-pnxmh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9457'
Oct 29 21:48:28.203: INFO: stderr: ""
Oct 29 21:48:28.203: INFO: stdout: ""
Oct 29 21:48:28.203: INFO: update-demo-nautilus-pnxmh is created but not running
Oct 29 21:48:33.204: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9457'
Oct 29 21:48:33.357: INFO: stderr: ""
Oct 29 21:48:33.357: INFO: stdout: "update-demo-nautilus-pnxmh update-demo-nautilus-vc5pb "
Oct 29 21:48:33.357: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig get pods update-demo-nautilus-pnxmh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9457'
Oct 29 21:48:33.491: INFO: stderr: ""
Oct 29 21:48:33.491: INFO: stdout: "true"
Oct 29 21:48:33.491: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig get pods update-demo-nautilus-pnxmh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9457'
Oct 29 21:48:33.626: INFO: stderr: ""
Oct 29 21:48:33.626: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Oct 29 21:48:33.626: INFO: validating pod update-demo-nautilus-pnxmh
Oct 29 21:48:33.650: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 29 21:48:33.650: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 29 21:48:33.650: INFO: update-demo-nautilus-pnxmh is verified up and running
Oct 29 21:48:33.650: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig get pods update-demo-nautilus-vc5pb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9457'
Oct 29 21:48:33.783: INFO: stderr: ""
Oct 29 21:48:33.783: INFO: stdout: "true"
Oct 29 21:48:33.783: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig get pods update-demo-nautilus-vc5pb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9457'
Oct 29 21:48:33.914: INFO: stderr: ""
Oct 29 21:48:33.914: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Oct 29 21:48:33.915: INFO: validating pod update-demo-nautilus-vc5pb
Oct 29 21:48:33.938: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 29 21:48:33.938: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 29 21:48:33.938: INFO: update-demo-nautilus-vc5pb is verified up and running
STEP: using delete to clean up resources
Oct 29 21:48:33.938: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig delete --grace-period=0 --force -f - --namespace=kubectl-9457'
Oct 29 21:48:34.091: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 29 21:48:34.091: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Oct 29 21:48:34.091: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9457'
Oct 29 21:48:34.252: INFO: stderr: "No resources found in kubectl-9457 namespace.\n"
Oct 29 21:48:34.252: INFO: stdout: ""
Oct 29 21:48:34.252: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig get pods -l name=update-demo --namespace=kubectl-9457 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Oct 29 21:48:34.412: INFO: stderr: ""
Oct 29 21:48:34.412: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:48:34.412: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-9457" for this suite.


 [SLOW TEST:7.224 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:306
    should create and stop a replication controller  [Conformance]
    /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":-1,"completed":7,"skipped":39,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Pods
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:48:32.844: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Oct 29 21:48:37.700: INFO: Successfully updated pod "pod-update-activedeadlineseconds-f6ea01d1-b689-42c3-8d43-6e836e407b0a"
Oct 29 21:48:37.700: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-f6ea01d1-b689-42c3-8d43-6e836e407b0a" in namespace "pods-3079" to be "terminated due to deadline exceeded"
Oct 29 21:48:37.724: INFO: Pod "pod-update-activedeadlineseconds-f6ea01d1-b689-42c3-8d43-6e836e407b0a": Phase="Running", Reason="", readiness=true. Elapsed: 24.249421ms
Oct 29 21:48:39.813: INFO: Pod "pod-update-activedeadlineseconds-f6ea01d1-b689-42c3-8d43-6e836e407b0a": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.112896023s
Oct 29 21:48:39.813: INFO: Pod "pod-update-activedeadlineseconds-f6ea01d1-b689-42c3-8d43-6e836e407b0a" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:48:39.813: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-3079" for this suite.


 [SLOW TEST:7.084 seconds]
[k8s.io] Pods
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":-1,"completed":6,"skipped":132,"failed":0}

SSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:48:17.971: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 29 21:48:18.904: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 29 21:48:20.984: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739604879, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739604879, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739604879, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739604879, loc:(*time.Location)(0x77098a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 29 21:48:24.044: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
Oct 29 21:48:24.157: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:24.309: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:24.408: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:24.508: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:24.619: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:24.720: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:24.811: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:24.907: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:25.012: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:25.110: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:25.237: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:25.318: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:25.412: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:25.508: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:25.609: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:25.713: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:25.807: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:25.908: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:26.014: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:26.108: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:26.208: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:26.316: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:26.413: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:26.512: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:26.608: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:26.733: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:26.808: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:26.921: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:27.017: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:27.108: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:27.209: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:27.310: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:27.430: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:27.507: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:27.609: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:27.707: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:27.806: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:27.907: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:28.007: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:28.108: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:28.209: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:28.310: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:28.419: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:28.508: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:28.614: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:28.708: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:28.812: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:28.907: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:29.007: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:29.107: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:29.208: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:29.308: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:29.412: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:29.507: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:29.608: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:29.740: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:29.818: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:29.909: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:30.012: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:30.110: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:30.212: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:30.318: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:30.415: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:30.510: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:30.610: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:30.711: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:30.807: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:30.912: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:31.008: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:31.108: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:31.209: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:31.310: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:31.409: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:31.513: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:31.617: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:31.712: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:31.815: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:31.908: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:32.008: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:32.116: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:32.208: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:32.312: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:32.411: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:32.520: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:32.611: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:32.714: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:32.849: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:32.927: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:33.039: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:33.110: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:33.222: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:33.308: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:33.409: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:33.511: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:33.611: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:33.723: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:33.813: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:33.907: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:34.008: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:34.108: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:34.207: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:34.308: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:34.410: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:34.507: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:34.636: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:34.723: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:34.807: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:34.908: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:35.007: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:35.112: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:35.213: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:35.312: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:35.407: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:35.510: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:35.609: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:35.709: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:35.809: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:35.907: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:36.007: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:36.115: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:36.207: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:36.313: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:36.408: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:36.508: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:36.607: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:36.708: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:36.815: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:36.907: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:37.006: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:37.107: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:37.207: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:37.307: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:37.407: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:37.508: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:37.608: INFO: Waiting for webhook configuration to be ready...
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Oct 29 21:48:41.754: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig attach --namespace=webhook-8334 to-be-attached-pod -i -c=container1'
Oct 29 21:48:41.962: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:48:41.988: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-8334" for this suite.
STEP: Destroying namespace "webhook-8334-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102


 [SLOW TEST:24.276 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
[BeforeEach] [k8s.io] [sig-node] PreStop
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:48:34.559: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:171
[It] should call prestop when killing a pod  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating server pod server in namespace prestop-6472
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-6472
STEP: Deleting pre-stop pod
Oct 29 21:48:47.932: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:48:47.958: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "prestop-6472" for this suite.


 [SLOW TEST:13.514 seconds]
[k8s.io] [sig-node] PreStop
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should call prestop when killing a pod  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":-1,"completed":8,"skipped":96,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:48:39.944: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:48:56.598: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "resourcequota-7626" for this suite.


 [SLOW TEST:16.761 seconds]
[sig-api-machinery] ResourceQuota
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":-1,"completed":7,"skipped":149,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:48:56.721: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Oct 29 21:48:56.923: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d8232454-4b5a-4451-a4dd-bc791db77e3b" in namespace "projected-1537" to be "Succeeded or Failed"
Oct 29 21:48:56.960: INFO: Pod "downwardapi-volume-d8232454-4b5a-4451-a4dd-bc791db77e3b": Phase="Pending", Reason="", readiness=false. Elapsed: 37.179484ms
Oct 29 21:48:58.982: INFO: Pod "downwardapi-volume-d8232454-4b5a-4451-a4dd-bc791db77e3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059798011s
Oct 29 21:49:01.005: INFO: Pod "downwardapi-volume-d8232454-4b5a-4451-a4dd-bc791db77e3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.081886732s
STEP: Saw pod success
Oct 29 21:49:01.005: INFO: Pod "downwardapi-volume-d8232454-4b5a-4451-a4dd-bc791db77e3b" satisfied condition "Succeeded or Failed"
Oct 29 21:49:01.026: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod downwardapi-volume-d8232454-4b5a-4451-a4dd-bc791db77e3b container client-container: <nil>
STEP: delete the pod
Oct 29 21:49:01.088: INFO: Waiting for pod downwardapi-volume-d8232454-4b5a-4451-a4dd-bc791db77e3b to disappear
Oct 29 21:49:01.114: INFO: Pod downwardapi-volume-d8232454-4b5a-4451-a4dd-bc791db77e3b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:49:01.114: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-1537" for this suite.


------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":-1,"completed":3,"skipped":33,"failed":0}
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:48:42.249: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 29 21:48:42.873: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739604903, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739604903, loc:(*time.Location)(0x77098a0)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-cbccbf6bb\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739604903, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739604903, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Oct 29 21:48:44.895: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739604903, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739604903, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739604903, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739604903, loc:(*time.Location)(0x77098a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 29 21:48:47.923: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 21:48:47.945: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Registering the custom resource webhook via the AdmissionRegistration API
Oct 29 21:48:48.079: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:48.247: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:48.344: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:48.465: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:48.576: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:48.647: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:48.753: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:48.879: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:49.042: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:49.131: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:49.235: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:49.330: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:49.432: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:49.533: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:49.632: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:49.731: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:49.918: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:50.031: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:50.131: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:50.233: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:50.339: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:50.479: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:50.551: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:50.640: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:50.740: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:50.852: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:50.950: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:51.066: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:51.138: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:51.233: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:51.342: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:51.441: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:51.531: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:51.659: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:51.792: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:51.957: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:52.060: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:52.134: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:52.238: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:52.333: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:52.436: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:52.538: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:52.634: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:52.733: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:52.831: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:52.931: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:53.032: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:53.135: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:53.233: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:53.335: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:53.435: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:53.538: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:53.633: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:53.733: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:53.830: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:53.930: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:54.034: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:54.130: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:54.231: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:54.333: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:54.434: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:54.534: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:54.658: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:54.732: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:54.837: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:54.932: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:55.031: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:55.131: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:55.230: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:55.339: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:55.432: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:55.530: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:55.637: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:55.731: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:55.837: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:55.930: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:56.030: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:56.131: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:56.236: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:56.336: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:56.441: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:56.537: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:56.642: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:56.734: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:56.846: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:56.950: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:57.037: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:57.140: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:57.232: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:57.338: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:57.435: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:57.531: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:57.634: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:57.730: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:57.833: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:57.930: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:58.030: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:58.130: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:58.229: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:58.332: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:58.430: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:58.531: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:58.630: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:58.731: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:58.831: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:58.932: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:59.048: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:59.135: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:59.232: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:59.334: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:59.430: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:59.531: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:59.638: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:59.734: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:59.840: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:48:59.976: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:49:00.053: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:49:00.147: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:49:00.244: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:49:00.330: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:49:00.438: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:49:00.531: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:49:00.635: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:49:00.732: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:49:00.830: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:49:00.933: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:49:01.042: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:49:01.158: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:49:01.229: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:49:01.348: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:49:01.432: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:49:01.537: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:49:02.446: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-8" for this suite.
STEP: Destroying namespace "webhook-8-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102


 [SLOW TEST:20.505 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":-1,"completed":4,"skipped":33,"failed":0}

SSS
------------------------------
[BeforeEach] [sig-api-machinery] Secrets
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:49:02.759: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:49:03.552: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-4225" for this suite.


------------------------------
{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":-1,"completed":5,"skipped":36,"failed":0}

SSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:48:48.105: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-5595
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-5595
STEP: creating replication controller externalsvc in namespace services-5595
I1029 21:48:48.348666   53210 runners.go:190] Created replication controller with name: externalsvc, namespace: services-5595, replica count: 2
I1029 21:48:51.399145   53210 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1029 21:48:54.399359   53210 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Oct 29 21:48:54.467: INFO: Creating new exec pod
Oct 29 21:48:58.546: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=services-5595 execpodw5dtp -- /bin/sh -x -c nslookup clusterip-service.services-5595.svc.cluster.local'
Oct 29 21:48:58.931: INFO: stderr: "+ nslookup clusterip-service.services-5595.svc.cluster.local\n"
Oct 29 21:48:58.931: INFO: stdout: "Server:\t\t172.30.0.10\nAddress:\t172.30.0.10#53\n\nclusterip-service.services-5595.svc.cluster.local\tcanonical name = externalsvc.services-5595.svc.cluster.local.\nName:\texternalsvc.services-5595.svc.cluster.local\nAddress: 172.30.77.147\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-5595, will wait for the garbage collector to delete the pods
Oct 29 21:48:59.028: INFO: Deleting ReplicationController externalsvc took: 25.719502ms
Oct 29 21:48:59.128: INFO: Terminating ReplicationController externalsvc pods took: 100.290427ms
Oct 29 21:49:07.164: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:49:07.199: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-5595" for this suite.
[AfterEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786


 [SLOW TEST:19.180 seconds]
[sig-network] Services
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
[BeforeEach] [sig-api-machinery] Garbage collector
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:49:03.618: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 21:49:03.908: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"d901cdf0-cc82-4836-9652-5e903cf5974a", Controller:(*bool)(0xc002789556), BlockOwnerDeletion:(*bool)(0xc002789557)}}
Oct 29 21:49:03.934: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"83a2df32-5ce1-462a-b961-b8fc6e285232", Controller:(*bool)(0xc002789892), BlockOwnerDeletion:(*bool)(0xc002789893)}}
Oct 29 21:49:03.960: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"c1536a74-398f-4bec-a914-ddedb22f6247", Controller:(*bool)(0xc0037d5f02), BlockOwnerDeletion:(*bool)(0xc0037d5f03)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:49:09.005: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "gc-6003" for this suite.


 [SLOW TEST:5.491 seconds]
[sig-api-machinery] Garbage collector
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":-1,"completed":6,"skipped":52,"failed":0}

SSSSSS
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":-1,"completed":9,"skipped":139,"failed":0}
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:49:07.287: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create deployment with httpd image
Oct 29 21:49:07.465: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig create -f -'
Oct 29 21:49:08.255: INFO: stderr: ""
Oct 29 21:49:08.255: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Oct 29 21:49:08.255: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig diff -f -'
Oct 29 21:49:09.071: INFO: rc: 1
Oct 29 21:49:09.071: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig delete -f -'
Oct 29 21:49:09.233: INFO: stderr: ""
Oct 29 21:49:09.233: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:49:09.233: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-1658" for this suite.


------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":-1,"completed":10,"skipped":139,"failed":0}

SSS
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":-1,"completed":8,"skipped":172,"failed":0}
[BeforeEach] [sig-apps] Job
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:49:01.220: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:49:11.475: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "job-9234" for this suite.


 [SLOW TEST:10.361 seconds]
[sig-apps] Job
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":-1,"completed":9,"skipped":172,"failed":0}

SSSS
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:49:09.370: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-f762d3dd-5eb1-44ea-9ecf-27c312256359
STEP: Creating a pod to test consume configMaps
Oct 29 21:49:09.617: INFO: Waiting up to 5m0s for pod "pod-configmaps-1d3da09c-011f-48ff-bc81-ffee4b4d916c" in namespace "configmap-3870" to be "Succeeded or Failed"
Oct 29 21:49:09.638: INFO: Pod "pod-configmaps-1d3da09c-011f-48ff-bc81-ffee4b4d916c": Phase="Pending", Reason="", readiness=false. Elapsed: 21.07441ms
Oct 29 21:49:11.681: INFO: Pod "pod-configmaps-1d3da09c-011f-48ff-bc81-ffee4b4d916c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.064540812s
Oct 29 21:49:13.703: INFO: Pod "pod-configmaps-1d3da09c-011f-48ff-bc81-ffee4b4d916c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.086230646s
STEP: Saw pod success
Oct 29 21:49:13.703: INFO: Pod "pod-configmaps-1d3da09c-011f-48ff-bc81-ffee4b4d916c" satisfied condition "Succeeded or Failed"
Oct 29 21:49:13.725: INFO: Trying to get logs from node ip-10-0-234-238.us-east-2.compute.internal pod pod-configmaps-1d3da09c-011f-48ff-bc81-ffee4b4d916c container configmap-volume-test: <nil>
STEP: delete the pod
Oct 29 21:49:13.783: INFO: Waiting for pod pod-configmaps-1d3da09c-011f-48ff-bc81-ffee4b4d916c to disappear
Oct 29 21:49:13.803: INFO: Pod pod-configmaps-1d3da09c-011f-48ff-bc81-ffee4b4d916c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:49:13.803: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-3870" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":-1,"completed":11,"skipped":142,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Events
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:49:13.927: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
Oct 29 21:49:14.183: INFO: created test-event-1
Oct 29 21:49:14.210: INFO: created test-event-2
Oct 29 21:49:14.235: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Oct 29 21:49:14.258: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Oct 29 21:49:14.299: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:49:14.322: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "events-6216" for this suite.


------------------------------
{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":-1,"completed":12,"skipped":170,"failed":0}

SSSSS
------------------------------
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:49:09.115: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Oct 29 21:49:09.276: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:49:18.967: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "init-container-5904" for this suite.


 [SLOW TEST:9.957 seconds]
[k8s.io] InitContainer [NodeConformance]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should invoke init containers on a RestartAlways pod [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":-1,"completed":7,"skipped":58,"failed":0}

SSSSSSS
------------------------------
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:49:14.394: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Oct 29 21:49:20.979: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1355 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 29 21:49:20.979: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
Oct 29 21:49:21.173: INFO: Exec stderr: ""
Oct 29 21:49:21.173: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1355 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 29 21:49:21.173: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
Oct 29 21:49:21.366: INFO: Exec stderr: ""
Oct 29 21:49:21.366: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1355 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 29 21:49:21.366: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
Oct 29 21:49:21.565: INFO: Exec stderr: ""
Oct 29 21:49:21.565: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1355 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 29 21:49:21.565: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
Oct 29 21:49:21.760: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Oct 29 21:49:21.760: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1355 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 29 21:49:21.760: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
Oct 29 21:49:21.957: INFO: Exec stderr: ""
Oct 29 21:49:21.957: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1355 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 29 21:49:21.957: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
Oct 29 21:49:22.149: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Oct 29 21:49:22.149: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1355 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 29 21:49:22.149: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
Oct 29 21:49:22.357: INFO: Exec stderr: ""
Oct 29 21:49:22.357: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1355 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 29 21:49:22.357: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
Oct 29 21:49:22.571: INFO: Exec stderr: ""
Oct 29 21:49:22.571: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1355 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 29 21:49:22.571: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
Oct 29 21:49:22.768: INFO: Exec stderr: ""
Oct 29 21:49:22.768: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1355 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 29 21:49:22.768: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
Oct 29 21:49:22.958: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:49:22.958: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-1355" for this suite.


 [SLOW TEST:8.666 seconds]
[k8s.io] KubeletManagedEtcHosts
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":13,"skipped":175,"failed":0}

SSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] Job
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:49:23.075: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Oct 29 21:49:27.838: INFO: Successfully updated pod "adopt-release-ff7nf"
STEP: Checking that the Job readopts the Pod
Oct 29 21:49:27.838: INFO: Waiting up to 15m0s for pod "adopt-release-ff7nf" in namespace "job-6928" to be "adopted"
Oct 29 21:49:27.858: INFO: Pod "adopt-release-ff7nf": Phase="Running", Reason="", readiness=true. Elapsed: 20.513802ms
Oct 29 21:49:27.858: INFO: Pod "adopt-release-ff7nf" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Oct 29 21:49:28.412: INFO: Successfully updated pod "adopt-release-ff7nf"
STEP: Checking that the Job releases the Pod
Oct 29 21:49:28.412: INFO: Waiting up to 15m0s for pod "adopt-release-ff7nf" in namespace "job-6928" to be "released"
Oct 29 21:49:28.434: INFO: Pod "adopt-release-ff7nf": Phase="Running", Reason="", readiness=true. Elapsed: 22.608732ms
Oct 29 21:49:28.434: INFO: Pod "adopt-release-ff7nf" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:49:28.434: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "job-6928" for this suite.


 [SLOW TEST:5.443 seconds]
[sig-apps] Job
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":-1,"completed":14,"skipped":190,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:49:19.080: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should create services for rc  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Oct 29 21:49:19.244: INFO: namespace kubectl-480
Oct 29 21:49:19.244: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig create -f - --namespace=kubectl-480'
Oct 29 21:49:19.746: INFO: stderr: ""
Oct 29 21:49:19.746: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Oct 29 21:49:20.769: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 29 21:49:20.769: INFO: Found 0 / 1
Oct 29 21:49:21.767: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 29 21:49:21.767: INFO: Found 0 / 1
Oct 29 21:49:22.768: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 29 21:49:22.768: INFO: Found 0 / 1
Oct 29 21:49:23.768: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 29 21:49:23.768: INFO: Found 1 / 1
Oct 29 21:49:23.768: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Oct 29 21:49:23.788: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 29 21:49:23.788: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Oct 29 21:49:23.789: INFO: wait on agnhost-primary startup in kubectl-480 
Oct 29 21:49:23.789: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig logs agnhost-primary-kh2h4 agnhost-primary --namespace=kubectl-480'
Oct 29 21:49:23.963: INFO: stderr: ""
Oct 29 21:49:23.963: INFO: stdout: "Paused\n"
STEP: exposing RC
Oct 29 21:49:23.963: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-480'
Oct 29 21:49:24.145: INFO: stderr: ""
Oct 29 21:49:24.145: INFO: stdout: "service/rm2 exposed\n"
Oct 29 21:49:24.167: INFO: Service rm2 in namespace kubectl-480 found.
STEP: exposing service
Oct 29 21:49:26.210: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-480'
Oct 29 21:49:26.405: INFO: stderr: ""
Oct 29 21:49:26.405: INFO: stdout: "service/rm3 exposed\n"
Oct 29 21:49:26.427: INFO: Service rm3 in namespace kubectl-480 found.
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:49:28.470: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-480" for this suite.


 [SLOW TEST:9.492 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1246
    should create services for rc  [Conformance]
    /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":-1,"completed":8,"skipped":65,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected configMap
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:47:59.268: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename projected
Oct 29 21:47:59.532: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 21:47:59.661: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating projection with configMap that has name projected-configmap-test-upd-f6d21eca-7903-4394-95d0-2f3b7f3aa649
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-f6d21eca-7903-4394-95d0-2f3b7f3aa649
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:49:33.176: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-9130" for this suite.


 [SLOW TEST:94.010 seconds]
[sig-storage] Projected configMap
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":-1,"completed":1,"skipped":9,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:49:28.594: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-cc3fd794-c01a-4714-bc6a-f0153ad05b00
STEP: Creating a pod to test consume configMaps
Oct 29 21:49:28.866: INFO: Waiting up to 5m0s for pod "pod-configmaps-b991e415-34d0-46a8-b8bc-63ee0e4e3282" in namespace "configmap-9547" to be "Succeeded or Failed"
Oct 29 21:49:28.924: INFO: Pod "pod-configmaps-b991e415-34d0-46a8-b8bc-63ee0e4e3282": Phase="Pending", Reason="", readiness=false. Elapsed: 58.240179ms
Oct 29 21:49:30.945: INFO: Pod "pod-configmaps-b991e415-34d0-46a8-b8bc-63ee0e4e3282": Phase="Pending", Reason="", readiness=false. Elapsed: 2.079637658s
Oct 29 21:49:32.967: INFO: Pod "pod-configmaps-b991e415-34d0-46a8-b8bc-63ee0e4e3282": Phase="Pending", Reason="", readiness=false. Elapsed: 4.100994656s
Oct 29 21:49:34.988: INFO: Pod "pod-configmaps-b991e415-34d0-46a8-b8bc-63ee0e4e3282": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.1223322s
STEP: Saw pod success
Oct 29 21:49:34.988: INFO: Pod "pod-configmaps-b991e415-34d0-46a8-b8bc-63ee0e4e3282" satisfied condition "Succeeded or Failed"
Oct 29 21:49:35.009: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-configmaps-b991e415-34d0-46a8-b8bc-63ee0e4e3282 container configmap-volume-test: <nil>
STEP: delete the pod
Oct 29 21:49:35.084: INFO: Waiting for pod pod-configmaps-b991e415-34d0-46a8-b8bc-63ee0e4e3282 to disappear
Oct 29 21:49:35.115: INFO: Pod pod-configmaps-b991e415-34d0-46a8-b8bc-63ee0e4e3282 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:49:35.115: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-9547" for this suite.


 [SLOW TEST:6.626 seconds]
[sig-storage] ConfigMap
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":-1,"completed":9,"skipped":92,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:49:35.252: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:49:35.463: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9437" for this suite.


------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":-1,"completed":10,"skipped":128,"failed":0}

SSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:49:33.305: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Oct 29 21:49:33.482: INFO: Waiting up to 5m0s for pod "downwardapi-volume-86e94fd1-8c87-4234-9981-e548904ecc2f" in namespace "projected-2136" to be "Succeeded or Failed"
Oct 29 21:49:33.505: INFO: Pod "downwardapi-volume-86e94fd1-8c87-4234-9981-e548904ecc2f": Phase="Pending", Reason="", readiness=false. Elapsed: 23.049162ms
Oct 29 21:49:35.536: INFO: Pod "downwardapi-volume-86e94fd1-8c87-4234-9981-e548904ecc2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053444393s
Oct 29 21:49:37.560: INFO: Pod "downwardapi-volume-86e94fd1-8c87-4234-9981-e548904ecc2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.078323295s
STEP: Saw pod success
Oct 29 21:49:37.560: INFO: Pod "downwardapi-volume-86e94fd1-8c87-4234-9981-e548904ecc2f" satisfied condition "Succeeded or Failed"
Oct 29 21:49:37.588: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod downwardapi-volume-86e94fd1-8c87-4234-9981-e548904ecc2f container client-container: <nil>
STEP: delete the pod
Oct 29 21:49:37.659: INFO: Waiting for pod downwardapi-volume-86e94fd1-8c87-4234-9981-e548904ecc2f to disappear
Oct 29 21:49:37.680: INFO: Pod downwardapi-volume-86e94fd1-8c87-4234-9981-e548904ecc2f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:49:37.681: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-2136" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":2,"skipped":40,"failed":0}

SSSSSS
------------------------------
[BeforeEach] [sig-network] Ingress API
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:49:37.800: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Oct 29 21:49:38.119: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Oct 29 21:49:38.159: INFO: starting watch
STEP: patching
STEP: updating
Oct 29 21:49:38.227: INFO: waiting for watch events with expected annotations
Oct 29 21:49:38.228: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:49:38.429: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "ingress-1214" for this suite.


------------------------------
{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":-1,"completed":3,"skipped":46,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] Deployment
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:49:28.551: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support proportional scaling [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 21:49:28.699: INFO: Creating deployment "webserver-deployment"
Oct 29 21:49:28.729: INFO: Waiting for observed generation 1
Oct 29 21:49:30.785: INFO: Waiting for all required pods to come up
Oct 29 21:49:30.825: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Oct 29 21:49:34.887: INFO: Waiting for deployment "webserver-deployment" to complete
Oct 29 21:49:34.942: INFO: Updating deployment "webserver-deployment" with a non-existent image
Oct 29 21:49:34.998: INFO: Updating deployment webserver-deployment
Oct 29 21:49:34.998: INFO: Waiting for observed generation 2
Oct 29 21:49:37.042: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Oct 29 21:49:37.064: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Oct 29 21:49:37.084: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Oct 29 21:49:37.146: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Oct 29 21:49:37.146: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Oct 29 21:49:37.168: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Oct 29 21:49:37.208: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Oct 29 21:49:37.208: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Oct 29 21:49:37.252: INFO: Updating deployment webserver-deployment
Oct 29 21:49:37.252: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Oct 29 21:49:37.302: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Oct 29 21:49:39.345: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Oct 29 21:49:39.391: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-1917 /apis/apps/v1/namespaces/deployment-1917/deployments/webserver-deployment e149a359-c25e-4289-9f0e-9cde0c3e679d 224980 3 2020-10-29 21:49:09 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-10-29 21:49:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-10-29 21:49:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002e50448 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-10-29 21:49:17 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2020-10-29 21:49:18 +0000 UTC,LastTransitionTime:2020-10-29 21:49:09 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Oct 29 21:49:39.413: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-1917 /apis/apps/v1/namespaces/deployment-1917/replicasets/webserver-deployment-795d758f88 82a1d61a-7847-4b01-b6ff-8b45c324a784 224979 3 2020-10-29 21:49:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment e149a359-c25e-4289-9f0e-9cde0c3e679d 0xc002e508b7 0xc002e508b8}] []  [{kube-controller-manager Update apps/v1 2020-10-29 21:49:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e149a359-c25e-4289-9f0e-9cde0c3e679d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002e50a68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Oct 29 21:49:39.413: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Oct 29 21:49:39.413: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-dd94f59b7  deployment-1917 /apis/apps/v1/namespaces/deployment-1917/replicasets/webserver-deployment-dd94f59b7 d5769155-9a98-4a27-bd91-9ccd2e399ef5 224933 3 2020-10-29 21:49:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment e149a359-c25e-4289-9f0e-9cde0c3e679d 0xc002e50ad7 0xc002e50ad8}] []  [{kube-controller-manager Update apps/v1 2020-10-29 21:49:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e149a359-c25e-4289-9f0e-9cde0c3e679d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: dd94f59b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002e50b48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Oct 29 21:49:39.491: INFO: Pod "webserver-deployment-795d758f88-45jfr" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-45jfr webserver-deployment-795d758f88- deployment-1917 /api/v1/namespaces/deployment-1917/pods/webserver-deployment-795d758f88-45jfr 209602d0-4a09-40a3-be63-0bae2fc83670 225151 0 2020-10-29 21:49:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.131.1.147"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.131.1.147"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 82a1d61a-7847-4b01-b6ff-8b45c324a784 0xc002fde857 0xc002fde858}] []  [{kube-controller-manager Update v1 2020-10-29 21:49:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"82a1d61a-7847-4b01-b6ff-8b45c324a784\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {multus Update v1 2020-10-29 21:49:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2020-10-29 21:49:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.1.147\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lk8pr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lk8pr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lk8pr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-142-212.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c85,c80,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-ftpxv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.142.212,PodIP:10.131.1.147,StartTime:2020-10-29 21:49:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error reading manifest 404 in docker.io/library/webserver: errors:
denied: requested access to the resource is denied
unauthorized: authentication required
,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.1.147,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 29 21:49:39.491: INFO: Pod "webserver-deployment-795d758f88-fqljq" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-fqljq webserver-deployment-795d758f88- deployment-1917 /api/v1/namespaces/deployment-1917/pods/webserver-deployment-795d758f88-fqljq ca786a85-ee26-4529-9675-3880d7d23e18 225154 0 2020-10-29 21:49:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.131.1.145"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.131.1.145"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 82a1d61a-7847-4b01-b6ff-8b45c324a784 0xc002fdea67 0xc002fdea68}] []  [{kube-controller-manager Update v1 2020-10-29 21:49:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"82a1d61a-7847-4b01-b6ff-8b45c324a784\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {multus Update v1 2020-10-29 21:49:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2020-10-29 21:49:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.1.145\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lk8pr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lk8pr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lk8pr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-142-212.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c85,c80,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-ftpxv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.142.212,PodIP:10.131.1.145,StartTime:2020-10-29 21:49:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error reading manifest 404 in docker.io/library/webserver: errors:
denied: requested access to the resource is denied
unauthorized: authentication required
,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.1.145,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 29 21:49:39.492: INFO: Pod "webserver-deployment-795d758f88-g74wz" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-g74wz webserver-deployment-795d758f88- deployment-1917 /api/v1/namespaces/deployment-1917/pods/webserver-deployment-795d758f88-g74wz d0e0fc05-eb7a-43cd-86ab-6c505d265999 224971 0 2020-10-29 21:49:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 82a1d61a-7847-4b01-b6ff-8b45c324a784 0xc002fdec77 0xc002fdec78}] []  [{kube-controller-manager Update v1 2020-10-29 21:49:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"82a1d61a-7847-4b01-b6ff-8b45c324a784\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-10-29 21:49:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lk8pr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lk8pr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lk8pr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-157-225.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c85,c80,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-ftpxv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.157.225,PodIP:,StartTime:2020-10-29 21:49:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 29 21:49:39.492: INFO: Pod "webserver-deployment-795d758f88-j4z4c" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-j4z4c webserver-deployment-795d758f88- deployment-1917 /api/v1/namespaces/deployment-1917/pods/webserver-deployment-795d758f88-j4z4c bfe12cf8-c04c-49d5-8888-a28f69fb8d18 225123 0 2020-10-29 21:49:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.128.2.230"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.128.2.230"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 82a1d61a-7847-4b01-b6ff-8b45c324a784 0xc002fdee37 0xc002fdee38}] []  [{kube-controller-manager Update v1 2020-10-29 21:49:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"82a1d61a-7847-4b01-b6ff-8b45c324a784\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-10-29 21:49:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:hostIP":{},"f:startTime":{}}}} {multus Update v1 2020-10-29 21:49:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}},"f:status":{"f:containerStatuses":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lk8pr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lk8pr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lk8pr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-234-238.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c85,c80,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-ftpxv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.234.238,PodIP:,StartTime:2020-10-29 21:49:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 29 21:49:39.492: INFO: Pod "webserver-deployment-795d758f88-j8vfz" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-j8vfz webserver-deployment-795d758f88- deployment-1917 /api/v1/namespaces/deployment-1917/pods/webserver-deployment-795d758f88-j8vfz 7af788be-5f99-4048-b3c1-322782c1611f 224952 0 2020-10-29 21:49:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 82a1d61a-7847-4b01-b6ff-8b45c324a784 0xc002fdf017 0xc002fdf018}] []  [{kube-controller-manager Update v1 2020-10-29 21:49:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"82a1d61a-7847-4b01-b6ff-8b45c324a784\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-10-29 21:49:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lk8pr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lk8pr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lk8pr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-157-225.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c85,c80,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-ftpxv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.157.225,PodIP:,StartTime:2020-10-29 21:49:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 29 21:49:39.492: INFO: Pod "webserver-deployment-795d758f88-kpxjq" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-kpxjq webserver-deployment-795d758f88- deployment-1917 /api/v1/namespaces/deployment-1917/pods/webserver-deployment-795d758f88-kpxjq 4ed17bcf-fdd1-410a-a209-a8eef72a10f1 225139 0 2020-10-29 21:49:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.131.1.146"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.131.1.146"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 82a1d61a-7847-4b01-b6ff-8b45c324a784 0xc002fdf1d7 0xc002fdf1d8}] []  [{kube-controller-manager Update v1 2020-10-29 21:49:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"82a1d61a-7847-4b01-b6ff-8b45c324a784\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {multus Update v1 2020-10-29 21:49:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2020-10-29 21:49:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.1.146\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lk8pr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lk8pr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lk8pr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-142-212.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c85,c80,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-ftpxv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.142.212,PodIP:10.131.1.146,StartTime:2020-10-29 21:49:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error reading manifest 404 in docker.io/library/webserver: errors:
denied: requested access to the resource is denied
unauthorized: authentication required
,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.1.146,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 29 21:49:39.492: INFO: Pod "webserver-deployment-795d758f88-mkdsp" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-mkdsp webserver-deployment-795d758f88- deployment-1917 /api/v1/namespaces/deployment-1917/pods/webserver-deployment-795d758f88-mkdsp f9b71885-aff2-463b-91a7-ea0262c8b0f7 224981 0 2020-10-29 21:49:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 82a1d61a-7847-4b01-b6ff-8b45c324a784 0xc002fdf3e7 0xc002fdf3e8}] []  [{kube-controller-manager Update v1 2020-10-29 21:49:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"82a1d61a-7847-4b01-b6ff-8b45c324a784\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-10-29 21:49:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lk8pr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lk8pr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lk8pr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-142-212.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c85,c80,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-ftpxv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.142.212,PodIP:,StartTime:2020-10-29 21:49:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 29 21:49:39.492: INFO: Pod "webserver-deployment-795d758f88-n54r9" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-n54r9 webserver-deployment-795d758f88- deployment-1917 /api/v1/namespaces/deployment-1917/pods/webserver-deployment-795d758f88-n54r9 48b08c96-5336-4b2a-8ecb-fef98522e791 224972 0 2020-10-29 21:49:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 82a1d61a-7847-4b01-b6ff-8b45c324a784 0xc002fdf5a7 0xc002fdf5a8}] []  [{kube-controller-manager Update v1 2020-10-29 21:49:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"82a1d61a-7847-4b01-b6ff-8b45c324a784\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-10-29 21:49:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lk8pr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lk8pr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lk8pr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-158-72.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c85,c80,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-ftpxv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.158.72,PodIP:,StartTime:2020-10-29 21:49:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 29 21:49:39.493: INFO: Pod "webserver-deployment-795d758f88-nhd4v" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-nhd4v webserver-deployment-795d758f88- deployment-1917 /api/v1/namespaces/deployment-1917/pods/webserver-deployment-795d758f88-nhd4v c625a919-d465-45b5-a581-5cd50ecf7dd3 225135 0 2020-10-29 21:49:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.130.2.77"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.130.2.77"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 82a1d61a-7847-4b01-b6ff-8b45c324a784 0xc002fdf767 0xc002fdf768}] []  [{kube-controller-manager Update v1 2020-10-29 21:49:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"82a1d61a-7847-4b01-b6ff-8b45c324a784\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-10-29 21:49:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:hostIP":{},"f:startTime":{}}}} {multus Update v1 2020-10-29 21:49:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}},"f:status":{"f:containerStatuses":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lk8pr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lk8pr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lk8pr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-158-72.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c85,c80,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-ftpxv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.158.72,PodIP:,StartTime:2020-10-29 21:49:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 29 21:49:39.493: INFO: Pod "webserver-deployment-795d758f88-pj8f2" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-pj8f2 webserver-deployment-795d758f88- deployment-1917 /api/v1/namespaces/deployment-1917/pods/webserver-deployment-795d758f88-pj8f2 b0d5a585-1190-4e24-b7aa-1c3d16a69c9b 225125 0 2020-10-29 21:49:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.128.2.228"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.128.2.228"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 82a1d61a-7847-4b01-b6ff-8b45c324a784 0xc002fdf947 0xc002fdf948}] []  [{kube-controller-manager Update v1 2020-10-29 21:49:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"82a1d61a-7847-4b01-b6ff-8b45c324a784\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {multus Update v1 2020-10-29 21:49:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2020-10-29 21:49:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.228\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lk8pr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lk8pr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lk8pr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-234-238.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c85,c80,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-ftpxv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.234.238,PodIP:10.128.2.228,StartTime:2020-10-29 21:49:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error reading manifest 404 in docker.io/library/webserver: errors:
denied: requested access to the resource is denied
unauthorized: authentication required
,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.228,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 29 21:49:39.493: INFO: Pod "webserver-deployment-795d758f88-rzcnq" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-rzcnq webserver-deployment-795d758f88- deployment-1917 /api/v1/namespaces/deployment-1917/pods/webserver-deployment-795d758f88-rzcnq 80680a9c-7a56-489c-8d79-ca3472a3be14 224967 0 2020-10-29 21:49:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 82a1d61a-7847-4b01-b6ff-8b45c324a784 0xc002fdfb67 0xc002fdfb68}] []  [{kube-controller-manager Update v1 2020-10-29 21:49:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"82a1d61a-7847-4b01-b6ff-8b45c324a784\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-10-29 21:49:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lk8pr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lk8pr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lk8pr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-157-225.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c85,c80,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-ftpxv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.157.225,PodIP:,StartTime:2020-10-29 21:49:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 29 21:49:39.493: INFO: Pod "webserver-deployment-795d758f88-xpxg7" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-xpxg7 webserver-deployment-795d758f88- deployment-1917 /api/v1/namespaces/deployment-1917/pods/webserver-deployment-795d758f88-xpxg7 2239e269-856f-4cc7-9826-5f5331a6ef64 225142 0 2020-10-29 21:49:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.130.2.78"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.130.2.78"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 82a1d61a-7847-4b01-b6ff-8b45c324a784 0xc002fdfd27 0xc002fdfd28}] []  [{kube-controller-manager Update v1 2020-10-29 21:49:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"82a1d61a-7847-4b01-b6ff-8b45c324a784\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-10-29 21:49:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:hostIP":{},"f:startTime":{}}}} {multus Update v1 2020-10-29 21:49:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}},"f:status":{"f:containerStatuses":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lk8pr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lk8pr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lk8pr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-158-72.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c85,c80,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-ftpxv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.158.72,PodIP:,StartTime:2020-10-29 21:49:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 29 21:49:39.493: INFO: Pod "webserver-deployment-795d758f88-z4qzr" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-z4qzr webserver-deployment-795d758f88- deployment-1917 /api/v1/namespaces/deployment-1917/pods/webserver-deployment-795d758f88-z4qzr 7f975687-9cbf-4965-b8e5-c41c4b918176 225025 0 2020-10-29 21:49:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.128.2.229"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.128.2.229"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 82a1d61a-7847-4b01-b6ff-8b45c324a784 0xc002fdff07 0xc002fdff08}] []  [{kube-controller-manager Update v1 2020-10-29 21:49:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"82a1d61a-7847-4b01-b6ff-8b45c324a784\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-10-29 21:49:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:hostIP":{},"f:startTime":{}}}} {multus Update v1 2020-10-29 21:49:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}},"f:status":{"f:containerStatuses":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lk8pr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lk8pr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lk8pr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-234-238.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c85,c80,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-ftpxv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.234.238,PodIP:,StartTime:2020-10-29 21:49:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 29 21:49:39.493: INFO: Pod "webserver-deployment-dd94f59b7-2q862" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-2q862 webserver-deployment-dd94f59b7- deployment-1917 /api/v1/namespaces/deployment-1917/pods/webserver-deployment-dd94f59b7-2q862 5ceb184f-3283-4f28-accb-f7c418845959 225152 0 2020-10-29 21:49:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.130.2.80"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.130.2.80"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 d5769155-9a98-4a27-bd91-9ccd2e399ef5 0xc0037c80e7 0xc0037c80e8}] []  [{kube-controller-manager Update v1 2020-10-29 21:49:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5769155-9a98-4a27-bd91-9ccd2e399ef5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-10-29 21:49:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:hostIP":{},"f:startTime":{}}}} {multus Update v1 2020-10-29 21:49:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}},"f:status":{"f:containerStatuses":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lk8pr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lk8pr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lk8pr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-158-72.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c85,c80,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-ftpxv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.158.72,PodIP:,StartTime:2020-10-29 21:49:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 29 21:49:39.493: INFO: Pod "webserver-deployment-dd94f59b7-5kl56" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-5kl56 webserver-deployment-dd94f59b7- deployment-1917 /api/v1/namespaces/deployment-1917/pods/webserver-deployment-dd94f59b7-5kl56 2142b90b-1e74-434d-96f8-032ef3d95447 225133 0 2020-10-29 21:49:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.129.2.75"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.129.2.75"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 d5769155-9a98-4a27-bd91-9ccd2e399ef5 0xc0037c82a7 0xc0037c82a8}] []  [{kube-controller-manager Update v1 2020-10-29 21:49:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5769155-9a98-4a27-bd91-9ccd2e399ef5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-10-29 21:49:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:hostIP":{},"f:startTime":{}}}} {multus Update v1 2020-10-29 21:49:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}},"f:status":{"f:containerStatuses":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lk8pr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lk8pr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lk8pr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-157-225.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c85,c80,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-ftpxv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.157.225,PodIP:,StartTime:2020-10-29 21:49:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 29 21:49:39.494: INFO: Pod "webserver-deployment-dd94f59b7-6plw7" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-6plw7 webserver-deployment-dd94f59b7- deployment-1917 /api/v1/namespaces/deployment-1917/pods/webserver-deployment-dd94f59b7-6plw7 3a8735c6-fe15-436b-82a3-8444dc44ba91 224530 0 2020-10-29 21:49:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.128.2.227"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.128.2.227"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 d5769155-9a98-4a27-bd91-9ccd2e399ef5 0xc0037c8467 0xc0037c8468}] []  [{kube-controller-manager Update v1 2020-10-29 21:49:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5769155-9a98-4a27-bd91-9ccd2e399ef5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {multus Update v1 2020-10-29 21:49:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2020-10-29 21:49:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.227\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lk8pr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lk8pr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lk8pr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-234-238.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c85,c80,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-ftpxv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.234.238,PodIP:10.128.2.227,StartTime:2020-10-29 21:49:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-10-29 21:49:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://2441f31e4904ffa8305c6052404de9c95e6d3331d7a3f51c82d47e13946a4122,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.227,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 29 21:49:39.494: INFO: Pod "webserver-deployment-dd94f59b7-8ndfz" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-8ndfz webserver-deployment-dd94f59b7- deployment-1917 /api/v1/namespaces/deployment-1917/pods/webserver-deployment-dd94f59b7-8ndfz 47a6e7dd-7ba2-47b2-ac2e-ce43a5d516ec 225147 0 2020-10-29 21:49:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.131.1.150"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.131.1.150"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 d5769155-9a98-4a27-bd91-9ccd2e399ef5 0xc0037c8657 0xc0037c8658}] []  [{kube-controller-manager Update v1 2020-10-29 21:49:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5769155-9a98-4a27-bd91-9ccd2e399ef5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-10-29 21:49:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:hostIP":{},"f:startTime":{}}}} {multus Update v1 2020-10-29 21:49:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}},"f:status":{"f:containerStatuses":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lk8pr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lk8pr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lk8pr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-142-212.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c85,c80,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-ftpxv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.142.212,PodIP:,StartTime:2020-10-29 21:49:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 29 21:49:39.494: INFO: Pod "webserver-deployment-dd94f59b7-8v6hn" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-8v6hn webserver-deployment-dd94f59b7- deployment-1917 /api/v1/namespaces/deployment-1917/pods/webserver-deployment-dd94f59b7-8v6hn 21e8f1b7-b270-48b4-a33e-e2a664e3a0f7 224886 0 2020-10-29 21:49:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 d5769155-9a98-4a27-bd91-9ccd2e399ef5 0xc0037c8817 0xc0037c8818}] []  [{kube-controller-manager Update v1 2020-10-29 21:49:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5769155-9a98-4a27-bd91-9ccd2e399ef5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-10-29 21:49:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lk8pr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lk8pr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lk8pr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-142-212.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c85,c80,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-ftpxv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.142.212,PodIP:,StartTime:2020-10-29 21:49:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 29 21:49:39.494: INFO: Pod "webserver-deployment-dd94f59b7-9g4bh" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-9g4bh webserver-deployment-dd94f59b7- deployment-1917 /api/v1/namespaces/deployment-1917/pods/webserver-deployment-dd94f59b7-9g4bh a342ebba-a26c-432e-a3e0-2b09a665404b 224546 0 2020-10-29 21:49:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.131.1.141"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.131.1.141"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 d5769155-9a98-4a27-bd91-9ccd2e399ef5 0xc0037c89b7 0xc0037c89b8}] []  [{kube-controller-manager Update v1 2020-10-29 21:49:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5769155-9a98-4a27-bd91-9ccd2e399ef5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {multus Update v1 2020-10-29 21:49:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2020-10-29 21:49:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.1.141\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lk8pr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lk8pr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lk8pr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-142-212.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c85,c80,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-ftpxv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.142.212,PodIP:10.131.1.141,StartTime:2020-10-29 21:49:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-10-29 21:49:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://8d7e45ab039a7c7f44fa8efacae7345baec0fcad2fba590217ef987364fbffad,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.1.141,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 29 21:49:39.494: INFO: Pod "webserver-deployment-dd94f59b7-9kq9l" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-9kq9l webserver-deployment-dd94f59b7- deployment-1917 /api/v1/namespaces/deployment-1917/pods/webserver-deployment-dd94f59b7-9kq9l 1789ed3a-3ff2-464f-8490-c7ec6007853f 224536 0 2020-10-29 21:49:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.128.2.223"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.128.2.223"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 d5769155-9a98-4a27-bd91-9ccd2e399ef5 0xc0037c8ba7 0xc0037c8ba8}] []  [{kube-controller-manager Update v1 2020-10-29 21:49:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5769155-9a98-4a27-bd91-9ccd2e399ef5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {multus Update v1 2020-10-29 21:49:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2020-10-29 21:49:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.223\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lk8pr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lk8pr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lk8pr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-234-238.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c85,c80,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-ftpxv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.234.238,PodIP:10.128.2.223,StartTime:2020-10-29 21:49:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-10-29 21:49:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://0b431c9631d8985b5a07529f263d09483388a2adf87c068ee88ad5c918d00c4f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.223,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 29 21:49:39.494: INFO: Pod "webserver-deployment-dd94f59b7-b7gpk" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-b7gpk webserver-deployment-dd94f59b7- deployment-1917 /api/v1/namespaces/deployment-1917/pods/webserver-deployment-dd94f59b7-b7gpk 70c481d0-b6c3-4329-b019-d787f4609eed 225121 0 2020-10-29 21:49:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.129.2.76"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.129.2.76"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 d5769155-9a98-4a27-bd91-9ccd2e399ef5 0xc0037c8d87 0xc0037c8d88}] []  [{kube-controller-manager Update v1 2020-10-29 21:49:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5769155-9a98-4a27-bd91-9ccd2e399ef5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-10-29 21:49:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:hostIP":{},"f:startTime":{}}}} {multus Update v1 2020-10-29 21:49:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}},"f:status":{"f:containerStatuses":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lk8pr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lk8pr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lk8pr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-157-225.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c85,c80,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-ftpxv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.157.225,PodIP:,StartTime:2020-10-29 21:49:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 29 21:49:39.494: INFO: Pod "webserver-deployment-dd94f59b7-clfmj" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-clfmj webserver-deployment-dd94f59b7- deployment-1917 /api/v1/namespaces/deployment-1917/pods/webserver-deployment-dd94f59b7-clfmj 7ecd8106-d0af-438b-b48e-f02a21f4f28c 224945 0 2020-10-29 21:49:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 d5769155-9a98-4a27-bd91-9ccd2e399ef5 0xc0037c8f47 0xc0037c8f48}] []  [{kube-controller-manager Update v1 2020-10-29 21:49:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5769155-9a98-4a27-bd91-9ccd2e399ef5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-10-29 21:49:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lk8pr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lk8pr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lk8pr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-157-225.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c85,c80,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-ftpxv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.157.225,PodIP:,StartTime:2020-10-29 21:49:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 29 21:49:39.495: INFO: Pod "webserver-deployment-dd94f59b7-dgxhn" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-dgxhn webserver-deployment-dd94f59b7- deployment-1917 /api/v1/namespaces/deployment-1917/pods/webserver-deployment-dd94f59b7-dgxhn 7c89769d-6b5c-494e-a357-1a2453cb9697 224949 0 2020-10-29 21:49:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 d5769155-9a98-4a27-bd91-9ccd2e399ef5 0xc0037c90e7 0xc0037c90e8}] []  [{kube-controller-manager Update v1 2020-10-29 21:49:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5769155-9a98-4a27-bd91-9ccd2e399ef5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-10-29 21:49:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lk8pr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lk8pr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lk8pr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-157-225.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c85,c80,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-ftpxv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.157.225,PodIP:,StartTime:2020-10-29 21:49:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 29 21:49:39.495: INFO: Pod "webserver-deployment-dd94f59b7-ggmpq" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-ggmpq webserver-deployment-dd94f59b7- deployment-1917 /api/v1/namespaces/deployment-1917/pods/webserver-deployment-dd94f59b7-ggmpq d7adba71-828d-4e95-9e9f-3fbcf01bb9df 224942 0 2020-10-29 21:49:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 d5769155-9a98-4a27-bd91-9ccd2e399ef5 0xc0037c9297 0xc0037c9298}] []  [{kube-controller-manager Update v1 2020-10-29 21:49:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5769155-9a98-4a27-bd91-9ccd2e399ef5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-10-29 21:49:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lk8pr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lk8pr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lk8pr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-142-212.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c85,c80,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-ftpxv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.142.212,PodIP:,StartTime:2020-10-29 21:49:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 29 21:49:39.495: INFO: Pod "webserver-deployment-dd94f59b7-gx5tj" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-gx5tj webserver-deployment-dd94f59b7- deployment-1917 /api/v1/namespaces/deployment-1917/pods/webserver-deployment-dd94f59b7-gx5tj f5e7f486-359c-4def-84c1-f280eee4cbde 224555 0 2020-10-29 21:49:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.131.1.138"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.131.1.138"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 d5769155-9a98-4a27-bd91-9ccd2e399ef5 0xc0037c9457 0xc0037c9458}] []  [{kube-controller-manager Update v1 2020-10-29 21:49:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5769155-9a98-4a27-bd91-9ccd2e399ef5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {multus Update v1 2020-10-29 21:49:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2020-10-29 21:49:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.1.138\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lk8pr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lk8pr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lk8pr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-142-212.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c85,c80,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-ftpxv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.142.212,PodIP:10.131.1.138,StartTime:2020-10-29 21:49:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-10-29 21:49:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://57ec6f3cf5039e044c0e6b86f331e19479aedd1b979e961c0ae2ad1c260bf883,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.1.138,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 29 21:49:39.495: INFO: Pod "webserver-deployment-dd94f59b7-h2tgv" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-h2tgv webserver-deployment-dd94f59b7- deployment-1917 /api/v1/namespaces/deployment-1917/pods/webserver-deployment-dd94f59b7-h2tgv 564b513d-e3c1-45f0-876e-6726a3e58380 225119 0 2020-10-29 21:49:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.130.2.76"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.130.2.76"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 d5769155-9a98-4a27-bd91-9ccd2e399ef5 0xc0037c9667 0xc0037c9668}] []  [{kube-controller-manager Update v1 2020-10-29 21:49:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5769155-9a98-4a27-bd91-9ccd2e399ef5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-10-29 21:49:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:hostIP":{},"f:startTime":{}}}} {multus Update v1 2020-10-29 21:49:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}},"f:status":{"f:containerStatuses":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lk8pr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lk8pr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lk8pr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-158-72.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c85,c80,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-ftpxv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.158.72,PodIP:,StartTime:2020-10-29 21:49:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 29 21:49:39.495: INFO: Pod "webserver-deployment-dd94f59b7-hzz9b" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-hzz9b webserver-deployment-dd94f59b7- deployment-1917 /api/v1/namespaces/deployment-1917/pods/webserver-deployment-dd94f59b7-hzz9b 54bba5e3-92fd-4b60-b3c5-345e80198ea0 224936 0 2020-10-29 21:49:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 d5769155-9a98-4a27-bd91-9ccd2e399ef5 0xc0037c9837 0xc0037c9838}] []  [{kube-controller-manager Update v1 2020-10-29 21:49:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5769155-9a98-4a27-bd91-9ccd2e399ef5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-10-29 21:49:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lk8pr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lk8pr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lk8pr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-157-225.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c85,c80,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-ftpxv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.157.225,PodIP:,StartTime:2020-10-29 21:49:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 29 21:49:39.495: INFO: Pod "webserver-deployment-dd94f59b7-l5c29" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-l5c29 webserver-deployment-dd94f59b7- deployment-1917 /api/v1/namespaces/deployment-1917/pods/webserver-deployment-dd94f59b7-l5c29 1eb8def3-1c51-4131-baf0-a2279cafa3e0 224565 0 2020-10-29 21:49:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.128.2.224"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.128.2.224"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 d5769155-9a98-4a27-bd91-9ccd2e399ef5 0xc0037c99d7 0xc0037c99d8}] []  [{kube-controller-manager Update v1 2020-10-29 21:49:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5769155-9a98-4a27-bd91-9ccd2e399ef5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {multus Update v1 2020-10-29 21:49:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2020-10-29 21:49:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.224\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lk8pr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lk8pr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lk8pr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-234-238.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c85,c80,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-ftpxv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.234.238,PodIP:10.128.2.224,StartTime:2020-10-29 21:49:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-10-29 21:49:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://fc35a56701671d40f08f1d1451cc85e37ffae7f23e7782821acc1e0f798016cf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.224,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 29 21:49:39.495: INFO: Pod "webserver-deployment-dd94f59b7-lklt4" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-lklt4 webserver-deployment-dd94f59b7- deployment-1917 /api/v1/namespaces/deployment-1917/pods/webserver-deployment-dd94f59b7-lklt4 d00d532d-a47e-44ea-a830-a0b5352c4ee6 224540 0 2020-10-29 21:49:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.128.2.226"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.128.2.226"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 d5769155-9a98-4a27-bd91-9ccd2e399ef5 0xc0037c9bb7 0xc0037c9bb8}] []  [{kube-controller-manager Update v1 2020-10-29 21:49:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5769155-9a98-4a27-bd91-9ccd2e399ef5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {multus Update v1 2020-10-29 21:49:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2020-10-29 21:49:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.226\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lk8pr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lk8pr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lk8pr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-234-238.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c85,c80,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-ftpxv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.234.238,PodIP:10.128.2.226,StartTime:2020-10-29 21:49:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-10-29 21:49:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://8c43b6680e3e194f9d76b8309b155534fdb7362f6fb3ef8b933e776f26387c53,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.226,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 29 21:49:39.496: INFO: Pod "webserver-deployment-dd94f59b7-nf7ls" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-nf7ls webserver-deployment-dd94f59b7- deployment-1917 /api/v1/namespaces/deployment-1917/pods/webserver-deployment-dd94f59b7-nf7ls 43614304-aa23-4a21-bb0e-f8c59f344c77 224550 0 2020-10-29 21:49:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.131.1.139"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.131.1.139"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 d5769155-9a98-4a27-bd91-9ccd2e399ef5 0xc0037c9d97 0xc0037c9d98}] []  [{kube-controller-manager Update v1 2020-10-29 21:49:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5769155-9a98-4a27-bd91-9ccd2e399ef5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {multus Update v1 2020-10-29 21:49:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2020-10-29 21:49:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.1.139\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lk8pr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lk8pr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lk8pr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-142-212.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c85,c80,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-ftpxv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.142.212,PodIP:10.131.1.139,StartTime:2020-10-29 21:49:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-10-29 21:49:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://e5cf22486690198050b20c3a076be35fcaff166765f8127d40647f2b333f80cc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.1.139,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 29 21:49:39.496: INFO: Pod "webserver-deployment-dd94f59b7-q7cnp" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-q7cnp webserver-deployment-dd94f59b7- deployment-1917 /api/v1/namespaces/deployment-1917/pods/webserver-deployment-dd94f59b7-q7cnp 0663b126-8d06-41ed-abd7-a2aebb4174b4 224890 0 2020-10-29 21:49:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 d5769155-9a98-4a27-bd91-9ccd2e399ef5 0xc0037c9f77 0xc0037c9f78}] []  [{kube-controller-manager Update v1 2020-10-29 21:49:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5769155-9a98-4a27-bd91-9ccd2e399ef5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-10-29 21:49:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lk8pr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lk8pr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lk8pr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-158-72.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c85,c80,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-ftpxv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.158.72,PodIP:,StartTime:2020-10-29 21:49:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 29 21:49:39.496: INFO: Pod "webserver-deployment-dd94f59b7-vbhgv" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-vbhgv webserver-deployment-dd94f59b7- deployment-1917 /api/v1/namespaces/deployment-1917/pods/webserver-deployment-dd94f59b7-vbhgv fe331efb-a2dc-452b-b5b5-ea13cb4514d0 224532 0 2020-10-29 21:49:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.128.2.225"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.128.2.225"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 d5769155-9a98-4a27-bd91-9ccd2e399ef5 0xc00286a2f7 0xc00286a2f8}] []  [{kube-controller-manager Update v1 2020-10-29 21:49:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5769155-9a98-4a27-bd91-9ccd2e399ef5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {multus Update v1 2020-10-29 21:49:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2020-10-29 21:49:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.225\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lk8pr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lk8pr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lk8pr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-234-238.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c85,c80,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-ftpxv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.234.238,PodIP:10.128.2.225,StartTime:2020-10-29 21:49:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-10-29 21:49:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://fb2d1e4519b639afa6cac51e627828cef9b4db471d6173d464a3da8b70ee43af,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.225,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 29 21:49:39.496: INFO: Pod "webserver-deployment-dd94f59b7-xh5gv" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-xh5gv webserver-deployment-dd94f59b7- deployment-1917 /api/v1/namespaces/deployment-1917/pods/webserver-deployment-dd94f59b7-xh5gv b0d5cf61-8acb-4996-806b-1a609b380586 224944 0 2020-10-29 21:49:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 d5769155-9a98-4a27-bd91-9ccd2e399ef5 0xc00286a7b7 0xc00286a7b8}] []  [{kube-controller-manager Update v1 2020-10-29 21:49:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5769155-9a98-4a27-bd91-9ccd2e399ef5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-10-29 21:49:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lk8pr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lk8pr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lk8pr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-158-72.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c85,c80,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-ftpxv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:49:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.158.72,PodIP:,StartTime:2020-10-29 21:49:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:49:39.496: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "deployment-1917" for this suite.


 [SLOW TEST:10.994 seconds]
[sig-apps] Deployment
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":-1,"completed":15,"skipped":231,"failed":0}

SSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:49:35.533: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Oct 29 21:49:35.746: INFO: Waiting up to 5m0s for pod "pod-aade59ec-6c53-4335-90b4-6332f03973f3" in namespace "emptydir-5235" to be "Succeeded or Failed"
Oct 29 21:49:35.767: INFO: Pod "pod-aade59ec-6c53-4335-90b4-6332f03973f3": Phase="Pending", Reason="", readiness=false. Elapsed: 20.917147ms
Oct 29 21:49:37.788: INFO: Pod "pod-aade59ec-6c53-4335-90b4-6332f03973f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042234197s
Oct 29 21:49:39.809: INFO: Pod "pod-aade59ec-6c53-4335-90b4-6332f03973f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063363339s
STEP: Saw pod success
Oct 29 21:49:39.809: INFO: Pod "pod-aade59ec-6c53-4335-90b4-6332f03973f3" satisfied condition "Succeeded or Failed"
Oct 29 21:49:39.839: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-aade59ec-6c53-4335-90b4-6332f03973f3 container test-container: <nil>
STEP: delete the pod
Oct 29 21:49:39.892: INFO: Waiting for pod pod-aade59ec-6c53-4335-90b4-6332f03973f3 to disappear
Oct 29 21:49:39.913: INFO: Pod pod-aade59ec-6c53-4335-90b4-6332f03973f3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:49:39.913: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-5235" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":11,"skipped":142,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Security Context
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:49:38.500: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 21:49:38.752: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-34a71809-fceb-4f60-8af6-c20f0daaaf24" in namespace "security-context-test-1568" to be "Succeeded or Failed"
Oct 29 21:49:38.774: INFO: Pod "busybox-readonly-false-34a71809-fceb-4f60-8af6-c20f0daaaf24": Phase="Pending", Reason="", readiness=false. Elapsed: 22.157212ms
Oct 29 21:49:40.796: INFO: Pod "busybox-readonly-false-34a71809-fceb-4f60-8af6-c20f0daaaf24": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044118774s
Oct 29 21:49:42.818: INFO: Pod "busybox-readonly-false-34a71809-fceb-4f60-8af6-c20f0daaaf24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065399778s
Oct 29 21:49:42.818: INFO: Pod "busybox-readonly-false-34a71809-fceb-4f60-8af6-c20f0daaaf24" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:49:42.818: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "security-context-test-1568" for this suite.


------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":-1,"completed":4,"skipped":76,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-node] PodTemplates
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:49:42.955: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-node] PodTemplates
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:49:43.273: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "podtemplate-9330" for this suite.


------------------------------
{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":-1,"completed":5,"skipped":117,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:49:39.575: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-bfc94eed-6519-4d91-ac21-75f72463aa9d
STEP: Creating a pod to test consume configMaps
Oct 29 21:49:39.771: INFO: Waiting up to 5m0s for pod "pod-configmaps-0f97de01-2076-4f70-98e0-f5ef339b42f4" in namespace "configmap-1664" to be "Succeeded or Failed"
Oct 29 21:49:39.791: INFO: Pod "pod-configmaps-0f97de01-2076-4f70-98e0-f5ef339b42f4": Phase="Pending", Reason="", readiness=false. Elapsed: 20.795919ms
Oct 29 21:49:41.814: INFO: Pod "pod-configmaps-0f97de01-2076-4f70-98e0-f5ef339b42f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042990262s
Oct 29 21:49:43.835: INFO: Pod "pod-configmaps-0f97de01-2076-4f70-98e0-f5ef339b42f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.064873754s
STEP: Saw pod success
Oct 29 21:49:43.835: INFO: Pod "pod-configmaps-0f97de01-2076-4f70-98e0-f5ef339b42f4" satisfied condition "Succeeded or Failed"
Oct 29 21:49:43.857: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-configmaps-0f97de01-2076-4f70-98e0-f5ef339b42f4 container configmap-volume-test: <nil>
STEP: delete the pod
Oct 29 21:49:43.911: INFO: Waiting for pod pod-configmaps-0f97de01-2076-4f70-98e0-f5ef339b42f4 to disappear
Oct 29 21:49:43.933: INFO: Pod pod-configmaps-0f97de01-2076-4f70-98e0-f5ef339b42f4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:49:43.933: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-1664" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":16,"skipped":246,"failed":0}

SS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:49:40.038: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Oct 29 21:49:40.377: INFO: Waiting up to 5m0s for pod "downwardapi-volume-222cae9f-c232-4601-9662-ed2541eae02b" in namespace "projected-1659" to be "Succeeded or Failed"
Oct 29 21:49:40.399: INFO: Pod "downwardapi-volume-222cae9f-c232-4601-9662-ed2541eae02b": Phase="Pending", Reason="", readiness=false. Elapsed: 22.052369ms
Oct 29 21:49:42.420: INFO: Pod "downwardapi-volume-222cae9f-c232-4601-9662-ed2541eae02b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043699523s
Oct 29 21:49:44.441: INFO: Pod "downwardapi-volume-222cae9f-c232-4601-9662-ed2541eae02b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.064829132s
STEP: Saw pod success
Oct 29 21:49:44.441: INFO: Pod "downwardapi-volume-222cae9f-c232-4601-9662-ed2541eae02b" satisfied condition "Succeeded or Failed"
Oct 29 21:49:44.464: INFO: Trying to get logs from node ip-10-0-234-238.us-east-2.compute.internal pod downwardapi-volume-222cae9f-c232-4601-9662-ed2541eae02b container client-container: <nil>
STEP: delete the pod
Oct 29 21:49:44.521: INFO: Waiting for pod downwardapi-volume-222cae9f-c232-4601-9662-ed2541eae02b to disappear
Oct 29 21:49:44.543: INFO: Pod downwardapi-volume-222cae9f-c232-4601-9662-ed2541eae02b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:49:44.543: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-1659" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":-1,"completed":12,"skipped":173,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Garbage collector
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:49:11.587: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W1029 21:49:51.924405   53213 metrics_grabber.go:83] Can't find any pods in namespace kube-system to grab metrics from
W1029 21:49:51.924419   53213 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1029 21:49:51.924425   53213 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1029 21:49:51.924431   53213 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Oct 29 21:49:51.924: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Oct 29 21:49:51.924: INFO: Deleting pod "simpletest.rc-2fl68" in namespace "gc-1905"
Oct 29 21:49:51.953: INFO: Deleting pod "simpletest.rc-52j9p" in namespace "gc-1905"
Oct 29 21:49:51.982: INFO: Deleting pod "simpletest.rc-5rq8g" in namespace "gc-1905"
Oct 29 21:49:52.011: INFO: Deleting pod "simpletest.rc-76x5r" in namespace "gc-1905"
Oct 29 21:49:52.045: INFO: Deleting pod "simpletest.rc-c2gj4" in namespace "gc-1905"
Oct 29 21:49:52.104: INFO: Deleting pod "simpletest.rc-drttx" in namespace "gc-1905"
Oct 29 21:49:52.133: INFO: Deleting pod "simpletest.rc-mj8pt" in namespace "gc-1905"
Oct 29 21:49:52.160: INFO: Deleting pod "simpletest.rc-mnlvr" in namespace "gc-1905"
Oct 29 21:49:52.190: INFO: Deleting pod "simpletest.rc-zjxss" in namespace "gc-1905"
Oct 29 21:49:52.219: INFO: Deleting pod "simpletest.rc-zmgx7" in namespace "gc-1905"
[AfterEach] [sig-api-machinery] Garbage collector
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:49:52.247: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "gc-1905" for this suite.


 [SLOW TEST:40.729 seconds]
[sig-api-machinery] Garbage collector
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":-1,"completed":10,"skipped":176,"failed":0}

SSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Garbage collector
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:49:44.680: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1029 21:49:55.220812   53211 metrics_grabber.go:83] Can't find any pods in namespace kube-system to grab metrics from
W1029 21:49:55.220827   53211 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1029 21:49:55.220834   53211 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1029 21:49:55.220840   53211 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Oct 29 21:49:55.220: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Oct 29 21:49:55.220: INFO: Deleting pod "simpletest-rc-to-be-deleted-blbqp" in namespace "gc-3438"
Oct 29 21:49:55.249: INFO: Deleting pod "simpletest-rc-to-be-deleted-crzlh" in namespace "gc-3438"
Oct 29 21:49:55.278: INFO: Deleting pod "simpletest-rc-to-be-deleted-fjvsg" in namespace "gc-3438"
Oct 29 21:49:55.312: INFO: Deleting pod "simpletest-rc-to-be-deleted-kbplx" in namespace "gc-3438"
Oct 29 21:49:55.340: INFO: Deleting pod "simpletest-rc-to-be-deleted-njgjp" in namespace "gc-3438"
[AfterEach] [sig-api-machinery] Garbage collector
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:49:55.373: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "gc-3438" for this suite.


 [SLOW TEST:10.777 seconds]
[sig-api-machinery] Garbage collector
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":-1,"completed":13,"skipped":211,"failed":0}

SS
------------------------------
[BeforeEach] [k8s.io] Variable Expansion
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:49:52.322: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in volume subpath
Oct 29 21:49:52.554: INFO: Waiting up to 5m0s for pod "var-expansion-44076e39-c6b0-43dd-9c86-a5056ba63105" in namespace "var-expansion-6726" to be "Succeeded or Failed"
Oct 29 21:49:52.575: INFO: Pod "var-expansion-44076e39-c6b0-43dd-9c86-a5056ba63105": Phase="Pending", Reason="", readiness=false. Elapsed: 21.101444ms
Oct 29 21:49:54.596: INFO: Pod "var-expansion-44076e39-c6b0-43dd-9c86-a5056ba63105": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042617258s
Oct 29 21:49:56.618: INFO: Pod "var-expansion-44076e39-c6b0-43dd-9c86-a5056ba63105": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.064622911s
STEP: Saw pod success
Oct 29 21:49:56.618: INFO: Pod "var-expansion-44076e39-c6b0-43dd-9c86-a5056ba63105" satisfied condition "Succeeded or Failed"
Oct 29 21:49:56.639: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod var-expansion-44076e39-c6b0-43dd-9c86-a5056ba63105 container dapi-container: <nil>
STEP: delete the pod
Oct 29 21:49:56.695: INFO: Waiting for pod var-expansion-44076e39-c6b0-43dd-9c86-a5056ba63105 to disappear
Oct 29 21:49:56.716: INFO: Pod var-expansion-44076e39-c6b0-43dd-9c86-a5056ba63105 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:49:56.716: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "var-expansion-6726" for this suite.


------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":-1,"completed":11,"skipped":181,"failed":0}
[BeforeEach] [k8s.io] Docker Containers
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:49:56.834: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override all
Oct 29 21:49:57.020: INFO: Waiting up to 5m0s for pod "client-containers-52dbb8d7-9212-4ec1-8f26-f74f50048935" in namespace "containers-4900" to be "Succeeded or Failed"
Oct 29 21:49:57.046: INFO: Pod "client-containers-52dbb8d7-9212-4ec1-8f26-f74f50048935": Phase="Pending", Reason="", readiness=false. Elapsed: 26.188615ms
Oct 29 21:49:59.069: INFO: Pod "client-containers-52dbb8d7-9212-4ec1-8f26-f74f50048935": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04846575s
Oct 29 21:50:01.091: INFO: Pod "client-containers-52dbb8d7-9212-4ec1-8f26-f74f50048935": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.070818925s
STEP: Saw pod success
Oct 29 21:50:01.091: INFO: Pod "client-containers-52dbb8d7-9212-4ec1-8f26-f74f50048935" satisfied condition "Succeeded or Failed"
Oct 29 21:50:01.113: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod client-containers-52dbb8d7-9212-4ec1-8f26-f74f50048935 container test-container: <nil>
STEP: delete the pod
Oct 29 21:50:01.175: INFO: Waiting for pod client-containers-52dbb8d7-9212-4ec1-8f26-f74f50048935 to disappear
Oct 29 21:50:01.197: INFO: Pod client-containers-52dbb8d7-9212-4ec1-8f26-f74f50048935 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:50:01.197: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "containers-4900" for this suite.


------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":-1,"completed":12,"skipped":181,"failed":0}

SSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] DNS
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:50:01.314: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8694.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8694.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8694.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8694.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8694.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8694.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct 29 21:50:05.797: INFO: DNS probes using dns-8694/dns-test-0006f364-1ca4-4937-9162-dd2277d9f6f5 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:50:05.828: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "dns-8694" for this suite.


------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":-1,"completed":13,"skipped":192,"failed":0}

SS
------------------------------
[BeforeEach] [sig-network] Networking
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:49:43.359: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-4953
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Oct 29 21:49:43.485: INFO: Waiting up to 10m0s for all (but 3) nodes to be schedulable
Oct 29 21:49:43.751: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Oct 29 21:49:45.773: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Oct 29 21:49:47.772: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 29 21:49:49.773: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 29 21:49:51.773: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 29 21:49:53.773: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 29 21:49:55.775: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 29 21:49:57.773: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 29 21:49:59.773: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 29 21:50:01.775: INFO: The status of Pod netserver-0 is Running (Ready = true)
Oct 29 21:50:01.826: INFO: The status of Pod netserver-1 is Running (Ready = true)
Oct 29 21:50:01.874: INFO: The status of Pod netserver-2 is Running (Ready = false)
Oct 29 21:50:03.896: INFO: The status of Pod netserver-2 is Running (Ready = false)
Oct 29 21:50:05.896: INFO: The status of Pod netserver-2 is Running (Ready = true)
Oct 29 21:50:05.938: INFO: The status of Pod netserver-3 is Running (Ready = true)
STEP: Creating test pods
Oct 29 21:50:10.173: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.131.1.155:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4953 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 29 21:50:10.173: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
Oct 29 21:50:10.483: INFO: Found all expected endpoints: [netserver-0]
Oct 29 21:50:10.504: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.129.2.83:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4953 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 29 21:50:10.504: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
Oct 29 21:50:10.713: INFO: Found all expected endpoints: [netserver-1]
Oct 29 21:50:10.742: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.130.2.82:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4953 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 29 21:50:10.742: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
Oct 29 21:50:10.964: INFO: Found all expected endpoints: [netserver-2]
Oct 29 21:50:10.986: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.128.2.232:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4953 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 29 21:50:10.986: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
Oct 29 21:50:11.182: INFO: Found all expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:50:11.182: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pod-network-test-4953" for this suite.


 [SLOW TEST:27.931 seconds]
[sig-network] Networking
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":6,"skipped":166,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:49:55.461: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 29 21:49:56.832: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739604977, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739604977, loc:(*time.Location)(0x77098a0)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-cbccbf6bb\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739604977, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739604977, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Oct 29 21:49:58.854: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739604977, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739604977, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739604977, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739604977, loc:(*time.Location)(0x77098a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 29 21:50:01.882: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
Oct 29 21:50:01.999: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:02.150: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:02.253: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:02.353: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:02.450: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:02.553: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:02.649: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:02.749: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:02.849: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:02.950: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:03.049: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:03.150: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:03.249: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:03.350: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:03.450: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:03.551: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:03.649: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:03.751: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:03.849: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:03.954: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:04.058: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:04.149: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:04.249: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:04.352: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:04.449: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:04.550: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:04.650: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:04.751: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:04.864: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:04.955: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:05.063: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:05.151: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:05.251: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:05.364: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:05.452: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:05.550: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:05.650: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:05.750: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:05.851: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:05.952: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:06.068: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:06.153: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:06.252: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:06.359: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:06.464: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:06.578: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:06.654: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:06.757: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:06.873: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:06.968: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:07.049: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:07.150: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:07.249: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:07.363: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:07.450: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:07.549: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:07.650: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:07.750: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:07.849: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:07.957: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:08.058: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:08.153: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:08.255: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:08.357: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:08.452: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:08.552: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:08.653: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:08.750: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:08.853: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:08.992: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:09.055: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:09.159: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:09.256: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:09.352: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:09.524: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:09.652: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:09.760: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:09.850: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:09.950: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:10.052: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:10.160: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:10.252: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:10.353: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:10.451: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:10.554: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:10.718: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:10.849: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:10.953: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:11.052: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:11.151: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:11.257: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:11.378: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:11.495: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:11.571: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:11.657: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:11.766: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:11.864: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:11.956: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:12.060: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:12.156: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:12.256: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:12.354: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:12.456: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:12.551: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:12.650: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:12.750: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:12.849: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:12.953: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:13.049: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:13.150: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:13.258: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:13.351: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:13.449: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:13.553: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:13.651: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:13.750: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:13.850: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:13.951: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:14.052: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:14.153: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:14.250: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:14.349: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:14.449: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:14.550: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:14.649: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:14.751: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:14.851: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:14.955: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:15.052: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:15.156: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:15.258: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:15.359: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:15.451: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:15.550: INFO: Waiting for webhook configuration to be ready...
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:50:15.703: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-4000" for this suite.
STEP: Destroying namespace "webhook-4000-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102


 [SLOW TEST:20.485 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
[BeforeEach] [sig-storage] Subpath
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:50:05.917: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-secret-6dpt
STEP: Creating a pod to test atomic-volume-subpath
Oct 29 21:50:06.158: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-6dpt" in namespace "subpath-7111" to be "Succeeded or Failed"
Oct 29 21:50:06.180: INFO: Pod "pod-subpath-test-secret-6dpt": Phase="Pending", Reason="", readiness=false. Elapsed: 21.758644ms
Oct 29 21:50:08.202: INFO: Pod "pod-subpath-test-secret-6dpt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043873388s
Oct 29 21:50:10.224: INFO: Pod "pod-subpath-test-secret-6dpt": Phase="Running", Reason="", readiness=true. Elapsed: 4.066017817s
Oct 29 21:50:12.246: INFO: Pod "pod-subpath-test-secret-6dpt": Phase="Running", Reason="", readiness=true. Elapsed: 6.087701481s
Oct 29 21:50:14.268: INFO: Pod "pod-subpath-test-secret-6dpt": Phase="Running", Reason="", readiness=true. Elapsed: 8.110056818s
Oct 29 21:50:16.290: INFO: Pod "pod-subpath-test-secret-6dpt": Phase="Running", Reason="", readiness=true. Elapsed: 10.131801149s
Oct 29 21:50:18.312: INFO: Pod "pod-subpath-test-secret-6dpt": Phase="Running", Reason="", readiness=true. Elapsed: 12.15418615s
Oct 29 21:50:20.335: INFO: Pod "pod-subpath-test-secret-6dpt": Phase="Running", Reason="", readiness=true. Elapsed: 14.176678909s
Oct 29 21:50:22.357: INFO: Pod "pod-subpath-test-secret-6dpt": Phase="Running", Reason="", readiness=true. Elapsed: 16.19850399s
Oct 29 21:50:24.379: INFO: Pod "pod-subpath-test-secret-6dpt": Phase="Running", Reason="", readiness=true. Elapsed: 18.220926081s
Oct 29 21:50:26.401: INFO: Pod "pod-subpath-test-secret-6dpt": Phase="Running", Reason="", readiness=true. Elapsed: 20.242931033s
Oct 29 21:50:28.423: INFO: Pod "pod-subpath-test-secret-6dpt": Phase="Running", Reason="", readiness=true. Elapsed: 22.264596431s
Oct 29 21:50:30.446: INFO: Pod "pod-subpath-test-secret-6dpt": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.287338498s
STEP: Saw pod success
Oct 29 21:50:30.446: INFO: Pod "pod-subpath-test-secret-6dpt" satisfied condition "Succeeded or Failed"
Oct 29 21:50:30.467: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-subpath-test-secret-6dpt container test-container-subpath-secret-6dpt: <nil>
STEP: delete the pod
Oct 29 21:50:30.620: INFO: Waiting for pod pod-subpath-test-secret-6dpt to disappear
Oct 29 21:50:30.643: INFO: Pod pod-subpath-test-secret-6dpt no longer exists
STEP: Deleting pod pod-subpath-test-secret-6dpt
Oct 29 21:50:30.643: INFO: Deleting pod "pod-subpath-test-secret-6dpt" in namespace "subpath-7111"
[AfterEach] [sig-storage] Subpath
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:50:30.667: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "subpath-7111" for this suite.


 [SLOW TEST:24.857 seconds]
[sig-storage] Subpath
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":-1,"completed":14,"skipped":194,"failed":0}

S
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:50:11.312: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
Oct 29 21:50:12.041: INFO: role binding webhook-auth-reader already exists
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 29 21:50:12.157: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739604992, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739604992, loc:(*time.Location)(0x77098a0)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-cbccbf6bb\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739604992, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739604992, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Oct 29 21:50:14.178: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739604992, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739604992, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739604992, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739604992, loc:(*time.Location)(0x77098a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 29 21:50:17.214: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
Oct 29 21:50:17.327: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:17.483: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:17.594: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:17.679: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:17.779: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:17.879: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:17.977: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:18.079: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:18.178: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:18.280: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:18.380: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:18.480: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:18.590: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:18.683: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:18.779: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:18.877: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:18.978: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:19.078: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:19.177: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:19.277: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:19.377: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:19.477: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:19.578: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:19.679: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:19.778: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:19.887: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:19.979: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:20.077: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:20.179: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:20.277: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:20.377: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:20.478: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:20.578: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:20.680: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:20.779: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:20.963: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:21.087: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:21.187: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:21.280: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:21.386: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:21.489: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:21.586: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:21.700: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:21.783: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:21.880: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:21.982: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:22.080: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:22.187: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:22.279: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:22.378: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:22.478: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:22.579: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:22.678: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:22.777: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:22.879: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:22.978: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:23.079: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:23.178: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:23.278: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:23.379: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:23.479: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:23.578: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:23.679: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:23.778: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:23.878: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:23.979: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:24.081: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:24.182: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:24.278: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:24.378: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:24.478: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:24.578: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:24.680: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:24.778: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:24.878: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:24.978: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:25.080: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:25.178: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:25.278: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:25.379: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:25.479: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:25.579: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:25.678: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:25.778: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:25.878: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:25.978: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:26.080: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:26.179: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:26.279: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:26.385: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:26.479: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:26.581: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:26.680: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:26.781: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:26.878: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:26.983: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:27.079: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:27.190: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:27.280: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:27.378: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:27.478: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:27.579: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:27.678: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:27.778: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:27.882: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:27.977: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:28.078: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:28.193: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:28.279: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:28.383: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:28.483: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:28.580: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:28.678: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:28.778: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:28.878: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:28.978: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:29.077: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:29.178: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:29.278: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:29.379: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:29.479: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:29.580: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:29.679: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:29.778: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:29.878: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:29.978: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:30.079: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:30.179: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:30.278: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:30.377: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:30.477: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:50:30.624: INFO: Waiting for webhook configuration to be ready...
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:50:30.897: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-5350" for this suite.
STEP: Destroying namespace "webhook-5350-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102


 [SLOW TEST:19.878 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":-1,"completed":7,"skipped":194,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Garbage collector
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:50:31.222: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1029 21:50:37.664651   53212 metrics_grabber.go:83] Can't find any pods in namespace kube-system to grab metrics from
W1029 21:50:37.664666   53212 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1029 21:50:37.664672   53212 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1029 21:50:37.664678   53212 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Oct 29 21:50:37.664: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:50:37.664: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "gc-9111" for this suite.


 [SLOW TEST:6.546 seconds]
[sig-api-machinery] Garbage collector
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":-1,"completed":8,"skipped":230,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":-1,"completed":14,"skipped":213,"failed":0}
[BeforeEach] [sig-network] Networking
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:50:15.948: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-1708
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Oct 29 21:50:16.090: INFO: Waiting up to 10m0s for all (but 3) nodes to be schedulable
Oct 29 21:50:16.386: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Oct 29 21:50:18.407: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Oct 29 21:50:20.407: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 29 21:50:22.407: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 29 21:50:24.407: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 29 21:50:26.407: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 29 21:50:28.407: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 29 21:50:30.407: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 29 21:50:32.409: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 29 21:50:34.407: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 29 21:50:36.408: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 29 21:50:38.408: INFO: The status of Pod netserver-0 is Running (Ready = true)
Oct 29 21:50:38.459: INFO: The status of Pod netserver-1 is Running (Ready = true)
Oct 29 21:50:38.506: INFO: The status of Pod netserver-2 is Running (Ready = true)
Oct 29 21:50:38.554: INFO: The status of Pod netserver-3 is Running (Ready = true)
STEP: Creating test pods
Oct 29 21:50:42.713: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.1.177:8080/dial?request=hostname&protocol=http&host=10.131.1.168&port=8080&tries=1'] Namespace:pod-network-test-1708 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 29 21:50:42.713: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
Oct 29 21:50:42.932: INFO: Waiting for responses: map[]
Oct 29 21:50:42.955: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.1.177:8080/dial?request=hostname&protocol=http&host=10.129.2.86&port=8080&tries=1'] Namespace:pod-network-test-1708 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 29 21:50:42.955: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
Oct 29 21:50:43.168: INFO: Waiting for responses: map[]
Oct 29 21:50:43.189: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.1.177:8080/dial?request=hostname&protocol=http&host=10.130.2.85&port=8080&tries=1'] Namespace:pod-network-test-1708 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 29 21:50:43.189: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
Oct 29 21:50:43.441: INFO: Waiting for responses: map[]
Oct 29 21:50:43.462: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.1.177:8080/dial?request=hostname&protocol=http&host=10.128.2.236&port=8080&tries=1'] Namespace:pod-network-test-1708 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 29 21:50:43.462: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
Oct 29 21:50:43.676: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:50:43.676: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pod-network-test-1708" for this suite.


 [SLOW TEST:27.839 seconds]
[sig-network] Networking
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":-1,"completed":15,"skipped":213,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Watchers
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:49:44.038: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Oct 29 21:49:44.283: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9145 /api/v1/namespaces/watch-9145/configmaps/e2e-watch-test-configmap-a 98014dab-a7a0-4440-9d4d-3a081675c429 225568 0 2020-10-29 21:49:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-10-29 21:49:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 29 21:49:44.283: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9145 /api/v1/namespaces/watch-9145/configmaps/e2e-watch-test-configmap-a 98014dab-a7a0-4440-9d4d-3a081675c429 225568 0 2020-10-29 21:49:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-10-29 21:49:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Oct 29 21:49:54.328: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9145 /api/v1/namespaces/watch-9145/configmaps/e2e-watch-test-configmap-a 98014dab-a7a0-4440-9d4d-3a081675c429 226348 0 2020-10-29 21:49:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-10-29 21:49:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 29 21:49:54.329: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9145 /api/v1/namespaces/watch-9145/configmaps/e2e-watch-test-configmap-a 98014dab-a7a0-4440-9d4d-3a081675c429 226348 0 2020-10-29 21:49:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-10-29 21:49:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Oct 29 21:50:04.381: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9145 /api/v1/namespaces/watch-9145/configmaps/e2e-watch-test-configmap-a 98014dab-a7a0-4440-9d4d-3a081675c429 226920 0 2020-10-29 21:49:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-10-29 21:49:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 29 21:50:04.381: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9145 /api/v1/namespaces/watch-9145/configmaps/e2e-watch-test-configmap-a 98014dab-a7a0-4440-9d4d-3a081675c429 226920 0 2020-10-29 21:49:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-10-29 21:49:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Oct 29 21:50:14.406: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9145 /api/v1/namespaces/watch-9145/configmaps/e2e-watch-test-configmap-a 98014dab-a7a0-4440-9d4d-3a081675c429 227423 0 2020-10-29 21:49:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-10-29 21:49:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 29 21:50:14.406: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9145 /api/v1/namespaces/watch-9145/configmaps/e2e-watch-test-configmap-a 98014dab-a7a0-4440-9d4d-3a081675c429 227423 0 2020-10-29 21:49:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-10-29 21:49:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Oct 29 21:50:24.431: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9145 /api/v1/namespaces/watch-9145/configmaps/e2e-watch-test-configmap-b a88b7ab9-a58a-4637-ba93-7174b9c640a1 227894 0 2020-10-29 21:50:04 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-10-29 21:50:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 29 21:50:24.431: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9145 /api/v1/namespaces/watch-9145/configmaps/e2e-watch-test-configmap-b a88b7ab9-a58a-4637-ba93-7174b9c640a1 227894 0 2020-10-29 21:50:04 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-10-29 21:50:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Oct 29 21:50:34.458: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9145 /api/v1/namespaces/watch-9145/configmaps/e2e-watch-test-configmap-b a88b7ab9-a58a-4637-ba93-7174b9c640a1 228266 0 2020-10-29 21:50:04 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-10-29 21:50:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 29 21:50:34.458: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9145 /api/v1/namespaces/watch-9145/configmaps/e2e-watch-test-configmap-b a88b7ab9-a58a-4637-ba93-7174b9c640a1 228266 0 2020-10-29 21:50:04 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-10-29 21:50:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:50:44.458: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "watch-9145" for this suite.


 [SLOW TEST:60.538 seconds]
[sig-api-machinery] Watchers
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":-1,"completed":17,"skipped":248,"failed":0}

SSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Secrets
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:50:44.588: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-a78e4e83-4f8c-48ee-bb63-ec31adb75719
STEP: Creating a pod to test consume secrets
Oct 29 21:50:44.774: INFO: Waiting up to 5m0s for pod "pod-secrets-b8d82434-a838-493e-a827-cade004c4772" in namespace "secrets-9772" to be "Succeeded or Failed"
Oct 29 21:50:44.796: INFO: Pod "pod-secrets-b8d82434-a838-493e-a827-cade004c4772": Phase="Pending", Reason="", readiness=false. Elapsed: 21.723465ms
Oct 29 21:50:46.822: INFO: Pod "pod-secrets-b8d82434-a838-493e-a827-cade004c4772": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048013719s
Oct 29 21:50:48.843: INFO: Pod "pod-secrets-b8d82434-a838-493e-a827-cade004c4772": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.068908853s
STEP: Saw pod success
Oct 29 21:50:48.843: INFO: Pod "pod-secrets-b8d82434-a838-493e-a827-cade004c4772" satisfied condition "Succeeded or Failed"
Oct 29 21:50:48.863: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-secrets-b8d82434-a838-493e-a827-cade004c4772 container secret-volume-test: <nil>
STEP: delete the pod
Oct 29 21:50:48.914: INFO: Waiting for pod pod-secrets-b8d82434-a838-493e-a827-cade004c4772 to disappear
Oct 29 21:50:48.935: INFO: Pod pod-secrets-b8d82434-a838-493e-a827-cade004c4772 no longer exists
[AfterEach] [sig-storage] Secrets
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:50:48.935: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-9772" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":-1,"completed":18,"skipped":263,"failed":0}

SSSSS
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:50:43.880: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:51:00.435: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "resourcequota-2412" for this suite.


 [SLOW TEST:16.658 seconds]
[sig-api-machinery] ResourceQuota
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":-1,"completed":16,"skipped":340,"failed":0}

SSSSSS
------------------------------
[BeforeEach] [k8s.io] Pods
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:51:00.543: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 21:51:04.883: INFO: Waiting up to 5m0s for pod "client-envvars-02fcb26c-fde2-4eaa-b8c7-e72b7d0fa9ec" in namespace "pods-6081" to be "Succeeded or Failed"
Oct 29 21:51:04.904: INFO: Pod "client-envvars-02fcb26c-fde2-4eaa-b8c7-e72b7d0fa9ec": Phase="Pending", Reason="", readiness=false. Elapsed: 20.641847ms
Oct 29 21:51:06.928: INFO: Pod "client-envvars-02fcb26c-fde2-4eaa-b8c7-e72b7d0fa9ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044662652s
Oct 29 21:51:08.949: INFO: Pod "client-envvars-02fcb26c-fde2-4eaa-b8c7-e72b7d0fa9ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.06625992s
STEP: Saw pod success
Oct 29 21:51:08.950: INFO: Pod "client-envvars-02fcb26c-fde2-4eaa-b8c7-e72b7d0fa9ec" satisfied condition "Succeeded or Failed"
Oct 29 21:51:08.971: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod client-envvars-02fcb26c-fde2-4eaa-b8c7-e72b7d0fa9ec container env3cont: <nil>
STEP: delete the pod
Oct 29 21:51:09.021: INFO: Waiting for pod client-envvars-02fcb26c-fde2-4eaa-b8c7-e72b7d0fa9ec to disappear
Oct 29 21:51:09.041: INFO: Pod client-envvars-02fcb26c-fde2-4eaa-b8c7-e72b7d0fa9ec no longer exists
[AfterEach] [k8s.io] Pods
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:51:09.041: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-6081" for this suite.


 [SLOW TEST:8.609 seconds]
[k8s.io] Pods
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should contain environment variables for services [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":-1,"completed":17,"skipped":346,"failed":0}

S
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:50:30.779: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Oct 29 21:50:30.987: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
Oct 29 21:50:41.854: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:51:10.056: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7799" for this suite.


 [SLOW TEST:39.378 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":-1,"completed":15,"skipped":195,"failed":0}

SSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:51:09.154: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Oct 29 21:51:09.350: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9af06766-d763-49ad-9f3e-59c00c5d2f43" in namespace "downward-api-8769" to be "Succeeded or Failed"
Oct 29 21:51:09.377: INFO: Pod "downwardapi-volume-9af06766-d763-49ad-9f3e-59c00c5d2f43": Phase="Pending", Reason="", readiness=false. Elapsed: 26.791903ms
Oct 29 21:51:11.399: INFO: Pod "downwardapi-volume-9af06766-d763-49ad-9f3e-59c00c5d2f43": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048423376s
Oct 29 21:51:13.436: INFO: Pod "downwardapi-volume-9af06766-d763-49ad-9f3e-59c00c5d2f43": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.085747067s
STEP: Saw pod success
Oct 29 21:51:13.436: INFO: Pod "downwardapi-volume-9af06766-d763-49ad-9f3e-59c00c5d2f43" satisfied condition "Succeeded or Failed"
Oct 29 21:51:13.458: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod downwardapi-volume-9af06766-d763-49ad-9f3e-59c00c5d2f43 container client-container: <nil>
STEP: delete the pod
Oct 29 21:51:13.514: INFO: Waiting for pod downwardapi-volume-9af06766-d763-49ad-9f3e-59c00c5d2f43 to disappear
Oct 29 21:51:13.534: INFO: Pod downwardapi-volume-9af06766-d763-49ad-9f3e-59c00c5d2f43 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:51:13.534: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-8769" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":-1,"completed":18,"skipped":347,"failed":0}

SSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:51:10.167: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 21:51:10.291: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Oct 29 21:51:19.169: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig --namespace=crd-publish-openapi-6989 create -f -'
Oct 29 21:51:23.972: INFO: stderr: ""
Oct 29 21:51:23.972: INFO: stdout: "e2e-test-crd-publish-openapi-6484-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Oct 29 21:51:23.972: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig --namespace=crd-publish-openapi-6989 delete e2e-test-crd-publish-openapi-6484-crds test-foo'
Oct 29 21:51:24.129: INFO: stderr: ""
Oct 29 21:51:24.129: INFO: stdout: "e2e-test-crd-publish-openapi-6484-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Oct 29 21:51:24.129: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig --namespace=crd-publish-openapi-6989 apply -f -'
Oct 29 21:51:24.738: INFO: stderr: ""
Oct 29 21:51:24.738: INFO: stdout: "e2e-test-crd-publish-openapi-6484-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Oct 29 21:51:24.738: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig --namespace=crd-publish-openapi-6989 delete e2e-test-crd-publish-openapi-6484-crds test-foo'
Oct 29 21:51:24.935: INFO: stderr: ""
Oct 29 21:51:24.935: INFO: stdout: "e2e-test-crd-publish-openapi-6484-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Oct 29 21:51:24.935: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig --namespace=crd-publish-openapi-6989 create -f -'
Oct 29 21:51:25.594: INFO: rc: 1
Oct 29 21:51:25.594: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig --namespace=crd-publish-openapi-6989 apply -f -'
Oct 29 21:51:26.229: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Oct 29 21:51:26.230: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig --namespace=crd-publish-openapi-6989 create -f -'
Oct 29 21:51:26.862: INFO: rc: 1
Oct 29 21:51:26.862: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig --namespace=crd-publish-openapi-6989 apply -f -'
Oct 29 21:51:27.315: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Oct 29 21:51:27.316: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig explain e2e-test-crd-publish-openapi-6484-crds'
Oct 29 21:51:27.924: INFO: stderr: ""
Oct 29 21:51:27.924: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6484-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Oct 29 21:51:27.924: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig explain e2e-test-crd-publish-openapi-6484-crds.metadata'
Oct 29 21:51:28.525: INFO: stderr: ""
Oct 29 21:51:28.525: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6484-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Oct 29 21:51:28.526: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig explain e2e-test-crd-publish-openapi-6484-crds.spec'
Oct 29 21:51:28.989: INFO: stderr: ""
Oct 29 21:51:28.989: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6484-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Oct 29 21:51:28.989: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig explain e2e-test-crd-publish-openapi-6484-crds.spec.bars'
Oct 29 21:51:29.843: INFO: stderr: ""
Oct 29 21:51:29.843: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6484-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Oct 29 21:51:29.843: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig explain e2e-test-crd-publish-openapi-6484-crds.spec.bars2'
Oct 29 21:51:30.447: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:51:37.839: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6989" for this suite.


 [SLOW TEST:27.778 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":-1,"completed":16,"skipped":209,"failed":0}

SSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:50:37.797: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 21:50:37.969: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Creating first CR 
Oct 29 21:50:38.734: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-10-29T21:50:19Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-10-29T21:50:19Z]] name:name1 resourceVersion:228482 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:6f79d739-db5d-4b5b-9085-a05dcd8159c5] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Oct 29 21:50:48.760: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-10-29T21:50:29Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-10-29T21:50:29Z]] name:name2 resourceVersion:228752 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:a2ba292b-732a-43f3-af0b-655f36c9efc5] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Oct 29 21:50:58.786: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-10-29T21:50:19Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-10-29T21:50:39Z]] name:name1 resourceVersion:229002 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:6f79d739-db5d-4b5b-9085-a05dcd8159c5] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Oct 29 21:51:08.811: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-10-29T21:50:29Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-10-29T21:50:49Z]] name:name2 resourceVersion:229158 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:a2ba292b-732a-43f3-af0b-655f36c9efc5] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Oct 29 21:51:18.836: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-10-29T21:50:19Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-10-29T21:50:39Z]] name:name1 resourceVersion:229463 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:6f79d739-db5d-4b5b-9085-a05dcd8159c5] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Oct 29 21:51:28.864: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-10-29T21:50:29Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-10-29T21:50:49Z]] name:name2 resourceVersion:229554 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:a2ba292b-732a-43f3-af0b-655f36c9efc5] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:51:39.428: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-watch-9558" for this suite.


 [SLOW TEST:61.734 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":-1,"completed":9,"skipped":271,"failed":0}

SSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] ReplicaSet
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:51:39.545: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Oct 29 21:51:43.936: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:51:44.029: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "replicaset-560" for this suite.


------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":-1,"completed":10,"skipped":290,"failed":0}

SSSSSSSS
------------------------------
[BeforeEach] [sig-network] Networking
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:51:13.656: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-9768
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Oct 29 21:51:13.835: INFO: Waiting up to 10m0s for all (but 3) nodes to be schedulable
Oct 29 21:51:14.084: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Oct 29 21:51:16.105: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Oct 29 21:51:18.105: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 29 21:51:20.106: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 29 21:51:22.105: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 29 21:51:24.105: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 29 21:51:26.105: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 29 21:51:28.105: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 29 21:51:30.106: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 29 21:51:32.106: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 29 21:51:34.105: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 29 21:51:36.105: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 29 21:51:38.105: INFO: The status of Pod netserver-0 is Running (Ready = true)
Oct 29 21:51:38.149: INFO: The status of Pod netserver-1 is Running (Ready = true)
Oct 29 21:51:38.191: INFO: The status of Pod netserver-2 is Running (Ready = true)
Oct 29 21:51:38.233: INFO: The status of Pod netserver-3 is Running (Ready = true)
STEP: Creating test pods
Oct 29 21:51:42.444: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.131.1.183 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9768 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 29 21:51:42.444: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
Oct 29 21:51:43.646: INFO: Found all expected endpoints: [netserver-0]
Oct 29 21:51:43.667: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.129.2.87 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9768 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 29 21:51:43.667: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
Oct 29 21:51:44.880: INFO: Found all expected endpoints: [netserver-1]
Oct 29 21:51:44.902: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.130.2.86 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9768 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 29 21:51:44.902: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
Oct 29 21:51:46.113: INFO: Found all expected endpoints: [netserver-2]
Oct 29 21:51:46.134: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.128.2.240 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9768 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 29 21:51:46.134: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
Oct 29 21:51:47.350: INFO: Found all expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:51:47.350: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pod-network-test-9768" for this suite.


 [SLOW TEST:33.796 seconds]
[sig-network] Networking
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":19,"skipped":353,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:51:44.141: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Oct 29 21:51:44.366: INFO: Waiting up to 5m0s for pod "downwardapi-volume-846a7529-fe4c-468b-92e5-e9f123131d30" in namespace "projected-1172" to be "Succeeded or Failed"
Oct 29 21:51:44.388: INFO: Pod "downwardapi-volume-846a7529-fe4c-468b-92e5-e9f123131d30": Phase="Pending", Reason="", readiness=false. Elapsed: 21.905951ms
Oct 29 21:51:46.409: INFO: Pod "downwardapi-volume-846a7529-fe4c-468b-92e5-e9f123131d30": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043586647s
Oct 29 21:51:48.432: INFO: Pod "downwardapi-volume-846a7529-fe4c-468b-92e5-e9f123131d30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065882851s
STEP: Saw pod success
Oct 29 21:51:48.432: INFO: Pod "downwardapi-volume-846a7529-fe4c-468b-92e5-e9f123131d30" satisfied condition "Succeeded or Failed"
Oct 29 21:51:48.455: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod downwardapi-volume-846a7529-fe4c-468b-92e5-e9f123131d30 container client-container: <nil>
STEP: delete the pod
Oct 29 21:51:48.525: INFO: Waiting for pod downwardapi-volume-846a7529-fe4c-468b-92e5-e9f123131d30 to disappear
Oct 29 21:51:48.546: INFO: Pod downwardapi-volume-846a7529-fe4c-468b-92e5-e9f123131d30 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:51:48.546: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-1172" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":-1,"completed":11,"skipped":298,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Docker Containers
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:51:48.693: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override command
Oct 29 21:51:48.996: INFO: Waiting up to 5m0s for pod "client-containers-ecb6a575-3940-45ef-958a-dd35228e338a" in namespace "containers-6768" to be "Succeeded or Failed"
Oct 29 21:51:49.017: INFO: Pod "client-containers-ecb6a575-3940-45ef-958a-dd35228e338a": Phase="Pending", Reason="", readiness=false. Elapsed: 21.156902ms
Oct 29 21:51:51.039: INFO: Pod "client-containers-ecb6a575-3940-45ef-958a-dd35228e338a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043213433s
Oct 29 21:51:53.061: INFO: Pod "client-containers-ecb6a575-3940-45ef-958a-dd35228e338a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065315487s
STEP: Saw pod success
Oct 29 21:51:53.061: INFO: Pod "client-containers-ecb6a575-3940-45ef-958a-dd35228e338a" satisfied condition "Succeeded or Failed"
Oct 29 21:51:53.082: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod client-containers-ecb6a575-3940-45ef-958a-dd35228e338a container test-container: <nil>
STEP: delete the pod
Oct 29 21:51:53.139: INFO: Waiting for pod client-containers-ecb6a575-3940-45ef-958a-dd35228e338a to disappear
Oct 29 21:51:53.160: INFO: Pod client-containers-ecb6a575-3940-45ef-958a-dd35228e338a no longer exists
[AfterEach] [k8s.io] Docker Containers
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:51:53.160: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "containers-6768" for this suite.


------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":-1,"completed":12,"skipped":354,"failed":0}

SSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:51:53.271: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Oct 29 21:51:53.488: INFO: Waiting up to 5m0s for pod "pod-41740841-2146-488e-9318-e1160411f20f" in namespace "emptydir-5343" to be "Succeeded or Failed"
Oct 29 21:51:53.509: INFO: Pod "pod-41740841-2146-488e-9318-e1160411f20f": Phase="Pending", Reason="", readiness=false. Elapsed: 21.228503ms
Oct 29 21:51:55.532: INFO: Pod "pod-41740841-2146-488e-9318-e1160411f20f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043619166s
Oct 29 21:51:57.553: INFO: Pod "pod-41740841-2146-488e-9318-e1160411f20f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.064998421s
STEP: Saw pod success
Oct 29 21:51:57.553: INFO: Pod "pod-41740841-2146-488e-9318-e1160411f20f" satisfied condition "Succeeded or Failed"
Oct 29 21:51:57.574: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-41740841-2146-488e-9318-e1160411f20f container test-container: <nil>
STEP: delete the pod
Oct 29 21:51:57.627: INFO: Waiting for pod pod-41740841-2146-488e-9318-e1160411f20f to disappear
Oct 29 21:51:57.648: INFO: Pod pod-41740841-2146-488e-9318-e1160411f20f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:51:57.649: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-5343" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":13,"skipped":363,"failed":0}

SS
------------------------------
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:51:57.754: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:52:02.155: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9191" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":-1,"completed":14,"skipped":365,"failed":0}

SSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-node] Downward API
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:52:02.271: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Oct 29 21:52:02.451: INFO: Waiting up to 5m0s for pod "downward-api-1ec48cbe-b910-48dd-9d19-e354017e3ead" in namespace "downward-api-5342" to be "Succeeded or Failed"
Oct 29 21:52:02.472: INFO: Pod "downward-api-1ec48cbe-b910-48dd-9d19-e354017e3ead": Phase="Pending", Reason="", readiness=false. Elapsed: 21.690781ms
Oct 29 21:52:04.494: INFO: Pod "downward-api-1ec48cbe-b910-48dd-9d19-e354017e3ead": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043561568s
Oct 29 21:52:06.518: INFO: Pod "downward-api-1ec48cbe-b910-48dd-9d19-e354017e3ead": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067385594s
STEP: Saw pod success
Oct 29 21:52:06.518: INFO: Pod "downward-api-1ec48cbe-b910-48dd-9d19-e354017e3ead" satisfied condition "Succeeded or Failed"
Oct 29 21:52:06.540: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod downward-api-1ec48cbe-b910-48dd-9d19-e354017e3ead container dapi-container: <nil>
STEP: delete the pod
Oct 29 21:52:06.599: INFO: Waiting for pod downward-api-1ec48cbe-b910-48dd-9d19-e354017e3ead to disappear
Oct 29 21:52:06.620: INFO: Pod downward-api-1ec48cbe-b910-48dd-9d19-e354017e3ead no longer exists
[AfterEach] [sig-node] Downward API
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:52:06.620: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-5342" for this suite.


------------------------------
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":-1,"completed":15,"skipped":379,"failed":0}

SSSSSSS
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:52:06.736: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Oct 29 21:52:07.060: INFO: Waiting up to 5m0s for pod "downwardapi-volume-85baf3ad-2891-43b2-832a-8257b536525c" in namespace "downward-api-5532" to be "Succeeded or Failed"
Oct 29 21:52:07.082: INFO: Pod "downwardapi-volume-85baf3ad-2891-43b2-832a-8257b536525c": Phase="Pending", Reason="", readiness=false. Elapsed: 22.694331ms
Oct 29 21:52:09.103: INFO: Pod "downwardapi-volume-85baf3ad-2891-43b2-832a-8257b536525c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043704327s
Oct 29 21:52:11.146: INFO: Pod "downwardapi-volume-85baf3ad-2891-43b2-832a-8257b536525c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.085800167s
STEP: Saw pod success
Oct 29 21:52:11.146: INFO: Pod "downwardapi-volume-85baf3ad-2891-43b2-832a-8257b536525c" satisfied condition "Succeeded or Failed"
Oct 29 21:52:11.167: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod downwardapi-volume-85baf3ad-2891-43b2-832a-8257b536525c container client-container: <nil>
STEP: delete the pod
Oct 29 21:52:11.221: INFO: Waiting for pod downwardapi-volume-85baf3ad-2891-43b2-832a-8257b536525c to disappear
Oct 29 21:52:11.241: INFO: Pod downwardapi-volume-85baf3ad-2891-43b2-832a-8257b536525c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:52:11.241: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-5532" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":16,"skipped":386,"failed":0}

SSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Secrets
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:52:11.355: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-8e090e79-5aa8-4d6f-a69d-a0e55dc53413
STEP: Creating a pod to test consume secrets
Oct 29 21:52:11.584: INFO: Waiting up to 5m0s for pod "pod-secrets-a88a17d1-de91-411c-94cd-c889b97e1a8f" in namespace "secrets-2430" to be "Succeeded or Failed"
Oct 29 21:52:11.606: INFO: Pod "pod-secrets-a88a17d1-de91-411c-94cd-c889b97e1a8f": Phase="Pending", Reason="", readiness=false. Elapsed: 22.279448ms
Oct 29 21:52:13.629: INFO: Pod "pod-secrets-a88a17d1-de91-411c-94cd-c889b97e1a8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044537329s
Oct 29 21:52:15.651: INFO: Pod "pod-secrets-a88a17d1-de91-411c-94cd-c889b97e1a8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066351976s
STEP: Saw pod success
Oct 29 21:52:15.651: INFO: Pod "pod-secrets-a88a17d1-de91-411c-94cd-c889b97e1a8f" satisfied condition "Succeeded or Failed"
Oct 29 21:52:15.672: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-secrets-a88a17d1-de91-411c-94cd-c889b97e1a8f container secret-volume-test: <nil>
STEP: delete the pod
Oct 29 21:52:15.724: INFO: Waiting for pod pod-secrets-a88a17d1-de91-411c-94cd-c889b97e1a8f to disappear
Oct 29 21:52:15.745: INFO: Pod pod-secrets-a88a17d1-de91-411c-94cd-c889b97e1a8f no longer exists
[AfterEach] [sig-storage] Secrets
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:52:15.745: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-2430" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":17,"skipped":394,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:52:15.875: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 21:52:15.992: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig create -f - --namespace=kubectl-3712'
Oct 29 21:52:16.792: INFO: stderr: ""
Oct 29 21:52:16.792: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Oct 29 21:52:16.792: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig create -f - --namespace=kubectl-3712'
Oct 29 21:52:17.453: INFO: stderr: ""
Oct 29 21:52:17.453: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Oct 29 21:52:18.476: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 29 21:52:18.476: INFO: Found 0 / 1
Oct 29 21:52:19.477: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 29 21:52:19.477: INFO: Found 1 / 1
Oct 29 21:52:19.477: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Oct 29 21:52:19.498: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 29 21:52:19.498: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Oct 29 21:52:19.498: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig describe pod agnhost-primary-9sk75 --namespace=kubectl-3712'
Oct 29 21:52:19.695: INFO: stderr: ""
Oct 29 21:52:19.695: INFO: stdout: "Name:         agnhost-primary-9sk75\nNamespace:    kubectl-3712\nPriority:     0\nNode:         ip-10-0-142-212.us-east-2.compute.internal/10.0.142.212\nStart Time:   Thu, 29 Oct 2020 21:51:57 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  k8s.v1.cni.cncf.io/network-status:\n                [{\n                    \"name\": \"\",\n                    \"interface\": \"eth0\",\n                    \"ips\": [\n                        \"10.131.1.196\"\n                    ],\n                    \"default\": true,\n                    \"dns\": {}\n                }]\n              k8s.v1.cni.cncf.io/networks-status:\n                [{\n                    \"name\": \"\",\n                    \"interface\": \"eth0\",\n                    \"ips\": [\n                        \"10.131.1.196\"\n                    ],\n                    \"default\": true,\n                    \"dns\": {}\n                }]\n              openshift.io/scc: anyuid\nStatus:       Running\nIP:           10.131.1.196\nIPs:\n  IP:           10.131.1.196\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://451d4fb6cd07a864f5fd6ca0a00390da0710fc537585d8c3fc2a71e9f32f986a\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 29 Oct 2020 21:51:59 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-vts2c (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-vts2c:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-vts2c\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason          Age   From               Message\n  ----    ------          ----  ----               -------\n  Normal  Scheduled       22s   default-scheduler  Successfully assigned kubectl-3712/agnhost-primary-9sk75 to ip-10-0-142-212.us-east-2.compute.internal\n  Normal  AddedInterface  20s   multus             Add eth0 [10.131.1.196/23]\n  Normal  Pulled          20s   kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.20\" already present on machine\n  Normal  Created         20s   kubelet            Created container agnhost-primary\n  Normal  Started         20s   kubelet            Started container agnhost-primary\n"
Oct 29 21:52:19.695: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig describe rc agnhost-primary --namespace=kubectl-3712'
Oct 29 21:52:19.912: INFO: stderr: ""
Oct 29 21:52:19.912: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-3712\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  22s   replication-controller  Created pod: agnhost-primary-9sk75\n"
Oct 29 21:52:19.912: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig describe service agnhost-primary --namespace=kubectl-3712'
Oct 29 21:52:20.129: INFO: stderr: ""
Oct 29 21:52:20.129: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-3712\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP:                172.30.56.96\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.131.1.196:6379\nSession Affinity:  None\nEvents:            <none>\n"
Oct 29 21:52:20.208: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig describe node ip-10-0-142-212.us-east-2.compute.internal'
Oct 29 21:52:20.564: INFO: stderr: ""
Oct 29 21:52:20.564: INFO: stdout: "Name:               ip-10-0-142-212.us-east-2.compute.internal\nRoles:              worker\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=m5.xlarge\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-east-2\n                    failure-domain.beta.kubernetes.io/zone=us-east-2a\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-10-0-142-212\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/worker=\n                    node.kubernetes.io/instance-type=m5.xlarge\n                    node.openshift.io/os_id=rhcos\n                    topology.ebs.csi.aws.com/zone=us-east-2a\n                    topology.kubernetes.io/region=us-east-2\n                    topology.kubernetes.io/zone=us-east-2a\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"ebs.csi.aws.com\":\"i-03781cb9a8092da57\"}\n                    machine.openshift.io/machine: openshift-machine-api/jeder-461-cncf2-pvrph-worker-us-east-2a-jhsl2\n                    machineconfiguration.openshift.io/currentConfig: rendered-worker-f26c105cafab97113f99101cc57ac048\n                    machineconfiguration.openshift.io/desiredConfig: rendered-worker-f26c105cafab97113f99101cc57ac048\n                    machineconfiguration.openshift.io/reason: \n                    machineconfiguration.openshift.io/state: Done\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 29 Oct 2020 17:30:43 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-10-0-142-212.us-east-2.compute.internal\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 29 Oct 2020 21:51:51 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Thu, 29 Oct 2020 21:51:11 +0000   Thu, 29 Oct 2020 17:44:37 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Thu, 29 Oct 2020 21:51:11 +0000   Thu, 29 Oct 2020 17:44:37 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Thu, 29 Oct 2020 21:51:11 +0000   Thu, 29 Oct 2020 17:44:37 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Thu, 29 Oct 2020 21:51:11 +0000   Thu, 29 Oct 2020 17:44:37 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:   10.0.142.212\n  Hostname:     ip-10-0-142-212.us-east-2.compute.internal\n  InternalDNS:  ip-10-0-142-212.us-east-2.compute.internal\nCapacity:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         4\n  ephemeral-storage:           314020844Ki\n  example.com/fakecpu:         1k\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      15944120Ki\n  pods:                        250\nAllocatable:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         3\n  ephemeral-storage:           288327867528\n  example.com/fakecpu:         1k\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      14793144Ki\n  pods:                        250\nSystem Info:\n  Machine ID:                             ec20bdb61ee45ac6e8e755af80323fd3\n  System UUID:                            ec20bdb6-1ee4-5ac6-e8e7-55af80323fd3\n  Boot ID:                                31e57073-a38a-44a7-8d88-3091338f4da5\n  Kernel Version:                         4.18.0-193.24.1.el8_2.dt1.x86_64\n  OS Image:                               Red Hat Enterprise Linux CoreOS 46.82.202010091720-0 (Ootpa)\n  Operating System:                       linux\n  Architecture:                           amd64\n  Container Runtime Version:              cri-o://1.19.0-20.rhaos4.6.git97d715e.el8\n  Kubelet Version:                        v1.19.0+d59ce34\n  Kube-Proxy Version:                     v1.19.0+d59ce34\nPodCIDR:                                  10.128.3.0/24\nPodCIDRs:                                 10.128.3.0/24\nProviderID:                               aws:///us-east-2a/i-03781cb9a8092da57\nNon-terminated Pods:                      (14 in total)\n  Namespace                               Name                                                  CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                               ----                                                  ------------  ----------  ---------------  -------------  ---\n  kubectl-3712                            agnhost-primary-9sk75                                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         23s\n  openshift-cluster-csi-drivers           aws-ebs-csi-driver-node-5gmf4                         30m (1%)      0 (0%)      150Mi (1%)       0 (0%)         4h21m\n  openshift-cluster-node-tuning-operator  tuned-d6fwx                                           10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         4h21m\n  openshift-dns                           dns-default-b89gt                                     65m (2%)      0 (0%)      110Mi (0%)       512Mi (3%)     4h21m\n  openshift-image-registry                node-ca-rq9vr                                         10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         4h21m\n  openshift-machine-config-operator       machine-config-daemon-l4r47                           40m (1%)      0 (0%)      100Mi (0%)       0 (0%)         4h21m\n  openshift-monitoring                    node-exporter-p2phw                                   9m (0%)       0 (0%)      210Mi (1%)       0 (0%)         4h21m\n  openshift-monitoring                    sre-dns-latency-exporter-jl57s                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         4m42s\n  openshift-multus                        multus-jst6k                                          10m (0%)      0 (0%)      150Mi (1%)       0 (0%)         4h21m\n  openshift-multus                        network-metrics-daemon-xrvxh                          20m (0%)      0 (0%)      120Mi (0%)       0 (0%)         4h21m\n  openshift-sdn                           ovs-66jns                                             100m (3%)     0 (0%)      400Mi (2%)       0 (0%)         4h21m\n  openshift-sdn                           sdn-lbx27                                             110m (3%)     0 (0%)      220Mi (1%)       0 (0%)         4h21m\n  services-6247                           affinity-clusterip-timeout-mprbw                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         55s\n  var-expansion-3846                      var-expansion-1956ec53-52b3-409d-99fb-21cf80c64e31    0 (0%)        0 (0%)      0 (0%)           0 (0%)         111s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests      Limits\n  --------                    --------      ------\n  cpu                         404m (13%)    0 (0%)\n  memory                      1520Mi (10%)  512Mi (3%)\n  ephemeral-storage           0 (0%)        0 (0%)\n  hugepages-1Gi               0 (0%)        0 (0%)\n  hugepages-2Mi               0 (0%)        0 (0%)\n  attachable-volumes-aws-ebs  0             0\n  example.com/fakecpu         0             0\nEvents:                       <none>\n"
Oct 29 21:52:20.564: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig describe namespace kubectl-3712'
Oct 29 21:52:20.799: INFO: stderr: ""
Oct 29 21:52:20.799: INFO: stdout: "Name:         kubectl-3712\nLabels:       e2e-framework=kubectl\n              e2e-run=e09a18d3-a705-47b6-a191-3aa184ca31b3\nAnnotations:  openshift.io/sa.scc.mcs: s0:c88,c32\n              openshift.io/sa.scc.supplemental-groups: 1007720000/10000\n              openshift.io/sa.scc.uid-range: 1007720000/10000\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:52:20.799: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-3712" for this suite.


 [SLOW TEST:5.018 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1105
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":-1,"completed":18,"skipped":430,"failed":0}

SSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Security Context
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:52:20.903: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 21:52:21.168: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-e8447966-e007-4eff-9e65-49f986c12075" in namespace "security-context-test-2116" to be "Succeeded or Failed"
Oct 29 21:52:21.190: INFO: Pod "busybox-privileged-false-e8447966-e007-4eff-9e65-49f986c12075": Phase="Pending", Reason="", readiness=false. Elapsed: 22.559279ms
Oct 29 21:52:23.211: INFO: Pod "busybox-privileged-false-e8447966-e007-4eff-9e65-49f986c12075": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043193888s
Oct 29 21:52:25.233: INFO: Pod "busybox-privileged-false-e8447966-e007-4eff-9e65-49f986c12075": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065138807s
Oct 29 21:52:25.233: INFO: Pod "busybox-privileged-false-e8447966-e007-4eff-9e65-49f986c12075" satisfied condition "Succeeded or Failed"
Oct 29 21:52:25.256: INFO: Got logs for pod "busybox-privileged-false-e8447966-e007-4eff-9e65-49f986c12075": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:52:25.256: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "security-context-test-2116" for this suite.


------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":19,"skipped":440,"failed":0}

SSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:51:37.954: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-6247
Oct 29 21:51:40.179: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=services-6247 kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Oct 29 21:51:40.523: INFO: rc: 7
Oct 29 21:51:40.552: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Oct 29 21:51:40.575: INFO: Pod kube-proxy-mode-detector still exists
Oct 29 21:51:42.575: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Oct 29 21:51:42.602: INFO: Pod kube-proxy-mode-detector still exists
Oct 29 21:51:44.575: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Oct 29 21:51:44.596: INFO: Pod kube-proxy-mode-detector no longer exists
Oct 29 21:51:44.596: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=services-6247 kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-clusterip-timeout in namespace services-6247
STEP: creating replication controller affinity-clusterip-timeout in namespace services-6247
I1029 21:51:44.653506   53213 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-6247, replica count: 3
I1029 21:51:47.703875   53213 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1029 21:51:50.704053   53213 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 29 21:51:50.749: INFO: Creating new exec pod
Oct 29 21:51:55.847: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=services-6247 execpod-affinityrf4gh -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Oct 29 21:51:56.197: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Oct 29 21:51:56.197: INFO: stdout: ""
Oct 29 21:51:56.197: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=services-6247 execpod-affinityrf4gh -- /bin/sh -x -c nc -zv -t -w 2 172.30.92.74 80'
Oct 29 21:51:56.563: INFO: stderr: "+ nc -zv -t -w 2 172.30.92.74 80\nConnection to 172.30.92.74 80 port [tcp/http] succeeded!\n"
Oct 29 21:51:56.563: INFO: stdout: ""
Oct 29 21:51:56.563: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=services-6247 execpod-affinityrf4gh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.92.74:80/ ; done'
Oct 29 21:51:56.966: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.92.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.92.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.92.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.92.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.92.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.92.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.92.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.92.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.92.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.92.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.92.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.92.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.92.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.92.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.92.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.92.74:80/\n"
Oct 29 21:51:56.966: INFO: stdout: "\naffinity-clusterip-timeout-kh79j\naffinity-clusterip-timeout-kh79j\naffinity-clusterip-timeout-kh79j\naffinity-clusterip-timeout-kh79j\naffinity-clusterip-timeout-kh79j\naffinity-clusterip-timeout-kh79j\naffinity-clusterip-timeout-kh79j\naffinity-clusterip-timeout-kh79j\naffinity-clusterip-timeout-kh79j\naffinity-clusterip-timeout-kh79j\naffinity-clusterip-timeout-kh79j\naffinity-clusterip-timeout-kh79j\naffinity-clusterip-timeout-kh79j\naffinity-clusterip-timeout-kh79j\naffinity-clusterip-timeout-kh79j\naffinity-clusterip-timeout-kh79j"
Oct 29 21:51:56.966: INFO: Received response from host: affinity-clusterip-timeout-kh79j
Oct 29 21:51:56.966: INFO: Received response from host: affinity-clusterip-timeout-kh79j
Oct 29 21:51:56.966: INFO: Received response from host: affinity-clusterip-timeout-kh79j
Oct 29 21:51:56.966: INFO: Received response from host: affinity-clusterip-timeout-kh79j
Oct 29 21:51:56.966: INFO: Received response from host: affinity-clusterip-timeout-kh79j
Oct 29 21:51:56.966: INFO: Received response from host: affinity-clusterip-timeout-kh79j
Oct 29 21:51:56.966: INFO: Received response from host: affinity-clusterip-timeout-kh79j
Oct 29 21:51:56.966: INFO: Received response from host: affinity-clusterip-timeout-kh79j
Oct 29 21:51:56.966: INFO: Received response from host: affinity-clusterip-timeout-kh79j
Oct 29 21:51:56.966: INFO: Received response from host: affinity-clusterip-timeout-kh79j
Oct 29 21:51:56.966: INFO: Received response from host: affinity-clusterip-timeout-kh79j
Oct 29 21:51:56.966: INFO: Received response from host: affinity-clusterip-timeout-kh79j
Oct 29 21:51:56.966: INFO: Received response from host: affinity-clusterip-timeout-kh79j
Oct 29 21:51:56.966: INFO: Received response from host: affinity-clusterip-timeout-kh79j
Oct 29 21:51:56.966: INFO: Received response from host: affinity-clusterip-timeout-kh79j
Oct 29 21:51:56.966: INFO: Received response from host: affinity-clusterip-timeout-kh79j
Oct 29 21:51:56.966: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=services-6247 execpod-affinityrf4gh -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.30.92.74:80/'
Oct 29 21:51:57.319: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.30.92.74:80/\n"
Oct 29 21:51:57.319: INFO: stdout: "affinity-clusterip-timeout-kh79j"
Oct 29 21:52:12.319: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=services-6247 execpod-affinityrf4gh -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.30.92.74:80/'
Oct 29 21:52:12.669: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.30.92.74:80/\n"
Oct 29 21:52:12.669: INFO: stdout: "affinity-clusterip-timeout-mprbw"
Oct 29 21:52:12.669: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-6247, will wait for the garbage collector to delete the pods
Oct 29 21:52:12.794: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 24.6017ms
Oct 29 21:52:12.894: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.131059ms
[AfterEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:52:27.139: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-6247" for this suite.
[AfterEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786


 [SLOW TEST:49.271 seconds]
[sig-network] Services
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":-1,"completed":17,"skipped":218,"failed":0}

SSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:51:47.471: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Oct 29 21:51:47.617: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:52:28.243: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5181" for this suite.


 [SLOW TEST:40.880 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":-1,"completed":20,"skipped":382,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Security Context
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:52:25.375: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 21:52:25.547: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-1cb786ec-2c57-4581-8d34-d1a432db481c" in namespace "security-context-test-4787" to be "Succeeded or Failed"
Oct 29 21:52:25.570: INFO: Pod "alpine-nnp-false-1cb786ec-2c57-4581-8d34-d1a432db481c": Phase="Pending", Reason="", readiness=false. Elapsed: 23.26662ms
Oct 29 21:52:27.597: INFO: Pod "alpine-nnp-false-1cb786ec-2c57-4581-8d34-d1a432db481c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050002539s
Oct 29 21:52:29.619: INFO: Pod "alpine-nnp-false-1cb786ec-2c57-4581-8d34-d1a432db481c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.072341315s
Oct 29 21:52:29.619: INFO: Pod "alpine-nnp-false-1cb786ec-2c57-4581-8d34-d1a432db481c" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:52:29.643: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "security-context-test-4787" for this suite.


------------------------------
{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":20,"skipped":455,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Watchers
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:52:27.230: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Oct 29 21:52:27.547: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6551 /api/v1/namespaces/watch-6551/configmaps/e2e-watch-test-label-changed a115b50e-3c72-48d5-be16-1c4a1488526a 231075 0 2020-10-29 21:52:07 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-10-29 21:52:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 29 21:52:27.547: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6551 /api/v1/namespaces/watch-6551/configmaps/e2e-watch-test-label-changed a115b50e-3c72-48d5-be16-1c4a1488526a 231077 0 2020-10-29 21:52:07 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-10-29 21:52:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 29 21:52:27.547: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6551 /api/v1/namespaces/watch-6551/configmaps/e2e-watch-test-label-changed a115b50e-3c72-48d5-be16-1c4a1488526a 231080 0 2020-10-29 21:52:07 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-10-29 21:52:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Oct 29 21:52:37.714: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6551 /api/v1/namespaces/watch-6551/configmaps/e2e-watch-test-label-changed a115b50e-3c72-48d5-be16-1c4a1488526a 231466 0 2020-10-29 21:52:07 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-10-29 21:52:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 29 21:52:37.714: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6551 /api/v1/namespaces/watch-6551/configmaps/e2e-watch-test-label-changed a115b50e-3c72-48d5-be16-1c4a1488526a 231467 0 2020-10-29 21:52:07 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-10-29 21:52:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 29 21:52:37.715: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6551 /api/v1/namespaces/watch-6551/configmaps/e2e-watch-test-label-changed a115b50e-3c72-48d5-be16-1c4a1488526a 231469 0 2020-10-29 21:52:07 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-10-29 21:52:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:52:37.715: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "watch-6551" for this suite.


 [SLOW TEST:10.589 seconds]
[sig-api-machinery] Watchers
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":-1,"completed":18,"skipped":222,"failed":0}

SSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected configMap
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:52:37.831: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-3de0b586-2abf-4c12-83c1-974f56535900
STEP: Creating a pod to test consume configMaps
Oct 29 21:52:38.290: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0326282a-c94c-4b86-95de-fe42047374ef" in namespace "projected-482" to be "Succeeded or Failed"
Oct 29 21:52:38.311: INFO: Pod "pod-projected-configmaps-0326282a-c94c-4b86-95de-fe42047374ef": Phase="Pending", Reason="", readiness=false. Elapsed: 21.703371ms
Oct 29 21:52:40.333: INFO: Pod "pod-projected-configmaps-0326282a-c94c-4b86-95de-fe42047374ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043730259s
Oct 29 21:52:42.356: INFO: Pod "pod-projected-configmaps-0326282a-c94c-4b86-95de-fe42047374ef": Phase="Pending", Reason="", readiness=false. Elapsed: 4.066501976s
Oct 29 21:52:44.380: INFO: Pod "pod-projected-configmaps-0326282a-c94c-4b86-95de-fe42047374ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.089933192s
STEP: Saw pod success
Oct 29 21:52:44.380: INFO: Pod "pod-projected-configmaps-0326282a-c94c-4b86-95de-fe42047374ef" satisfied condition "Succeeded or Failed"
Oct 29 21:52:44.401: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-projected-configmaps-0326282a-c94c-4b86-95de-fe42047374ef container projected-configmap-volume-test: <nil>
STEP: delete the pod
Oct 29 21:52:44.473: INFO: Waiting for pod pod-projected-configmaps-0326282a-c94c-4b86-95de-fe42047374ef to disappear
Oct 29 21:52:44.494: INFO: Pod pod-projected-configmaps-0326282a-c94c-4b86-95de-fe42047374ef no longer exists
[AfterEach] [sig-storage] Projected configMap
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:52:44.494: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-482" for this suite.


 [SLOW TEST:6.776 seconds]
[sig-storage] Projected configMap
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":-1,"completed":19,"skipped":237,"failed":0}

SSSSSSS
------------------------------
[BeforeEach] [k8s.io] Pods
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:52:44.613: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be updated [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Oct 29 21:52:49.426: INFO: Successfully updated pod "pod-update-a636f7e5-1e32-4c26-ad93-9ddf585e330d"
STEP: verifying the updated pod is in kubernetes
Oct 29 21:52:49.469: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:52:49.469: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-8108" for this suite.


------------------------------
{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":-1,"completed":20,"skipped":244,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] DNS
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:52:49.572: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1357.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1357.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1357.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1357.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct 29 21:52:53.905: INFO: DNS probes using dns-test-3dbd1ca1-8367-4361-ac66-673ad2b207c9 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1357.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1357.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1357.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1357.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct 29 21:52:58.122: INFO: DNS probes using dns-test-2ad910d0-ce21-4fb2-8902-defef37f6bdf succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1357.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1357.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1357.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1357.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct 29 21:53:02.373: INFO: DNS probes using dns-test-eb170c4c-f597-499d-b495-454231e4456f succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:53:02.444: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "dns-1357" for this suite.


 [SLOW TEST:12.961 seconds]
[sig-network] DNS
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":-1,"completed":21,"skipped":268,"failed":0}

SSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:53:02.543: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Oct 29 21:53:06.872: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-128 PodName:pod-sharedvolume-3a3e95cf-3809-4f61-8439-035c8a6ab3c5 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 29 21:53:06.872: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
Oct 29 21:53:07.078: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:53:07.078: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-128" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":-1,"completed":22,"skipped":277,"failed":0}

S
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:53:07.186: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-20ce0483-497e-40ce-87f7-4c3f24ab7869
STEP: Creating a pod to test consume configMaps
Oct 29 21:53:07.394: INFO: Waiting up to 5m0s for pod "pod-configmaps-da78f8f2-82c6-4c5c-8ec7-2656c3fc2a41" in namespace "configmap-1490" to be "Succeeded or Failed"
Oct 29 21:53:07.416: INFO: Pod "pod-configmaps-da78f8f2-82c6-4c5c-8ec7-2656c3fc2a41": Phase="Pending", Reason="", readiness=false. Elapsed: 22.15822ms
Oct 29 21:53:09.445: INFO: Pod "pod-configmaps-da78f8f2-82c6-4c5c-8ec7-2656c3fc2a41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050729124s
Oct 29 21:53:11.468: INFO: Pod "pod-configmaps-da78f8f2-82c6-4c5c-8ec7-2656c3fc2a41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.073960275s
STEP: Saw pod success
Oct 29 21:53:11.468: INFO: Pod "pod-configmaps-da78f8f2-82c6-4c5c-8ec7-2656c3fc2a41" satisfied condition "Succeeded or Failed"
Oct 29 21:53:11.490: INFO: Trying to get logs from node ip-10-0-234-238.us-east-2.compute.internal pod pod-configmaps-da78f8f2-82c6-4c5c-8ec7-2656c3fc2a41 container configmap-volume-test: <nil>
STEP: delete the pod
Oct 29 21:53:11.549: INFO: Waiting for pod pod-configmaps-da78f8f2-82c6-4c5c-8ec7-2656c3fc2a41 to disappear
Oct 29 21:53:11.571: INFO: Pod pod-configmaps-da78f8f2-82c6-4c5c-8ec7-2656c3fc2a41 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:53:11.571: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-1490" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":-1,"completed":23,"skipped":278,"failed":0}

SSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:53:11.695: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Oct 29 21:53:11.897: INFO: Waiting up to 5m0s for pod "pod-96352e2b-674f-4530-bec9-1b1dfe16bf1a" in namespace "emptydir-8311" to be "Succeeded or Failed"
Oct 29 21:53:11.920: INFO: Pod "pod-96352e2b-674f-4530-bec9-1b1dfe16bf1a": Phase="Pending", Reason="", readiness=false. Elapsed: 22.611844ms
Oct 29 21:53:13.942: INFO: Pod "pod-96352e2b-674f-4530-bec9-1b1dfe16bf1a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044823549s
Oct 29 21:53:15.965: INFO: Pod "pod-96352e2b-674f-4530-bec9-1b1dfe16bf1a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067210155s
STEP: Saw pod success
Oct 29 21:53:15.965: INFO: Pod "pod-96352e2b-674f-4530-bec9-1b1dfe16bf1a" satisfied condition "Succeeded or Failed"
Oct 29 21:53:15.986: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-96352e2b-674f-4530-bec9-1b1dfe16bf1a container test-container: <nil>
STEP: delete the pod
Oct 29 21:53:16.037: INFO: Waiting for pod pod-96352e2b-674f-4530-bec9-1b1dfe16bf1a to disappear
Oct 29 21:53:16.059: INFO: Pod pod-96352e2b-674f-4530-bec9-1b1dfe16bf1a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:53:16.059: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-8311" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":24,"skipped":298,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Container Runtime
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:53:16.211: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Oct 29 21:53:20.542: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:53:20.598: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-runtime-7095" for this suite.


------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":-1,"completed":25,"skipped":369,"failed":0}

S
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:53:20.703: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Oct 29 21:53:20.883: INFO: Waiting up to 5m0s for pod "downwardapi-volume-95be6839-7749-4114-8d76-f18eff10ec6f" in namespace "downward-api-4241" to be "Succeeded or Failed"
Oct 29 21:53:20.910: INFO: Pod "downwardapi-volume-95be6839-7749-4114-8d76-f18eff10ec6f": Phase="Pending", Reason="", readiness=false. Elapsed: 27.037507ms
Oct 29 21:53:22.932: INFO: Pod "downwardapi-volume-95be6839-7749-4114-8d76-f18eff10ec6f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049136068s
Oct 29 21:53:24.954: INFO: Pod "downwardapi-volume-95be6839-7749-4114-8d76-f18eff10ec6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.071309841s
STEP: Saw pod success
Oct 29 21:53:24.955: INFO: Pod "downwardapi-volume-95be6839-7749-4114-8d76-f18eff10ec6f" satisfied condition "Succeeded or Failed"
Oct 29 21:53:24.976: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod downwardapi-volume-95be6839-7749-4114-8d76-f18eff10ec6f container client-container: <nil>
STEP: delete the pod
Oct 29 21:53:25.030: INFO: Waiting for pod downwardapi-volume-95be6839-7749-4114-8d76-f18eff10ec6f to disappear
Oct 29 21:53:25.052: INFO: Pod downwardapi-volume-95be6839-7749-4114-8d76-f18eff10ec6f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:53:25.052: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-4241" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":-1,"completed":26,"skipped":370,"failed":0}

SSSSS
------------------------------
[BeforeEach] [k8s.io] Variable Expansion
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:50:49.045: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod with failed condition
STEP: updating the pod
Oct 29 21:52:49.836: INFO: Successfully updated pod "var-expansion-1956ec53-52b3-409d-99fb-21cf80c64e31"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Oct 29 21:52:51.952: INFO: Deleting pod "var-expansion-1956ec53-52b3-409d-99fb-21cf80c64e31" in namespace "var-expansion-3846"
Oct 29 21:52:51.976: INFO: Wait up to 5m0s for pod "var-expansion-1956ec53-52b3-409d-99fb-21cf80c64e31" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:53:28.017: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "var-expansion-3846" for this suite.


 [SLOW TEST:159.078 seconds]
[k8s.io] Variable Expansion
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":-1,"completed":19,"skipped":268,"failed":0}

SSS
------------------------------
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:53:28.127: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Oct 29 21:53:28.261: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:53:34.845: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "init-container-9972" for this suite.


 [SLOW TEST:6.825 seconds]
[k8s.io] InitContainer [NodeConformance]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":-1,"completed":20,"skipped":271,"failed":0}

SSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:53:34.962: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 21:53:35.085: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:53:35.701: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5731" for this suite.


------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":-1,"completed":21,"skipped":282,"failed":0}

SSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected configMap
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:52:28.372: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 21:52:28.540: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-f8d5b55c-516c-402c-8aa2-cc73ba30f0be
STEP: Creating configMap with name cm-test-opt-upd-cb184aff-7e6a-4c03-b0df-ba1d4763a49b
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-f8d5b55c-516c-402c-8aa2-cc73ba30f0be
STEP: Updating configmap cm-test-opt-upd-cb184aff-7e6a-4c03-b0df-ba1d4763a49b
STEP: Creating configMap with name cm-test-opt-create-0029c271-0b57-478f-bd8f-dcdafa567520
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:53:45.837: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-9082" for this suite.


 [SLOW TEST:77.552 seconds]
[sig-storage] Projected configMap
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":-1,"completed":21,"skipped":410,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] DNS
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:53:45.950: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Oct 29 21:53:46.101: INFO: Created pod &Pod{ObjectMeta:{dns-8781  dns-8781 /api/v1/namespaces/dns-8781/pods/dns-8781 15245ded-fd06-46ef-b4ca-ac4cef19659d 233162 0 2020-10-29 21:53:26 +0000 UTC <nil> <nil> map[] map[openshift.io/scc:anyuid] [] []  [{e2e.test Update v1 2020-10-29 21:53:26 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7mp2n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7mp2n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7mp2n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c89,c34,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-m5csk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 29 21:53:46.122: INFO: The status of Pod dns-8781 is Pending, waiting for it to be Running (with Ready = true)
Oct 29 21:53:48.144: INFO: The status of Pod dns-8781 is Pending, waiting for it to be Running (with Ready = true)
Oct 29 21:53:50.144: INFO: The status of Pod dns-8781 is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Oct 29 21:53:50.144: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-8781 PodName:dns-8781 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 29 21:53:50.144: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Verifying customized DNS server is configured on pod...
Oct 29 21:53:50.352: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-8781 PodName:dns-8781 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 29 21:53:50.352: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
Oct 29 21:53:50.556: INFO: Deleting pod dns-8781...
[AfterEach] [sig-network] DNS
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:53:50.586: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "dns-8781" for this suite.


------------------------------
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":-1,"completed":22,"skipped":446,"failed":0}

SSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:53:35.810: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-6679
STEP: creating service affinity-clusterip-transition in namespace services-6679
STEP: creating replication controller affinity-clusterip-transition in namespace services-6679
I1029 21:53:36.233470   53210 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-6679, replica count: 3
I1029 21:53:39.283867   53210 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1029 21:53:42.284147   53210 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 29 21:53:42.329: INFO: Creating new exec pod
Oct 29 21:53:47.427: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=services-6679 execpod-affinityl2g78 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
Oct 29 21:53:47.797: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Oct 29 21:53:47.797: INFO: stdout: ""
Oct 29 21:53:47.797: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=services-6679 execpod-affinityl2g78 -- /bin/sh -x -c nc -zv -t -w 2 172.30.168.173 80'
Oct 29 21:53:48.143: INFO: stderr: "+ nc -zv -t -w 2 172.30.168.173 80\nConnection to 172.30.168.173 80 port [tcp/http] succeeded!\n"
Oct 29 21:53:48.143: INFO: stdout: ""
Oct 29 21:53:48.189: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=services-6679 execpod-affinityl2g78 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.168.173:80/ ; done'
Oct 29 21:53:48.712: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.168.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.168.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.168.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.168.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.168.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.168.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.168.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.168.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.168.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.168.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.168.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.168.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.168.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.168.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.168.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.168.173:80/\n"
Oct 29 21:53:48.712: INFO: stdout: "\naffinity-clusterip-transition-j2mb4\naffinity-clusterip-transition-5dqmv\naffinity-clusterip-transition-mw6fq\naffinity-clusterip-transition-5dqmv\naffinity-clusterip-transition-mw6fq\naffinity-clusterip-transition-mw6fq\naffinity-clusterip-transition-5dqmv\naffinity-clusterip-transition-5dqmv\naffinity-clusterip-transition-5dqmv\naffinity-clusterip-transition-5dqmv\naffinity-clusterip-transition-5dqmv\naffinity-clusterip-transition-mw6fq\naffinity-clusterip-transition-mw6fq\naffinity-clusterip-transition-5dqmv\naffinity-clusterip-transition-j2mb4\naffinity-clusterip-transition-j2mb4"
Oct 29 21:53:48.712: INFO: Received response from host: affinity-clusterip-transition-j2mb4
Oct 29 21:53:48.712: INFO: Received response from host: affinity-clusterip-transition-5dqmv
Oct 29 21:53:48.712: INFO: Received response from host: affinity-clusterip-transition-mw6fq
Oct 29 21:53:48.712: INFO: Received response from host: affinity-clusterip-transition-5dqmv
Oct 29 21:53:48.712: INFO: Received response from host: affinity-clusterip-transition-mw6fq
Oct 29 21:53:48.712: INFO: Received response from host: affinity-clusterip-transition-mw6fq
Oct 29 21:53:48.712: INFO: Received response from host: affinity-clusterip-transition-5dqmv
Oct 29 21:53:48.712: INFO: Received response from host: affinity-clusterip-transition-5dqmv
Oct 29 21:53:48.712: INFO: Received response from host: affinity-clusterip-transition-5dqmv
Oct 29 21:53:48.712: INFO: Received response from host: affinity-clusterip-transition-5dqmv
Oct 29 21:53:48.712: INFO: Received response from host: affinity-clusterip-transition-5dqmv
Oct 29 21:53:48.712: INFO: Received response from host: affinity-clusterip-transition-mw6fq
Oct 29 21:53:48.712: INFO: Received response from host: affinity-clusterip-transition-mw6fq
Oct 29 21:53:48.712: INFO: Received response from host: affinity-clusterip-transition-5dqmv
Oct 29 21:53:48.712: INFO: Received response from host: affinity-clusterip-transition-j2mb4
Oct 29 21:53:48.712: INFO: Received response from host: affinity-clusterip-transition-j2mb4
Oct 29 21:53:48.757: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=services-6679 execpod-affinityl2g78 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.168.173:80/ ; done'
Oct 29 21:53:49.153: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.168.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.168.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.168.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.168.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.168.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.168.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.168.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.168.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.168.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.168.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.168.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.168.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.168.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.168.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.168.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.168.173:80/\n"
Oct 29 21:53:49.153: INFO: stdout: "\naffinity-clusterip-transition-mw6fq\naffinity-clusterip-transition-mw6fq\naffinity-clusterip-transition-mw6fq\naffinity-clusterip-transition-mw6fq\naffinity-clusterip-transition-mw6fq\naffinity-clusterip-transition-mw6fq\naffinity-clusterip-transition-mw6fq\naffinity-clusterip-transition-mw6fq\naffinity-clusterip-transition-mw6fq\naffinity-clusterip-transition-mw6fq\naffinity-clusterip-transition-mw6fq\naffinity-clusterip-transition-mw6fq\naffinity-clusterip-transition-mw6fq\naffinity-clusterip-transition-mw6fq\naffinity-clusterip-transition-mw6fq\naffinity-clusterip-transition-mw6fq"
Oct 29 21:53:49.153: INFO: Received response from host: affinity-clusterip-transition-mw6fq
Oct 29 21:53:49.153: INFO: Received response from host: affinity-clusterip-transition-mw6fq
Oct 29 21:53:49.154: INFO: Received response from host: affinity-clusterip-transition-mw6fq
Oct 29 21:53:49.154: INFO: Received response from host: affinity-clusterip-transition-mw6fq
Oct 29 21:53:49.154: INFO: Received response from host: affinity-clusterip-transition-mw6fq
Oct 29 21:53:49.154: INFO: Received response from host: affinity-clusterip-transition-mw6fq
Oct 29 21:53:49.154: INFO: Received response from host: affinity-clusterip-transition-mw6fq
Oct 29 21:53:49.154: INFO: Received response from host: affinity-clusterip-transition-mw6fq
Oct 29 21:53:49.154: INFO: Received response from host: affinity-clusterip-transition-mw6fq
Oct 29 21:53:49.154: INFO: Received response from host: affinity-clusterip-transition-mw6fq
Oct 29 21:53:49.154: INFO: Received response from host: affinity-clusterip-transition-mw6fq
Oct 29 21:53:49.154: INFO: Received response from host: affinity-clusterip-transition-mw6fq
Oct 29 21:53:49.154: INFO: Received response from host: affinity-clusterip-transition-mw6fq
Oct 29 21:53:49.154: INFO: Received response from host: affinity-clusterip-transition-mw6fq
Oct 29 21:53:49.154: INFO: Received response from host: affinity-clusterip-transition-mw6fq
Oct 29 21:53:49.154: INFO: Received response from host: affinity-clusterip-transition-mw6fq
Oct 29 21:53:49.154: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-6679, will wait for the garbage collector to delete the pods
Oct 29 21:53:49.278: INFO: Deleting ReplicationController affinity-clusterip-transition took: 23.717019ms
Oct 29 21:53:49.378: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.198076ms
[AfterEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:53:57.128: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-6679" for this suite.
[AfterEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786


 [SLOW TEST:21.403 seconds]
[sig-network] Services
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":-1,"completed":22,"skipped":303,"failed":0}

SSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected configMap
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:53:57.224: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-41b841f5-f345-4fbc-9878-54cb9a519ef7
STEP: Creating a pod to test consume configMaps
Oct 29 21:53:57.479: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-af4a65c7-ac71-47b1-a7f9-3f18b4fe1ca9" in namespace "projected-2407" to be "Succeeded or Failed"
Oct 29 21:53:57.500: INFO: Pod "pod-projected-configmaps-af4a65c7-ac71-47b1-a7f9-3f18b4fe1ca9": Phase="Pending", Reason="", readiness=false. Elapsed: 20.904918ms
Oct 29 21:53:59.521: INFO: Pod "pod-projected-configmaps-af4a65c7-ac71-47b1-a7f9-3f18b4fe1ca9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042044296s
Oct 29 21:54:01.544: INFO: Pod "pod-projected-configmaps-af4a65c7-ac71-47b1-a7f9-3f18b4fe1ca9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.064492787s
STEP: Saw pod success
Oct 29 21:54:01.544: INFO: Pod "pod-projected-configmaps-af4a65c7-ac71-47b1-a7f9-3f18b4fe1ca9" satisfied condition "Succeeded or Failed"
Oct 29 21:54:01.571: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-projected-configmaps-af4a65c7-ac71-47b1-a7f9-3f18b4fe1ca9 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Oct 29 21:54:01.624: INFO: Waiting for pod pod-projected-configmaps-af4a65c7-ac71-47b1-a7f9-3f18b4fe1ca9 to disappear
Oct 29 21:54:01.645: INFO: Pod pod-projected-configmaps-af4a65c7-ac71-47b1-a7f9-3f18b4fe1ca9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:54:01.645: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-2407" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":-1,"completed":23,"skipped":314,"failed":0}

SS
------------------------------
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:53:50.699: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Oct 29 21:53:59.062: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Oct 29 21:53:59.084: INFO: Pod pod-with-poststart-exec-hook still exists
Oct 29 21:54:01.084: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Oct 29 21:54:01.107: INFO: Pod pod-with-poststart-exec-hook still exists
Oct 29 21:54:03.084: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Oct 29 21:54:03.106: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:54:03.106: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4527" for this suite.


 [SLOW TEST:12.522 seconds]
[k8s.io] Container Lifecycle Hook
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":-1,"completed":23,"skipped":455,"failed":0}

SS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:54:03.224: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Oct 29 21:54:03.430: INFO: Waiting up to 5m0s for pod "pod-d264a34b-5d80-4ea0-b2f2-fd69a4dc57da" in namespace "emptydir-4902" to be "Succeeded or Failed"
Oct 29 21:54:03.452: INFO: Pod "pod-d264a34b-5d80-4ea0-b2f2-fd69a4dc57da": Phase="Pending", Reason="", readiness=false. Elapsed: 22.247167ms
Oct 29 21:54:05.474: INFO: Pod "pod-d264a34b-5d80-4ea0-b2f2-fd69a4dc57da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044397148s
Oct 29 21:54:07.496: INFO: Pod "pod-d264a34b-5d80-4ea0-b2f2-fd69a4dc57da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066369123s
STEP: Saw pod success
Oct 29 21:54:07.496: INFO: Pod "pod-d264a34b-5d80-4ea0-b2f2-fd69a4dc57da" satisfied condition "Succeeded or Failed"
Oct 29 21:54:07.518: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-d264a34b-5d80-4ea0-b2f2-fd69a4dc57da container test-container: <nil>
STEP: delete the pod
Oct 29 21:54:07.588: INFO: Waiting for pod pod-d264a34b-5d80-4ea0-b2f2-fd69a4dc57da to disappear
Oct 29 21:54:07.610: INFO: Pod pod-d264a34b-5d80-4ea0-b2f2-fd69a4dc57da no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:54:07.610: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-4902" for this suite.


------------------------------
[BeforeEach] [sig-network] DNS
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:54:01.752: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2159.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-2159.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2159.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2159.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2159.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-2159.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2159.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-2159.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2159.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2159.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-2159.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2159.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-2159.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2159.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-2159.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2159.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-2159.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2159.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct 29 21:54:06.123: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2159.svc.cluster.local from pod dns-2159/dns-test-d6b25258-196b-48cf-83cd-f55a719ef0a9: the server could not find the requested resource (get pods dns-test-d6b25258-196b-48cf-83cd-f55a719ef0a9)
Oct 29 21:54:06.315: INFO: Lookups using dns-2159/dns-test-d6b25258-196b-48cf-83cd-f55a719ef0a9 failed for: [wheezy_udp@dns-test-service-2.dns-2159.svc.cluster.local]

Oct 29 21:54:11.579: INFO: DNS probes using dns-2159/dns-test-d6b25258-196b-48cf-83cd-f55a719ef0a9 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:54:11.666: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "dns-2159" for this suite.


 [SLOW TEST:10.000 seconds]
[sig-network] DNS
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":-1,"completed":24,"skipped":316,"failed":0}

SSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:54:11.764: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should support proxy with --port 0  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting the proxy server
Oct 29 21:54:11.962: INFO: Asynchronously running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:54:12.091: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-3806" for this suite.


------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":-1,"completed":25,"skipped":328,"failed":0}

SSSSSSSSSSSSSSSSSSSS
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":24,"skipped":457,"failed":0}
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:54:07.715: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 29 21:54:08.735: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605229, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605229, loc:(*time.Location)(0x77098a0)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-cbccbf6bb\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605229, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605229, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Oct 29 21:54:10.757: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605229, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605229, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605229, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605229, loc:(*time.Location)(0x77098a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 29 21:54:13.787: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a mutating webhook configuration
Oct 29 21:54:13.894: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:14.047: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:14.151: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:14.255: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:14.356: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:14.457: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:14.549: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:14.647: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:14.748: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:14.850: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:14.947: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:15.049: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:15.151: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:15.246: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:15.348: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:15.447: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:15.547: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:15.649: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:15.747: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:15.846: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:15.946: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:16.045: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:16.146: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:16.252: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:16.359: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:16.450: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:16.590: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:16.659: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:16.776: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:16.859: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:16.956: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:17.046: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:17.156: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:17.257: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:17.349: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:17.448: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:17.548: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:17.648: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:17.749: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:17.858: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:17.947: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:18.046: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:18.146: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:18.247: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:18.345: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:18.447: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:18.547: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:18.648: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:18.746: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:18.846: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:18.948: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:19.046: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:19.149: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:19.247: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:19.348: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:19.446: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:19.545: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:19.649: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:19.752: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:19.853: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:19.948: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:20.046: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:20.145: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:20.248: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:20.345: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:20.446: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:20.550: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:20.646: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:20.746: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:20.846: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:20.946: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:21.046: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:21.167: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:21.247: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:21.360: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:21.448: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:21.548: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:21.649: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:21.750: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:21.848: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:21.950: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:22.046: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:22.146: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:22.245: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:22.347: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:22.445: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:22.546: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:22.646: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:22.745: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:22.849: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:22.946: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:23.047: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:23.145: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:23.246: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:23.408: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:23.545: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:23.646: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:23.747: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:23.846: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:23.948: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:24.046: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:24.147: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:24.246: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:24.358: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:24.446: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:24.547: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:24.648: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:24.746: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:24.846: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:24.947: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:25.045: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:25.146: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:25.246: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:25.347: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:25.446: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:25.546: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:25.647: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:25.749: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:25.847: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:25.946: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:26.046: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:26.148: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:26.248: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:26.354: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:26.451: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:26.551: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:26.648: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:26.748: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:26.848: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:26.950: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:27.050: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:27.153: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:27.246: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:27.346: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:27.446: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:54:27.547: INFO: Waiting for webhook configuration to be ready...
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:54:27.837: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-6605" for this suite.
STEP: Destroying namespace "webhook-6605-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102


 [SLOW TEST:20.365 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:54:12.154: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Update Demo
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:308
[It] should scale a replication controller  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Oct 29 21:54:12.328: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig create -f - --namespace=kubectl-1743'
Oct 29 21:54:13.092: INFO: stderr: ""
Oct 29 21:54:13.092: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Oct 29 21:54:13.092: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1743'
Oct 29 21:54:13.327: INFO: stderr: ""
Oct 29 21:54:13.327: INFO: stdout: "update-demo-nautilus-fsrms update-demo-nautilus-hxc5f "
Oct 29 21:54:13.327: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig get pods update-demo-nautilus-fsrms -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1743'
Oct 29 21:54:13.467: INFO: stderr: ""
Oct 29 21:54:13.467: INFO: stdout: ""
Oct 29 21:54:13.467: INFO: update-demo-nautilus-fsrms is created but not running
Oct 29 21:54:18.467: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1743'
Oct 29 21:54:18.624: INFO: stderr: ""
Oct 29 21:54:18.624: INFO: stdout: "update-demo-nautilus-fsrms update-demo-nautilus-hxc5f "
Oct 29 21:54:18.624: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig get pods update-demo-nautilus-fsrms -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1743'
Oct 29 21:54:18.761: INFO: stderr: ""
Oct 29 21:54:18.761: INFO: stdout: "true"
Oct 29 21:54:18.761: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig get pods update-demo-nautilus-fsrms -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1743'
Oct 29 21:54:18.899: INFO: stderr: ""
Oct 29 21:54:18.899: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Oct 29 21:54:18.899: INFO: validating pod update-demo-nautilus-fsrms
Oct 29 21:54:18.924: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 29 21:54:18.924: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 29 21:54:18.924: INFO: update-demo-nautilus-fsrms is verified up and running
Oct 29 21:54:18.924: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig get pods update-demo-nautilus-hxc5f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1743'
Oct 29 21:54:19.065: INFO: stderr: ""
Oct 29 21:54:19.065: INFO: stdout: "true"
Oct 29 21:54:19.065: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig get pods update-demo-nautilus-hxc5f -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1743'
Oct 29 21:54:19.205: INFO: stderr: ""
Oct 29 21:54:19.205: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Oct 29 21:54:19.205: INFO: validating pod update-demo-nautilus-hxc5f
Oct 29 21:54:19.228: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 29 21:54:19.228: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 29 21:54:19.228: INFO: update-demo-nautilus-hxc5f is verified up and running
STEP: scaling down the replication controller
Oct 29 21:54:19.483: INFO: scanned /home/jeder for discovery docs: <nil>
Oct 29 21:54:19.483: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-1743'
Oct 29 21:54:19.682: INFO: stderr: ""
Oct 29 21:54:19.682: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Oct 29 21:54:19.683: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1743'
Oct 29 21:54:19.837: INFO: stderr: ""
Oct 29 21:54:19.837: INFO: stdout: "update-demo-nautilus-fsrms update-demo-nautilus-hxc5f "
STEP: Replicas for name=update-demo: expected=1 actual=2
Oct 29 21:54:24.838: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1743'
Oct 29 21:54:24.981: INFO: stderr: ""
Oct 29 21:54:24.981: INFO: stdout: "update-demo-nautilus-fsrms "
Oct 29 21:54:24.981: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig get pods update-demo-nautilus-fsrms -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1743'
Oct 29 21:54:25.121: INFO: stderr: ""
Oct 29 21:54:25.121: INFO: stdout: "true"
Oct 29 21:54:25.121: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig get pods update-demo-nautilus-fsrms -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1743'
Oct 29 21:54:25.260: INFO: stderr: ""
Oct 29 21:54:25.260: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Oct 29 21:54:25.260: INFO: validating pod update-demo-nautilus-fsrms
Oct 29 21:54:25.284: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 29 21:54:25.284: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 29 21:54:25.284: INFO: update-demo-nautilus-fsrms is verified up and running
STEP: scaling up the replication controller
Oct 29 21:54:25.531: INFO: scanned /home/jeder for discovery docs: <nil>
Oct 29 21:54:25.531: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-1743'
Oct 29 21:54:26.756: INFO: stderr: ""
Oct 29 21:54:26.756: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Oct 29 21:54:26.756: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1743'
Oct 29 21:54:26.912: INFO: stderr: ""
Oct 29 21:54:26.912: INFO: stdout: "update-demo-nautilus-92qn5 update-demo-nautilus-fsrms "
Oct 29 21:54:26.912: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig get pods update-demo-nautilus-92qn5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1743'
Oct 29 21:54:27.048: INFO: stderr: ""
Oct 29 21:54:27.049: INFO: stdout: ""
Oct 29 21:54:27.049: INFO: update-demo-nautilus-92qn5 is created but not running
Oct 29 21:54:32.049: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1743'
Oct 29 21:54:32.217: INFO: stderr: ""
Oct 29 21:54:32.217: INFO: stdout: "update-demo-nautilus-92qn5 update-demo-nautilus-fsrms "
Oct 29 21:54:32.217: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig get pods update-demo-nautilus-92qn5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1743'
Oct 29 21:54:32.361: INFO: stderr: ""
Oct 29 21:54:32.361: INFO: stdout: "true"
Oct 29 21:54:32.361: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig get pods update-demo-nautilus-92qn5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1743'
Oct 29 21:54:32.503: INFO: stderr: ""
Oct 29 21:54:32.503: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Oct 29 21:54:32.503: INFO: validating pod update-demo-nautilus-92qn5
Oct 29 21:54:32.526: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 29 21:54:32.526: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 29 21:54:32.526: INFO: update-demo-nautilus-92qn5 is verified up and running
Oct 29 21:54:32.526: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig get pods update-demo-nautilus-fsrms -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1743'
Oct 29 21:54:32.663: INFO: stderr: ""
Oct 29 21:54:32.663: INFO: stdout: "true"
Oct 29 21:54:32.663: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig get pods update-demo-nautilus-fsrms -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1743'
Oct 29 21:54:32.807: INFO: stderr: ""
Oct 29 21:54:32.807: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Oct 29 21:54:32.807: INFO: validating pod update-demo-nautilus-fsrms
Oct 29 21:54:32.828: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 29 21:54:32.828: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 29 21:54:32.828: INFO: update-demo-nautilus-fsrms is verified up and running
STEP: using delete to clean up resources
Oct 29 21:54:32.828: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig delete --grace-period=0 --force -f - --namespace=kubectl-1743'
Oct 29 21:54:32.991: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 29 21:54:32.991: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Oct 29 21:54:32.991: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1743'
Oct 29 21:54:33.157: INFO: stderr: "No resources found in kubectl-1743 namespace.\n"
Oct 29 21:54:33.157: INFO: stdout: ""
Oct 29 21:54:33.157: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig get pods -l name=update-demo --namespace=kubectl-1743 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Oct 29 21:54:33.313: INFO: stderr: ""
Oct 29 21:54:33.313: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:54:33.313: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-1743" for this suite.


 [SLOW TEST:21.261 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:306
    should scale a replication controller  [Conformance]
    /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":-1,"completed":26,"skipped":348,"failed":0}

SS
------------------------------
[BeforeEach] [sig-instrumentation] Events API
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:54:33.419: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:54:33.912: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "events-8325" for this suite.


------------------------------
{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":-1,"completed":27,"skipped":350,"failed":0}

SSSSS
------------------------------
[BeforeEach] [k8s.io] Docker Containers
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:54:33.964: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override arguments
Oct 29 21:54:34.174: INFO: Waiting up to 5m0s for pod "client-containers-090ec259-ea08-4a49-b867-614228bdb221" in namespace "containers-2803" to be "Succeeded or Failed"
Oct 29 21:54:34.196: INFO: Pod "client-containers-090ec259-ea08-4a49-b867-614228bdb221": Phase="Pending", Reason="", readiness=false. Elapsed: 22.226918ms
Oct 29 21:54:36.217: INFO: Pod "client-containers-090ec259-ea08-4a49-b867-614228bdb221": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043116776s
Oct 29 21:54:38.239: INFO: Pod "client-containers-090ec259-ea08-4a49-b867-614228bdb221": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065277028s
STEP: Saw pod success
Oct 29 21:54:38.239: INFO: Pod "client-containers-090ec259-ea08-4a49-b867-614228bdb221" satisfied condition "Succeeded or Failed"
Oct 29 21:54:38.260: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod client-containers-090ec259-ea08-4a49-b867-614228bdb221 container test-container: <nil>
STEP: delete the pod
Oct 29 21:54:38.312: INFO: Waiting for pod client-containers-090ec259-ea08-4a49-b867-614228bdb221 to disappear
Oct 29 21:54:38.333: INFO: Pod client-containers-090ec259-ea08-4a49-b867-614228bdb221 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:54:38.333: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "containers-2803" for this suite.


------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":-1,"completed":28,"skipped":355,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Watchers
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:54:38.476: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:54:43.750: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "watch-6269" for this suite.


 [SLOW TEST:5.376 seconds]
[sig-api-machinery] Watchers
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":-1,"completed":29,"skipped":412,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":-1,"completed":25,"skipped":457,"failed":0}
[BeforeEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:54:28.082: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-9660
STEP: creating service affinity-nodeport-transition in namespace services-9660
STEP: creating replication controller affinity-nodeport-transition in namespace services-9660
I1029 21:54:28.338032   53211 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-9660, replica count: 3
I1029 21:54:31.388346   53211 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1029 21:54:34.388519   53211 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 29 21:54:34.491: INFO: Creating new exec pod
Oct 29 21:54:39.677: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=services-9660 execpod-affinitycl7z5 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
Oct 29 21:54:40.024: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Oct 29 21:54:40.024: INFO: stdout: ""
Oct 29 21:54:40.024: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=services-9660 execpod-affinitycl7z5 -- /bin/sh -x -c nc -zv -t -w 2 172.30.208.134 80'
Oct 29 21:54:40.359: INFO: stderr: "+ nc -zv -t -w 2 172.30.208.134 80\nConnection to 172.30.208.134 80 port [tcp/http] succeeded!\n"
Oct 29 21:54:40.359: INFO: stdout: ""
Oct 29 21:54:40.359: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=services-9660 execpod-affinitycl7z5 -- /bin/sh -x -c nc -zv -t -w 2 10.0.158.72 31455'
Oct 29 21:54:40.692: INFO: stderr: "+ nc -zv -t -w 2 10.0.158.72 31455\nConnection to 10.0.158.72 31455 port [tcp/31455] succeeded!\n"
Oct 29 21:54:40.692: INFO: stdout: ""
Oct 29 21:54:40.692: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=services-9660 execpod-affinitycl7z5 -- /bin/sh -x -c nc -zv -t -w 2 10.0.142.212 31455'
Oct 29 21:54:41.029: INFO: stderr: "+ nc -zv -t -w 2 10.0.142.212 31455\nConnection to 10.0.142.212 31455 port [tcp/31455] succeeded!\n"
Oct 29 21:54:41.029: INFO: stdout: ""
Oct 29 21:54:41.080: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=services-9660 execpod-affinitycl7z5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.142.212:31455/ ; done'
Oct 29 21:54:41.482: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31455/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31455/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31455/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31455/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31455/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31455/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31455/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31455/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31455/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31455/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31455/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31455/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31455/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31455/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31455/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31455/\n"
Oct 29 21:54:41.482: INFO: stdout: "\naffinity-nodeport-transition-rtfx4\naffinity-nodeport-transition-7j28w\naffinity-nodeport-transition-5gj2r\naffinity-nodeport-transition-rtfx4\naffinity-nodeport-transition-5gj2r\naffinity-nodeport-transition-rtfx4\naffinity-nodeport-transition-7j28w\naffinity-nodeport-transition-rtfx4\naffinity-nodeport-transition-rtfx4\naffinity-nodeport-transition-7j28w\naffinity-nodeport-transition-rtfx4\naffinity-nodeport-transition-7j28w\naffinity-nodeport-transition-7j28w\naffinity-nodeport-transition-rtfx4\naffinity-nodeport-transition-7j28w\naffinity-nodeport-transition-5gj2r"
Oct 29 21:54:41.482: INFO: Received response from host: affinity-nodeport-transition-rtfx4
Oct 29 21:54:41.482: INFO: Received response from host: affinity-nodeport-transition-7j28w
Oct 29 21:54:41.482: INFO: Received response from host: affinity-nodeport-transition-5gj2r
Oct 29 21:54:41.482: INFO: Received response from host: affinity-nodeport-transition-rtfx4
Oct 29 21:54:41.482: INFO: Received response from host: affinity-nodeport-transition-5gj2r
Oct 29 21:54:41.482: INFO: Received response from host: affinity-nodeport-transition-rtfx4
Oct 29 21:54:41.482: INFO: Received response from host: affinity-nodeport-transition-7j28w
Oct 29 21:54:41.482: INFO: Received response from host: affinity-nodeport-transition-rtfx4
Oct 29 21:54:41.482: INFO: Received response from host: affinity-nodeport-transition-rtfx4
Oct 29 21:54:41.482: INFO: Received response from host: affinity-nodeport-transition-7j28w
Oct 29 21:54:41.482: INFO: Received response from host: affinity-nodeport-transition-rtfx4
Oct 29 21:54:41.482: INFO: Received response from host: affinity-nodeport-transition-7j28w
Oct 29 21:54:41.482: INFO: Received response from host: affinity-nodeport-transition-7j28w
Oct 29 21:54:41.482: INFO: Received response from host: affinity-nodeport-transition-rtfx4
Oct 29 21:54:41.482: INFO: Received response from host: affinity-nodeport-transition-7j28w
Oct 29 21:54:41.482: INFO: Received response from host: affinity-nodeport-transition-5gj2r
Oct 29 21:54:41.529: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=services-9660 execpod-affinitycl7z5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.142.212:31455/ ; done'
Oct 29 21:54:41.940: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31455/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31455/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31455/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31455/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31455/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31455/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31455/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31455/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31455/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31455/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31455/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31455/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31455/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31455/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31455/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31455/\n"
Oct 29 21:54:41.940: INFO: stdout: "\naffinity-nodeport-transition-7j28w\naffinity-nodeport-transition-7j28w\naffinity-nodeport-transition-7j28w\naffinity-nodeport-transition-7j28w\naffinity-nodeport-transition-7j28w\naffinity-nodeport-transition-7j28w\naffinity-nodeport-transition-7j28w\naffinity-nodeport-transition-7j28w\naffinity-nodeport-transition-7j28w\naffinity-nodeport-transition-7j28w\naffinity-nodeport-transition-7j28w\naffinity-nodeport-transition-7j28w\naffinity-nodeport-transition-7j28w\naffinity-nodeport-transition-7j28w\naffinity-nodeport-transition-7j28w\naffinity-nodeport-transition-7j28w"
Oct 29 21:54:41.940: INFO: Received response from host: affinity-nodeport-transition-7j28w
Oct 29 21:54:41.940: INFO: Received response from host: affinity-nodeport-transition-7j28w
Oct 29 21:54:41.940: INFO: Received response from host: affinity-nodeport-transition-7j28w
Oct 29 21:54:41.940: INFO: Received response from host: affinity-nodeport-transition-7j28w
Oct 29 21:54:41.940: INFO: Received response from host: affinity-nodeport-transition-7j28w
Oct 29 21:54:41.940: INFO: Received response from host: affinity-nodeport-transition-7j28w
Oct 29 21:54:41.940: INFO: Received response from host: affinity-nodeport-transition-7j28w
Oct 29 21:54:41.940: INFO: Received response from host: affinity-nodeport-transition-7j28w
Oct 29 21:54:41.940: INFO: Received response from host: affinity-nodeport-transition-7j28w
Oct 29 21:54:41.940: INFO: Received response from host: affinity-nodeport-transition-7j28w
Oct 29 21:54:41.940: INFO: Received response from host: affinity-nodeport-transition-7j28w
Oct 29 21:54:41.940: INFO: Received response from host: affinity-nodeport-transition-7j28w
Oct 29 21:54:41.940: INFO: Received response from host: affinity-nodeport-transition-7j28w
Oct 29 21:54:41.940: INFO: Received response from host: affinity-nodeport-transition-7j28w
Oct 29 21:54:41.940: INFO: Received response from host: affinity-nodeport-transition-7j28w
Oct 29 21:54:41.940: INFO: Received response from host: affinity-nodeport-transition-7j28w
Oct 29 21:54:41.940: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-9660, will wait for the garbage collector to delete the pods
Oct 29 21:54:42.072: INFO: Deleting ReplicationController affinity-nodeport-transition took: 24.924276ms
Oct 29 21:54:42.172: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.271842ms
[AfterEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:54:57.126: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-9660" for this suite.
[AfterEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786


 [SLOW TEST:29.132 seconds]
[sig-network] Services
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":-1,"completed":26,"skipped":457,"failed":0}

SSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] StatefulSet
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:53:25.162: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-8686
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Oct 29 21:53:25.383: INFO: Found 1 stateful pods, waiting for 3
Oct 29 21:53:35.407: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Oct 29 21:53:35.407: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Oct 29 21:53:35.407: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Oct 29 21:53:35.556: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Oct 29 21:53:35.675: INFO: Updating stateful set ss2
Oct 29 21:53:35.722: INFO: Waiting for Pod statefulset-8686/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Oct 29 21:53:45.852: INFO: Found 2 stateful pods, waiting for 3
Oct 29 21:53:55.875: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Oct 29 21:53:55.875: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Oct 29 21:53:55.875: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Oct 29 21:53:55.980: INFO: Updating stateful set ss2
Oct 29 21:53:56.024: INFO: Waiting for Pod statefulset-8686/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Oct 29 21:54:06.068: INFO: Waiting for Pod statefulset-8686/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Oct 29 21:54:16.124: INFO: Updating stateful set ss2
Oct 29 21:54:16.167: INFO: Waiting for StatefulSet statefulset-8686/ss2 to complete update
Oct 29 21:54:16.167: INFO: Waiting for Pod statefulset-8686/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Oct 29 21:54:26.211: INFO: Waiting for StatefulSet statefulset-8686/ss2 to complete update
Oct 29 21:54:26.211: INFO: Waiting for Pod statefulset-8686/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Oct 29 21:54:36.211: INFO: Deleting all statefulset in ns statefulset-8686
Oct 29 21:54:36.232: INFO: Scaling statefulset ss2 to 0
Oct 29 21:55:06.322: INFO: Waiting for statefulset status.replicas updated to 0
Oct 29 21:55:06.343: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:55:06.412: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "statefulset-8686" for this suite.


 [SLOW TEST:101.335 seconds]
[sig-apps] StatefulSet
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":-1,"completed":27,"skipped":375,"failed":0}

SSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Watchers
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:55:06.509: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Oct 29 21:55:06.843: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8810 /api/v1/namespaces/watch-8810/configmaps/e2e-watch-test-resource-version ead22f9a-7bf3-4ff5-99f6-f1c27c12d5ae 235817 0 2020-10-29 21:54:47 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2020-10-29 21:54:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 29 21:55:06.843: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8810 /api/v1/namespaces/watch-8810/configmaps/e2e-watch-test-resource-version ead22f9a-7bf3-4ff5-99f6-f1c27c12d5ae 235819 0 2020-10-29 21:54:47 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2020-10-29 21:54:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:55:06.843: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "watch-8810" for this suite.


------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":-1,"completed":28,"skipped":388,"failed":0}

SSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:54:57.222: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 29 21:54:58.264: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605278, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605278, loc:(*time.Location)(0x77098a0)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-cbccbf6bb\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605278, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605278, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Oct 29 21:55:00.286: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605278, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605278, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605278, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605278, loc:(*time.Location)(0x77098a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 29 21:55:03.330: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a validating webhook configuration
Oct 29 21:55:03.435: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:03.600: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:03.698: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:03.830: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:03.897: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:03.994: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:04.093: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:04.215: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:04.296: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:04.396: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:04.489: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:04.588: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:04.689: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:04.787: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:04.887: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:04.988: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:05.090: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:05.189: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:05.286: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:05.387: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:05.486: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:05.587: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:05.686: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:05.786: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:05.886: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:05.986: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:06.089: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:06.187: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:06.288: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:06.387: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:06.489: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:06.593: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:06.706: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:06.791: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:06.887: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:07.013: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:07.107: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:07.186: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:07.291: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:07.387: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:07.488: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:07.587: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:07.686: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:07.786: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:07.887: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:07.988: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:08.087: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:08.187: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:08.287: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:08.387: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:08.487: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:08.592: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:08.688: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:08.789: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:08.887: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:08.989: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:09.087: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:09.190: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:09.287: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:09.387: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:09.486: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:09.587: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:09.687: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:09.787: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:09.887: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:10.012: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:10.087: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:10.192: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:10.297: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:10.387: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:10.488: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:10.590: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:10.699: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:10.799: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:10.894: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:10.989: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:11.087: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:11.187: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:11.295: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:11.390: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:11.489: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:11.591: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:11.694: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:11.791: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:11.892: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:11.988: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:12.098: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:12.194: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:12.288: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:12.394: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:12.488: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:12.590: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:12.698: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:12.789: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:12.891: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:13.003: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:13.089: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:13.190: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:13.290: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:13.398: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:13.496: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:13.600: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:13.691: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:13.789: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:13.942: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:14.117: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:14.209: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:14.306: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:14.405: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:14.492: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:14.597: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:14.737: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:14.888: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:14.988: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:15.089: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:15.186: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:15.287: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:15.388: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:15.487: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:15.615: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:15.694: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:15.787: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:15.887: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:15.986: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:16.087: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:16.195: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:16.288: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:16.390: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:16.494: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:55:16.588: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:55:16.864: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-4518" for this suite.
STEP: Destroying namespace "webhook-4518-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102


 [SLOW TEST:19.894 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":-1,"completed":27,"skipped":467,"failed":0}

SSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:54:43.883: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Oct 29 21:54:44.033: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:55:19.675: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1145" for this suite.


 [SLOW TEST:35.894 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":-1,"completed":30,"skipped":455,"failed":0}

SSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:55:19.782: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Oct 29 21:55:20.018: INFO: Waiting up to 5m0s for pod "pod-fde88b4f-a6ad-4428-9a02-02039bd4759b" in namespace "emptydir-9004" to be "Succeeded or Failed"
Oct 29 21:55:20.040: INFO: Pod "pod-fde88b4f-a6ad-4428-9a02-02039bd4759b": Phase="Pending", Reason="", readiness=false. Elapsed: 22.070322ms
Oct 29 21:55:22.062: INFO: Pod "pod-fde88b4f-a6ad-4428-9a02-02039bd4759b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043403875s
Oct 29 21:55:24.085: INFO: Pod "pod-fde88b4f-a6ad-4428-9a02-02039bd4759b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066578215s
STEP: Saw pod success
Oct 29 21:55:24.085: INFO: Pod "pod-fde88b4f-a6ad-4428-9a02-02039bd4759b" satisfied condition "Succeeded or Failed"
Oct 29 21:55:24.106: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-fde88b4f-a6ad-4428-9a02-02039bd4759b container test-container: <nil>
STEP: delete the pod
Oct 29 21:55:24.156: INFO: Waiting for pod pod-fde88b4f-a6ad-4428-9a02-02039bd4759b to disappear
Oct 29 21:55:24.177: INFO: Pod pod-fde88b4f-a6ad-4428-9a02-02039bd4759b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:55:24.177: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-9004" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":31,"skipped":460,"failed":0}

SSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:55:24.294: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating cluster-info
Oct 29 21:55:24.396: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig cluster-info'
Oct 29 21:55:24.551: INFO: stderr: ""
Oct 29 21:55:24.551: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://api.jeder-461-cncf2.c2g2.p1.openshiftapps.com:6443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:55:24.551: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-7683" for this suite.


------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":-1,"completed":32,"skipped":468,"failed":0}

SSSSSSSS
------------------------------
[BeforeEach] [sig-apps] Deployment
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:55:17.121: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should delete old replica sets [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 21:55:17.331: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Oct 29 21:55:22.354: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Oct 29 21:55:22.354: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Oct 29 21:55:26.538: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-8673 /apis/apps/v1/namespaces/deployment-8673/deployments/test-cleanup-deployment ad386771-28e3-4753-ab2a-cde6c04d1ee2 236600 1 2020-10-29 21:55:02 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2020-10-29 21:55:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-10-29 21:55:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000836a98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-10-29 21:55:02 +0000 UTC,LastTransitionTime:2020-10-29 21:55:02 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-5d446bdd47" has successfully progressed.,LastUpdateTime:2020-10-29 21:55:06 +0000 UTC,LastTransitionTime:2020-10-29 21:55:02 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Oct 29 21:55:26.560: INFO: New ReplicaSet "test-cleanup-deployment-5d446bdd47" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-5d446bdd47  deployment-8673 /apis/apps/v1/namespaces/deployment-8673/replicasets/test-cleanup-deployment-5d446bdd47 5bdebb3a-1380-4061-8e70-d3f180769688 236589 1 2020-10-29 21:55:02 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment ad386771-28e3-4753-ab2a-cde6c04d1ee2 0xc000836f57 0xc000836f58}] []  [{kube-controller-manager Update apps/v1 2020-10-29 21:55:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ad386771-28e3-4753-ab2a-cde6c04d1ee2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 5d446bdd47,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000836ff8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Oct 29 21:55:26.582: INFO: Pod "test-cleanup-deployment-5d446bdd47-mfbvv" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-5d446bdd47-mfbvv test-cleanup-deployment-5d446bdd47- deployment-8673 /api/v1/namespaces/deployment-8673/pods/test-cleanup-deployment-5d446bdd47-mfbvv 67fb852a-6e9a-40b3-8be3-d7fd008819f6 236588 0 2020-10-29 21:55:02 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.131.1.239"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.131.1.239"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-cleanup-deployment-5d446bdd47 5bdebb3a-1380-4061-8e70-d3f180769688 0xc000837417 0xc000837418}] []  [{kube-controller-manager Update v1 2020-10-29 21:55:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5bdebb3a-1380-4061-8e70-d3f180769688\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-10-29 21:55:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.1.239\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}} {multus Update v1 2020-10-29 21:55:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6qbpb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6qbpb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6qbpb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-142-212.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c90,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-jnnmm,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:55:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:55:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:55:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:55:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.142.212,PodIP:10.131.1.239,StartTime:2020-10-29 21:55:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-10-29 21:55:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:cri-o://5ddcfc5e386e0e99c3964fc2af40fb2102cb23e23b3fffb8c7ef0cd4e4666ce6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.1.239,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:55:26.582: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "deployment-8673" for this suite.


 [SLOW TEST:9.570 seconds]
[sig-apps] Deployment
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":-1,"completed":28,"skipped":470,"failed":0}

SSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:55:24.605: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl label
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1333
STEP: creating the pod
Oct 29 21:55:24.730: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig create -f - --namespace=kubectl-2243'
Oct 29 21:55:25.490: INFO: stderr: ""
Oct 29 21:55:25.490: INFO: stdout: "pod/pause created\n"
Oct 29 21:55:25.490: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Oct 29 21:55:25.490: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-2243" to be "running and ready"
Oct 29 21:55:25.511: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 20.522408ms
Oct 29 21:55:27.532: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042196303s
Oct 29 21:55:29.553: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.063314618s
Oct 29 21:55:29.553: INFO: Pod "pause" satisfied condition "running and ready"
Oct 29 21:55:29.553: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: adding the label testing-label with value testing-label-value to a pod
Oct 29 21:55:29.554: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig label pods pause testing-label=testing-label-value --namespace=kubectl-2243'
Oct 29 21:55:29.730: INFO: stderr: ""
Oct 29 21:55:29.730: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Oct 29 21:55:29.730: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig get pod pause -L testing-label --namespace=kubectl-2243'
Oct 29 21:55:29.866: INFO: stderr: ""
Oct 29 21:55:29.866: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Oct 29 21:55:29.866: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig label pods pause testing-label- --namespace=kubectl-2243'
Oct 29 21:55:30.037: INFO: stderr: ""
Oct 29 21:55:30.037: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Oct 29 21:55:30.037: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig get pod pause -L testing-label --namespace=kubectl-2243'
Oct 29 21:55:30.170: INFO: stderr: ""
Oct 29 21:55:30.170: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] Kubectl label
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1340
STEP: using delete to clean up resources
Oct 29 21:55:30.170: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig delete --grace-period=0 --force -f - --namespace=kubectl-2243'
Oct 29 21:55:30.334: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 29 21:55:30.334: INFO: stdout: "pod \"pause\" force deleted\n"
Oct 29 21:55:30.334: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig get rc,svc -l name=pause --no-headers --namespace=kubectl-2243'
Oct 29 21:55:30.491: INFO: stderr: "No resources found in kubectl-2243 namespace.\n"
Oct 29 21:55:30.491: INFO: stdout: ""
Oct 29 21:55:30.491: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig get pods -l name=pause --namespace=kubectl-2243 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Oct 29 21:55:30.630: INFO: stderr: ""
Oct 29 21:55:30.630: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:55:30.630: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-2243" for this suite.


 [SLOW TEST:6.130 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1330
    should update the label on a resource  [Conformance]
    /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":-1,"completed":33,"skipped":476,"failed":0}

SSSSS
------------------------------
[BeforeEach] [sig-storage] Subpath
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:55:06.899: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-sgg2
STEP: Creating a pod to test atomic-volume-subpath
Oct 29 21:55:07.118: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-sgg2" in namespace "subpath-2635" to be "Succeeded or Failed"
Oct 29 21:55:07.139: INFO: Pod "pod-subpath-test-configmap-sgg2": Phase="Pending", Reason="", readiness=false. Elapsed: 21.389647ms
Oct 29 21:55:09.162: INFO: Pod "pod-subpath-test-configmap-sgg2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044329788s
Oct 29 21:55:11.184: INFO: Pod "pod-subpath-test-configmap-sgg2": Phase="Running", Reason="", readiness=true. Elapsed: 4.06632029s
Oct 29 21:55:13.206: INFO: Pod "pod-subpath-test-configmap-sgg2": Phase="Running", Reason="", readiness=true. Elapsed: 6.088012311s
Oct 29 21:55:15.229: INFO: Pod "pod-subpath-test-configmap-sgg2": Phase="Running", Reason="", readiness=true. Elapsed: 8.111075212s
Oct 29 21:55:17.252: INFO: Pod "pod-subpath-test-configmap-sgg2": Phase="Running", Reason="", readiness=true. Elapsed: 10.134061927s
Oct 29 21:55:19.274: INFO: Pod "pod-subpath-test-configmap-sgg2": Phase="Running", Reason="", readiness=true. Elapsed: 12.156128738s
Oct 29 21:55:21.296: INFO: Pod "pod-subpath-test-configmap-sgg2": Phase="Running", Reason="", readiness=true. Elapsed: 14.178388274s
Oct 29 21:55:23.319: INFO: Pod "pod-subpath-test-configmap-sgg2": Phase="Running", Reason="", readiness=true. Elapsed: 16.201007073s
Oct 29 21:55:25.345: INFO: Pod "pod-subpath-test-configmap-sgg2": Phase="Running", Reason="", readiness=true. Elapsed: 18.22710641s
Oct 29 21:55:27.367: INFO: Pod "pod-subpath-test-configmap-sgg2": Phase="Running", Reason="", readiness=true. Elapsed: 20.249020514s
Oct 29 21:55:29.389: INFO: Pod "pod-subpath-test-configmap-sgg2": Phase="Running", Reason="", readiness=true. Elapsed: 22.270745465s
Oct 29 21:55:31.412: INFO: Pod "pod-subpath-test-configmap-sgg2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.294203318s
STEP: Saw pod success
Oct 29 21:55:31.412: INFO: Pod "pod-subpath-test-configmap-sgg2" satisfied condition "Succeeded or Failed"
Oct 29 21:55:31.435: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-subpath-test-configmap-sgg2 container test-container-subpath-configmap-sgg2: <nil>
STEP: delete the pod
Oct 29 21:55:31.495: INFO: Waiting for pod pod-subpath-test-configmap-sgg2 to disappear
Oct 29 21:55:31.516: INFO: Pod pod-subpath-test-configmap-sgg2 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-sgg2
Oct 29 21:55:31.516: INFO: Deleting pod "pod-subpath-test-configmap-sgg2" in namespace "subpath-2635"
[AfterEach] [sig-storage] Subpath
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:55:31.539: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "subpath-2635" for this suite.


 [SLOW TEST:24.748 seconds]
[sig-storage] Subpath
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":-1,"completed":29,"skipped":397,"failed":0}

SSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:55:30.741: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 21:55:30.888: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-fcbbff41-3d07-47a3-8fb2-2c8594b8f95f
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:55:35.054: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-7993" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":-1,"completed":34,"skipped":481,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:55:35.161: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Oct 29 21:55:35.325: INFO: Waiting up to 5m0s for pod "pod-e6e5df41-6b0a-4147-8c55-02430d187c69" in namespace "emptydir-1791" to be "Succeeded or Failed"
Oct 29 21:55:35.346: INFO: Pod "pod-e6e5df41-6b0a-4147-8c55-02430d187c69": Phase="Pending", Reason="", readiness=false. Elapsed: 21.28227ms
Oct 29 21:55:37.367: INFO: Pod "pod-e6e5df41-6b0a-4147-8c55-02430d187c69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042452957s
Oct 29 21:55:39.388: INFO: Pod "pod-e6e5df41-6b0a-4147-8c55-02430d187c69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063411793s
STEP: Saw pod success
Oct 29 21:55:39.388: INFO: Pod "pod-e6e5df41-6b0a-4147-8c55-02430d187c69" satisfied condition "Succeeded or Failed"
Oct 29 21:55:39.410: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-e6e5df41-6b0a-4147-8c55-02430d187c69 container test-container: <nil>
STEP: delete the pod
Oct 29 21:55:39.461: INFO: Waiting for pod pod-e6e5df41-6b0a-4147-8c55-02430d187c69 to disappear
Oct 29 21:55:39.481: INFO: Pod pod-e6e5df41-6b0a-4147-8c55-02430d187c69 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:55:39.481: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-1791" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":35,"skipped":508,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:55:39.615: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Oct 29 21:55:39.783: INFO: Waiting up to 5m0s for pod "downwardapi-volume-69594485-cb39-4100-b4f6-4e3e3f8d8d6d" in namespace "downward-api-8386" to be "Succeeded or Failed"
Oct 29 21:55:39.824: INFO: Pod "downwardapi-volume-69594485-cb39-4100-b4f6-4e3e3f8d8d6d": Phase="Pending", Reason="", readiness=false. Elapsed: 40.651395ms
Oct 29 21:55:41.845: INFO: Pod "downwardapi-volume-69594485-cb39-4100-b4f6-4e3e3f8d8d6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.062329728s
Oct 29 21:55:43.867: INFO: Pod "downwardapi-volume-69594485-cb39-4100-b4f6-4e3e3f8d8d6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.083790431s
STEP: Saw pod success
Oct 29 21:55:43.867: INFO: Pod "downwardapi-volume-69594485-cb39-4100-b4f6-4e3e3f8d8d6d" satisfied condition "Succeeded or Failed"
Oct 29 21:55:43.892: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod downwardapi-volume-69594485-cb39-4100-b4f6-4e3e3f8d8d6d container client-container: <nil>
STEP: delete the pod
Oct 29 21:55:43.950: INFO: Waiting for pod downwardapi-volume-69594485-cb39-4100-b4f6-4e3e3f8d8d6d to disappear
Oct 29 21:55:43.971: INFO: Pod downwardapi-volume-69594485-cb39-4100-b4f6-4e3e3f8d8d6d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:55:43.971: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-8386" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":-1,"completed":36,"skipped":550,"failed":0}

SSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:55:44.087: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Oct 29 21:55:44.270: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4438d557-26e4-41de-a536-bcb139423b38" in namespace "projected-6376" to be "Succeeded or Failed"
Oct 29 21:55:44.291: INFO: Pod "downwardapi-volume-4438d557-26e4-41de-a536-bcb139423b38": Phase="Pending", Reason="", readiness=false. Elapsed: 21.520077ms
Oct 29 21:55:46.313: INFO: Pod "downwardapi-volume-4438d557-26e4-41de-a536-bcb139423b38": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042906733s
Oct 29 21:55:48.336: INFO: Pod "downwardapi-volume-4438d557-26e4-41de-a536-bcb139423b38": Phase="Pending", Reason="", readiness=false. Elapsed: 4.066346477s
Oct 29 21:55:50.358: INFO: Pod "downwardapi-volume-4438d557-26e4-41de-a536-bcb139423b38": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.088094206s
STEP: Saw pod success
Oct 29 21:55:50.358: INFO: Pod "downwardapi-volume-4438d557-26e4-41de-a536-bcb139423b38" satisfied condition "Succeeded or Failed"
Oct 29 21:55:50.379: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod downwardapi-volume-4438d557-26e4-41de-a536-bcb139423b38 container client-container: <nil>
STEP: delete the pod
Oct 29 21:55:50.437: INFO: Waiting for pod downwardapi-volume-4438d557-26e4-41de-a536-bcb139423b38 to disappear
Oct 29 21:55:50.457: INFO: Pod downwardapi-volume-4438d557-26e4-41de-a536-bcb139423b38 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:55:50.457: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-6376" for this suite.


 [SLOW TEST:6.470 seconds]
[sig-storage] Projected downwardAPI
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should provide container's memory request [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":-1,"completed":37,"skipped":564,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Subpath
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:55:26.697: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-projected-zccc
STEP: Creating a pod to test atomic-volume-subpath
Oct 29 21:55:26.947: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-zccc" in namespace "subpath-3463" to be "Succeeded or Failed"
Oct 29 21:55:26.969: INFO: Pod "pod-subpath-test-projected-zccc": Phase="Pending", Reason="", readiness=false. Elapsed: 21.817029ms
Oct 29 21:55:28.991: INFO: Pod "pod-subpath-test-projected-zccc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044126388s
Oct 29 21:55:31.013: INFO: Pod "pod-subpath-test-projected-zccc": Phase="Running", Reason="", readiness=true. Elapsed: 4.066287002s
Oct 29 21:55:33.036: INFO: Pod "pod-subpath-test-projected-zccc": Phase="Running", Reason="", readiness=true. Elapsed: 6.088783563s
Oct 29 21:55:35.059: INFO: Pod "pod-subpath-test-projected-zccc": Phase="Running", Reason="", readiness=true. Elapsed: 8.111766781s
Oct 29 21:55:37.081: INFO: Pod "pod-subpath-test-projected-zccc": Phase="Running", Reason="", readiness=true. Elapsed: 10.133862357s
Oct 29 21:55:39.103: INFO: Pod "pod-subpath-test-projected-zccc": Phase="Running", Reason="", readiness=true. Elapsed: 12.155913153s
Oct 29 21:55:41.125: INFO: Pod "pod-subpath-test-projected-zccc": Phase="Running", Reason="", readiness=true. Elapsed: 14.177829545s
Oct 29 21:55:43.147: INFO: Pod "pod-subpath-test-projected-zccc": Phase="Running", Reason="", readiness=true. Elapsed: 16.199637859s
Oct 29 21:55:45.168: INFO: Pod "pod-subpath-test-projected-zccc": Phase="Running", Reason="", readiness=true. Elapsed: 18.22138079s
Oct 29 21:55:47.190: INFO: Pod "pod-subpath-test-projected-zccc": Phase="Running", Reason="", readiness=true. Elapsed: 20.243469057s
Oct 29 21:55:49.213: INFO: Pod "pod-subpath-test-projected-zccc": Phase="Running", Reason="", readiness=true. Elapsed: 22.266014025s
Oct 29 21:55:51.235: INFO: Pod "pod-subpath-test-projected-zccc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.287653439s
STEP: Saw pod success
Oct 29 21:55:51.235: INFO: Pod "pod-subpath-test-projected-zccc" satisfied condition "Succeeded or Failed"
Oct 29 21:55:51.256: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-subpath-test-projected-zccc container test-container-subpath-projected-zccc: <nil>
STEP: delete the pod
Oct 29 21:55:51.327: INFO: Waiting for pod pod-subpath-test-projected-zccc to disappear
Oct 29 21:55:51.349: INFO: Pod pod-subpath-test-projected-zccc no longer exists
STEP: Deleting pod pod-subpath-test-projected-zccc
Oct 29 21:55:51.349: INFO: Deleting pod "pod-subpath-test-projected-zccc" in namespace "subpath-3463"
[AfterEach] [sig-storage] Subpath
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:55:51.370: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "subpath-3463" for this suite.


 [SLOW TEST:24.787 seconds]
[sig-storage] Subpath
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":-1,"completed":29,"skipped":475,"failed":0}

SSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] StatefulSet
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:55:31.657: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-5432
[It] should have a working scale subresource [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating statefulset ss in namespace statefulset-5432
Oct 29 21:55:31.947: INFO: Found 0 stateful pods, waiting for 1
Oct 29 21:55:41.970: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Oct 29 21:55:42.082: INFO: Deleting all statefulset in ns statefulset-5432
Oct 29 21:55:42.103: INFO: Scaling statefulset ss to 0
Oct 29 21:55:52.191: INFO: Waiting for statefulset status.replicas updated to 0
Oct 29 21:55:52.213: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:55:52.280: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "statefulset-5432" for this suite.


 [SLOW TEST:20.708 seconds]
[sig-apps] StatefulSet
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should have a working scale subresource [Conformance]
    /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":-1,"completed":30,"skipped":407,"failed":0}
[BeforeEach] [sig-network] DNS
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:55:52.366: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-187.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-187.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-187.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-187.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-187.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-187.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct 29 21:55:56.824: INFO: DNS probes using dns-187/dns-test-0070721a-1f28-4d90-8a3b-73c434438a6b succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:55:56.893: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "dns-187" for this suite.


------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":-1,"completed":31,"skipped":407,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:55:50.591: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Oct 29 21:55:55.481: INFO: Successfully updated pod "annotationupdatee2a25682-d806-455c-8c37-b7d1b984bf77"
[AfterEach] [sig-storage] Downward API volume
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:55:57.528: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-6629" for this suite.


 [SLOW TEST:7.040 seconds]
[sig-storage] Downward API volume
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update annotations on modification [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":-1,"completed":38,"skipped":616,"failed":0}

SSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected configMap
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:55:57.014: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-577a4f14-174c-4282-be6e-d6a350faf59d
STEP: Creating a pod to test consume configMaps
Oct 29 21:55:57.212: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0cf5ce92-9afd-4186-a223-abe01018b0eb" in namespace "projected-3964" to be "Succeeded or Failed"
Oct 29 21:55:57.233: INFO: Pod "pod-projected-configmaps-0cf5ce92-9afd-4186-a223-abe01018b0eb": Phase="Pending", Reason="", readiness=false. Elapsed: 21.310651ms
Oct 29 21:55:59.255: INFO: Pod "pod-projected-configmaps-0cf5ce92-9afd-4186-a223-abe01018b0eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043488909s
Oct 29 21:56:01.278: INFO: Pod "pod-projected-configmaps-0cf5ce92-9afd-4186-a223-abe01018b0eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065668398s
STEP: Saw pod success
Oct 29 21:56:01.278: INFO: Pod "pod-projected-configmaps-0cf5ce92-9afd-4186-a223-abe01018b0eb" satisfied condition "Succeeded or Failed"
Oct 29 21:56:01.299: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-projected-configmaps-0cf5ce92-9afd-4186-a223-abe01018b0eb container projected-configmap-volume-test: <nil>
STEP: delete the pod
Oct 29 21:56:01.355: INFO: Waiting for pod pod-projected-configmaps-0cf5ce92-9afd-4186-a223-abe01018b0eb to disappear
Oct 29 21:56:01.376: INFO: Pod pod-projected-configmaps-0cf5ce92-9afd-4186-a223-abe01018b0eb no longer exists
[AfterEach] [sig-storage] Projected configMap
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:56:01.376: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-3964" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":-1,"completed":32,"skipped":448,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:55:57.640: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Oct 29 21:55:57.822: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3bd73027-7835-4f40-abb5-68d43c18df45" in namespace "downward-api-1351" to be "Succeeded or Failed"
Oct 29 21:55:57.844: INFO: Pod "downwardapi-volume-3bd73027-7835-4f40-abb5-68d43c18df45": Phase="Pending", Reason="", readiness=false. Elapsed: 22.271409ms
Oct 29 21:55:59.866: INFO: Pod "downwardapi-volume-3bd73027-7835-4f40-abb5-68d43c18df45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043802931s
Oct 29 21:56:01.904: INFO: Pod "downwardapi-volume-3bd73027-7835-4f40-abb5-68d43c18df45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.081622854s
STEP: Saw pod success
Oct 29 21:56:01.904: INFO: Pod "downwardapi-volume-3bd73027-7835-4f40-abb5-68d43c18df45" satisfied condition "Succeeded or Failed"
Oct 29 21:56:01.924: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod downwardapi-volume-3bd73027-7835-4f40-abb5-68d43c18df45 container client-container: <nil>
STEP: delete the pod
Oct 29 21:56:01.991: INFO: Waiting for pod downwardapi-volume-3bd73027-7835-4f40-abb5-68d43c18df45 to disappear
Oct 29 21:56:02.012: INFO: Pod downwardapi-volume-3bd73027-7835-4f40-abb5-68d43c18df45 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:56:02.012: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-1351" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":-1,"completed":39,"skipped":625,"failed":0}

SSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-auth] ServiceAccounts
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:56:02.126: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
Oct 29 21:56:02.920: INFO: created pod pod-service-account-defaultsa
Oct 29 21:56:02.920: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Oct 29 21:56:02.954: INFO: created pod pod-service-account-mountsa
Oct 29 21:56:02.954: INFO: pod pod-service-account-mountsa service account token volume mount: true
Oct 29 21:56:02.990: INFO: created pod pod-service-account-nomountsa
Oct 29 21:56:02.990: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Oct 29 21:56:03.022: INFO: created pod pod-service-account-defaultsa-mountspec
Oct 29 21:56:03.022: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Oct 29 21:56:03.052: INFO: created pod pod-service-account-mountsa-mountspec
Oct 29 21:56:03.052: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Oct 29 21:56:03.090: INFO: created pod pod-service-account-nomountsa-mountspec
Oct 29 21:56:03.090: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Oct 29 21:56:03.133: INFO: created pod pod-service-account-defaultsa-nomountspec
Oct 29 21:56:03.133: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Oct 29 21:56:03.178: INFO: created pod pod-service-account-mountsa-nomountspec
Oct 29 21:56:03.178: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Oct 29 21:56:03.288: INFO: created pod pod-service-account-nomountsa-nomountspec
Oct 29 21:56:03.288: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:56:03.288: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "svcaccounts-2699" for this suite.


------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":-1,"completed":40,"skipped":638,"failed":0}

SSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Secrets
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:56:01.515: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-12616f2e-1930-4cc6-b9a2-e8c447be37b5
STEP: Creating a pod to test consume secrets
Oct 29 21:56:01.974: INFO: Waiting up to 5m0s for pod "pod-secrets-d2c8b13a-f575-4d0f-aa0a-c9ccd66c9f30" in namespace "secrets-1866" to be "Succeeded or Failed"
Oct 29 21:56:01.996: INFO: Pod "pod-secrets-d2c8b13a-f575-4d0f-aa0a-c9ccd66c9f30": Phase="Pending", Reason="", readiness=false. Elapsed: 21.976758ms
Oct 29 21:56:04.018: INFO: Pod "pod-secrets-d2c8b13a-f575-4d0f-aa0a-c9ccd66c9f30": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043429825s
Oct 29 21:56:06.040: INFO: Pod "pod-secrets-d2c8b13a-f575-4d0f-aa0a-c9ccd66c9f30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066223087s
STEP: Saw pod success
Oct 29 21:56:06.040: INFO: Pod "pod-secrets-d2c8b13a-f575-4d0f-aa0a-c9ccd66c9f30" satisfied condition "Succeeded or Failed"
Oct 29 21:56:06.062: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-secrets-d2c8b13a-f575-4d0f-aa0a-c9ccd66c9f30 container secret-volume-test: <nil>
STEP: delete the pod
Oct 29 21:56:06.122: INFO: Waiting for pod pod-secrets-d2c8b13a-f575-4d0f-aa0a-c9ccd66c9f30 to disappear
Oct 29 21:56:06.153: INFO: Pod pod-secrets-d2c8b13a-f575-4d0f-aa0a-c9ccd66c9f30 no longer exists
[AfterEach] [sig-storage] Secrets
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:56:06.153: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-1866" for this suite.
STEP: Destroying namespace "secret-namespace-2996" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":-1,"completed":33,"skipped":473,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:56:03.365: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 29 21:56:04.257: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605344, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605344, loc:(*time.Location)(0x77098a0)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-cbccbf6bb\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605344, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605344, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Oct 29 21:56:06.279: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605344, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605344, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605344, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605344, loc:(*time.Location)(0x77098a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 29 21:56:08.279: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605344, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605344, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605344, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605344, loc:(*time.Location)(0x77098a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 29 21:56:11.308: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 21:56:11.334: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3699-crds.webhook.example.com via the AdmissionRegistration API
Oct 29 21:56:12.005: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:12.164: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:12.261: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:12.364: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:12.459: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:12.577: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:12.705: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:12.770: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:12.871: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:12.979: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:13.066: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:13.167: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:13.267: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:13.394: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:13.461: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:13.561: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:13.665: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:13.760: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:13.867: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:13.959: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:14.064: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:14.161: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:14.267: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:14.360: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:14.469: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:14.576: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:14.680: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:14.769: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:14.884: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:14.963: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:15.062: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:15.164: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:15.287: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:15.371: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:15.479: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:15.560: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:15.665: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:15.779: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:15.862: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:15.958: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:16.057: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:16.163: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:16.280: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:16.367: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:16.471: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:16.573: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:16.663: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:16.761: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:16.866: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:16.958: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:17.058: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:17.167: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:17.277: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:17.362: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:17.459: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:17.558: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:17.658: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:17.756: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:17.862: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:17.961: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:18.056: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:18.158: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:18.257: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:18.356: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:18.456: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:18.557: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:18.657: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:18.755: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:18.859: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:18.958: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:19.056: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:19.204: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:19.255: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:19.356: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:19.456: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:19.556: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:19.661: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:19.765: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:19.857: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:19.958: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:20.056: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:20.159: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:20.257: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:20.355: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:20.504: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:20.555: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:20.657: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:20.756: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:20.858: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:20.957: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:21.057: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:21.157: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:21.257: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:21.365: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:21.458: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:21.584: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:21.667: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:21.826: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:21.961: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:22.056: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:22.157: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:22.256: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:22.358: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:22.455: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:22.556: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:56:23.445: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-9483" for this suite.
STEP: Destroying namespace "webhook-9483-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102


 [SLOW TEST:20.392 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":-1,"completed":41,"skipped":656,"failed":0}

SSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:56:06.317: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl logs
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1415
STEP: creating an pod
Oct 29 21:56:06.504: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.20 --namespace=kubectl-5826 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Oct 29 21:56:06.675: INFO: stderr: ""
Oct 29 21:56:06.675: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Waiting for log generator to start.
Oct 29 21:56:06.675: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Oct 29 21:56:06.675: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-5826" to be "running and ready, or succeeded"
Oct 29 21:56:06.697: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 22.317256ms
Oct 29 21:56:08.723: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047644085s
Oct 29 21:56:10.745: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.069656844s
Oct 29 21:56:10.745: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Oct 29 21:56:10.745: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Oct 29 21:56:10.745: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig logs logs-generator logs-generator --namespace=kubectl-5826'
Oct 29 21:56:10.916: INFO: stderr: ""
Oct 29 21:56:10.916: INFO: stdout: "I1029 21:55:50.147020       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/pzf8 570\nI1029 21:55:50.347132       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/pwtx 427\nI1029 21:55:50.547111       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/nff 458\nI1029 21:55:50.747114       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/mc48 544\nI1029 21:55:50.947127       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/2s4 242\nI1029 21:55:51.147113       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/t5s 289\n"
STEP: limiting log lines
Oct 29 21:56:10.916: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig logs logs-generator logs-generator --namespace=kubectl-5826 --tail=1'
Oct 29 21:56:11.085: INFO: stderr: ""
Oct 29 21:56:11.085: INFO: stdout: "I1029 21:55:51.347126       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/p6m5 229\n"
Oct 29 21:56:11.085: INFO: got output "I1029 21:55:51.347126       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/p6m5 229\n"
STEP: limiting log bytes
Oct 29 21:56:11.086: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig logs logs-generator logs-generator --namespace=kubectl-5826 --limit-bytes=1'
Oct 29 21:56:11.246: INFO: stderr: ""
Oct 29 21:56:11.246: INFO: stdout: "I"
Oct 29 21:56:11.246: INFO: got output "I"
STEP: exposing timestamps
Oct 29 21:56:11.246: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig logs logs-generator logs-generator --namespace=kubectl-5826 --tail=1 --timestamps'
Oct 29 21:56:11.434: INFO: stderr: ""
Oct 29 21:56:11.434: INFO: stdout: "2020-10-29T21:55:51.747169937Z I1029 21:55:51.747124       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/f7k 466\n"
Oct 29 21:56:11.434: INFO: got output "2020-10-29T21:55:51.747169937Z I1029 21:55:51.747124       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/f7k 466\n"
STEP: restricting to a time range
Oct 29 21:56:13.935: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig logs logs-generator logs-generator --namespace=kubectl-5826 --since=1s'
Oct 29 21:56:14.104: INFO: stderr: ""
Oct 29 21:56:14.104: INFO: stdout: "I1029 21:55:53.547106       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/hw4 475\nI1029 21:55:53.747101       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/vcx5 572\nI1029 21:55:53.947109       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/l94w 556\nI1029 21:55:54.147118       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/5vg 584\nI1029 21:55:54.347105       1 logs_generator.go:76] 21 POST /api/v1/namespaces/ns/pods/wf2 555\n"
Oct 29 21:56:14.104: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig logs logs-generator logs-generator --namespace=kubectl-5826 --since=24h'
Oct 29 21:56:14.281: INFO: stderr: ""
Oct 29 21:56:14.281: INFO: stdout: "I1029 21:55:50.147020       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/pzf8 570\nI1029 21:55:50.347132       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/pwtx 427\nI1029 21:55:50.547111       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/nff 458\nI1029 21:55:50.747114       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/mc48 544\nI1029 21:55:50.947127       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/2s4 242\nI1029 21:55:51.147113       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/t5s 289\nI1029 21:55:51.347126       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/p6m5 229\nI1029 21:55:51.547116       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/mf8 445\nI1029 21:55:51.747124       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/f7k 466\nI1029 21:55:51.947113       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/psds 542\nI1029 21:55:52.147120       1 logs_generator.go:76] 10 GET /api/v1/namespaces/default/pods/v59r 383\nI1029 21:55:52.347119       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/c7db 580\nI1029 21:55:52.547215       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/lvw 209\nI1029 21:55:52.747104       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/cdvs 244\nI1029 21:55:52.947116       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/wc6 269\nI1029 21:55:53.147109       1 logs_generator.go:76] 15 GET /api/v1/namespaces/default/pods/9kjq 560\nI1029 21:55:53.347104       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/5lh 286\nI1029 21:55:53.547106       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/hw4 475\nI1029 21:55:53.747101       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/vcx5 572\nI1029 21:55:53.947109       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/l94w 556\nI1029 21:55:54.147118       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/5vg 584\nI1029 21:55:54.347105       1 logs_generator.go:76] 21 POST /api/v1/namespaces/ns/pods/wf2 555\nI1029 21:55:54.547110       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/default/pods/8hxv 339\n"
[AfterEach] Kubectl logs
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
Oct 29 21:56:14.281: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig delete pod logs-generator --namespace=kubectl-5826'
Oct 29 21:56:26.572: INFO: stderr: ""
Oct 29 21:56:26.572: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:56:26.572: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-5826" for this suite.


 [SLOW TEST:20.377 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1411
    should be able to retrieve and filter logs  [Conformance]
    /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":-1,"completed":34,"skipped":517,"failed":0}

SSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Probing container
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:52:29.770: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-11a90155-14af-4f0f-b4f8-974ae99b010d in namespace container-probe-7523
Oct 29 21:52:33.994: INFO: Started pod busybox-11a90155-14af-4f0f-b4f8-974ae99b010d in namespace container-probe-7523
STEP: checking the pod's current state and verifying that restartCount is present
Oct 29 21:52:34.015: INFO: Initial restart count of pod busybox-11a90155-14af-4f0f-b4f8-974ae99b010d is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:56:34.853: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-probe-7523" for this suite.


 [SLOW TEST:245.184 seconds]
[k8s.io] Probing container
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":-1,"completed":21,"skipped":484,"failed":0}

SSSS
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:56:26.706: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:56:44.129: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "resourcequota-8418" for this suite.


 [SLOW TEST:17.527 seconds]
[sig-api-machinery] ResourceQuota
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":-1,"completed":35,"skipped":535,"failed":0}

SSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:56:34.959: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to create a functioning NodePort service [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service nodeport-test with type=NodePort in namespace services-6550
STEP: creating replication controller nodeport-test in namespace services-6550
I1029 21:56:35.145468   53212 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-6550, replica count: 2
I1029 21:56:38.195979   53212 runners.go:190] nodeport-test Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1029 21:56:41.196320   53212 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 29 21:56:41.196: INFO: Creating new exec pod
Oct 29 21:56:46.353: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=services-6550 execpodq7vlw -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Oct 29 21:56:46.705: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Oct 29 21:56:46.705: INFO: stdout: ""
Oct 29 21:56:46.706: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=services-6550 execpodq7vlw -- /bin/sh -x -c nc -zv -t -w 2 172.30.37.221 80'
Oct 29 21:56:47.039: INFO: stderr: "+ nc -zv -t -w 2 172.30.37.221 80\nConnection to 172.30.37.221 80 port [tcp/http] succeeded!\n"
Oct 29 21:56:47.039: INFO: stdout: ""
Oct 29 21:56:47.039: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=services-6550 execpodq7vlw -- /bin/sh -x -c nc -zv -t -w 2 10.0.234.238 32496'
Oct 29 21:56:47.377: INFO: stderr: "+ nc -zv -t -w 2 10.0.234.238 32496\nConnection to 10.0.234.238 32496 port [tcp/32496] succeeded!\n"
Oct 29 21:56:47.377: INFO: stdout: ""
Oct 29 21:56:47.377: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=services-6550 execpodq7vlw -- /bin/sh -x -c nc -zv -t -w 2 10.0.158.72 32496'
Oct 29 21:56:47.707: INFO: stderr: "+ nc -zv -t -w 2 10.0.158.72 32496\nConnection to 10.0.158.72 32496 port [tcp/32496] succeeded!\n"
Oct 29 21:56:47.707: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:56:47.707: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-6550" for this suite.
[AfterEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786


 [SLOW TEST:12.851 seconds]
[sig-network] Services
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":-1,"completed":22,"skipped":488,"failed":0}

SSSS
------------------------------
[BeforeEach] [sig-apps] Deployment
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:56:44.242: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 21:56:44.402: INFO: Creating deployment "test-recreate-deployment"
Oct 29 21:56:44.428: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Oct 29 21:56:44.474: INFO: Waiting deployment "test-recreate-deployment" to complete
Oct 29 21:56:44.495: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605384, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605384, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605384, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605384, loc:(*time.Location)(0x77098a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 29 21:56:46.516: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605384, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605384, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605384, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605384, loc:(*time.Location)(0x77098a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 29 21:56:48.517: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Oct 29 21:56:48.562: INFO: Updating deployment test-recreate-deployment
Oct 29 21:56:48.562: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Oct 29 21:56:48.664: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-6089 /apis/apps/v1/namespaces/deployment-6089/deployments/test-recreate-deployment b6b881c7-a7bc-406b-a5ad-885f12a46ab7 239498 2 2020-10-29 21:56:24 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-10-29 21:56:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-10-29 21:56:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002968638 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-10-29 21:56:29 +0000 UTC,LastTransitionTime:2020-10-29 21:56:29 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f79dd4667" is progressing.,LastUpdateTime:2020-10-29 21:56:29 +0000 UTC,LastTransitionTime:2020-10-29 21:56:24 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Oct 29 21:56:48.686: INFO: New ReplicaSet "test-recreate-deployment-f79dd4667" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f79dd4667  deployment-6089 /apis/apps/v1/namespaces/deployment-6089/replicasets/test-recreate-deployment-f79dd4667 5e15be11-a647-4069-84aa-91740219be7d 239496 1 2020-10-29 21:56:29 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment b6b881c7-a7bc-406b-a5ad-885f12a46ab7 0xc002968b30 0xc002968b31}] []  [{kube-controller-manager Update apps/v1 2020-10-29 21:56:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6b881c7-a7bc-406b-a5ad-885f12a46ab7\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f79dd4667,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002968ba8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Oct 29 21:56:48.686: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Oct 29 21:56:48.686: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-c96cf48f  deployment-6089 /apis/apps/v1/namespaces/deployment-6089/replicasets/test-recreate-deployment-c96cf48f 4cc8fec9-cc4f-4daf-a6a4-706de15a7f05 239487 2 2020-10-29 21:56:24 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment b6b881c7-a7bc-406b-a5ad-885f12a46ab7 0xc002968a3f 0xc002968a50}] []  [{kube-controller-manager Update apps/v1 2020-10-29 21:56:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6b881c7-a7bc-406b-a5ad-885f12a46ab7\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c96cf48f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002968ac8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Oct 29 21:56:48.707: INFO: Pod "test-recreate-deployment-f79dd4667-2dq8c" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f79dd4667-2dq8c test-recreate-deployment-f79dd4667- deployment-6089 /api/v1/namespaces/deployment-6089/pods/test-recreate-deployment-f79dd4667-2dq8c d17183bc-04ec-4785-bed9-33051d31b4c8 239499 0 2020-10-29 21:56:29 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-recreate-deployment-f79dd4667 5e15be11-a647-4069-84aa-91740219be7d 0xc002969087 0xc002969088}] []  [{kube-controller-manager Update v1 2020-10-29 21:56:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e15be11-a647-4069-84aa-91740219be7d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-10-29 21:56:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hnqz9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hnqz9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hnqz9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-142-212.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c91,c65,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-grdw5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:56:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:56:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:56:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 21:56:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.142.212,PodIP:,StartTime:2020-10-29 21:56:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:56:48.707: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "deployment-6089" for this suite.


------------------------------
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":-1,"completed":36,"skipped":546,"failed":0}

SSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected secret
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:56:47.814: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-e1a98ab8-17e4-4968-85ce-490d57d7bf6b
STEP: Creating a pod to test consume secrets
Oct 29 21:56:48.032: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d31135c2-0ab4-464e-8380-1d9f877a3a3c" in namespace "projected-9491" to be "Succeeded or Failed"
Oct 29 21:56:48.080: INFO: Pod "pod-projected-secrets-d31135c2-0ab4-464e-8380-1d9f877a3a3c": Phase="Pending", Reason="", readiness=false. Elapsed: 47.98002ms
Oct 29 21:56:50.101: INFO: Pod "pod-projected-secrets-d31135c2-0ab4-464e-8380-1d9f877a3a3c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.069067336s
Oct 29 21:56:52.123: INFO: Pod "pod-projected-secrets-d31135c2-0ab4-464e-8380-1d9f877a3a3c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.090700286s
STEP: Saw pod success
Oct 29 21:56:52.123: INFO: Pod "pod-projected-secrets-d31135c2-0ab4-464e-8380-1d9f877a3a3c" satisfied condition "Succeeded or Failed"
Oct 29 21:56:52.144: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-projected-secrets-d31135c2-0ab4-464e-8380-1d9f877a3a3c container projected-secret-volume-test: <nil>
STEP: delete the pod
Oct 29 21:56:52.212: INFO: Waiting for pod pod-projected-secrets-d31135c2-0ab4-464e-8380-1d9f877a3a3c to disappear
Oct 29 21:56:52.233: INFO: Pod pod-projected-secrets-d31135c2-0ab4-464e-8380-1d9f877a3a3c no longer exists
[AfterEach] [sig-storage] Projected secret
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:56:52.233: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-9491" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":23,"skipped":492,"failed":0}

SSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Secrets
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:56:52.347: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-d83b53d8-391f-4d2f-9c73-4ed6121873c8
STEP: Creating a pod to test consume secrets
Oct 29 21:56:52.548: INFO: Waiting up to 5m0s for pod "pod-secrets-9936edfd-6ffc-4ce0-a91b-51af306bd6bb" in namespace "secrets-6984" to be "Succeeded or Failed"
Oct 29 21:56:52.572: INFO: Pod "pod-secrets-9936edfd-6ffc-4ce0-a91b-51af306bd6bb": Phase="Pending", Reason="", readiness=false. Elapsed: 23.082639ms
Oct 29 21:56:54.593: INFO: Pod "pod-secrets-9936edfd-6ffc-4ce0-a91b-51af306bd6bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044872375s
Oct 29 21:56:56.619: INFO: Pod "pod-secrets-9936edfd-6ffc-4ce0-a91b-51af306bd6bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.070521507s
STEP: Saw pod success
Oct 29 21:56:56.619: INFO: Pod "pod-secrets-9936edfd-6ffc-4ce0-a91b-51af306bd6bb" satisfied condition "Succeeded or Failed"
Oct 29 21:56:56.660: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-secrets-9936edfd-6ffc-4ce0-a91b-51af306bd6bb container secret-volume-test: <nil>
STEP: delete the pod
Oct 29 21:56:56.776: INFO: Waiting for pod pod-secrets-9936edfd-6ffc-4ce0-a91b-51af306bd6bb to disappear
Oct 29 21:56:56.804: INFO: Pod pod-secrets-9936edfd-6ffc-4ce0-a91b-51af306bd6bb no longer exists
[AfterEach] [sig-storage] Secrets
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:56:56.804: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-6984" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":-1,"completed":24,"skipped":502,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-node] ConfigMap
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:56:56.935: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap that has name configmap-test-emptyKey-8c7895b6-6587-4da8-9f93-76da0bfbef3d
[AfterEach] [sig-node] ConfigMap
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:56:57.118: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-2842" for this suite.


------------------------------
{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":-1,"completed":25,"skipped":528,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:56:57.193: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-3102e327-63e4-47e7-915c-bea88535fefb
STEP: Creating a pod to test consume configMaps
Oct 29 21:56:57.439: INFO: Waiting up to 5m0s for pod "pod-configmaps-083c08d5-6b76-4c98-8250-33768bcd2d90" in namespace "configmap-8011" to be "Succeeded or Failed"
Oct 29 21:56:57.463: INFO: Pod "pod-configmaps-083c08d5-6b76-4c98-8250-33768bcd2d90": Phase="Pending", Reason="", readiness=false. Elapsed: 23.671289ms
Oct 29 21:56:59.485: INFO: Pod "pod-configmaps-083c08d5-6b76-4c98-8250-33768bcd2d90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045330767s
Oct 29 21:57:01.510: INFO: Pod "pod-configmaps-083c08d5-6b76-4c98-8250-33768bcd2d90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.070353027s
STEP: Saw pod success
Oct 29 21:57:01.510: INFO: Pod "pod-configmaps-083c08d5-6b76-4c98-8250-33768bcd2d90" satisfied condition "Succeeded or Failed"
Oct 29 21:57:01.531: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-configmaps-083c08d5-6b76-4c98-8250-33768bcd2d90 container configmap-volume-test: <nil>
STEP: delete the pod
Oct 29 21:57:01.606: INFO: Waiting for pod pod-configmaps-083c08d5-6b76-4c98-8250-33768bcd2d90 to disappear
Oct 29 21:57:01.628: INFO: Pod pod-configmaps-083c08d5-6b76-4c98-8250-33768bcd2d90 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:57:01.628: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-8011" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":-1,"completed":26,"skipped":552,"failed":0}

SSSS
------------------------------
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:57:01.736: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Oct 29 21:57:02.783: INFO: starting watch
STEP: patching
STEP: updating
Oct 29 21:57:02.850: INFO: waiting for watch events with expected annotations
Oct 29 21:57:02.850: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:57:03.110: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "certificates-5054" for this suite.


------------------------------
{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":-1,"completed":27,"skipped":556,"failed":0}

SSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:57:03.203: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if v1 is in available api versions  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating api versions
Oct 29 21:57:03.337: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig api-versions'
Oct 29 21:57:03.711: INFO: stderr: ""
Oct 29 21:57:03.711: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps.openshift.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nauthorization.openshift.io/v1\nautoscaling.openshift.io/v1\nautoscaling.openshift.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncloudcredential.openshift.io/v1\ncloudingress.managed.openshift.io/v1alpha1\nconfig.openshift.io/v1\nconsole.openshift.io/v1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nflowcontrol.apiserver.k8s.io/v1alpha1\nhelm.openshift.io/v1beta1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\ningress.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nmachine.openshift.io/v1beta1\nmachineconfiguration.openshift.io/v1\nmanaged.openshift.io/v1alpha1\nmanaged.openshift.io/v1alpha2\nmetal3.io/v1alpha1\nmetrics.k8s.io/v1beta1\nmigration.k8s.io/v1alpha1\nmonitoring.coreos.com/v1\nnetwork.openshift.io/v1\nnetwork.operator.openshift.io/v1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\noauth.openshift.io/v1\noperator.openshift.io/v1\noperator.openshift.io/v1alpha1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\npackages.operators.coreos.com/v1\npolicy/v1beta1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nsecurity.internal.openshift.io/v1\nsecurity.openshift.io/v1\nsnapshot.storage.k8s.io/v1beta1\nsplunkforwarder.managed.openshift.io/v1alpha1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nupgrade.managed.openshift.io/v1alpha1\nuser.openshift.io/v1\nv1\nvelero.io/v1\nwhereabouts.cni.cncf.io/v1alpha1\n"
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:57:03.711: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-9938" for this suite.


------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":-1,"completed":28,"skipped":567,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:57:03.820: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Oct 29 21:57:03.971: INFO: Waiting up to 5m0s for pod "pod-09268e76-fe0a-464f-9c9b-d7ab9dd47c42" in namespace "emptydir-110" to be "Succeeded or Failed"
Oct 29 21:57:03.993: INFO: Pod "pod-09268e76-fe0a-464f-9c9b-d7ab9dd47c42": Phase="Pending", Reason="", readiness=false. Elapsed: 21.542352ms
Oct 29 21:57:06.015: INFO: Pod "pod-09268e76-fe0a-464f-9c9b-d7ab9dd47c42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043571452s
Oct 29 21:57:08.037: INFO: Pod "pod-09268e76-fe0a-464f-9c9b-d7ab9dd47c42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065350106s
STEP: Saw pod success
Oct 29 21:57:08.037: INFO: Pod "pod-09268e76-fe0a-464f-9c9b-d7ab9dd47c42" satisfied condition "Succeeded or Failed"
Oct 29 21:57:08.058: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-09268e76-fe0a-464f-9c9b-d7ab9dd47c42 container test-container: <nil>
STEP: delete the pod
Oct 29 21:57:08.113: INFO: Waiting for pod pod-09268e76-fe0a-464f-9c9b-d7ab9dd47c42 to disappear
Oct 29 21:57:08.134: INFO: Pod pod-09268e76-fe0a-464f-9c9b-d7ab9dd47c42 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:57:08.134: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-110" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":29,"skipped":620,"failed":0}

SSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:57:08.266: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Oct 29 21:57:08.478: INFO: Waiting up to 5m0s for pod "pod-2a20f41d-0712-415d-ab59-d554a6ba2069" in namespace "emptydir-6920" to be "Succeeded or Failed"
Oct 29 21:57:08.499: INFO: Pod "pod-2a20f41d-0712-415d-ab59-d554a6ba2069": Phase="Pending", Reason="", readiness=false. Elapsed: 20.907823ms
Oct 29 21:57:10.521: INFO: Pod "pod-2a20f41d-0712-415d-ab59-d554a6ba2069": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043589799s
Oct 29 21:57:12.543: INFO: Pod "pod-2a20f41d-0712-415d-ab59-d554a6ba2069": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.06549906s
STEP: Saw pod success
Oct 29 21:57:12.543: INFO: Pod "pod-2a20f41d-0712-415d-ab59-d554a6ba2069" satisfied condition "Succeeded or Failed"
Oct 29 21:57:12.565: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-2a20f41d-0712-415d-ab59-d554a6ba2069 container test-container: <nil>
STEP: delete the pod
Oct 29 21:57:12.619: INFO: Waiting for pod pod-2a20f41d-0712-415d-ab59-d554a6ba2069 to disappear
Oct 29 21:57:12.639: INFO: Pod pod-2a20f41d-0712-415d-ab59-d554a6ba2069 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:57:12.639: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-6920" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":30,"skipped":631,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:57:12.776: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should add annotations for pods in rc  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Oct 29 21:57:12.908: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig create -f - --namespace=kubectl-9630'
Oct 29 21:57:13.572: INFO: stderr: ""
Oct 29 21:57:13.572: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Oct 29 21:57:14.595: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 29 21:57:14.595: INFO: Found 0 / 1
Oct 29 21:57:15.594: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 29 21:57:15.594: INFO: Found 0 / 1
Oct 29 21:57:16.596: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 29 21:57:16.596: INFO: Found 1 / 1
Oct 29 21:57:16.596: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Oct 29 21:57:16.617: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 29 21:57:16.617: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Oct 29 21:57:16.618: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig patch pod agnhost-primary-5xk97 --namespace=kubectl-9630 -p {"metadata":{"annotations":{"x":"y"}}}'
Oct 29 21:57:16.790: INFO: stderr: ""
Oct 29 21:57:16.790: INFO: stdout: "pod/agnhost-primary-5xk97 patched\n"
STEP: checking annotations
Oct 29 21:57:16.811: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 29 21:57:16.811: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:57:16.811: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-9630" for this suite.


------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":-1,"completed":31,"skipped":670,"failed":0}

SSSSSS
------------------------------
[BeforeEach] [sig-apps] ReplicationController
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:57:16.921: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 21:57:17.048: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Oct 29 21:57:18.218: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:57:18.240: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "replication-controller-5355" for this suite.


------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":-1,"completed":32,"skipped":676,"failed":0}

SSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:56:48.818: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 29 21:56:49.587: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605389, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605389, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605389, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605389, loc:(*time.Location)(0x77098a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 29 21:56:51.608: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605389, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605389, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605389, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605389, loc:(*time.Location)(0x77098a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 29 21:56:54.644: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
Oct 29 21:56:54.752: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:54.904: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:55.005: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:55.105: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:55.202: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:55.308: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:55.404: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:55.513: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:55.607: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:55.709: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:55.805: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:55.902: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:56.001: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:56.104: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:56.202: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:56.312: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:56.406: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:56.512: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:56.619: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:56.716: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:56.820: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:56.907: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:57.011: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:57.111: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:57.204: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:57.319: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:57.455: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:57.609: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:57.705: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:57.811: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:57.905: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:58.003: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:58.101: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:58.201: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:58.303: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:58.402: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:58.502: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:58.603: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:58.702: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:58.803: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:58.904: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:59.003: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:59.102: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:59.201: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:59.302: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:59.456: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:59.603: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:59.703: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:59.804: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:56:59.903: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:00.003: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:00.103: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:00.203: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:00.306: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:00.415: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:00.504: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:00.602: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:00.702: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:00.810: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:00.909: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:01.002: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:01.110: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:01.205: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:01.302: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:01.407: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:01.503: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:01.608: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:01.715: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:01.810: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:01.912: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:02.010: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:02.103: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:02.222: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:02.343: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:02.406: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:02.510: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:02.604: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:02.702: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:02.806: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:02.905: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:03.002: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:03.102: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:03.202: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:03.331: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:03.405: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:03.509: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:03.605: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:03.717: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:03.801: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:03.923: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:04.005: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:04.102: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:04.203: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:04.302: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:04.403: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:04.502: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:04.602: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:04.704: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:04.805: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:04.902: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:05.003: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:05.104: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:05.205: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:05.304: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:05.410: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:05.503: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:05.602: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:05.703: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:05.804: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:05.903: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:06.002: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:06.102: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:06.204: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:06.304: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:06.403: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:06.506: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:06.616: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:06.710: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:06.804: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:06.906: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:07.010: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:07.101: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:07.201: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:07.303: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:07.420: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:07.503: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:07.602: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:07.703: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:07.802: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:07.904: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:08.001: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:08.105: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:08.203: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:08.314: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:08.419: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:08.505: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:57:08.604: INFO: Waiting for webhook configuration to be ready...
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:57:18.989: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-2894" for this suite.
STEP: Destroying namespace "webhook-2894-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102


 [SLOW TEST:30.408 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":-1,"completed":37,"skipped":554,"failed":0}

SSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-node] Downward API
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:57:18.356: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Oct 29 21:57:18.561: INFO: Waiting up to 5m0s for pod "downward-api-76b115f7-799c-4949-a2a6-0607429a6cbc" in namespace "downward-api-2180" to be "Succeeded or Failed"
Oct 29 21:57:18.583: INFO: Pod "downward-api-76b115f7-799c-4949-a2a6-0607429a6cbc": Phase="Pending", Reason="", readiness=false. Elapsed: 21.559243ms
Oct 29 21:57:20.607: INFO: Pod "downward-api-76b115f7-799c-4949-a2a6-0607429a6cbc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045966183s
Oct 29 21:57:22.631: INFO: Pod "downward-api-76b115f7-799c-4949-a2a6-0607429a6cbc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.069792299s
STEP: Saw pod success
Oct 29 21:57:22.631: INFO: Pod "downward-api-76b115f7-799c-4949-a2a6-0607429a6cbc" satisfied condition "Succeeded or Failed"
Oct 29 21:57:22.655: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod downward-api-76b115f7-799c-4949-a2a6-0607429a6cbc container dapi-container: <nil>
STEP: delete the pod
Oct 29 21:57:22.712: INFO: Waiting for pod downward-api-76b115f7-799c-4949-a2a6-0607429a6cbc to disappear
Oct 29 21:57:22.734: INFO: Pod downward-api-76b115f7-799c-4949-a2a6-0607429a6cbc no longer exists
[AfterEach] [sig-node] Downward API
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:57:22.734: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-2180" for this suite.


------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":-1,"completed":33,"skipped":685,"failed":0}

SSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:57:19.240: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Oct 29 21:57:19.421: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e73ee510-d0f9-4f52-9136-6e26a9a405dc" in namespace "projected-9105" to be "Succeeded or Failed"
Oct 29 21:57:19.443: INFO: Pod "downwardapi-volume-e73ee510-d0f9-4f52-9136-6e26a9a405dc": Phase="Pending", Reason="", readiness=false. Elapsed: 21.766514ms
Oct 29 21:57:21.465: INFO: Pod "downwardapi-volume-e73ee510-d0f9-4f52-9136-6e26a9a405dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044014891s
Oct 29 21:57:23.486: INFO: Pod "downwardapi-volume-e73ee510-d0f9-4f52-9136-6e26a9a405dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065027574s
STEP: Saw pod success
Oct 29 21:57:23.486: INFO: Pod "downwardapi-volume-e73ee510-d0f9-4f52-9136-6e26a9a405dc" satisfied condition "Succeeded or Failed"
Oct 29 21:57:23.507: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod downwardapi-volume-e73ee510-d0f9-4f52-9136-6e26a9a405dc container client-container: <nil>
STEP: delete the pod
Oct 29 21:57:23.597: INFO: Waiting for pod downwardapi-volume-e73ee510-d0f9-4f52-9136-6e26a9a405dc to disappear
Oct 29 21:57:23.619: INFO: Pod downwardapi-volume-e73ee510-d0f9-4f52-9136-6e26a9a405dc no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:57:23.619: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-9105" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":-1,"completed":38,"skipped":571,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Aggregator
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:57:22.857: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Oct 29 21:57:22.988: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the sample API server.
Oct 29 21:57:23.611: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605423, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605423, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605423, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605423, loc:(*time.Location)(0x77098a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 29 21:57:25.634: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605423, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605423, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605423, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605423, loc:(*time.Location)(0x77098a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 29 21:57:42.705: INFO: Waited 15.045321892s for the sample-apiserver to be ready to handle requests.
I1029 21:57:43.906591   53212 request.go:645] Throttling request took 1.046016597s, request: GET:https://api.jeder-461-cncf2.c2g2.p1.openshiftapps.com:6443/apis/operators.coreos.com/v1alpha2?timeout=32s
[AfterEach] [sig-api-machinery] Aggregator
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:57:45.687: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "aggregator-855" for this suite.


 [SLOW TEST:22.916 seconds]
[sig-api-machinery] Aggregator
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":-1,"completed":34,"skipped":702,"failed":0}

SSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] ReplicationController
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:57:45.786: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating replication controller my-hostname-basic-a0ae4e5a-d9ed-475e-8dea-51bf59ea0667
Oct 29 21:57:46.023: INFO: Pod name my-hostname-basic-a0ae4e5a-d9ed-475e-8dea-51bf59ea0667: Found 0 pods out of 1
Oct 29 21:57:51.046: INFO: Pod name my-hostname-basic-a0ae4e5a-d9ed-475e-8dea-51bf59ea0667: Found 1 pods out of 1
Oct 29 21:57:51.046: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-a0ae4e5a-d9ed-475e-8dea-51bf59ea0667" are running
Oct 29 21:57:51.068: INFO: Pod "my-hostname-basic-a0ae4e5a-d9ed-475e-8dea-51bf59ea0667-t2z2c" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-10-29 21:57:26 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-10-29 21:57:28 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-10-29 21:57:28 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-10-29 21:57:26 +0000 UTC Reason: Message:}])
Oct 29 21:57:51.068: INFO: Trying to dial the pod
Oct 29 21:57:56.136: INFO: Controller my-hostname-basic-a0ae4e5a-d9ed-475e-8dea-51bf59ea0667: Got expected result from replica 1 [my-hostname-basic-a0ae4e5a-d9ed-475e-8dea-51bf59ea0667-t2z2c]: "my-hostname-basic-a0ae4e5a-d9ed-475e-8dea-51bf59ea0667-t2z2c", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:57:56.136: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "replication-controller-6093" for this suite.


 [SLOW TEST:10.458 seconds]
[sig-apps] ReplicationController
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":-1,"completed":35,"skipped":717,"failed":0}

SSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Pods
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:57:56.260: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 21:57:56.391: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:58:00.676: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-748" for this suite.


------------------------------
[BeforeEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:57:23.742: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-4441
Oct 29 21:57:29.141: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=services-4441 kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Oct 29 21:57:29.497: INFO: rc: 7
Oct 29 21:57:29.523: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Oct 29 21:57:29.545: INFO: Pod kube-proxy-mode-detector still exists
Oct 29 21:57:31.545: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Oct 29 21:57:31.569: INFO: Pod kube-proxy-mode-detector no longer exists
Oct 29 21:57:31.569: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=services-4441 kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-nodeport-timeout in namespace services-4441
STEP: creating replication controller affinity-nodeport-timeout in namespace services-4441
I1029 21:57:31.634709   53213 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-4441, replica count: 3
I1029 21:57:34.685027   53213 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 29 21:57:34.788: INFO: Creating new exec pod
Oct 29 21:57:39.955: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=services-4441 execpod-affinity9sv2f -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
Oct 29 21:57:40.286: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Oct 29 21:57:40.286: INFO: stdout: ""
Oct 29 21:57:40.287: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=services-4441 execpod-affinity9sv2f -- /bin/sh -x -c nc -zv -t -w 2 172.30.112.179 80'
Oct 29 21:57:40.612: INFO: stderr: "+ nc -zv -t -w 2 172.30.112.179 80\nConnection to 172.30.112.179 80 port [tcp/http] succeeded!\n"
Oct 29 21:57:40.612: INFO: stdout: ""
Oct 29 21:57:40.612: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=services-4441 execpod-affinity9sv2f -- /bin/sh -x -c nc -zv -t -w 2 10.0.157.225 30516'
Oct 29 21:57:40.940: INFO: stderr: "+ nc -zv -t -w 2 10.0.157.225 30516\nConnection to 10.0.157.225 30516 port [tcp/30516] succeeded!\n"
Oct 29 21:57:40.940: INFO: stdout: ""
Oct 29 21:57:40.940: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=services-4441 execpod-affinity9sv2f -- /bin/sh -x -c nc -zv -t -w 2 10.0.158.72 30516'
Oct 29 21:57:41.277: INFO: stderr: "+ nc -zv -t -w 2 10.0.158.72 30516\nConnection to 10.0.158.72 30516 port [tcp/30516] succeeded!\n"
Oct 29 21:57:41.277: INFO: stdout: ""
Oct 29 21:57:41.277: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=services-4441 execpod-affinity9sv2f -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.142.212:30516/ ; done'
Oct 29 21:57:41.670: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:30516/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:30516/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:30516/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:30516/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:30516/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:30516/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:30516/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:30516/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:30516/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:30516/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:30516/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:30516/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:30516/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:30516/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:30516/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:30516/\n"
Oct 29 21:57:41.670: INFO: stdout: "\naffinity-nodeport-timeout-654cj\naffinity-nodeport-timeout-654cj\naffinity-nodeport-timeout-654cj\naffinity-nodeport-timeout-654cj\naffinity-nodeport-timeout-654cj\naffinity-nodeport-timeout-654cj\naffinity-nodeport-timeout-654cj\naffinity-nodeport-timeout-654cj\naffinity-nodeport-timeout-654cj\naffinity-nodeport-timeout-654cj\naffinity-nodeport-timeout-654cj\naffinity-nodeport-timeout-654cj\naffinity-nodeport-timeout-654cj\naffinity-nodeport-timeout-654cj\naffinity-nodeport-timeout-654cj\naffinity-nodeport-timeout-654cj"
Oct 29 21:57:41.670: INFO: Received response from host: affinity-nodeport-timeout-654cj
Oct 29 21:57:41.670: INFO: Received response from host: affinity-nodeport-timeout-654cj
Oct 29 21:57:41.670: INFO: Received response from host: affinity-nodeport-timeout-654cj
Oct 29 21:57:41.670: INFO: Received response from host: affinity-nodeport-timeout-654cj
Oct 29 21:57:41.670: INFO: Received response from host: affinity-nodeport-timeout-654cj
Oct 29 21:57:41.670: INFO: Received response from host: affinity-nodeport-timeout-654cj
Oct 29 21:57:41.670: INFO: Received response from host: affinity-nodeport-timeout-654cj
Oct 29 21:57:41.670: INFO: Received response from host: affinity-nodeport-timeout-654cj
Oct 29 21:57:41.670: INFO: Received response from host: affinity-nodeport-timeout-654cj
Oct 29 21:57:41.670: INFO: Received response from host: affinity-nodeport-timeout-654cj
Oct 29 21:57:41.670: INFO: Received response from host: affinity-nodeport-timeout-654cj
Oct 29 21:57:41.670: INFO: Received response from host: affinity-nodeport-timeout-654cj
Oct 29 21:57:41.670: INFO: Received response from host: affinity-nodeport-timeout-654cj
Oct 29 21:57:41.670: INFO: Received response from host: affinity-nodeport-timeout-654cj
Oct 29 21:57:41.670: INFO: Received response from host: affinity-nodeport-timeout-654cj
Oct 29 21:57:41.670: INFO: Received response from host: affinity-nodeport-timeout-654cj
Oct 29 21:57:41.670: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=services-4441 execpod-affinity9sv2f -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.142.212:30516/'
Oct 29 21:57:42.030: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.142.212:30516/\n"
Oct 29 21:57:42.030: INFO: stdout: "affinity-nodeport-timeout-654cj"
Oct 29 21:57:57.031: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=services-4441 execpod-affinity9sv2f -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.142.212:30516/'
Oct 29 21:57:57.377: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.142.212:30516/\n"
Oct 29 21:57:57.377: INFO: stdout: "affinity-nodeport-timeout-w4tkp"
Oct 29 21:57:57.377: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-4441, will wait for the garbage collector to delete the pods
Oct 29 21:57:57.508: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 26.047933ms
Oct 29 21:57:57.608: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.290915ms
[AfterEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:58:06.760: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-4441" for this suite.
[AfterEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786


 [SLOW TEST:43.125 seconds]
[sig-network] Services
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":-1,"completed":39,"skipped":595,"failed":0}

S
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:58:06.870: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should create and stop a working application  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating all guestbook components
Oct 29 21:58:07.043: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Oct 29 21:58:07.043: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig create -f - --namespace=kubectl-4357'
Oct 29 21:58:07.725: INFO: stderr: ""
Oct 29 21:58:07.725: INFO: stdout: "service/agnhost-replica created\n"
Oct 29 21:58:07.725: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Oct 29 21:58:07.725: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig create -f - --namespace=kubectl-4357'
Oct 29 21:58:08.429: INFO: stderr: ""
Oct 29 21:58:08.429: INFO: stdout: "service/agnhost-primary created\n"
Oct 29 21:58:08.429: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Oct 29 21:58:08.429: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig create -f - --namespace=kubectl-4357'
Oct 29 21:58:09.121: INFO: stderr: ""
Oct 29 21:58:09.121: INFO: stdout: "service/frontend created\n"
Oct 29 21:58:09.121: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Oct 29 21:58:09.122: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig create -f - --namespace=kubectl-4357'
Oct 29 21:58:09.761: INFO: stderr: ""
Oct 29 21:58:09.761: INFO: stdout: "deployment.apps/frontend created\n"
Oct 29 21:58:09.761: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Oct 29 21:58:09.761: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig create -f - --namespace=kubectl-4357'
Oct 29 21:58:10.416: INFO: stderr: ""
Oct 29 21:58:10.416: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Oct 29 21:58:10.416: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Oct 29 21:58:10.416: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig create -f - --namespace=kubectl-4357'
Oct 29 21:58:10.897: INFO: stderr: ""
Oct 29 21:58:10.897: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Oct 29 21:58:10.897: INFO: Waiting for all frontend pods to be Running.
Oct 29 21:58:15.947: INFO: Waiting for frontend to serve content.
Oct 29 21:58:15.977: INFO: Trying to add a new entry to the guestbook.
Oct 29 21:58:16.007: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Oct 29 21:58:16.034: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig delete --grace-period=0 --force -f - --namespace=kubectl-4357'
Oct 29 21:58:16.205: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 29 21:58:16.205: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Oct 29 21:58:16.205: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig delete --grace-period=0 --force -f - --namespace=kubectl-4357'
Oct 29 21:58:16.386: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 29 21:58:16.386: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Oct 29 21:58:16.386: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig delete --grace-period=0 --force -f - --namespace=kubectl-4357'
Oct 29 21:58:16.595: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 29 21:58:16.595: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Oct 29 21:58:16.595: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig delete --grace-period=0 --force -f - --namespace=kubectl-4357'
Oct 29 21:58:16.752: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 29 21:58:16.752: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Oct 29 21:58:16.753: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig delete --grace-period=0 --force -f - --namespace=kubectl-4357'
Oct 29 21:58:16.916: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 29 21:58:16.916: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Oct 29 21:58:16.916: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig delete --grace-period=0 --force -f - --namespace=kubectl-4357'
Oct 29 21:58:17.073: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 29 21:58:17.073: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:58:17.073: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-4357" for this suite.


 [SLOW TEST:10.306 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:351
    should create and stop a working application  [Conformance]
    /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":-1,"completed":40,"skipped":596,"failed":0}
[BeforeEach] [sig-auth] ServiceAccounts
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:58:17.178: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
STEP: reading a file in the container
Oct 29 21:58:21.982: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl exec --namespace=svcaccounts-5586 pod-service-account-e94d2d51-331f-4dd3-bac5-ef7291b44bd5 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Oct 29 21:58:22.332: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl exec --namespace=svcaccounts-5586 pod-service-account-e94d2d51-331f-4dd3-bac5-ef7291b44bd5 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Oct 29 21:58:22.668: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl exec --namespace=svcaccounts-5586 pod-service-account-e94d2d51-331f-4dd3-bac5-ef7291b44bd5 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:58:23.025: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "svcaccounts-5586" for this suite.


 [SLOW TEST:5.950 seconds]
[sig-auth] ServiceAccounts
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":-1,"completed":41,"skipped":596,"failed":0}

SSSSSSSSSSSSSSSSSS
------------------------------
{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":-1,"completed":36,"skipped":736,"failed":0}
[BeforeEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:58:00.783: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-9324
STEP: creating service affinity-nodeport in namespace services-9324
STEP: creating replication controller affinity-nodeport in namespace services-9324
I1029 21:58:01.007101   53212 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-9324, replica count: 3
I1029 21:58:04.057594   53212 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1029 21:58:07.057940   53212 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 29 21:58:07.164: INFO: Creating new exec pod
Oct 29 21:58:12.329: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=services-9324 execpod-affinityfwsg2 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
Oct 29 21:58:12.760: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Oct 29 21:58:12.760: INFO: stdout: ""
Oct 29 21:58:12.760: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=services-9324 execpod-affinityfwsg2 -- /bin/sh -x -c nc -zv -t -w 2 172.30.141.222 80'
Oct 29 21:58:13.114: INFO: stderr: "+ nc -zv -t -w 2 172.30.141.222 80\nConnection to 172.30.141.222 80 port [tcp/http] succeeded!\n"
Oct 29 21:58:13.114: INFO: stdout: ""
Oct 29 21:58:13.114: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=services-9324 execpod-affinityfwsg2 -- /bin/sh -x -c nc -zv -t -w 2 10.0.234.238 31638'
Oct 29 21:58:13.463: INFO: stderr: "+ nc -zv -t -w 2 10.0.234.238 31638\nConnection to 10.0.234.238 31638 port [tcp/31638] succeeded!\n"
Oct 29 21:58:13.463: INFO: stdout: ""
Oct 29 21:58:13.463: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=services-9324 execpod-affinityfwsg2 -- /bin/sh -x -c nc -zv -t -w 2 10.0.142.212 31638'
Oct 29 21:58:13.827: INFO: stderr: "+ nc -zv -t -w 2 10.0.142.212 31638\nConnection to 10.0.142.212 31638 port [tcp/31638] succeeded!\n"
Oct 29 21:58:13.827: INFO: stdout: ""
Oct 29 21:58:13.827: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=services-9324 execpod-affinityfwsg2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.142.212:31638/ ; done'
Oct 29 21:58:14.245: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31638/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.142.212:31638/\n"
Oct 29 21:58:14.245: INFO: stdout: "\naffinity-nodeport-282pm\naffinity-nodeport-282pm\naffinity-nodeport-282pm\naffinity-nodeport-282pm\naffinity-nodeport-282pm\naffinity-nodeport-282pm\naffinity-nodeport-282pm\naffinity-nodeport-282pm\naffinity-nodeport-282pm\naffinity-nodeport-282pm\naffinity-nodeport-282pm\naffinity-nodeport-282pm\naffinity-nodeport-282pm\naffinity-nodeport-282pm\naffinity-nodeport-282pm\naffinity-nodeport-282pm"
Oct 29 21:58:14.245: INFO: Received response from host: affinity-nodeport-282pm
Oct 29 21:58:14.245: INFO: Received response from host: affinity-nodeport-282pm
Oct 29 21:58:14.245: INFO: Received response from host: affinity-nodeport-282pm
Oct 29 21:58:14.245: INFO: Received response from host: affinity-nodeport-282pm
Oct 29 21:58:14.245: INFO: Received response from host: affinity-nodeport-282pm
Oct 29 21:58:14.245: INFO: Received response from host: affinity-nodeport-282pm
Oct 29 21:58:14.245: INFO: Received response from host: affinity-nodeport-282pm
Oct 29 21:58:14.245: INFO: Received response from host: affinity-nodeport-282pm
Oct 29 21:58:14.245: INFO: Received response from host: affinity-nodeport-282pm
Oct 29 21:58:14.245: INFO: Received response from host: affinity-nodeport-282pm
Oct 29 21:58:14.245: INFO: Received response from host: affinity-nodeport-282pm
Oct 29 21:58:14.245: INFO: Received response from host: affinity-nodeport-282pm
Oct 29 21:58:14.245: INFO: Received response from host: affinity-nodeport-282pm
Oct 29 21:58:14.245: INFO: Received response from host: affinity-nodeport-282pm
Oct 29 21:58:14.245: INFO: Received response from host: affinity-nodeport-282pm
Oct 29 21:58:14.245: INFO: Received response from host: affinity-nodeport-282pm
Oct 29 21:58:14.245: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-9324, will wait for the garbage collector to delete the pods
Oct 29 21:58:14.380: INFO: Deleting ReplicationController affinity-nodeport took: 25.172254ms
Oct 29 21:58:14.480: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.199927ms
[AfterEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:58:27.127: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-9324" for this suite.
[AfterEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786


 [SLOW TEST:26.433 seconds]
[sig-network] Services
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:58:23.142: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:58:34.518: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "resourcequota-4951" for this suite.


 [SLOW TEST:11.480 seconds]
[sig-api-machinery] ResourceQuota
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":-1,"completed":42,"skipped":614,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected secret
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:58:34.641: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-1af73fd7-f35c-4d58-8240-62dbfd046ffc
STEP: Creating a pod to test consume secrets
Oct 29 21:58:34.899: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2fc1d011-62a2-4524-928b-ccd0b666a5c3" in namespace "projected-9598" to be "Succeeded or Failed"
Oct 29 21:58:34.920: INFO: Pod "pod-projected-secrets-2fc1d011-62a2-4524-928b-ccd0b666a5c3": Phase="Pending", Reason="", readiness=false. Elapsed: 20.694438ms
Oct 29 21:58:36.941: INFO: Pod "pod-projected-secrets-2fc1d011-62a2-4524-928b-ccd0b666a5c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042203394s
Oct 29 21:58:38.964: INFO: Pod "pod-projected-secrets-2fc1d011-62a2-4524-928b-ccd0b666a5c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.064388053s
STEP: Saw pod success
Oct 29 21:58:38.964: INFO: Pod "pod-projected-secrets-2fc1d011-62a2-4524-928b-ccd0b666a5c3" satisfied condition "Succeeded or Failed"
Oct 29 21:58:38.985: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-projected-secrets-2fc1d011-62a2-4524-928b-ccd0b666a5c3 container projected-secret-volume-test: <nil>
STEP: delete the pod
Oct 29 21:58:39.041: INFO: Waiting for pod pod-projected-secrets-2fc1d011-62a2-4524-928b-ccd0b666a5c3 to disappear
Oct 29 21:58:39.062: INFO: Pod pod-projected-secrets-2fc1d011-62a2-4524-928b-ccd0b666a5c3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:58:39.062: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-9598" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":-1,"completed":43,"skipped":640,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:58:39.197: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Oct 29 21:58:39.348: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:58:44.705: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "init-container-8431" for this suite.


 [SLOW TEST:5.613 seconds]
[k8s.io] InitContainer [NodeConformance]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should invoke init containers on a RestartNever pod [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":-1,"completed":44,"skipped":680,"failed":0}

SSSSSSS
------------------------------
[BeforeEach] [k8s.io] Kubelet
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:58:44.817: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:58:45.053: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubelet-test-4842" for this suite.


------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":-1,"completed":45,"skipped":687,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":-1,"completed":37,"skipped":736,"failed":0}
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:58:27.217: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 29 21:58:28.334: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605488, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605488, loc:(*time.Location)(0x77098a0)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-cbccbf6bb\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605488, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605488, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Oct 29 21:58:30.356: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605488, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605488, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605488, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605488, loc:(*time.Location)(0x77098a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 29 21:58:32.356: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605488, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605488, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605488, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605488, loc:(*time.Location)(0x77098a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 29 21:58:35.386: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
Oct 29 21:58:35.488: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:35.641: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:35.742: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:35.900: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:36.041: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:36.141: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:36.242: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:36.349: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:36.442: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:36.543: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:36.645: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:36.751: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:36.845: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:36.940: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:37.041: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:37.141: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:37.240: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:37.349: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:37.441: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:37.543: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:37.640: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:37.741: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:37.841: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:37.940: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:38.042: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:38.143: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:38.241: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:38.340: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:38.441: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:38.540: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:38.642: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:38.741: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:38.843: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:38.941: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:39.043: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:39.141: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:39.249: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:39.352: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:39.443: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:39.540: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:39.645: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:39.744: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:39.844: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:39.963: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:40.040: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:40.147: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:40.246: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:40.342: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:40.441: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:40.541: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:40.644: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:40.741: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:40.845: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:40.940: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:41.041: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:41.142: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:41.241: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:41.354: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:41.445: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:41.545: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:41.644: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:41.747: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:41.846: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:41.965: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:42.041: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:42.141: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:42.240: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:42.342: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:42.441: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:42.544: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:42.641: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:42.742: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:42.843: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:42.941: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:43.040: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:43.140: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:43.239: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:43.342: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:43.441: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:43.541: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:43.642: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:43.744: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:43.842: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:43.943: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:44.041: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:44.146: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:44.242: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:44.345: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:44.451: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:44.544: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:44.647: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:44.742: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:44.853: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:44.954: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:45.050: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:45.167: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:45.308: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:45.442: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:45.542: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:45.643: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:45.743: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:45.844: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:46.017: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:46.141: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:46.243: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:46.355: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:46.450: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:58:46.544: INFO: Waiting for webhook configuration to be ready...
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:58:46.766: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-4525" for this suite.
STEP: Destroying namespace "webhook-4525-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102


 [SLOW TEST:19.804 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":-1,"completed":38,"skipped":736,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] ReplicationController
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:58:45.119: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Oct 29 21:58:45.330: INFO: Pod name pod-release: Found 0 pods out of 1
Oct 29 21:58:50.352: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:58:50.440: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "replication-controller-2330" for this suite.


 [SLOW TEST:5.415 seconds]
[sig-apps] ReplicationController
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":-1,"completed":46,"skipped":710,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-node] Downward API
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:58:47.047: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Oct 29 21:58:47.273: INFO: Waiting up to 5m0s for pod "downward-api-3c2e0c2e-e86f-4ff8-b508-054c31128052" in namespace "downward-api-4664" to be "Succeeded or Failed"
Oct 29 21:58:47.302: INFO: Pod "downward-api-3c2e0c2e-e86f-4ff8-b508-054c31128052": Phase="Pending", Reason="", readiness=false. Elapsed: 28.119342ms
Oct 29 21:58:49.330: INFO: Pod "downward-api-3c2e0c2e-e86f-4ff8-b508-054c31128052": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056890724s
Oct 29 21:58:51.353: INFO: Pod "downward-api-3c2e0c2e-e86f-4ff8-b508-054c31128052": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.079668456s
STEP: Saw pod success
Oct 29 21:58:51.353: INFO: Pod "downward-api-3c2e0c2e-e86f-4ff8-b508-054c31128052" satisfied condition "Succeeded or Failed"
Oct 29 21:58:51.378: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod downward-api-3c2e0c2e-e86f-4ff8-b508-054c31128052 container dapi-container: <nil>
STEP: delete the pod
Oct 29 21:58:51.436: INFO: Waiting for pod downward-api-3c2e0c2e-e86f-4ff8-b508-054c31128052 to disappear
Oct 29 21:58:51.458: INFO: Pod downward-api-3c2e0c2e-e86f-4ff8-b508-054c31128052 no longer exists
[AfterEach] [sig-node] Downward API
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:58:51.458: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-4664" for this suite.


------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":-1,"completed":39,"skipped":765,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:58:51.590: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on node default medium
Oct 29 21:58:51.800: INFO: Waiting up to 5m0s for pod "pod-e6dd7edc-e5bc-45d5-bc1c-bed99b213de6" in namespace "emptydir-7489" to be "Succeeded or Failed"
Oct 29 21:58:51.828: INFO: Pod "pod-e6dd7edc-e5bc-45d5-bc1c-bed99b213de6": Phase="Pending", Reason="", readiness=false. Elapsed: 27.154649ms
Oct 29 21:58:53.850: INFO: Pod "pod-e6dd7edc-e5bc-45d5-bc1c-bed99b213de6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049383078s
Oct 29 21:58:55.873: INFO: Pod "pod-e6dd7edc-e5bc-45d5-bc1c-bed99b213de6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.072082173s
STEP: Saw pod success
Oct 29 21:58:55.873: INFO: Pod "pod-e6dd7edc-e5bc-45d5-bc1c-bed99b213de6" satisfied condition "Succeeded or Failed"
Oct 29 21:58:55.895: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-e6dd7edc-e5bc-45d5-bc1c-bed99b213de6 container test-container: <nil>
STEP: delete the pod
Oct 29 21:58:55.952: INFO: Waiting for pod pod-e6dd7edc-e5bc-45d5-bc1c-bed99b213de6 to disappear
Oct 29 21:58:55.973: INFO: Pod pod-e6dd7edc-e5bc-45d5-bc1c-bed99b213de6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:58:55.974: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-7489" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":40,"skipped":793,"failed":0}

SSS
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:58:56.172: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Oct 29 21:58:56.435: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bbe02710-6717-4c83-97a5-f8d601c8c698" in namespace "downward-api-2767" to be "Succeeded or Failed"
Oct 29 21:58:56.459: INFO: Pod "downwardapi-volume-bbe02710-6717-4c83-97a5-f8d601c8c698": Phase="Pending", Reason="", readiness=false. Elapsed: 24.495286ms
Oct 29 21:58:58.482: INFO: Pod "downwardapi-volume-bbe02710-6717-4c83-97a5-f8d601c8c698": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047488065s
Oct 29 21:59:00.505: INFO: Pod "downwardapi-volume-bbe02710-6717-4c83-97a5-f8d601c8c698": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.070348539s
STEP: Saw pod success
Oct 29 21:59:00.505: INFO: Pod "downwardapi-volume-bbe02710-6717-4c83-97a5-f8d601c8c698" satisfied condition "Succeeded or Failed"
Oct 29 21:59:00.527: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod downwardapi-volume-bbe02710-6717-4c83-97a5-f8d601c8c698 container client-container: <nil>
STEP: delete the pod
Oct 29 21:59:00.579: INFO: Waiting for pod downwardapi-volume-bbe02710-6717-4c83-97a5-f8d601c8c698 to disappear
Oct 29 21:59:00.603: INFO: Pod downwardapi-volume-bbe02710-6717-4c83-97a5-f8d601c8c698 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:59:00.603: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-2767" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":-1,"completed":41,"skipped":796,"failed":0}

SSSSSSSSS
------------------------------
[BeforeEach] [sig-node] PodTemplates
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:59:00.716: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pod templates
Oct 29 21:59:00.925: INFO: created test-podtemplate-1
Oct 29 21:59:00.952: INFO: created test-podtemplate-2
Oct 29 21:59:00.976: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Oct 29 21:59:01.001: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Oct 29 21:59:01.045: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:59:01.071: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "podtemplate-2390" for this suite.


------------------------------
{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":-1,"completed":42,"skipped":805,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:58:50.584: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:59:01.927: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "resourcequota-2374" for this suite.


 [SLOW TEST:11.452 seconds]
[sig-api-machinery] ResourceQuota
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":-1,"completed":47,"skipped":775,"failed":0}

SSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] server version
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:59:02.046: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Request ServerVersion
STEP: Confirm major version
Oct 29 21:59:02.258: INFO: Major version: 1
STEP: Confirm minor version
Oct 29 21:59:02.258: INFO: cleanMinorVersion: 19
Oct 29 21:59:02.258: INFO: Minor version: 19
[AfterEach] [sig-api-machinery] server version
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:59:02.258: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "server-version-3604" for this suite.


------------------------------
{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":-1,"completed":48,"skipped":786,"failed":0}

SSSSSS
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:59:01.137: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-b4f63729-5ca5-466f-a45e-15419679e45b
STEP: Creating a pod to test consume configMaps
Oct 29 21:59:01.429: INFO: Waiting up to 5m0s for pod "pod-configmaps-6e967cad-eefd-45fd-87b9-8f23c8810029" in namespace "configmap-2956" to be "Succeeded or Failed"
Oct 29 21:59:01.451: INFO: Pod "pod-configmaps-6e967cad-eefd-45fd-87b9-8f23c8810029": Phase="Pending", Reason="", readiness=false. Elapsed: 22.11038ms
Oct 29 21:59:03.474: INFO: Pod "pod-configmaps-6e967cad-eefd-45fd-87b9-8f23c8810029": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044597242s
Oct 29 21:59:05.496: INFO: Pod "pod-configmaps-6e967cad-eefd-45fd-87b9-8f23c8810029": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067007284s
STEP: Saw pod success
Oct 29 21:59:05.496: INFO: Pod "pod-configmaps-6e967cad-eefd-45fd-87b9-8f23c8810029" satisfied condition "Succeeded or Failed"
Oct 29 21:59:05.518: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-configmaps-6e967cad-eefd-45fd-87b9-8f23c8810029 container configmap-volume-test: <nil>
STEP: delete the pod
Oct 29 21:59:05.575: INFO: Waiting for pod pod-configmaps-6e967cad-eefd-45fd-87b9-8f23c8810029 to disappear
Oct 29 21:59:05.597: INFO: Pod pod-configmaps-6e967cad-eefd-45fd-87b9-8f23c8810029 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:59:05.597: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-2956" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":-1,"completed":43,"skipped":829,"failed":0}

SSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected configMap
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:59:05.719: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-fdd50397-76c9-47d1-8af9-1764232a2995
STEP: Creating a pod to test consume configMaps
Oct 29 21:59:05.917: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9955473f-7362-4979-951c-8ab963ceab2c" in namespace "projected-2812" to be "Succeeded or Failed"
Oct 29 21:59:05.939: INFO: Pod "pod-projected-configmaps-9955473f-7362-4979-951c-8ab963ceab2c": Phase="Pending", Reason="", readiness=false. Elapsed: 21.838159ms
Oct 29 21:59:07.962: INFO: Pod "pod-projected-configmaps-9955473f-7362-4979-951c-8ab963ceab2c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044551015s
Oct 29 21:59:09.984: INFO: Pod "pod-projected-configmaps-9955473f-7362-4979-951c-8ab963ceab2c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067091063s
STEP: Saw pod success
Oct 29 21:59:09.985: INFO: Pod "pod-projected-configmaps-9955473f-7362-4979-951c-8ab963ceab2c" satisfied condition "Succeeded or Failed"
Oct 29 21:59:10.007: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-projected-configmaps-9955473f-7362-4979-951c-8ab963ceab2c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Oct 29 21:59:10.060: INFO: Waiting for pod pod-projected-configmaps-9955473f-7362-4979-951c-8ab963ceab2c to disappear
Oct 29 21:59:10.081: INFO: Pod pod-projected-configmaps-9955473f-7362-4979-951c-8ab963ceab2c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:59:10.081: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-2812" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":-1,"completed":44,"skipped":849,"failed":0}

SSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] ReplicaSet
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:59:02.333: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 21:59:02.645: INFO: Creating ReplicaSet my-hostname-basic-260e5c06-ee09-4837-bc09-b3d8e9688055
Oct 29 21:59:02.717: INFO: Pod name my-hostname-basic-260e5c06-ee09-4837-bc09-b3d8e9688055: Found 0 pods out of 1
Oct 29 21:59:07.739: INFO: Pod name my-hostname-basic-260e5c06-ee09-4837-bc09-b3d8e9688055: Found 1 pods out of 1
Oct 29 21:59:07.739: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-260e5c06-ee09-4837-bc09-b3d8e9688055" is running
Oct 29 21:59:07.760: INFO: Pod "my-hostname-basic-260e5c06-ee09-4837-bc09-b3d8e9688055-kv7bp" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-10-29 21:58:43 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-10-29 21:58:46 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-10-29 21:58:46 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-10-29 21:58:43 +0000 UTC Reason: Message:}])
Oct 29 21:59:07.760: INFO: Trying to dial the pod
Oct 29 21:59:12.829: INFO: Controller my-hostname-basic-260e5c06-ee09-4837-bc09-b3d8e9688055: Got expected result from replica 1 [my-hostname-basic-260e5c06-ee09-4837-bc09-b3d8e9688055-kv7bp]: "my-hostname-basic-260e5c06-ee09-4837-bc09-b3d8e9688055-kv7bp", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:59:12.829: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "replicaset-3687" for this suite.


 [SLOW TEST:10.604 seconds]
[sig-apps] ReplicaSet
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":-1,"completed":49,"skipped":792,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] StatefulSet
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:56:23.766: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-5438
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Oct 29 21:56:24.075: INFO: Found 0 stateful pods, waiting for 3
Oct 29 21:56:34.097: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Oct 29 21:56:34.097: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Oct 29 21:56:34.097: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Oct 29 21:56:44.097: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Oct 29 21:56:44.097: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Oct 29 21:56:44.097: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Oct 29 21:56:44.160: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5438 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 29 21:56:44.513: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 29 21:56:44.513: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 29 21:56:44.513: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Oct 29 21:56:54.653: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Oct 29 21:56:54.725: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5438 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 29 21:56:55.096: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Oct 29 21:56:55.096: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Oct 29 21:56:55.096: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Oct 29 21:57:05.224: INFO: Waiting for StatefulSet statefulset-5438/ss2 to complete update
Oct 29 21:57:05.224: INFO: Waiting for Pod statefulset-5438/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Oct 29 21:57:05.224: INFO: Waiting for Pod statefulset-5438/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Oct 29 21:57:05.224: INFO: Waiting for Pod statefulset-5438/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Oct 29 21:57:15.266: INFO: Waiting for StatefulSet statefulset-5438/ss2 to complete update
Oct 29 21:57:15.266: INFO: Waiting for Pod statefulset-5438/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Oct 29 21:57:15.266: INFO: Waiting for Pod statefulset-5438/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Oct 29 21:57:25.267: INFO: Waiting for StatefulSet statefulset-5438/ss2 to complete update
Oct 29 21:57:25.267: INFO: Waiting for Pod statefulset-5438/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
Oct 29 21:57:35.266: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5438 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 29 21:57:35.633: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 29 21:57:35.633: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 29 21:57:35.633: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Oct 29 21:57:45.779: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Oct 29 21:57:55.897: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5438 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 29 21:57:56.235: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Oct 29 21:57:56.235: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Oct 29 21:57:56.235: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Oct 29 21:58:06.365: INFO: Waiting for StatefulSet statefulset-5438/ss2 to complete update
Oct 29 21:58:06.365: INFO: Waiting for Pod statefulset-5438/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Oct 29 21:58:06.365: INFO: Waiting for Pod statefulset-5438/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Oct 29 21:58:06.365: INFO: Waiting for Pod statefulset-5438/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Oct 29 21:58:16.407: INFO: Waiting for StatefulSet statefulset-5438/ss2 to complete update
Oct 29 21:58:16.407: INFO: Waiting for Pod statefulset-5438/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Oct 29 21:58:26.410: INFO: Waiting for StatefulSet statefulset-5438/ss2 to complete update
Oct 29 21:58:26.410: INFO: Waiting for Pod statefulset-5438/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Oct 29 21:58:36.416: INFO: Deleting all statefulset in ns statefulset-5438
Oct 29 21:58:36.437: INFO: Scaling statefulset ss2 to 0
Oct 29 21:59:16.527: INFO: Waiting for statefulset status.replicas updated to 0
Oct 29 21:59:16.549: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:59:16.618: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "statefulset-5438" for this suite.


 [SLOW TEST:172.943 seconds]
[sig-apps] StatefulSet
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform rolling updates and roll backs of template modifications [Conformance]
    /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":-1,"completed":42,"skipped":668,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:59:10.197: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Oct 29 21:59:15.039: INFO: Successfully updated pod "annotationupdatec55b517e-1564-4867-84f6-0cb8681f2149"
[AfterEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:59:17.091: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-7640" for this suite.


 [SLOW TEST:7.005 seconds]
[sig-storage] Projected downwardAPI
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":-1,"completed":45,"skipped":862,"failed":0}

SSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:59:17.212: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should support --unix-socket=/path  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Starting the proxy
Oct 29 21:59:17.452: INFO: Asynchronously running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig proxy --unix-socket=/tmp/kubectl-proxy-unix765500435/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:59:17.510: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-5154" for this suite.


------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":-1,"completed":46,"skipped":873,"failed":0}

SSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:59:12.954: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 29 21:59:13.633: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605533, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605533, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605534, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605533, loc:(*time.Location)(0x77098a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 29 21:59:15.658: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605533, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605533, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605534, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605533, loc:(*time.Location)(0x77098a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 29 21:59:18.710: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 21:59:18.731: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7226-crds.webhook.example.com via the AdmissionRegistration API
Oct 29 21:59:18.854: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:19.005: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:19.113: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:19.217: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:19.306: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:19.406: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:19.510: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:19.610: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:19.753: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:19.827: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:19.906: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:20.008: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:20.107: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:20.208: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:20.319: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:20.457: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:20.628: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:20.721: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:20.810: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:20.926: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:21.028: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:21.114: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:21.223: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:21.311: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:21.413: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:21.506: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:21.608: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:21.717: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:21.826: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:21.927: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:22.012: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:22.123: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:22.212: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:22.314: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:22.420: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:22.515: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:22.640: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:22.708: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:22.811: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:22.906: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:23.018: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:23.114: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:23.214: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:23.309: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:23.421: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:23.510: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:23.642: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:23.713: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:23.812: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:23.923: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:24.032: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:24.113: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:24.211: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:24.313: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:24.410: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:24.527: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:24.659: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:24.811: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:24.916: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:25.012: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:25.106: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:25.208: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:25.306: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:25.406: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:25.509: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:25.605: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:25.710: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:25.805: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:25.904: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:26.004: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:26.107: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:26.205: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:26.307: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:26.404: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:26.508: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:26.605: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:26.707: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:26.807: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:26.905: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:27.004: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:27.107: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:27.204: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:27.305: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:27.405: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:27.505: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:27.604: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:27.705: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:27.804: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:27.906: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:28.004: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:28.104: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:28.206: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:28.304: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:28.404: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:28.504: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:28.604: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:28.705: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:28.806: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:28.913: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:29.006: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:29.106: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:29.208: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:29.347: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:29.408: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:29.530: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:29.610: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:29.707: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:29.882: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:30.070: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:30.208: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:30.309: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:30.432: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:30.509: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:30.606: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:30.707: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:30.806: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:30.905: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:31.007: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:31.107: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:31.207: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:31.306: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:31.407: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:31.508: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:31.607: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:31.709: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:31.805: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:31.904: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:32.008: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:32.114: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:32.204: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:32.305: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:32.405: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:32.507: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:32.605: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:59:33.325: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-852" for this suite.
STEP: Destroying namespace "webhook-852-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102


 [SLOW TEST:20.673 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":-1,"completed":50,"skipped":817,"failed":0}

SSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:59:16.727: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
Oct 29 21:59:17.936: INFO: role binding webhook-auth-reader already exists
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 29 21:59:18.110: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605538, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605538, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605538, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605538, loc:(*time.Location)(0x77098a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 29 21:59:20.132: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605538, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605538, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605538, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605538, loc:(*time.Location)(0x77098a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 29 21:59:23.163: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
Oct 29 21:59:23.289: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:23.449: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:23.552: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:23.663: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:23.749: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:23.857: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:23.950: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:24.050: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:24.148: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:24.260: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:24.339: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:24.446: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:24.547: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:24.700: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:24.841: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:24.948: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:25.047: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:25.143: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:25.239: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:25.342: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:25.445: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:25.539: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:25.641: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:25.740: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:25.838: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:25.943: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:26.038: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:26.137: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:26.239: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:26.345: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:26.458: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:26.539: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:26.639: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:26.739: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:26.855: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:26.942: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:27.038: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:27.138: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:27.238: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:27.341: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:27.438: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:27.537: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:27.637: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:27.737: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:27.840: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:27.939: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:28.039: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:28.138: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:28.238: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:28.338: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:28.438: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:28.539: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:28.639: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:28.741: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:28.843: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:28.950: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:29.038: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:29.144: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:29.241: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:29.368: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:29.440: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:29.540: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:29.646: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:29.753: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:29.846: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:29.939: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:30.063: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:30.149: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:30.242: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:30.344: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:30.444: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:30.541: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:30.640: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:30.739: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:30.839: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:30.938: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:31.050: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:31.139: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:31.241: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:31.344: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:31.440: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:31.557: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:31.640: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:31.743: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:31.839: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:31.940: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:32.041: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:32.142: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:32.237: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:32.343: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:32.438: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:32.539: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:32.639: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:32.740: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:32.841: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:32.940: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:33.040: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:33.142: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:33.240: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:33.339: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:33.441: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:33.548: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:33.655: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:33.799: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:33.959: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:34.043: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:34.144: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:34.239: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:34.339: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:34.446: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:34.542: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:34.640: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:34.802: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:34.941: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:35.047: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:35.152: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:35.242: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:35.405: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:35.545: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:35.654: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:35.768: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:35.868: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:35.953: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:36.058: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:36.144: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:36.239: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:36.351: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:36.483: INFO: Waiting for webhook configuration to be ready...
Oct 29 21:59:36.573: INFO: Waiting for webhook configuration to be ready...
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:59:36.709: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-7687" for this suite.
STEP: Destroying namespace "webhook-7687-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102


 [SLOW TEST:20.220 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":-1,"completed":43,"skipped":691,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:59:17.573: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service nodeport-service with the type=NodePort in namespace services-5800
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-5800
STEP: creating replication controller externalsvc in namespace services-5800
I1029 21:59:17.870481   53212 runners.go:190] Created replication controller with name: externalsvc, namespace: services-5800, replica count: 2
I1029 21:59:20.920812   53212 runners.go:190] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1029 21:59:23.920947   53212 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Oct 29 21:59:24.034: INFO: Creating new exec pod
Oct 29 21:59:28.141: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=services-5800 execpodhd2rb -- /bin/sh -x -c nslookup nodeport-service.services-5800.svc.cluster.local'
Oct 29 21:59:28.511: INFO: stderr: "+ nslookup nodeport-service.services-5800.svc.cluster.local\n"
Oct 29 21:59:28.511: INFO: stdout: "Server:\t\t172.30.0.10\nAddress:\t172.30.0.10#53\n\nnodeport-service.services-5800.svc.cluster.local\tcanonical name = externalsvc.services-5800.svc.cluster.local.\nName:\texternalsvc.services-5800.svc.cluster.local\nAddress: 172.30.54.1\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-5800, will wait for the garbage collector to delete the pods
Oct 29 21:59:28.608: INFO: Deleting ReplicationController externalsvc took: 25.235853ms
Oct 29 21:59:28.709: INFO: Terminating ReplicationController externalsvc pods took: 100.307021ms
Oct 29 21:59:37.189: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:59:37.262: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-5800" for this suite.
[AfterEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786


 [SLOW TEST:19.803 seconds]
[sig-network] Services
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":-1,"completed":47,"skipped":879,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Secrets
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:59:36.975: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating secret secrets-6623/secret-test-1714e1c3-d337-4bdf-8004-70b537f74f73
STEP: Creating a pod to test consume secrets
Oct 29 21:59:37.279: INFO: Waiting up to 5m0s for pod "pod-configmaps-73342694-66d1-4bdc-a04c-0b316f5bb71e" in namespace "secrets-6623" to be "Succeeded or Failed"
Oct 29 21:59:37.301: INFO: Pod "pod-configmaps-73342694-66d1-4bdc-a04c-0b316f5bb71e": Phase="Pending", Reason="", readiness=false. Elapsed: 21.749921ms
Oct 29 21:59:39.325: INFO: Pod "pod-configmaps-73342694-66d1-4bdc-a04c-0b316f5bb71e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045417006s
Oct 29 21:59:41.346: INFO: Pod "pod-configmaps-73342694-66d1-4bdc-a04c-0b316f5bb71e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066868343s
STEP: Saw pod success
Oct 29 21:59:41.346: INFO: Pod "pod-configmaps-73342694-66d1-4bdc-a04c-0b316f5bb71e" satisfied condition "Succeeded or Failed"
Oct 29 21:59:41.367: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-configmaps-73342694-66d1-4bdc-a04c-0b316f5bb71e container env-test: <nil>
STEP: delete the pod
Oct 29 21:59:41.431: INFO: Waiting for pod pod-configmaps-73342694-66d1-4bdc-a04c-0b316f5bb71e to disappear
Oct 29 21:59:41.451: INFO: Pod pod-configmaps-73342694-66d1-4bdc-a04c-0b316f5bb71e no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:59:41.451: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-6623" for this suite.


------------------------------
{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":-1,"completed":44,"skipped":721,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Kubelet
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:59:37.395: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:59:41.660: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubelet-test-8744" for this suite.


------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":-1,"completed":48,"skipped":906,"failed":0}

SSSS
------------------------------
[BeforeEach] [k8s.io] Variable Expansion
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:59:41.588: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test env composition
Oct 29 21:59:41.816: INFO: Waiting up to 5m0s for pod "var-expansion-d81b747c-8eb4-4cf7-8fb5-9b1793a5e8a9" in namespace "var-expansion-2387" to be "Succeeded or Failed"
Oct 29 21:59:41.840: INFO: Pod "var-expansion-d81b747c-8eb4-4cf7-8fb5-9b1793a5e8a9": Phase="Pending", Reason="", readiness=false. Elapsed: 23.648203ms
Oct 29 21:59:43.862: INFO: Pod "var-expansion-d81b747c-8eb4-4cf7-8fb5-9b1793a5e8a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045567151s
Oct 29 21:59:45.883: INFO: Pod "var-expansion-d81b747c-8eb4-4cf7-8fb5-9b1793a5e8a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067043578s
STEP: Saw pod success
Oct 29 21:59:45.883: INFO: Pod "var-expansion-d81b747c-8eb4-4cf7-8fb5-9b1793a5e8a9" satisfied condition "Succeeded or Failed"
Oct 29 21:59:45.904: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod var-expansion-d81b747c-8eb4-4cf7-8fb5-9b1793a5e8a9 container dapi-container: <nil>
STEP: delete the pod
Oct 29 21:59:45.957: INFO: Waiting for pod var-expansion-d81b747c-8eb4-4cf7-8fb5-9b1793a5e8a9 to disappear
Oct 29 21:59:45.978: INFO: Pod var-expansion-d81b747c-8eb4-4cf7-8fb5-9b1793a5e8a9 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:59:45.978: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "var-expansion-2387" for this suite.


------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":-1,"completed":45,"skipped":765,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:59:41.780: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 29 21:59:42.566: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605562, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605562, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605562, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605562, loc:(*time.Location)(0x77098a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 29 21:59:44.591: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605562, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605562, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605562, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605562, loc:(*time.Location)(0x77098a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 29 21:59:46.712: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605562, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605562, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605562, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605562, loc:(*time.Location)(0x77098a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 29 21:59:49.647: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:59:49.730: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-1029" for this suite.
STEP: Destroying namespace "webhook-1029-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102


 [SLOW TEST:8.190 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":-1,"completed":49,"skipped":910,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-node] ConfigMap
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:59:49.987: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:59:50.322: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-2510" for this suite.


------------------------------
{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":-1,"completed":50,"skipped":933,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:59:46.109: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Oct 29 21:59:46.366: INFO: Waiting up to 5m0s for pod "pod-051b15c1-facb-4f89-b71a-c43cb9535185" in namespace "emptydir-5626" to be "Succeeded or Failed"
Oct 29 21:59:46.388: INFO: Pod "pod-051b15c1-facb-4f89-b71a-c43cb9535185": Phase="Pending", Reason="", readiness=false. Elapsed: 21.834371ms
Oct 29 21:59:48.412: INFO: Pod "pod-051b15c1-facb-4f89-b71a-c43cb9535185": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045424071s
Oct 29 21:59:50.434: INFO: Pod "pod-051b15c1-facb-4f89-b71a-c43cb9535185": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067492134s
STEP: Saw pod success
Oct 29 21:59:50.434: INFO: Pod "pod-051b15c1-facb-4f89-b71a-c43cb9535185" satisfied condition "Succeeded or Failed"
Oct 29 21:59:50.460: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-051b15c1-facb-4f89-b71a-c43cb9535185 container test-container: <nil>
STEP: delete the pod
Oct 29 21:59:50.571: INFO: Waiting for pod pod-051b15c1-facb-4f89-b71a-c43cb9535185 to disappear
Oct 29 21:59:50.594: INFO: Pod pod-051b15c1-facb-4f89-b71a-c43cb9535185 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 21:59:50.594: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-5626" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":46,"skipped":798,"failed":0}

SSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:59:50.716: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 21:59:50.910: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Oct 29 21:59:58.731: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig --namespace=crd-publish-openapi-1569 create -f -'
Oct 29 22:00:03.164: INFO: stderr: ""
Oct 29 22:00:03.164: INFO: stdout: "e2e-test-crd-publish-openapi-7097-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Oct 29 22:00:03.164: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig --namespace=crd-publish-openapi-1569 delete e2e-test-crd-publish-openapi-7097-crds test-cr'
Oct 29 22:00:03.329: INFO: stderr: ""
Oct 29 22:00:03.329: INFO: stdout: "e2e-test-crd-publish-openapi-7097-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Oct 29 22:00:03.329: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig --namespace=crd-publish-openapi-1569 apply -f -'
Oct 29 22:00:04.062: INFO: stderr: ""
Oct 29 22:00:04.062: INFO: stdout: "e2e-test-crd-publish-openapi-7097-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Oct 29 22:00:04.062: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig --namespace=crd-publish-openapi-1569 delete e2e-test-crd-publish-openapi-7097-crds test-cr'
Oct 29 22:00:04.229: INFO: stderr: ""
Oct 29 22:00:04.229: INFO: stdout: "e2e-test-crd-publish-openapi-7097-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Oct 29 22:00:04.229: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig explain e2e-test-crd-publish-openapi-7097-crds'
Oct 29 22:00:04.732: INFO: stderr: ""
Oct 29 22:00:04.732: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7097-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:00:11.512: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1569" for this suite.


 [SLOW TEST:20.900 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":-1,"completed":47,"skipped":812,"failed":0}

SSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Subpath
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:59:50.414: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-downwardapi-q5rw
STEP: Creating a pod to test atomic-volume-subpath
Oct 29 21:59:50.694: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-q5rw" in namespace "subpath-839" to be "Succeeded or Failed"
Oct 29 21:59:50.716: INFO: Pod "pod-subpath-test-downwardapi-q5rw": Phase="Pending", Reason="", readiness=false. Elapsed: 22.443359ms
Oct 29 21:59:52.739: INFO: Pod "pod-subpath-test-downwardapi-q5rw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044850941s
Oct 29 21:59:54.771: INFO: Pod "pod-subpath-test-downwardapi-q5rw": Phase="Running", Reason="", readiness=true. Elapsed: 4.077542727s
Oct 29 21:59:56.894: INFO: Pod "pod-subpath-test-downwardapi-q5rw": Phase="Running", Reason="", readiness=true. Elapsed: 6.199969431s
Oct 29 21:59:58.917: INFO: Pod "pod-subpath-test-downwardapi-q5rw": Phase="Running", Reason="", readiness=true. Elapsed: 8.222832581s
Oct 29 22:00:00.940: INFO: Pod "pod-subpath-test-downwardapi-q5rw": Phase="Running", Reason="", readiness=true. Elapsed: 10.245700515s
Oct 29 22:00:02.962: INFO: Pod "pod-subpath-test-downwardapi-q5rw": Phase="Running", Reason="", readiness=true. Elapsed: 12.268087994s
Oct 29 22:00:04.985: INFO: Pod "pod-subpath-test-downwardapi-q5rw": Phase="Running", Reason="", readiness=true. Elapsed: 14.291193117s
Oct 29 22:00:07.009: INFO: Pod "pod-subpath-test-downwardapi-q5rw": Phase="Running", Reason="", readiness=true. Elapsed: 16.314727314s
Oct 29 22:00:09.031: INFO: Pod "pod-subpath-test-downwardapi-q5rw": Phase="Running", Reason="", readiness=true. Elapsed: 18.337596117s
Oct 29 22:00:11.054: INFO: Pod "pod-subpath-test-downwardapi-q5rw": Phase="Running", Reason="", readiness=true. Elapsed: 20.359932454s
Oct 29 22:00:13.076: INFO: Pod "pod-subpath-test-downwardapi-q5rw": Phase="Running", Reason="", readiness=true. Elapsed: 22.381916389s
Oct 29 22:00:15.098: INFO: Pod "pod-subpath-test-downwardapi-q5rw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.404120203s
STEP: Saw pod success
Oct 29 22:00:15.098: INFO: Pod "pod-subpath-test-downwardapi-q5rw" satisfied condition "Succeeded or Failed"
Oct 29 22:00:15.120: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-subpath-test-downwardapi-q5rw container test-container-subpath-downwardapi-q5rw: <nil>
STEP: delete the pod
Oct 29 22:00:15.183: INFO: Waiting for pod pod-subpath-test-downwardapi-q5rw to disappear
Oct 29 22:00:15.205: INFO: Pod pod-subpath-test-downwardapi-q5rw no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-q5rw
Oct 29 22:00:15.205: INFO: Deleting pod "pod-subpath-test-downwardapi-q5rw" in namespace "subpath-839"
[AfterEach] [sig-storage] Subpath
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:00:15.226: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "subpath-839" for this suite.


 [SLOW TEST:24.920 seconds]
[sig-storage] Subpath
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":-1,"completed":51,"skipped":999,"failed":0}

SSSSSS
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:00:11.628: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Oct 29 22:00:11.857: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3fe34e2c-a7c5-45c0-90bf-35e2c787a3ed" in namespace "downward-api-6177" to be "Succeeded or Failed"
Oct 29 22:00:11.878: INFO: Pod "downwardapi-volume-3fe34e2c-a7c5-45c0-90bf-35e2c787a3ed": Phase="Pending", Reason="", readiness=false. Elapsed: 21.01438ms
Oct 29 22:00:13.909: INFO: Pod "downwardapi-volume-3fe34e2c-a7c5-45c0-90bf-35e2c787a3ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052468212s
Oct 29 22:00:15.931: INFO: Pod "downwardapi-volume-3fe34e2c-a7c5-45c0-90bf-35e2c787a3ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.073684285s
STEP: Saw pod success
Oct 29 22:00:15.931: INFO: Pod "downwardapi-volume-3fe34e2c-a7c5-45c0-90bf-35e2c787a3ed" satisfied condition "Succeeded or Failed"
Oct 29 22:00:15.951: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod downwardapi-volume-3fe34e2c-a7c5-45c0-90bf-35e2c787a3ed container client-container: <nil>
STEP: delete the pod
Oct 29 22:00:16.002: INFO: Waiting for pod downwardapi-volume-3fe34e2c-a7c5-45c0-90bf-35e2c787a3ed to disappear
Oct 29 22:00:16.022: INFO: Pod downwardapi-volume-3fe34e2c-a7c5-45c0-90bf-35e2c787a3ed no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:00:16.022: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-6177" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":48,"skipped":826,"failed":0}

SSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:00:15.340: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Oct 29 22:00:15.518: INFO: Waiting up to 5m0s for pod "pod-425e183e-e553-4bf5-8ca0-dadd76b72958" in namespace "emptydir-9470" to be "Succeeded or Failed"
Oct 29 22:00:15.557: INFO: Pod "pod-425e183e-e553-4bf5-8ca0-dadd76b72958": Phase="Pending", Reason="", readiness=false. Elapsed: 38.055218ms
Oct 29 22:00:17.581: INFO: Pod "pod-425e183e-e553-4bf5-8ca0-dadd76b72958": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06239148s
Oct 29 22:00:19.604: INFO: Pod "pod-425e183e-e553-4bf5-8ca0-dadd76b72958": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.085047956s
STEP: Saw pod success
Oct 29 22:00:19.604: INFO: Pod "pod-425e183e-e553-4bf5-8ca0-dadd76b72958" satisfied condition "Succeeded or Failed"
Oct 29 22:00:19.625: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-425e183e-e553-4bf5-8ca0-dadd76b72958 container test-container: <nil>
STEP: delete the pod
Oct 29 22:00:19.687: INFO: Waiting for pod pod-425e183e-e553-4bf5-8ca0-dadd76b72958 to disappear
Oct 29 22:00:19.709: INFO: Pod pod-425e183e-e553-4bf5-8ca0-dadd76b72958 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:00:19.709: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-9470" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":52,"skipped":1005,"failed":0}

SSSSSS
------------------------------
[BeforeEach] [sig-node] ConfigMap
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:00:19.851: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-6880/configmap-test-1fa463a8-1d42-4d85-a86a-624ffab26658
STEP: Creating a pod to test consume configMaps
Oct 29 22:00:20.099: INFO: Waiting up to 5m0s for pod "pod-configmaps-7a395bd9-ef9e-476d-aae2-99d913168e33" in namespace "configmap-6880" to be "Succeeded or Failed"
Oct 29 22:00:20.124: INFO: Pod "pod-configmaps-7a395bd9-ef9e-476d-aae2-99d913168e33": Phase="Pending", Reason="", readiness=false. Elapsed: 25.482465ms
Oct 29 22:00:22.146: INFO: Pod "pod-configmaps-7a395bd9-ef9e-476d-aae2-99d913168e33": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047535489s
Oct 29 22:00:24.169: INFO: Pod "pod-configmaps-7a395bd9-ef9e-476d-aae2-99d913168e33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.069756184s
STEP: Saw pod success
Oct 29 22:00:24.169: INFO: Pod "pod-configmaps-7a395bd9-ef9e-476d-aae2-99d913168e33" satisfied condition "Succeeded or Failed"
Oct 29 22:00:24.190: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-configmaps-7a395bd9-ef9e-476d-aae2-99d913168e33 container env-test: <nil>
STEP: delete the pod
Oct 29 22:00:24.247: INFO: Waiting for pod pod-configmaps-7a395bd9-ef9e-476d-aae2-99d913168e33 to disappear
Oct 29 22:00:24.269: INFO: Pod pod-configmaps-7a395bd9-ef9e-476d-aae2-99d913168e33 no longer exists
[AfterEach] [sig-node] ConfigMap
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:00:24.269: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-6880" for this suite.


------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":-1,"completed":53,"skipped":1011,"failed":0}

SSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:00:16.127: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 22:00:16.252: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Oct 29 22:00:23.555: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig --namespace=crd-publish-openapi-7392 create -f -'
Oct 29 22:00:28.247: INFO: stderr: ""
Oct 29 22:00:28.247: INFO: stdout: "e2e-test-crd-publish-openapi-7445-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Oct 29 22:00:28.247: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig --namespace=crd-publish-openapi-7392 delete e2e-test-crd-publish-openapi-7445-crds test-cr'
Oct 29 22:00:28.415: INFO: stderr: ""
Oct 29 22:00:28.415: INFO: stdout: "e2e-test-crd-publish-openapi-7445-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Oct 29 22:00:28.415: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig --namespace=crd-publish-openapi-7392 apply -f -'
Oct 29 22:00:29.100: INFO: stderr: ""
Oct 29 22:00:29.100: INFO: stdout: "e2e-test-crd-publish-openapi-7445-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Oct 29 22:00:29.100: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig --namespace=crd-publish-openapi-7392 delete e2e-test-crd-publish-openapi-7445-crds test-cr'
Oct 29 22:00:29.269: INFO: stderr: ""
Oct 29 22:00:29.269: INFO: stdout: "e2e-test-crd-publish-openapi-7445-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Oct 29 22:00:29.269: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig explain e2e-test-crd-publish-openapi-7445-crds'
Oct 29 22:00:29.695: INFO: stderr: ""
Oct 29 22:00:29.695: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7445-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:00:37.518: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7392" for this suite.


 [SLOW TEST:21.493 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":-1,"completed":49,"skipped":831,"failed":0}

SSSS
------------------------------
[BeforeEach] [sig-storage] Subpath
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:00:24.384: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-tg8d
STEP: Creating a pod to test atomic-volume-subpath
Oct 29 22:00:24.606: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-tg8d" in namespace "subpath-8465" to be "Succeeded or Failed"
Oct 29 22:00:24.637: INFO: Pod "pod-subpath-test-configmap-tg8d": Phase="Pending", Reason="", readiness=false. Elapsed: 30.180715ms
Oct 29 22:00:26.659: INFO: Pod "pod-subpath-test-configmap-tg8d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052729008s
Oct 29 22:00:28.683: INFO: Pod "pod-subpath-test-configmap-tg8d": Phase="Running", Reason="", readiness=true. Elapsed: 4.076996182s
Oct 29 22:00:30.706: INFO: Pod "pod-subpath-test-configmap-tg8d": Phase="Running", Reason="", readiness=true. Elapsed: 6.099851306s
Oct 29 22:00:32.729: INFO: Pod "pod-subpath-test-configmap-tg8d": Phase="Running", Reason="", readiness=true. Elapsed: 8.122270814s
Oct 29 22:00:34.752: INFO: Pod "pod-subpath-test-configmap-tg8d": Phase="Running", Reason="", readiness=true. Elapsed: 10.145341555s
Oct 29 22:00:36.775: INFO: Pod "pod-subpath-test-configmap-tg8d": Phase="Running", Reason="", readiness=true. Elapsed: 12.168597587s
Oct 29 22:00:38.799: INFO: Pod "pod-subpath-test-configmap-tg8d": Phase="Running", Reason="", readiness=true. Elapsed: 14.192167563s
Oct 29 22:00:40.821: INFO: Pod "pod-subpath-test-configmap-tg8d": Phase="Running", Reason="", readiness=true. Elapsed: 16.215021332s
Oct 29 22:00:42.843: INFO: Pod "pod-subpath-test-configmap-tg8d": Phase="Running", Reason="", readiness=true. Elapsed: 18.236924061s
Oct 29 22:00:44.866: INFO: Pod "pod-subpath-test-configmap-tg8d": Phase="Running", Reason="", readiness=true. Elapsed: 20.26006415s
Oct 29 22:00:46.890: INFO: Pod "pod-subpath-test-configmap-tg8d": Phase="Running", Reason="", readiness=true. Elapsed: 22.283236909s
Oct 29 22:00:48.912: INFO: Pod "pod-subpath-test-configmap-tg8d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.30605161s
STEP: Saw pod success
Oct 29 22:00:48.912: INFO: Pod "pod-subpath-test-configmap-tg8d" satisfied condition "Succeeded or Failed"
Oct 29 22:00:48.934: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-subpath-test-configmap-tg8d container test-container-subpath-configmap-tg8d: <nil>
STEP: delete the pod
Oct 29 22:00:48.988: INFO: Waiting for pod pod-subpath-test-configmap-tg8d to disappear
Oct 29 22:00:49.009: INFO: Pod pod-subpath-test-configmap-tg8d no longer exists
STEP: Deleting pod pod-subpath-test-configmap-tg8d
Oct 29 22:00:49.009: INFO: Deleting pod "pod-subpath-test-configmap-tg8d" in namespace "subpath-8465"
[AfterEach] [sig-storage] Subpath
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:00:49.031: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "subpath-8465" for this suite.


 [SLOW TEST:24.751 seconds]
[sig-storage] Subpath
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":-1,"completed":54,"skipped":1025,"failed":0}

SS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:00:37.625: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Oct 29 22:00:38.345: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605618, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605618, loc:(*time.Location)(0x77098a0)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-crd-conversion-webhook-deployment-85d57b96d6\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605618, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605618, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Oct 29 22:00:40.366: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605618, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605618, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605618, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605618, loc:(*time.Location)(0x77098a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 29 22:00:43.409: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 22:00:43.433: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
Oct 29 22:00:44.207: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:24Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:44.335: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:24Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:44.439: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:24Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:44.532: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:24Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:44.632: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:25Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:44.738: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:25Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:44.835: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:25Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:44.933: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:25Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:45.034: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:25Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:45.133: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:25Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:45.233: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:25Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:45.333: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:25Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:45.432: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:25Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:45.568: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:25Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:45.632: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:26Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:45.732: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:26Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:45.831: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:26Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:45.932: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:26Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:46.032: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:26Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:46.132: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:26Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:46.332: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:26Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:46.532: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:26Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:46.732: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:27Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:46.935: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:27Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:47.131: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:27Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:47.332: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:27Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:47.532: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:27Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:47.732: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:28Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:47.932: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:28Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:48.133: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:28Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:48.332: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:28Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:48.532: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:28Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:48.732: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:29Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:48.932: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:29Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:49.132: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:29Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:49.332: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:29Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:49.532: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:29Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:49.732: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:30Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:49.933: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:30Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:50.132: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:30Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:50.333: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:30Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:50.533: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:30Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:50.732: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:31Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:50.932: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:31Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:51.132: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:31Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:51.332: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:31Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:51.532: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:31Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:51.736: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:32Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:51.932: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:32Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:52.131: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:32Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:52.335: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:32Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:52.535: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:32Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:52.734: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:33Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:52.939: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:33Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:53.132: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:33Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:53.331: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:33Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:53.532: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:33Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:53.732: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:34Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:53.932: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:34Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:54.132: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:34Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:54.333: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:34Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:54.531: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:34Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:54.736: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:35Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:54.932: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:35Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:55.132: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:35Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:55.332: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:35Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:55.533: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:35Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:55.732: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:36Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:55.932: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:36Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:56.132: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:36Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:56.332: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:36Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:56.532: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:36Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:56.732: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:37Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:56.934: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:37Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:57.132: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:37Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:57.332: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:37Z is before 2020-10-29T22:00:38Z
Oct 29 22:00:57.532: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-5141-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3663.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:00:37Z is before 2020-10-29T22:00:38Z
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:00:59.312: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-webhook-3663" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137


 [SLOW TEST:21.945 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":-1,"completed":50,"skipped":835,"failed":0}

SSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:00:49.139: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:01:05.541: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "resourcequota-2558" for this suite.


 [SLOW TEST:16.508 seconds]
[sig-api-machinery] ResourceQuota
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":-1,"completed":55,"skipped":1027,"failed":0}

SSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:01:05.662: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Oct 29 22:01:05.854: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bdf6f4e2-d611-43cc-841f-14877c2400d9" in namespace "projected-5033" to be "Succeeded or Failed"
Oct 29 22:01:05.875: INFO: Pod "downwardapi-volume-bdf6f4e2-d611-43cc-841f-14877c2400d9": Phase="Pending", Reason="", readiness=false. Elapsed: 21.602572ms
Oct 29 22:01:07.898: INFO: Pod "downwardapi-volume-bdf6f4e2-d611-43cc-841f-14877c2400d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044381405s
Oct 29 22:01:09.922: INFO: Pod "downwardapi-volume-bdf6f4e2-d611-43cc-841f-14877c2400d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.06786702s
STEP: Saw pod success
Oct 29 22:01:09.922: INFO: Pod "downwardapi-volume-bdf6f4e2-d611-43cc-841f-14877c2400d9" satisfied condition "Succeeded or Failed"
Oct 29 22:01:09.943: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod downwardapi-volume-bdf6f4e2-d611-43cc-841f-14877c2400d9 container client-container: <nil>
STEP: delete the pod
Oct 29 22:01:09.996: INFO: Waiting for pod downwardapi-volume-bdf6f4e2-d611-43cc-841f-14877c2400d9 to disappear
Oct 29 22:01:10.018: INFO: Pod downwardapi-volume-bdf6f4e2-d611-43cc-841f-14877c2400d9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:01:10.018: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-5033" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":56,"skipped":1041,"failed":0}

SSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:01:10.138: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 22:01:10.262: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:01:11.220: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-855" for this suite.


------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":-1,"completed":57,"skipped":1058,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:01:11.331: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve multiport endpoints from pods  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service multi-endpoint-test in namespace services-7050
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7050 to expose endpoints map[]
Oct 29 22:01:11.663: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Oct 29 22:01:12.727: INFO: successfully validated that service multi-endpoint-test in namespace services-7050 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7050
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7050 to expose endpoints map[pod1:[100]]
Oct 29 22:01:15.880: INFO: successfully validated that service multi-endpoint-test in namespace services-7050 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-7050
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7050 to expose endpoints map[pod1:[100] pod2:[101]]
Oct 29 22:01:19.041: INFO: successfully validated that service multi-endpoint-test in namespace services-7050 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-7050
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7050 to expose endpoints map[pod2:[101]]
Oct 29 22:01:19.196: INFO: successfully validated that service multi-endpoint-test in namespace services-7050 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-7050
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7050 to expose endpoints map[]
Oct 29 22:01:19.289: INFO: successfully validated that service multi-endpoint-test in namespace services-7050 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:01:19.340: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-7050" for this suite.
[AfterEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786


 [SLOW TEST:8.114 seconds]
[sig-network] Services
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":-1,"completed":58,"skipped":1083,"failed":0}

SSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:00:59.582: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 29 22:01:00.332: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605640, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605640, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605640, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605640, loc:(*time.Location)(0x77098a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 29 22:01:02.355: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605640, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605640, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605640, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605640, loc:(*time.Location)(0x77098a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 29 22:01:05.384: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
Oct 29 22:01:05.871: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:06.058: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:06.151: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:06.267: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:06.378: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:06.554: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:06.652: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:06.753: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:06.854: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:06.960: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:07.053: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:07.149: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:07.249: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:07.353: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:07.450: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:07.551: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:07.650: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:07.757: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:07.924: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:08.064: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:08.165: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:08.261: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:08.355: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:08.459: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:08.551: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:08.651: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:08.749: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:08.853: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:08.953: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:09.050: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:09.148: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:09.249: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:09.355: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:09.456: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:09.565: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:09.656: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:09.751: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:09.850: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:09.951: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:10.054: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:10.153: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:10.263: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:10.359: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:10.471: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:10.567: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:10.664: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:10.843: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:10.967: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:11.055: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:11.177: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:11.358: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:11.505: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:11.722: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:11.940: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:12.096: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:12.316: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:12.525: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:12.685: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:12.865: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:12.986: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:13.156: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:13.260: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:13.359: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:13.452: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:13.572: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:13.756: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:13.850: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:13.956: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:14.067: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:14.151: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:14.257: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:14.370: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:14.461: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:14.635: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:14.759: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:14.861: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:14.967: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:15.064: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:15.158: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:15.268: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:15.363: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:15.456: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:15.588: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:15.809: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:15.954: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:16.053: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:16.151: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:16.274: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:16.474: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:16.657: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:16.758: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:16.853: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:16.954: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:17.060: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:17.166: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:17.250: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:17.353: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:17.454: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:17.552: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:17.652: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:17.755: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:17.860: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:17.951: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:18.064: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:18.165: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:18.269: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:18.370: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:18.457: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:18.555: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:18.656: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:18.753: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:18.853: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:18.951: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:19.050: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:19.174: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:19.387: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:19.580: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:01:19.939: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-1068" for this suite.
STEP: Destroying namespace "webhook-1068-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102


 [SLOW TEST:20.631 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":-1,"completed":51,"skipped":844,"failed":0}

SSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:01:19.457: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Oct 29 22:01:19.618: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
Oct 29 22:01:26.462: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:01:51.457: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1690" for this suite.


 [SLOW TEST:32.109 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":-1,"completed":59,"skipped":1096,"failed":0}

SSS
------------------------------
[BeforeEach] [sig-apps] StatefulSet
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:55:51.496: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-5520
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating stateful set ss in namespace statefulset-5520
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5520
Oct 29 21:55:51.734: INFO: Found 0 stateful pods, waiting for 1
Oct 29 21:56:01.782: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Oct 29 21:56:01.816: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 29 21:56:02.171: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 29 21:56:02.171: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 29 21:56:02.171: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Oct 29 21:56:02.194: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Oct 29 21:56:12.225: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Oct 29 21:56:12.225: INFO: Waiting for statefulset status.replicas updated to 0
Oct 29 21:56:12.317: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Oct 29 21:56:12.317: INFO: ss-0  ip-10-0-142-212.us-east-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:55:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:55:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:55:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:55:32 +0000 UTC  }]
Oct 29 21:56:12.317: INFO: 
Oct 29 21:56:12.317: INFO: StatefulSet ss has not reached scale 3, at 1
Oct 29 21:56:13.340: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.976708466s
Oct 29 21:56:14.457: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.953890695s
Oct 29 21:56:15.480: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.83663998s
Oct 29 21:56:16.502: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.813562285s
Oct 29 21:56:17.524: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.791414873s
Oct 29 21:56:18.546: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.769080957s
Oct 29 21:56:19.569: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.746947652s
Oct 29 21:56:20.591: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.724712941s
Oct 29 21:56:21.614: INFO: Verifying statefulset ss doesn't scale past 3 for another 702.665144ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5520
Oct 29 21:56:22.636: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 29 21:56:22.997: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Oct 29 21:56:22.997: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Oct 29 21:56:22.997: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Oct 29 21:56:22.997: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 29 21:56:23.326: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Oct 29 21:56:23.326: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Oct 29 21:56:23.326: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Oct 29 21:56:23.326: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 29 21:56:23.661: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Oct 29 21:56:23.661: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Oct 29 21:56:23.661: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Oct 29 21:56:23.690: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Oct 29 21:56:33.712: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Oct 29 21:56:33.712: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Oct 29 21:56:33.712: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Oct 29 21:56:33.734: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 29 21:56:34.086: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 29 21:56:34.086: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 29 21:56:34.086: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Oct 29 21:56:34.086: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 29 21:56:34.419: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 29 21:56:34.419: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 29 21:56:34.419: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Oct 29 21:56:34.419: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 29 21:56:34.752: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 29 21:56:34.752: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 29 21:56:34.752: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Oct 29 21:56:34.752: INFO: Waiting for statefulset status.replicas updated to 0
Oct 29 21:56:34.774: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Oct 29 21:56:44.821: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Oct 29 21:56:44.821: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Oct 29 21:56:44.821: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Oct 29 21:56:44.897: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Oct 29 21:56:44.897: INFO: ss-0  ip-10-0-142-212.us-east-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:55:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:56:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:56:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:55:32 +0000 UTC  }]
Oct 29 21:56:44.897: INFO: ss-1  ip-10-0-234-238.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:55:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:56:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:56:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:55:52 +0000 UTC  }]
Oct 29 21:56:44.897: INFO: ss-2  ip-10-0-142-212.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:55:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:56:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:56:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:55:52 +0000 UTC  }]
Oct 29 21:56:44.897: INFO: 
Oct 29 21:56:44.897: INFO: StatefulSet ss has not reached scale 0, at 3
Oct 29 21:56:45.919: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Oct 29 21:56:45.919: INFO: ss-0  ip-10-0-142-212.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:55:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:56:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:56:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:55:32 +0000 UTC  }]
Oct 29 21:56:45.919: INFO: ss-1  ip-10-0-234-238.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:55:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:56:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:56:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:55:52 +0000 UTC  }]
Oct 29 21:56:45.919: INFO: ss-2  ip-10-0-142-212.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:55:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:56:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:56:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:55:52 +0000 UTC  }]
Oct 29 21:56:45.919: INFO: 
Oct 29 21:56:45.919: INFO: StatefulSet ss has not reached scale 0, at 3
Oct 29 21:56:46.942: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Oct 29 21:56:46.942: INFO: ss-0  ip-10-0-142-212.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:55:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:56:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:56:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:55:32 +0000 UTC  }]
Oct 29 21:56:46.942: INFO: ss-1  ip-10-0-234-238.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:55:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:56:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:56:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:55:52 +0000 UTC  }]
Oct 29 21:56:46.942: INFO: ss-2  ip-10-0-142-212.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:55:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:56:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:56:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:55:52 +0000 UTC  }]
Oct 29 21:56:46.942: INFO: 
Oct 29 21:56:46.942: INFO: StatefulSet ss has not reached scale 0, at 3
Oct 29 21:56:47.964: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Oct 29 21:56:47.964: INFO: ss-1  ip-10-0-234-238.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:55:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:56:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:56:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:55:52 +0000 UTC  }]
Oct 29 21:56:47.964: INFO: 
Oct 29 21:56:47.964: INFO: StatefulSet ss has not reached scale 0, at 1
Oct 29 21:56:48.987: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Oct 29 21:56:48.987: INFO: ss-1  ip-10-0-234-238.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:55:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:56:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:56:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:55:52 +0000 UTC  }]
Oct 29 21:56:48.987: INFO: 
Oct 29 21:56:48.987: INFO: StatefulSet ss has not reached scale 0, at 1
Oct 29 21:56:50.009: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Oct 29 21:56:50.009: INFO: ss-1  ip-10-0-234-238.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:55:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:56:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:56:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:55:52 +0000 UTC  }]
Oct 29 21:56:50.009: INFO: 
Oct 29 21:56:50.009: INFO: StatefulSet ss has not reached scale 0, at 1
Oct 29 21:56:51.032: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Oct 29 21:56:51.032: INFO: ss-1  ip-10-0-234-238.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:55:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:56:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:56:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:55:52 +0000 UTC  }]
Oct 29 21:56:51.032: INFO: 
Oct 29 21:56:51.032: INFO: StatefulSet ss has not reached scale 0, at 1
Oct 29 21:56:52.054: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Oct 29 21:56:52.054: INFO: ss-1  ip-10-0-234-238.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:55:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:56:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:56:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:55:52 +0000 UTC  }]
Oct 29 21:56:52.054: INFO: 
Oct 29 21:56:52.054: INFO: StatefulSet ss has not reached scale 0, at 1
Oct 29 21:56:53.079: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Oct 29 21:56:53.079: INFO: ss-1  ip-10-0-234-238.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:55:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:56:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:56:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:55:52 +0000 UTC  }]
Oct 29 21:56:53.079: INFO: 
Oct 29 21:56:53.079: INFO: StatefulSet ss has not reached scale 0, at 1
Oct 29 21:56:54.102: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Oct 29 21:56:54.102: INFO: ss-1  ip-10-0-234-238.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:55:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:56:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:56:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-29 21:55:52 +0000 UTC  }]
Oct 29 21:56:54.102: INFO: 
Oct 29 21:56:54.102: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5520
Oct 29 21:56:55.124: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 29 21:56:55.345: INFO: rc: 1
Oct 29 21:56:55.345: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Oct 29 21:57:05.345: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 29 21:57:05.503: INFO: rc: 1
Oct 29 21:57:05.504: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 29 21:57:15.504: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 29 21:57:15.660: INFO: rc: 1
Oct 29 21:57:15.660: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 29 21:57:25.660: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 29 21:57:25.814: INFO: rc: 1
Oct 29 21:57:25.814: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 29 21:57:35.815: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 29 21:57:35.973: INFO: rc: 1
Oct 29 21:57:35.973: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 29 21:57:45.974: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 29 21:57:46.138: INFO: rc: 1
Oct 29 21:57:46.138: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 29 21:57:56.138: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 29 21:57:56.299: INFO: rc: 1
Oct 29 21:57:56.299: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 29 21:58:06.299: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 29 21:58:06.459: INFO: rc: 1
Oct 29 21:58:06.459: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 29 21:58:16.459: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 29 21:58:16.615: INFO: rc: 1
Oct 29 21:58:16.615: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 29 21:58:26.615: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 29 21:58:26.773: INFO: rc: 1
Oct 29 21:58:26.773: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 29 21:58:36.773: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 29 21:58:36.958: INFO: rc: 1
Oct 29 21:58:36.958: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 29 21:58:46.958: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 29 21:58:47.149: INFO: rc: 1
Oct 29 21:58:47.149: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 29 21:58:57.149: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 29 21:58:57.311: INFO: rc: 1
Oct 29 21:58:57.311: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 29 21:59:07.311: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 29 21:59:07.468: INFO: rc: 1
Oct 29 21:59:07.468: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 29 21:59:17.468: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 29 21:59:17.681: INFO: rc: 1
Oct 29 21:59:17.681: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 29 21:59:27.682: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 29 21:59:27.838: INFO: rc: 1
Oct 29 21:59:27.838: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 29 21:59:37.839: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 29 21:59:38.023: INFO: rc: 1
Oct 29 21:59:38.023: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 29 21:59:48.023: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 29 21:59:48.180: INFO: rc: 1
Oct 29 21:59:48.180: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 29 21:59:58.180: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 29 21:59:58.341: INFO: rc: 1
Oct 29 21:59:58.341: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 29 22:00:08.341: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 29 22:00:08.518: INFO: rc: 1
Oct 29 22:00:08.518: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 29 22:00:18.519: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 29 22:00:18.677: INFO: rc: 1
Oct 29 22:00:18.677: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 29 22:00:28.678: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 29 22:00:28.843: INFO: rc: 1
Oct 29 22:00:28.843: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 29 22:00:38.843: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 29 22:00:39.023: INFO: rc: 1
Oct 29 22:00:39.024: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 29 22:00:49.024: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 29 22:00:49.192: INFO: rc: 1
Oct 29 22:00:49.192: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 29 22:00:59.192: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 29 22:00:59.348: INFO: rc: 1
Oct 29 22:00:59.348: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 29 22:01:09.348: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 29 22:01:09.505: INFO: rc: 1
Oct 29 22:01:09.505: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 29 22:01:19.505: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 29 22:01:19.712: INFO: rc: 1
Oct 29 22:01:19.712: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 29 22:01:29.712: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 29 22:01:29.875: INFO: rc: 1
Oct 29 22:01:29.875: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 29 22:01:39.875: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 29 22:01:40.054: INFO: rc: 1
Oct 29 22:01:40.054: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 29 22:01:50.054: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 29 22:01:50.219: INFO: rc: 1
Oct 29 22:01:50.219: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Oct 29 22:02:00.219: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5520 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 29 22:02:00.374: INFO: rc: 1
Oct 29 22:02:00.374: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: 
Oct 29 22:02:00.374: INFO: Scaling statefulset ss to 0
Oct 29 22:02:00.440: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Oct 29 22:02:00.462: INFO: Deleting all statefulset in ns statefulset-5520
Oct 29 22:02:00.483: INFO: Scaling statefulset ss to 0
Oct 29 22:02:00.549: INFO: Waiting for statefulset status.replicas updated to 0
Oct 29 22:02:00.571: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:02:00.646: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "statefulset-5520" for this suite.


 [SLOW TEST:369.235 seconds]
[sig-apps] StatefulSet
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":-1,"completed":30,"skipped":490,"failed":0}

SSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:02:00.738: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Oct 29 22:02:00.923: INFO: Waiting up to 5m0s for pod "pod-136622a4-bdf0-4447-9c74-a48d831a05d6" in namespace "emptydir-5488" to be "Succeeded or Failed"
Oct 29 22:02:00.945: INFO: Pod "pod-136622a4-bdf0-4447-9c74-a48d831a05d6": Phase="Pending", Reason="", readiness=false. Elapsed: 21.79915ms
Oct 29 22:02:02.967: INFO: Pod "pod-136622a4-bdf0-4447-9c74-a48d831a05d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04402758s
Oct 29 22:02:04.989: INFO: Pod "pod-136622a4-bdf0-4447-9c74-a48d831a05d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066176407s
STEP: Saw pod success
Oct 29 22:02:04.989: INFO: Pod "pod-136622a4-bdf0-4447-9c74-a48d831a05d6" satisfied condition "Succeeded or Failed"
Oct 29 22:02:05.010: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-136622a4-bdf0-4447-9c74-a48d831a05d6 container test-container: <nil>
STEP: delete the pod
Oct 29 22:02:05.064: INFO: Waiting for pod pod-136622a4-bdf0-4447-9c74-a48d831a05d6 to disappear
Oct 29 22:02:05.086: INFO: Pod pod-136622a4-bdf0-4447-9c74-a48d831a05d6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:02:05.086: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-5488" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":31,"skipped":495,"failed":0}

SSS
------------------------------
[BeforeEach] [sig-storage] Projected secret
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:02:05.194: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-bcbaf032-fb6e-431b-a652-d5aedc2b337d
STEP: Creating a pod to test consume secrets
Oct 29 22:02:05.396: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-deca393d-e35b-41b8-b4e6-6a8a82586618" in namespace "projected-5797" to be "Succeeded or Failed"
Oct 29 22:02:05.420: INFO: Pod "pod-projected-secrets-deca393d-e35b-41b8-b4e6-6a8a82586618": Phase="Pending", Reason="", readiness=false. Elapsed: 23.130045ms
Oct 29 22:02:07.442: INFO: Pod "pod-projected-secrets-deca393d-e35b-41b8-b4e6-6a8a82586618": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045379851s
Oct 29 22:02:09.464: INFO: Pod "pod-projected-secrets-deca393d-e35b-41b8-b4e6-6a8a82586618": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067569691s
STEP: Saw pod success
Oct 29 22:02:09.464: INFO: Pod "pod-projected-secrets-deca393d-e35b-41b8-b4e6-6a8a82586618" satisfied condition "Succeeded or Failed"
Oct 29 22:02:09.486: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-projected-secrets-deca393d-e35b-41b8-b4e6-6a8a82586618 container projected-secret-volume-test: <nil>
STEP: delete the pod
Oct 29 22:02:09.538: INFO: Waiting for pod pod-projected-secrets-deca393d-e35b-41b8-b4e6-6a8a82586618 to disappear
Oct 29 22:02:09.559: INFO: Pod pod-projected-secrets-deca393d-e35b-41b8-b4e6-6a8a82586618 no longer exists
[AfterEach] [sig-storage] Projected secret
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:02:09.559: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-5797" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":-1,"completed":32,"skipped":498,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Kubelet
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:02:09.703: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:02:13.989: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubelet-test-9623" for this suite.


------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":33,"skipped":551,"failed":0}

S
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:02:14.094: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 22:02:14.295: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-80abf976-0a92-40da-9923-01985f319339
STEP: Creating configMap with name cm-test-opt-upd-f8315e82-a8c4-4f3b-b367-360cd61916b7
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-80abf976-0a92-40da-9923-01985f319339
STEP: Updating configmap cm-test-opt-upd-f8315e82-a8c4-4f3b-b367-360cd61916b7
STEP: Creating configMap with name cm-test-opt-create-106248ce-8978-4fe1-b7e5-3610a65fd4ca
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:02:22.708: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-369" for this suite.


 [SLOW TEST:8.717 seconds]
[sig-storage] ConfigMap
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":-1,"completed":34,"skipped":552,"failed":0}

S
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:01:51.570: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 29 22:01:52.591: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 29 22:01:54.657: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605692, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605692, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605693, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605692, loc:(*time.Location)(0x77098a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 29 22:01:56.681: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605692, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605692, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605693, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605692, loc:(*time.Location)(0x77098a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 29 22:01:59.715: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
Oct 29 22:01:59.819: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:01:59.972: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:00.071: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:00.172: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:00.271: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:00.372: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:00.471: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:00.572: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:00.676: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:00.775: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:00.895: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:00.974: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:01.077: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:01.181: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:01.272: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:01.381: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:01.474: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:01.601: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:01.679: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:01.773: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:01.872: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:01.973: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:02.071: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:02.172: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:02.271: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:02.375: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:02.474: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:02.571: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:02.672: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:02.773: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:02.876: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:02.971: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:03.071: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:03.171: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:03.272: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:03.376: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:03.518: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:03.572: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:03.679: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:03.773: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:03.878: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:03.974: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:04.071: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:04.173: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:04.271: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:04.372: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:04.473: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:04.571: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:04.673: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:04.773: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:04.872: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:04.972: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:05.071: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:05.171: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:05.292: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:05.371: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:05.480: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:05.572: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:05.672: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:05.777: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:05.874: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:05.976: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:06.073: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:06.172: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:06.274: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:06.375: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:06.475: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:06.583: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:06.676: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:06.793: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:06.884: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:06.973: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:07.078: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:07.174: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:07.275: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:07.372: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:07.473: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:07.573: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:07.673: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:07.772: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:07.886: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:07.972: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:08.078: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:08.174: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:08.272: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:08.372: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:08.471: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:08.573: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:08.671: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:08.775: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:08.870: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:08.971: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:09.071: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:09.172: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:09.273: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:09.371: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:09.471: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:09.571: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:09.678: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:09.794: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:09.893: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:09.973: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:10.074: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:10.172: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:10.273: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:10.380: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:10.471: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:10.579: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:10.672: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:10.771: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:10.874: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:10.972: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:11.072: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:11.171: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:11.274: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:11.395: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:11.478: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:02:11.575: INFO: Waiting for webhook configuration to be ready...
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:02:24.083: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-2901" for this suite.
STEP: Destroying namespace "webhook-2901-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102


 [SLOW TEST:32.775 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":-1,"completed":60,"skipped":1099,"failed":0}

SSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected secret
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:02:22.813: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 22:02:22.981: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-33a87e8c-5ada-4816-bacc-d706741af97e
STEP: Creating secret with name s-test-opt-upd-22bd1956-7d84-4fd9-bda8-17cfe3d790d6
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-33a87e8c-5ada-4816-bacc-d706741af97e
STEP: Updating secret s-test-opt-upd-22bd1956-7d84-4fd9-bda8-17cfe3d790d6
STEP: Creating secret with name s-test-opt-create-67bb561f-6e79-4373-980c-c83b521e1f76
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:02:29.386: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-4625" for this suite.


 [SLOW TEST:6.680 seconds]
[sig-storage] Projected secret
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":-1,"completed":35,"skipped":553,"failed":0}

SSSSSS
------------------------------
[BeforeEach] [sig-network] DNS
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:02:24.356: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2294.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2294.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2294.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2294.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2294.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2294.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2294.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2294.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2294.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2294.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2294.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2294.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2294.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 220.234.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.234.220_udp@PTR;check="$$(dig +tcp +noall +answer +search 220.234.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.234.220_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2294.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2294.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2294.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2294.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2294.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2294.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2294.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2294.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2294.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2294.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2294.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2294.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2294.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 220.234.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.234.220_udp@PTR;check="$$(dig +tcp +noall +answer +search 220.234.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.234.220_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct 29 22:02:28.715: INFO: Unable to read wheezy_udp@dns-test-service.dns-2294.svc.cluster.local from pod dns-2294/dns-test-de06ce18-d3ba-4e01-bbd6-9deaee22a65b: the server could not find the requested resource (get pods dns-test-de06ce18-d3ba-4e01-bbd6-9deaee22a65b)
Oct 29 22:02:28.737: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2294.svc.cluster.local from pod dns-2294/dns-test-de06ce18-d3ba-4e01-bbd6-9deaee22a65b: the server could not find the requested resource (get pods dns-test-de06ce18-d3ba-4e01-bbd6-9deaee22a65b)
Oct 29 22:02:28.762: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2294.svc.cluster.local from pod dns-2294/dns-test-de06ce18-d3ba-4e01-bbd6-9deaee22a65b: the server could not find the requested resource (get pods dns-test-de06ce18-d3ba-4e01-bbd6-9deaee22a65b)
Oct 29 22:02:28.784: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2294.svc.cluster.local from pod dns-2294/dns-test-de06ce18-d3ba-4e01-bbd6-9deaee22a65b: the server could not find the requested resource (get pods dns-test-de06ce18-d3ba-4e01-bbd6-9deaee22a65b)
Oct 29 22:02:28.973: INFO: Unable to read jessie_tcp@dns-test-service.dns-2294.svc.cluster.local from pod dns-2294/dns-test-de06ce18-d3ba-4e01-bbd6-9deaee22a65b: the server could not find the requested resource (get pods dns-test-de06ce18-d3ba-4e01-bbd6-9deaee22a65b)
Oct 29 22:02:29.020: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2294.svc.cluster.local from pod dns-2294/dns-test-de06ce18-d3ba-4e01-bbd6-9deaee22a65b: the server could not find the requested resource (get pods dns-test-de06ce18-d3ba-4e01-bbd6-9deaee22a65b)
Oct 29 22:02:29.154: INFO: Lookups using dns-2294/dns-test-de06ce18-d3ba-4e01-bbd6-9deaee22a65b failed for: [wheezy_udp@dns-test-service.dns-2294.svc.cluster.local wheezy_tcp@dns-test-service.dns-2294.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2294.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2294.svc.cluster.local jessie_tcp@dns-test-service.dns-2294.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2294.svc.cluster.local]

Oct 29 22:02:34.613: INFO: DNS probes using dns-2294/dns-test-de06ce18-d3ba-4e01-bbd6-9deaee22a65b succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:02:34.756: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "dns-2294" for this suite.


 [SLOW TEST:10.497 seconds]
[sig-network] DNS
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":-1,"completed":61,"skipped":1110,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Kubelet
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:02:34.872: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:02:39.235: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubelet-test-128" for this suite.


------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":-1,"completed":62,"skipped":1133,"failed":0}

SSSSSSS
------------------------------
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:02:29.499: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Oct 29 22:02:37.934: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Oct 29 22:02:37.959: INFO: Pod pod-with-prestop-http-hook still exists
Oct 29 22:02:39.960: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Oct 29 22:02:39.981: INFO: Pod pod-with-prestop-http-hook still exists
Oct 29 22:02:41.960: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Oct 29 22:02:41.982: INFO: Pod pod-with-prestop-http-hook still exists
Oct 29 22:02:43.960: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Oct 29 22:02:43.982: INFO: Pod pod-with-prestop-http-hook still exists
Oct 29 22:02:45.960: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Oct 29 22:02:45.982: INFO: Pod pod-with-prestop-http-hook still exists
Oct 29 22:02:47.960: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Oct 29 22:02:47.981: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:02:48.005: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7513" for this suite.


 [SLOW TEST:18.610 seconds]
[k8s.io] Container Lifecycle Hook
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":-1,"completed":36,"skipped":559,"failed":0}

SSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:02:48.116: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Oct 29 22:02:48.931: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605749, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605749, loc:(*time.Location)(0x77098a0)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-crd-conversion-webhook-deployment-85d57b96d6\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605749, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605749, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Oct 29 22:02:50.953: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605749, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605749, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605749, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605749, loc:(*time.Location)(0x77098a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 29 22:02:53.982: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 22:02:54.008: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
Oct 29 22:02:54.693: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:35Z is before 2020-10-29T22:02:48Z
Oct 29 22:02:54.818: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:35Z is before 2020-10-29T22:02:48Z
Oct 29 22:02:54.918: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:35Z is before 2020-10-29T22:02:48Z
Oct 29 22:02:55.018: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:35Z is before 2020-10-29T22:02:48Z
Oct 29 22:02:55.117: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:35Z is before 2020-10-29T22:02:48Z
Oct 29 22:02:55.218: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:35Z is before 2020-10-29T22:02:48Z
Oct 29 22:02:55.318: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:35Z is before 2020-10-29T22:02:48Z
Oct 29 22:02:55.418: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:35Z is before 2020-10-29T22:02:48Z
Oct 29 22:02:55.518: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:35Z is before 2020-10-29T22:02:48Z
Oct 29 22:02:55.618: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:36Z is before 2020-10-29T22:02:48Z
Oct 29 22:02:55.718: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:36Z is before 2020-10-29T22:02:48Z
Oct 29 22:02:55.817: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:36Z is before 2020-10-29T22:02:48Z
Oct 29 22:02:55.917: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:36Z is before 2020-10-29T22:02:48Z
Oct 29 22:02:56.018: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:36Z is before 2020-10-29T22:02:48Z
Oct 29 22:02:56.118: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:36Z is before 2020-10-29T22:02:48Z
Oct 29 22:02:56.220: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:36Z is before 2020-10-29T22:02:48Z
Oct 29 22:02:56.320: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:36Z is before 2020-10-29T22:02:48Z
Oct 29 22:02:56.419: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:36Z is before 2020-10-29T22:02:48Z
Oct 29 22:02:56.530: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:36Z is before 2020-10-29T22:02:48Z
Oct 29 22:02:56.629: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:37Z is before 2020-10-29T22:02:48Z
Oct 29 22:02:56.827: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:37Z is before 2020-10-29T22:02:48Z
Oct 29 22:02:57.028: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:37Z is before 2020-10-29T22:02:48Z
Oct 29 22:02:57.227: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:37Z is before 2020-10-29T22:02:48Z
Oct 29 22:02:57.427: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:37Z is before 2020-10-29T22:02:48Z
Oct 29 22:02:57.628: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:38Z is before 2020-10-29T22:02:48Z
Oct 29 22:02:57.827: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:38Z is before 2020-10-29T22:02:48Z
Oct 29 22:02:58.028: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:38Z is before 2020-10-29T22:02:48Z
Oct 29 22:02:58.227: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:38Z is before 2020-10-29T22:02:48Z
Oct 29 22:02:58.493: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:38Z is before 2020-10-29T22:02:48Z
Oct 29 22:02:58.627: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:39Z is before 2020-10-29T22:02:48Z
Oct 29 22:02:58.827: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:39Z is before 2020-10-29T22:02:48Z
Oct 29 22:02:59.027: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:39Z is before 2020-10-29T22:02:48Z
Oct 29 22:02:59.227: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:39Z is before 2020-10-29T22:02:48Z
Oct 29 22:02:59.427: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:39Z is before 2020-10-29T22:02:48Z
Oct 29 22:02:59.627: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:40Z is before 2020-10-29T22:02:48Z
Oct 29 22:02:59.827: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:40Z is before 2020-10-29T22:02:48Z
Oct 29 22:03:00.027: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:40Z is before 2020-10-29T22:02:48Z
Oct 29 22:03:00.227: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:40Z is before 2020-10-29T22:02:48Z
Oct 29 22:03:00.427: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:40Z is before 2020-10-29T22:02:48Z
Oct 29 22:03:00.627: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:41Z is before 2020-10-29T22:02:48Z
Oct 29 22:03:00.827: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:41Z is before 2020-10-29T22:02:48Z
Oct 29 22:03:01.029: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:41Z is before 2020-10-29T22:02:48Z
Oct 29 22:03:01.227: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:41Z is before 2020-10-29T22:02:48Z
Oct 29 22:03:01.427: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:41Z is before 2020-10-29T22:02:48Z
Oct 29 22:03:01.627: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:42Z is before 2020-10-29T22:02:48Z
Oct 29 22:03:01.827: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:42Z is before 2020-10-29T22:02:48Z
Oct 29 22:03:02.027: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:42Z is before 2020-10-29T22:02:48Z
Oct 29 22:03:02.227: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:42Z is before 2020-10-29T22:02:48Z
Oct 29 22:03:02.427: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:42Z is before 2020-10-29T22:02:48Z
Oct 29 22:03:02.626: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:43Z is before 2020-10-29T22:02:48Z
Oct 29 22:03:02.826: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:43Z is before 2020-10-29T22:02:48Z
Oct 29 22:03:03.027: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:43Z is before 2020-10-29T22:02:48Z
Oct 29 22:03:03.226: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:43Z is before 2020-10-29T22:02:48Z
Oct 29 22:03:03.427: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:43Z is before 2020-10-29T22:02:48Z
Oct 29 22:03:03.627: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:44Z is before 2020-10-29T22:02:48Z
Oct 29 22:03:03.826: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:44Z is before 2020-10-29T22:02:48Z
Oct 29 22:03:04.027: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:44Z is before 2020-10-29T22:02:48Z
Oct 29 22:03:04.226: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:44Z is before 2020-10-29T22:02:48Z
Oct 29 22:03:04.426: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:44Z is before 2020-10-29T22:02:48Z
Oct 29 22:03:04.627: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:45Z is before 2020-10-29T22:02:48Z
Oct 29 22:03:04.827: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:45Z is before 2020-10-29T22:02:48Z
Oct 29 22:03:05.027: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:45Z is before 2020-10-29T22:02:48Z
Oct 29 22:03:05.227: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:45Z is before 2020-10-29T22:02:48Z
Oct 29 22:03:05.427: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:45Z is before 2020-10-29T22:02:48Z
Oct 29 22:03:05.627: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:46Z is before 2020-10-29T22:02:48Z
Oct 29 22:03:05.827: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:46Z is before 2020-10-29T22:02:48Z
Oct 29 22:03:06.027: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:46Z is before 2020-10-29T22:02:48Z
Oct 29 22:03:06.227: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:46Z is before 2020-10-29T22:02:48Z
Oct 29 22:03:06.427: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:46Z is before 2020-10-29T22:02:48Z
Oct 29 22:03:06.627: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:47Z is before 2020-10-29T22:02:48Z
Oct 29 22:03:06.827: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:47Z is before 2020-10-29T22:02:48Z
Oct 29 22:03:07.027: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:47Z is before 2020-10-29T22:02:48Z
Oct 29 22:03:07.227: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:47Z is before 2020-10-29T22:02:48Z
Oct 29 22:03:07.427: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-7708-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4558.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-10-29T22:02:47Z is before 2020-10-29T22:02:48Z
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:03:08.810: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-webhook-4558" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137


 [SLOW TEST:20.958 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":-1,"completed":37,"skipped":565,"failed":0}

SS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:03:09.083: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 29 22:03:10.093: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605770, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605770, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605770, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605770, loc:(*time.Location)(0x77098a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 29 22:03:12.124: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605770, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605770, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605770, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605770, loc:(*time.Location)(0x77098a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 29 22:03:15.144: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 22:03:15.166: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2297-crds.webhook.example.com via the AdmissionRegistration API
Oct 29 22:03:15.330: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:15.489: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:15.582: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:15.682: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:15.782: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:15.891: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:15.982: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:16.083: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:16.187: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:16.288: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:16.384: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:16.485: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:16.586: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:16.709: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:16.789: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:16.884: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:16.983: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:17.082: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:17.182: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:17.284: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:17.384: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:17.483: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:17.605: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:17.702: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:17.798: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:17.895: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:17.984: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:18.086: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:18.183: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:18.293: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:18.426: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:18.525: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:18.673: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:18.796: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:18.893: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:18.983: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:19.083: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:19.184: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:19.302: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:19.381: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:19.483: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:19.582: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:19.692: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:19.787: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:19.882: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:19.984: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:20.086: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:20.182: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:20.281: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:20.382: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:20.481: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:20.586: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:20.682: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:20.782: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:20.881: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:20.981: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:21.083: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:21.194: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:21.286: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:21.386: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:21.485: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:21.587: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:21.685: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:21.783: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:21.883: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:21.981: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:22.081: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:22.184: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:22.282: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:22.382: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:22.481: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:22.582: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:22.683: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:22.788: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:22.888: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:22.993: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:23.088: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:23.181: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:23.281: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:23.381: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:23.481: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:23.581: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:23.683: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:23.783: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:23.880: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:23.983: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:24.085: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:24.184: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:24.281: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:24.383: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:24.481: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:24.582: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:24.700: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:24.783: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:24.881: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:24.982: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:25.082: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:25.183: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:25.282: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:25.381: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:25.481: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:25.583: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:25.681: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:25.781: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:25.882: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:25.981: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:26.083: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:26.181: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:26.283: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:26.385: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:26.487: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:26.582: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:26.684: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:26.783: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:26.882: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:26.980: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:27.081: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:27.181: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:27.281: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:27.389: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:27.482: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:27.582: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:27.681: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:27.781: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:27.882: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:27.982: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:28.082: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:28.181: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:28.283: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:28.381: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:28.480: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:03:28.582: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:03:29.418: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-9172" for this suite.
STEP: Destroying namespace "webhook-9172-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102


 [SLOW TEST:20.777 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":-1,"completed":38,"skipped":567,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Secrets
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:03:29.915: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-4eee0fa3-208a-4537-b0ff-07550a5d3c68
STEP: Creating a pod to test consume secrets
Oct 29 22:03:30.148: INFO: Waiting up to 5m0s for pod "pod-secrets-b821deb1-4ceb-400b-8ceb-cfe7304f22bf" in namespace "secrets-3259" to be "Succeeded or Failed"
Oct 29 22:03:30.170: INFO: Pod "pod-secrets-b821deb1-4ceb-400b-8ceb-cfe7304f22bf": Phase="Pending", Reason="", readiness=false. Elapsed: 22.007568ms
Oct 29 22:03:32.195: INFO: Pod "pod-secrets-b821deb1-4ceb-400b-8ceb-cfe7304f22bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047347426s
Oct 29 22:03:34.218: INFO: Pod "pod-secrets-b821deb1-4ceb-400b-8ceb-cfe7304f22bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.06979585s
STEP: Saw pod success
Oct 29 22:03:34.218: INFO: Pod "pod-secrets-b821deb1-4ceb-400b-8ceb-cfe7304f22bf" satisfied condition "Succeeded or Failed"
Oct 29 22:03:34.239: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-secrets-b821deb1-4ceb-400b-8ceb-cfe7304f22bf container secret-volume-test: <nil>
STEP: delete the pod
Oct 29 22:03:34.292: INFO: Waiting for pod pod-secrets-b821deb1-4ceb-400b-8ceb-cfe7304f22bf to disappear
Oct 29 22:03:34.313: INFO: Pod pod-secrets-b821deb1-4ceb-400b-8ceb-cfe7304f22bf no longer exists
[AfterEach] [sig-storage] Secrets
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:03:34.313: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-3259" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":39,"skipped":640,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Probing container
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 21:59:33.635: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod test-webserver-cb289eb7-3f37-42cd-aaca-fe391e20d0e0 in namespace container-probe-3601
Oct 29 21:59:38.060: INFO: Started pod test-webserver-cb289eb7-3f37-42cd-aaca-fe391e20d0e0 in namespace container-probe-3601
STEP: checking the pod's current state and verifying that restartCount is present
Oct 29 21:59:38.081: INFO: Initial restart count of pod test-webserver-cb289eb7-3f37-42cd-aaca-fe391e20d0e0 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:03:38.894: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-probe-3601" for this suite.


 [SLOW TEST:245.361 seconds]
[k8s.io] Probing container
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":-1,"completed":51,"skipped":822,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] DNS
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:03:34.448: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4639.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4639.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct 29 22:03:38.973: INFO: DNS probes using dns-4639/dns-test-cad11b03-ae9c-4d67-a2eb-2c12a54597e2 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:03:39.010: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "dns-4639" for this suite.


------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":-1,"completed":40,"skipped":665,"failed":0}

SSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:03:39.030: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Oct 29 22:03:39.280: INFO: Waiting up to 5m0s for pod "downwardapi-volume-399e4f65-4d74-4762-841d-39d3985285b1" in namespace "downward-api-7951" to be "Succeeded or Failed"
Oct 29 22:03:39.324: INFO: Pod "downwardapi-volume-399e4f65-4d74-4762-841d-39d3985285b1": Phase="Pending", Reason="", readiness=false. Elapsed: 43.517518ms
Oct 29 22:03:41.346: INFO: Pod "downwardapi-volume-399e4f65-4d74-4762-841d-39d3985285b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0651352s
Oct 29 22:03:43.367: INFO: Pod "downwardapi-volume-399e4f65-4d74-4762-841d-39d3985285b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.086549543s
STEP: Saw pod success
Oct 29 22:03:43.367: INFO: Pod "downwardapi-volume-399e4f65-4d74-4762-841d-39d3985285b1" satisfied condition "Succeeded or Failed"
Oct 29 22:03:43.388: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod downwardapi-volume-399e4f65-4d74-4762-841d-39d3985285b1 container client-container: <nil>
STEP: delete the pod
Oct 29 22:03:43.449: INFO: Waiting for pod downwardapi-volume-399e4f65-4d74-4762-841d-39d3985285b1 to disappear
Oct 29 22:03:43.470: INFO: Pod downwardapi-volume-399e4f65-4d74-4762-841d-39d3985285b1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:03:43.470: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-7951" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":-1,"completed":52,"skipped":871,"failed":0}

SSSS
------------------------------
[BeforeEach] [k8s.io] Probing container
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:03:43.576: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 22:03:43.838: INFO: The status of Pod test-webserver-203af22c-6fc4-4c1b-9ef1-8056b7a5e362 is Pending, waiting for it to be Running (with Ready = true)
Oct 29 22:03:45.860: INFO: The status of Pod test-webserver-203af22c-6fc4-4c1b-9ef1-8056b7a5e362 is Pending, waiting for it to be Running (with Ready = true)
Oct 29 22:03:47.859: INFO: The status of Pod test-webserver-203af22c-6fc4-4c1b-9ef1-8056b7a5e362 is Running (Ready = false)
Oct 29 22:03:49.860: INFO: The status of Pod test-webserver-203af22c-6fc4-4c1b-9ef1-8056b7a5e362 is Running (Ready = false)
Oct 29 22:03:51.860: INFO: The status of Pod test-webserver-203af22c-6fc4-4c1b-9ef1-8056b7a5e362 is Running (Ready = false)
Oct 29 22:03:53.860: INFO: The status of Pod test-webserver-203af22c-6fc4-4c1b-9ef1-8056b7a5e362 is Running (Ready = false)
Oct 29 22:03:55.860: INFO: The status of Pod test-webserver-203af22c-6fc4-4c1b-9ef1-8056b7a5e362 is Running (Ready = false)
Oct 29 22:03:57.860: INFO: The status of Pod test-webserver-203af22c-6fc4-4c1b-9ef1-8056b7a5e362 is Running (Ready = false)
Oct 29 22:03:59.860: INFO: The status of Pod test-webserver-203af22c-6fc4-4c1b-9ef1-8056b7a5e362 is Running (Ready = false)
Oct 29 22:04:01.860: INFO: The status of Pod test-webserver-203af22c-6fc4-4c1b-9ef1-8056b7a5e362 is Running (Ready = false)
Oct 29 22:04:03.859: INFO: The status of Pod test-webserver-203af22c-6fc4-4c1b-9ef1-8056b7a5e362 is Running (Ready = true)
Oct 29 22:04:03.880: INFO: Container started at 2020-10-29 22:03:26 +0000 UTC, pod became ready at 2020-10-29 22:03:42 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:04:03.880: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-probe-4206" for this suite.


 [SLOW TEST:20.409 seconds]
[k8s.io] Probing container
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":-1,"completed":53,"skipped":875,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] StatefulSet
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:02:39.349: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-5253
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-5253
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5253
Oct 29 22:02:39.591: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Oct 29 22:02:49.614: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Oct 29 22:02:49.636: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5253 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 29 22:02:50.000: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 29 22:02:50.000: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 29 22:02:50.000: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Oct 29 22:02:50.022: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Oct 29 22:03:00.044: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Oct 29 22:03:00.044: INFO: Waiting for statefulset status.replicas updated to 0
Oct 29 22:03:00.134: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999437s
Oct 29 22:03:01.156: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.978050816s
Oct 29 22:03:02.178: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.955797866s
Oct 29 22:03:03.201: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.933642904s
Oct 29 22:03:04.222: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.911467405s
Oct 29 22:03:05.245: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.889627599s
Oct 29 22:03:06.268: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.867164035s
Oct 29 22:03:07.292: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.84406251s
Oct 29 22:03:08.315: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.820475991s
Oct 29 22:03:09.356: INFO: Verifying statefulset ss doesn't scale past 1 for another 796.988931ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5253
Oct 29 22:03:10.381: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5253 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 29 22:03:10.737: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Oct 29 22:03:10.737: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Oct 29 22:03:10.737: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Oct 29 22:03:10.759: INFO: Found 1 stateful pods, waiting for 3
Oct 29 22:03:20.781: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Oct 29 22:03:20.781: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Oct 29 22:03:20.781: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Oct 29 22:03:20.825: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5253 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 29 22:03:21.174: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 29 22:03:21.174: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 29 22:03:21.174: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Oct 29 22:03:21.174: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5253 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 29 22:03:21.527: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 29 22:03:21.527: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 29 22:03:21.527: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Oct 29 22:03:21.527: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5253 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 29 22:03:21.861: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 29 22:03:21.861: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 29 22:03:21.861: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Oct 29 22:03:21.861: INFO: Waiting for statefulset status.replicas updated to 0
Oct 29 22:03:21.884: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Oct 29 22:03:31.929: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Oct 29 22:03:31.929: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Oct 29 22:03:31.929: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Oct 29 22:03:32.001: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999776s
Oct 29 22:03:33.025: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.976096934s
Oct 29 22:03:34.047: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.953072429s
Oct 29 22:03:35.073: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.930211499s
Oct 29 22:03:36.097: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.904777156s
Oct 29 22:03:37.121: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.880815919s
Oct 29 22:03:38.144: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.856905196s
Oct 29 22:03:39.201: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.834041953s
Oct 29 22:03:40.225: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.77600749s
Oct 29 22:03:41.250: INFO: Verifying statefulset ss doesn't scale past 3 for another 752.731789ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5253
Oct 29 22:03:42.273: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5253 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 29 22:03:42.604: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Oct 29 22:03:42.604: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Oct 29 22:03:42.604: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Oct 29 22:03:42.604: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5253 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 29 22:03:42.935: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Oct 29 22:03:42.935: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Oct 29 22:03:42.935: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Oct 29 22:03:42.935: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=statefulset-5253 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 29 22:03:43.265: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Oct 29 22:03:43.265: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Oct 29 22:03:43.265: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Oct 29 22:03:43.265: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Oct 29 22:04:13.358: INFO: Deleting all statefulset in ns statefulset-5253
Oct 29 22:04:13.380: INFO: Scaling statefulset ss to 0
Oct 29 22:04:13.455: INFO: Waiting for statefulset status.replicas updated to 0
Oct 29 22:04:13.476: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:04:13.545: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "statefulset-5253" for this suite.


 [SLOW TEST:94.282 seconds]
[sig-apps] StatefulSet
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":-1,"completed":63,"skipped":1140,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:04:04.012: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 29 22:04:05.218: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605825, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605825, loc:(*time.Location)(0x77098a0)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-cbccbf6bb\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605825, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605825, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Oct 29 22:04:07.239: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605825, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605825, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605825, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605825, loc:(*time.Location)(0x77098a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 29 22:04:10.272: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the crd webhook via the AdmissionRegistration API
Oct 29 22:04:10.376: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:10.531: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:10.627: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:10.728: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:10.826: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:10.927: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:11.026: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:11.126: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:11.229: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:11.335: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:11.427: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:11.529: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:11.626: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:11.730: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:11.828: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:11.927: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:12.028: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:12.128: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:12.228: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:12.328: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:12.427: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:12.527: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:12.626: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:12.735: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:12.827: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:12.927: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:13.029: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:13.127: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:13.227: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:13.328: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:13.426: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:13.526: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:13.628: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:13.732: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:13.839: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:13.928: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:14.031: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:14.128: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:14.228: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:14.328: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:14.427: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:14.527: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:14.627: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:14.727: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:14.829: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:14.928: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:15.027: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:15.127: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:15.229: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:15.333: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:15.431: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:15.542: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:15.626: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:15.729: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:15.826: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:15.927: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:16.029: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:16.127: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:16.230: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:16.350: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:16.435: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:16.532: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:16.627: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:16.738: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:16.827: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:16.976: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:17.026: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:17.127: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:17.226: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:17.330: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:17.428: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:17.526: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:17.626: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:17.739: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:17.844: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:17.948: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:18.028: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:18.126: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:18.231: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:18.326: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:18.427: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:18.526: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:18.637: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:18.728: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:18.831: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:18.947: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:19.026: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:19.135: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:19.226: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:19.328: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:19.427: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:19.527: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:19.629: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:19.730: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:19.827: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:19.946: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:20.032: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:20.134: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:20.227: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:20.327: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:20.428: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:20.528: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:20.629: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:20.733: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:20.826: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:20.926: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:21.026: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:21.128: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:21.228: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:21.336: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:21.438: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:21.529: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:21.627: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:21.730: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:21.826: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:21.927: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:22.027: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:22.127: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:22.226: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:22.344: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:22.426: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:22.530: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:22.648: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:22.729: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:22.830: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:22.926: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:23.031: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:23.127: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:23.242: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:23.326: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:23.426: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:23.526: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:04:23.625: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource definition that should be denied by the webhook
Oct 29 22:04:23.702: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:04:23.750: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-364" for this suite.
STEP: Destroying namespace "webhook-364-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102


 [SLOW TEST:19.953 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":-1,"completed":54,"skipped":910,"failed":0}
[BeforeEach] [sig-network] DNS
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:04:23.966: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8499 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8499;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8499 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8499;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8499.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8499.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8499.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8499.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8499.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8499.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8499.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8499.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8499.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8499.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8499.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8499.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8499.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 14.151.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.151.14_udp@PTR;check="$$(dig +tcp +noall +answer +search 14.151.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.151.14_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8499 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8499;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8499 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8499;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8499.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8499.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8499.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8499.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8499.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8499.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8499.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8499.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8499.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8499.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8499.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8499.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8499.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 14.151.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.151.14_udp@PTR;check="$$(dig +tcp +noall +answer +search 14.151.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.151.14_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct 29 22:04:28.356: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8499/dns-test-aa8f206b-523a-409d-8d36-83e733064655: the server could not find the requested resource (get pods dns-test-aa8f206b-523a-409d-8d36-83e733064655)
Oct 29 22:04:28.377: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8499/dns-test-aa8f206b-523a-409d-8d36-83e733064655: the server could not find the requested resource (get pods dns-test-aa8f206b-523a-409d-8d36-83e733064655)
Oct 29 22:04:28.398: INFO: Unable to read wheezy_udp@dns-test-service.dns-8499 from pod dns-8499/dns-test-aa8f206b-523a-409d-8d36-83e733064655: the server could not find the requested resource (get pods dns-test-aa8f206b-523a-409d-8d36-83e733064655)
Oct 29 22:04:28.420: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8499 from pod dns-8499/dns-test-aa8f206b-523a-409d-8d36-83e733064655: the server could not find the requested resource (get pods dns-test-aa8f206b-523a-409d-8d36-83e733064655)
Oct 29 22:04:28.441: INFO: Unable to read wheezy_udp@dns-test-service.dns-8499.svc from pod dns-8499/dns-test-aa8f206b-523a-409d-8d36-83e733064655: the server could not find the requested resource (get pods dns-test-aa8f206b-523a-409d-8d36-83e733064655)
Oct 29 22:04:28.462: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8499.svc from pod dns-8499/dns-test-aa8f206b-523a-409d-8d36-83e733064655: the server could not find the requested resource (get pods dns-test-aa8f206b-523a-409d-8d36-83e733064655)
Oct 29 22:04:28.661: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8499/dns-test-aa8f206b-523a-409d-8d36-83e733064655: the server could not find the requested resource (get pods dns-test-aa8f206b-523a-409d-8d36-83e733064655)
Oct 29 22:04:28.682: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8499/dns-test-aa8f206b-523a-409d-8d36-83e733064655: the server could not find the requested resource (get pods dns-test-aa8f206b-523a-409d-8d36-83e733064655)
Oct 29 22:04:28.704: INFO: Unable to read jessie_udp@dns-test-service.dns-8499 from pod dns-8499/dns-test-aa8f206b-523a-409d-8d36-83e733064655: the server could not find the requested resource (get pods dns-test-aa8f206b-523a-409d-8d36-83e733064655)
Oct 29 22:04:28.725: INFO: Unable to read jessie_tcp@dns-test-service.dns-8499 from pod dns-8499/dns-test-aa8f206b-523a-409d-8d36-83e733064655: the server could not find the requested resource (get pods dns-test-aa8f206b-523a-409d-8d36-83e733064655)
Oct 29 22:04:28.747: INFO: Unable to read jessie_udp@dns-test-service.dns-8499.svc from pod dns-8499/dns-test-aa8f206b-523a-409d-8d36-83e733064655: the server could not find the requested resource (get pods dns-test-aa8f206b-523a-409d-8d36-83e733064655)
Oct 29 22:04:28.768: INFO: Unable to read jessie_tcp@dns-test-service.dns-8499.svc from pod dns-8499/dns-test-aa8f206b-523a-409d-8d36-83e733064655: the server could not find the requested resource (get pods dns-test-aa8f206b-523a-409d-8d36-83e733064655)
Oct 29 22:04:28.789: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8499.svc from pod dns-8499/dns-test-aa8f206b-523a-409d-8d36-83e733064655: the server could not find the requested resource (get pods dns-test-aa8f206b-523a-409d-8d36-83e733064655)
Oct 29 22:04:28.946: INFO: Lookups using dns-8499/dns-test-aa8f206b-523a-409d-8d36-83e733064655 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8499 wheezy_tcp@dns-test-service.dns-8499 wheezy_udp@dns-test-service.dns-8499.svc wheezy_tcp@dns-test-service.dns-8499.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8499 jessie_tcp@dns-test-service.dns-8499 jessie_udp@dns-test-service.dns-8499.svc jessie_tcp@dns-test-service.dns-8499.svc jessie_udp@_http._tcp.dns-test-service.dns-8499.svc]

Oct 29 22:04:34.561: INFO: DNS probes using dns-8499/dns-test-aa8f206b-523a-409d-8d36-83e733064655 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:04:34.674: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "dns-8499" for this suite.


 [SLOW TEST:10.793 seconds]
[sig-network] DNS
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":-1,"completed":55,"skipped":910,"failed":0}

SSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:04:34.765: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:04:35.075: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "resourcequota-9030" for this suite.


------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":-1,"completed":56,"skipped":917,"failed":0}

SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] ReplicationController
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:04:35.142: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:04:39.423: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "replication-controller-7239" for this suite.


------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":-1,"completed":57,"skipped":939,"failed":0}

SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Probing container
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:04:13.654: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-5a07eeaa-bcb0-4464-841c-2dff0780459e in namespace container-probe-4759
Oct 29 22:04:17.918: INFO: Started pod liveness-5a07eeaa-bcb0-4464-841c-2dff0780459e in namespace container-probe-4759
STEP: checking the pod's current state and verifying that restartCount is present
Oct 29 22:04:17.941: INFO: Initial restart count of pod liveness-5a07eeaa-bcb0-4464-841c-2dff0780459e is 0
Oct 29 22:04:40.214: INFO: Restart count of pod container-probe-4759/liveness-5a07eeaa-bcb0-4464-841c-2dff0780459e is now 1 (22.27291796s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:04:40.251: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-probe-4759" for this suite.


 [SLOW TEST:26.707 seconds]
[k8s.io] Probing container
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":-1,"completed":64,"skipped":1172,"failed":0}

SSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:04:40.374: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should find a service from listing all namespaces [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching services
[AfterEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:04:40.526: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-3996" for this suite.
[AfterEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786


------------------------------
{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":-1,"completed":65,"skipped":1191,"failed":0}

SSSSS
------------------------------
[BeforeEach] [k8s.io] Security Context
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:04:39.544: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 22:04:39.731: INFO: Waiting up to 5m0s for pod "busybox-user-65534-d2c262c4-cc6d-4547-b380-ebf08472e59b" in namespace "security-context-test-9135" to be "Succeeded or Failed"
Oct 29 22:04:39.751: INFO: Pod "busybox-user-65534-d2c262c4-cc6d-4547-b380-ebf08472e59b": Phase="Pending", Reason="", readiness=false. Elapsed: 20.643215ms
Oct 29 22:04:41.773: INFO: Pod "busybox-user-65534-d2c262c4-cc6d-4547-b380-ebf08472e59b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042130416s
Oct 29 22:04:43.795: INFO: Pod "busybox-user-65534-d2c262c4-cc6d-4547-b380-ebf08472e59b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063819865s
Oct 29 22:04:43.795: INFO: Pod "busybox-user-65534-d2c262c4-cc6d-4547-b380-ebf08472e59b" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:04:43.795: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "security-context-test-9135" for this suite.


------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":58,"skipped":961,"failed":0}

SS
------------------------------
[BeforeEach] [sig-storage] Projected secret
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:04:40.580: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name projected-secret-test-7d55851b-5a4b-4abb-bf37-e7fe9923a61c
STEP: Creating a pod to test consume secrets
Oct 29 22:04:40.784: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e10b948d-4317-4bfb-88cd-c69f6da591eb" in namespace "projected-1288" to be "Succeeded or Failed"
Oct 29 22:04:40.808: INFO: Pod "pod-projected-secrets-e10b948d-4317-4bfb-88cd-c69f6da591eb": Phase="Pending", Reason="", readiness=false. Elapsed: 23.964801ms
Oct 29 22:04:42.832: INFO: Pod "pod-projected-secrets-e10b948d-4317-4bfb-88cd-c69f6da591eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04810738s
Oct 29 22:04:44.855: INFO: Pod "pod-projected-secrets-e10b948d-4317-4bfb-88cd-c69f6da591eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.071308648s
STEP: Saw pod success
Oct 29 22:04:44.855: INFO: Pod "pod-projected-secrets-e10b948d-4317-4bfb-88cd-c69f6da591eb" satisfied condition "Succeeded or Failed"
Oct 29 22:04:44.877: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-projected-secrets-e10b948d-4317-4bfb-88cd-c69f6da591eb container secret-volume-test: <nil>
STEP: delete the pod
Oct 29 22:04:44.933: INFO: Waiting for pod pod-projected-secrets-e10b948d-4317-4bfb-88cd-c69f6da591eb to disappear
Oct 29 22:04:44.955: INFO: Pod pod-projected-secrets-e10b948d-4317-4bfb-88cd-c69f6da591eb no longer exists
[AfterEach] [sig-storage] Projected secret
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:04:44.955: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-1288" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":-1,"completed":66,"skipped":1196,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:04:45.089: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-e19c4510-6ee8-4741-8a7e-fccc711b636c
STEP: Creating a pod to test consume configMaps
Oct 29 22:04:45.313: INFO: Waiting up to 5m0s for pod "pod-configmaps-d1dc1b43-bb3b-43f2-a91f-6fe6339da9b7" in namespace "configmap-5894" to be "Succeeded or Failed"
Oct 29 22:04:45.335: INFO: Pod "pod-configmaps-d1dc1b43-bb3b-43f2-a91f-6fe6339da9b7": Phase="Pending", Reason="", readiness=false. Elapsed: 22.90503ms
Oct 29 22:04:47.358: INFO: Pod "pod-configmaps-d1dc1b43-bb3b-43f2-a91f-6fe6339da9b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045884125s
Oct 29 22:04:49.392: INFO: Pod "pod-configmaps-d1dc1b43-bb3b-43f2-a91f-6fe6339da9b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.079874639s
STEP: Saw pod success
Oct 29 22:04:49.392: INFO: Pod "pod-configmaps-d1dc1b43-bb3b-43f2-a91f-6fe6339da9b7" satisfied condition "Succeeded or Failed"
Oct 29 22:04:49.473: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-configmaps-d1dc1b43-bb3b-43f2-a91f-6fe6339da9b7 container configmap-volume-test: <nil>
STEP: delete the pod
Oct 29 22:04:49.526: INFO: Waiting for pod pod-configmaps-d1dc1b43-bb3b-43f2-a91f-6fe6339da9b7 to disappear
Oct 29 22:04:49.547: INFO: Pod pod-configmaps-d1dc1b43-bb3b-43f2-a91f-6fe6339da9b7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:04:49.547: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-5894" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":67,"skipped":1227,"failed":0}

SSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:04:43.900: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Oct 29 22:04:48.735: INFO: Successfully updated pod "labelsupdate761c7deb-8a93-4407-9809-fbaceb64cff7"
[AfterEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:04:52.806: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-8477" for this suite.


 [SLOW TEST:9.010 seconds]
[sig-storage] Projected downwardAPI
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":-1,"completed":59,"skipped":963,"failed":0}

SSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Garbage collector
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:04:52.915: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: Gathering metrics
W1029 22:04:53.232577   53213 metrics_grabber.go:83] Can't find any pods in namespace kube-system to grab metrics from
W1029 22:04:53.232591   53213 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1029 22:04:53.232597   53213 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1029 22:04:53.232603   53213 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Oct 29 22:04:53.232: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:04:53.232: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "gc-225" for this suite.


------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":-1,"completed":60,"skipped":968,"failed":0}

SSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Watchers
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:04:53.306: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Oct 29 22:04:53.516: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4926 /api/v1/namespaces/watch-4926/configmaps/e2e-watch-test-watch-closed a1ab3586-f500-4950-a5de-542d871ea9be 254318 0 2020-10-29 22:04:33 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-10-29 22:04:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 29 22:04:53.516: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4926 /api/v1/namespaces/watch-4926/configmaps/e2e-watch-test-watch-closed a1ab3586-f500-4950-a5de-542d871ea9be 254322 0 2020-10-29 22:04:33 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-10-29 22:04:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Oct 29 22:04:53.603: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4926 /api/v1/namespaces/watch-4926/configmaps/e2e-watch-test-watch-closed a1ab3586-f500-4950-a5de-542d871ea9be 254323 0 2020-10-29 22:04:33 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-10-29 22:04:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 29 22:04:53.603: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4926 /api/v1/namespaces/watch-4926/configmaps/e2e-watch-test-watch-closed a1ab3586-f500-4950-a5de-542d871ea9be 254324 0 2020-10-29 22:04:33 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-10-29 22:04:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:04:53.603: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "watch-4926" for this suite.


------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":-1,"completed":61,"skipped":975,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:04:53.666: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl can dry-run update Pods [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Oct 29 22:04:53.791: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-3560'
Oct 29 22:04:53.962: INFO: stderr: ""
Oct 29 22:04:53.962: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Oct 29 22:04:53.962: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig get pod e2e-test-httpd-pod -o json --namespace=kubectl-3560'
Oct 29 22:04:54.100: INFO: stderr: ""
Oct 29 22:04:54.100: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2020-10-29T22:04:34Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-10-29T22:04:34Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:message\": {},\n                                \"f:reason\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:message\": {},\n                                \"f:reason\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-10-29T22:04:34Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-3560\",\n        \"resourceVersion\": \"254363\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-3560/pods/e2e-test-httpd-pod\",\n        \"uid\": \"79bd53a9-7458-4664-96ad-2562b6e73908\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-kfqhj\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"imagePullSecrets\": [\n            {\n                \"name\": \"default-dockercfg-6sgmb\"\n            }\n        ],\n        \"nodeName\": \"ip-10-0-142-212.us-east-2.compute.internal\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c97,c24\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-kfqhj\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-kfqhj\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-10-29T22:04:34Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-10-29T22:04:34Z\",\n                \"message\": \"containers with unready status: [e2e-test-httpd-pod]\",\n                \"reason\": \"ContainersNotReady\",\n                \"status\": \"False\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-10-29T22:04:34Z\",\n                \"message\": \"containers with unready status: [e2e-test-httpd-pod]\",\n                \"reason\": \"ContainersNotReady\",\n                \"status\": \"False\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-10-29T22:04:34Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": false,\n                \"restartCount\": 0,\n                \"started\": false,\n                \"state\": {\n                    \"waiting\": {\n                        \"reason\": \"ContainerCreating\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.142.212\",\n        \"phase\": \"Pending\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-10-29T22:04:34Z\"\n    }\n}\n"
Oct 29 22:04:54.100: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig replace -f - --dry-run server --namespace=kubectl-3560'
Oct 29 22:04:54.723: INFO: stderr: "W1029 22:04:54.151751   54706 helpers.go:553] --dry-run is deprecated and can be replaced with --dry-run=client.\n"
Oct 29 22:04:54.723: INFO: stdout: "pod/e2e-test-httpd-pod replaced (dry run)\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
Oct 29 22:04:54.747: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig delete pods e2e-test-httpd-pod --namespace=kubectl-3560'
Oct 29 22:05:01.350: INFO: stderr: ""
Oct 29 22:05:01.350: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:05:01.350: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-3560" for this suite.


 [SLOW TEST:7.786 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:919
    should check if kubectl can dry-run update Pods [Conformance]
    /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":-1,"completed":62,"skipped":998,"failed":0}

SSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected configMap
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:05:01.458: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-f366857e-3ae7-4b56-b4ad-4b6bae00690d
STEP: Creating a pod to test consume configMaps
Oct 29 22:05:01.803: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d93947ba-5ef8-4e06-a308-9d94dffd18e8" in namespace "projected-7610" to be "Succeeded or Failed"
Oct 29 22:05:01.824: INFO: Pod "pod-projected-configmaps-d93947ba-5ef8-4e06-a308-9d94dffd18e8": Phase="Pending", Reason="", readiness=false. Elapsed: 21.406577ms
Oct 29 22:05:03.846: INFO: Pod "pod-projected-configmaps-d93947ba-5ef8-4e06-a308-9d94dffd18e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0425953s
Oct 29 22:05:05.867: INFO: Pod "pod-projected-configmaps-d93947ba-5ef8-4e06-a308-9d94dffd18e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063711754s
STEP: Saw pod success
Oct 29 22:05:05.867: INFO: Pod "pod-projected-configmaps-d93947ba-5ef8-4e06-a308-9d94dffd18e8" satisfied condition "Succeeded or Failed"
Oct 29 22:05:05.888: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-projected-configmaps-d93947ba-5ef8-4e06-a308-9d94dffd18e8 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Oct 29 22:05:05.941: INFO: Waiting for pod pod-projected-configmaps-d93947ba-5ef8-4e06-a308-9d94dffd18e8 to disappear
Oct 29 22:05:05.962: INFO: Pod pod-projected-configmaps-d93947ba-5ef8-4e06-a308-9d94dffd18e8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:05:05.962: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-7610" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":-1,"completed":63,"skipped":1004,"failed":0}

SSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Variable Expansion
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:05:06.074: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's args
Oct 29 22:05:06.244: INFO: Waiting up to 5m0s for pod "var-expansion-4a00fbdd-dfc5-4f48-8a6e-b573784da740" in namespace "var-expansion-7201" to be "Succeeded or Failed"
Oct 29 22:05:06.278: INFO: Pod "var-expansion-4a00fbdd-dfc5-4f48-8a6e-b573784da740": Phase="Pending", Reason="", readiness=false. Elapsed: 33.741915ms
Oct 29 22:05:08.300: INFO: Pod "var-expansion-4a00fbdd-dfc5-4f48-8a6e-b573784da740": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056173157s
Oct 29 22:05:10.322: INFO: Pod "var-expansion-4a00fbdd-dfc5-4f48-8a6e-b573784da740": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.078355568s
STEP: Saw pod success
Oct 29 22:05:10.322: INFO: Pod "var-expansion-4a00fbdd-dfc5-4f48-8a6e-b573784da740" satisfied condition "Succeeded or Failed"
Oct 29 22:05:10.344: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod var-expansion-4a00fbdd-dfc5-4f48-8a6e-b573784da740 container dapi-container: <nil>
STEP: delete the pod
Oct 29 22:05:10.398: INFO: Waiting for pod var-expansion-4a00fbdd-dfc5-4f48-8a6e-b573784da740 to disappear
Oct 29 22:05:10.418: INFO: Pod var-expansion-4a00fbdd-dfc5-4f48-8a6e-b573784da740 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:05:10.419: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "var-expansion-7201" for this suite.


------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":-1,"completed":64,"skipped":1015,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Variable Expansion
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:05:10.600: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's command
Oct 29 22:05:10.772: INFO: Waiting up to 5m0s for pod "var-expansion-a698d006-367c-4833-8789-4354eb7fed87" in namespace "var-expansion-8630" to be "Succeeded or Failed"
Oct 29 22:05:10.801: INFO: Pod "var-expansion-a698d006-367c-4833-8789-4354eb7fed87": Phase="Pending", Reason="", readiness=false. Elapsed: 29.057424ms
Oct 29 22:05:12.822: INFO: Pod "var-expansion-a698d006-367c-4833-8789-4354eb7fed87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050403083s
Oct 29 22:05:14.845: INFO: Pod "var-expansion-a698d006-367c-4833-8789-4354eb7fed87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.07315674s
STEP: Saw pod success
Oct 29 22:05:14.845: INFO: Pod "var-expansion-a698d006-367c-4833-8789-4354eb7fed87" satisfied condition "Succeeded or Failed"
Oct 29 22:05:14.867: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod var-expansion-a698d006-367c-4833-8789-4354eb7fed87 container dapi-container: <nil>
STEP: delete the pod
Oct 29 22:05:14.918: INFO: Waiting for pod var-expansion-a698d006-367c-4833-8789-4354eb7fed87 to disappear
Oct 29 22:05:14.939: INFO: Pod var-expansion-a698d006-367c-4833-8789-4354eb7fed87 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:05:14.939: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "var-expansion-8630" for this suite.


------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":-1,"completed":65,"skipped":1128,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Probing container
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:01:20.228: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-378ab7e3-564e-484b-9414-85a2db01f526 in namespace container-probe-8373
Oct 29 22:01:25.874: INFO: Started pod liveness-378ab7e3-564e-484b-9414-85a2db01f526 in namespace container-probe-8373
STEP: checking the pod's current state and verifying that restartCount is present
Oct 29 22:01:25.895: INFO: Initial restart count of pod liveness-378ab7e3-564e-484b-9414-85a2db01f526 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:05:26.527: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-probe-8373" for this suite.


 [SLOW TEST:246.400 seconds]
[k8s.io] Probing container
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":-1,"completed":52,"skipped":859,"failed":0}

S
------------------------------
[BeforeEach] [k8s.io] Pods
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:05:26.631: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 22:05:26.781: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:05:30.954: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-7227" for this suite.


------------------------------
{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":-1,"completed":53,"skipped":860,"failed":0}

SSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Pods
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:05:31.063: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be submitted and removed [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Oct 29 22:05:31.252: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:05:46.577: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-7050" for this suite.


 [SLOW TEST:15.615 seconds]
[k8s.io] Pods
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be submitted and removed [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
[BeforeEach] [k8s.io] Variable Expansion
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:03:39.133: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 22:05:39.500: INFO: Deleting pod "var-expansion-f08c9765-67b1-431e-9f4a-e2ebfa31c80d" in namespace "var-expansion-6394"
Oct 29 22:05:39.529: INFO: Wait up to 5m0s for pod "var-expansion-f08c9765-67b1-431e-9f4a-e2ebfa31c80d" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:05:47.573: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "var-expansion-6394" for this suite.


 [SLOW TEST:128.548 seconds]
[k8s.io] Variable Expansion
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":-1,"completed":41,"skipped":679,"failed":0}

SSSSSSSS
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":-1,"completed":54,"skipped":868,"failed":0}
[BeforeEach] [k8s.io] Container Runtime
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:05:46.679: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Oct 29 22:05:49.996: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:05:50.045: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-runtime-6722" for this suite.


------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":-1,"completed":55,"skipped":868,"failed":0}

S
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:05:47.689: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl run pod
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
[It] should create a pod from an image when restart is Never  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Oct 29 22:05:47.804: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-8757'
Oct 29 22:05:47.957: INFO: stderr: ""
Oct 29 22:05:47.957: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1550
Oct 29 22:05:47.984: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig delete pods e2e-test-httpd-pod --namespace=kubectl-8757'
Oct 29 22:05:56.529: INFO: stderr: ""
Oct 29 22:05:56.529: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:05:56.529: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-8757" for this suite.


 [SLOW TEST:8.951 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1541
    should create a pod from an image when restart is Never  [Conformance]
    /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":-1,"completed":42,"skipped":687,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-scheduling] LimitRange
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:05:56.684: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Oct 29 22:05:56.921: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Oct 29 22:05:56.962: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Oct 29 22:05:56.962: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Oct 29 22:05:57.018: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Oct 29 22:05:57.018: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Oct 29 22:05:57.074: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Oct 29 22:05:57.074: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Oct 29 22:06:04.273: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:06:04.304: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "limitrange-1487" for this suite.


 [SLOW TEST:7.725 seconds]
[sig-scheduling] LimitRange
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":-1,"completed":43,"skipped":742,"failed":0}

SS
------------------------------
[BeforeEach] [sig-apps] Deployment
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:05:50.149: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support rollover [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 22:05:50.350: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Oct 29 22:05:54.392: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Oct 29 22:05:56.413: INFO: Creating deployment "test-rollover-deployment"
Oct 29 22:05:56.457: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Oct 29 22:05:58.502: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Oct 29 22:05:58.543: INFO: Ensure that both replica sets have 1 created replica
Oct 29 22:05:58.596: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Oct 29 22:05:58.643: INFO: Updating deployment test-rollover-deployment
Oct 29 22:05:58.644: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Oct 29 22:06:00.687: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Oct 29 22:06:00.729: INFO: Make sure deployment "test-rollover-deployment" is complete
Oct 29 22:06:00.771: INFO: all replica sets need to contain the pod-template-hash label
Oct 29 22:06:00.771: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605936, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605936, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605939, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605936, loc:(*time.Location)(0x77098a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 29 22:06:02.813: INFO: all replica sets need to contain the pod-template-hash label
Oct 29 22:06:02.813: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605936, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605936, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605942, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605936, loc:(*time.Location)(0x77098a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 29 22:06:04.814: INFO: all replica sets need to contain the pod-template-hash label
Oct 29 22:06:04.814: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605936, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605936, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605942, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605936, loc:(*time.Location)(0x77098a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 29 22:06:06.815: INFO: all replica sets need to contain the pod-template-hash label
Oct 29 22:06:06.815: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605936, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605936, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605942, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605936, loc:(*time.Location)(0x77098a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 29 22:06:08.813: INFO: all replica sets need to contain the pod-template-hash label
Oct 29 22:06:08.813: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605936, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605936, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605942, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605936, loc:(*time.Location)(0x77098a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 29 22:06:10.813: INFO: all replica sets need to contain the pod-template-hash label
Oct 29 22:06:10.813: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605936, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605936, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605942, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605936, loc:(*time.Location)(0x77098a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 29 22:06:12.813: INFO: 
Oct 29 22:06:12.813: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Oct 29 22:06:12.874: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-689 /apis/apps/v1/namespaces/deployment-689/deployments/test-rollover-deployment 083bc63f-681e-4aa7-b059-f340e58b5fa3 255938 2 2020-10-29 22:05:36 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-10-29 22:05:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-10-29 22:05:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002e51d18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-10-29 22:05:36 +0000 UTC,LastTransitionTime:2020-10-29 22:05:36 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-5797c7764" has successfully progressed.,LastUpdateTime:2020-10-29 22:05:52 +0000 UTC,LastTransitionTime:2020-10-29 22:05:36 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Oct 29 22:06:12.895: INFO: New ReplicaSet "test-rollover-deployment-5797c7764" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-5797c7764  deployment-689 /apis/apps/v1/namespaces/deployment-689/replicasets/test-rollover-deployment-5797c7764 2247e1f4-b6b0-4fa7-b0b4-1ac3ddd71784 255927 2 2020-10-29 22:05:39 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 083bc63f-681e-4aa7-b059-f340e58b5fa3 0xc0030e0230 0xc0030e0231}] []  [{kube-controller-manager Update apps/v1 2020-10-29 22:05:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"083bc63f-681e-4aa7-b059-f340e58b5fa3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5797c7764,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0030e02a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Oct 29 22:06:12.895: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Oct 29 22:06:12.895: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-689 /apis/apps/v1/namespaces/deployment-689/replicasets/test-rollover-controller 7e4db8f6-6a5c-4702-a058-4e99b4dc5e85 255937 2 2020-10-29 22:05:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 083bc63f-681e-4aa7-b059-f340e58b5fa3 0xc0030e0127 0xc0030e0128}] []  [{e2e.test Update apps/v1 2020-10-29 22:05:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-10-29 22:05:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"083bc63f-681e-4aa7-b059-f340e58b5fa3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0030e01c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Oct 29 22:06:12.895: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-689 /apis/apps/v1/namespaces/deployment-689/replicasets/test-rollover-deployment-78bc8b888c 0a687811-6be8-4206-aaae-f3a720c69168 255643 2 2020-10-29 22:05:36 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 083bc63f-681e-4aa7-b059-f340e58b5fa3 0xc0030e0317 0xc0030e0318}] []  [{kube-controller-manager Update apps/v1 2020-10-29 22:05:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"083bc63f-681e-4aa7-b059-f340e58b5fa3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0030e03a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Oct 29 22:06:12.916: INFO: Pod "test-rollover-deployment-5797c7764-nz7dq" is available:
&Pod{ObjectMeta:{test-rollover-deployment-5797c7764-nz7dq test-rollover-deployment-5797c7764- deployment-689 /api/v1/namespaces/deployment-689/pods/test-rollover-deployment-5797c7764-nz7dq 0e64f46c-efe9-4e62-83ce-4ad0181d12eb 255729 0 2020-10-29 22:05:39 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.131.0.124"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.131.0.124"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rollover-deployment-5797c7764 2247e1f4-b6b0-4fa7-b0b4-1ac3ddd71784 0xc0030e0947 0xc0030e0948}] []  [{kube-controller-manager Update v1 2020-10-29 22:05:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2247e1f4-b6b0-4fa7-b0b4-1ac3ddd71784\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {multus Update v1 2020-10-29 22:05:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2020-10-29 22:05:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.124\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-k6r4w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-k6r4w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-k6r4w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-142-212.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c97,c69,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-8mhb4,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 22:05:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 22:05:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 22:05:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 22:05:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.142.212,PodIP:10.131.0.124,StartTime:2020-10-29 22:05:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-10-29 22:05:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:cri-o://84a95a1b419a9271afa61266986b5bebd8d430734d22a012992eab96d0b3c9ab,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.124,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:06:12.916: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "deployment-689" for this suite.


 [SLOW TEST:22.859 seconds]
[sig-apps] Deployment
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":-1,"completed":56,"skipped":869,"failed":0}

SSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] ConfigMap
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:04:49.660: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 22:04:49.901: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-111620a6-4973-4c4d-8eaa-31e7d14bc7a6
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-111620a6-4973-4c4d-8eaa-31e7d14bc7a6
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:06:13.084: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-7186" for this suite.


 [SLOW TEST:83.554 seconds]
[sig-storage] ConfigMap
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":-1,"completed":68,"skipped":1234,"failed":0}

S
------------------------------
[BeforeEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:06:04.412: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-9838
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-9838
I1029 22:06:04.643380   53211 runners.go:190] Created replication controller with name: externalname-service, namespace: services-9838, replica count: 2
I1029 22:06:07.693736   53211 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 29 22:06:07.693: INFO: Creating new exec pod
Oct 29 22:06:12.907: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=services-9838 execpodqr2mc -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Oct 29 22:06:13.286: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Oct 29 22:06:13.287: INFO: stdout: ""
Oct 29 22:06:13.287: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=services-9838 execpodqr2mc -- /bin/sh -x -c nc -zv -t -w 2 172.30.83.1 80'
Oct 29 22:06:13.642: INFO: stderr: "+ nc -zv -t -w 2 172.30.83.1 80\nConnection to 172.30.83.1 80 port [tcp/http] succeeded!\n"
Oct 29 22:06:13.642: INFO: stdout: ""
Oct 29 22:06:13.642: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=services-9838 execpodqr2mc -- /bin/sh -x -c nc -zv -t -w 2 10.0.142.212 32103'
Oct 29 22:06:13.987: INFO: stderr: "+ nc -zv -t -w 2 10.0.142.212 32103\nConnection to 10.0.142.212 32103 port [tcp/32103] succeeded!\n"
Oct 29 22:06:13.987: INFO: stdout: ""
Oct 29 22:06:13.987: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=services-9838 execpodqr2mc -- /bin/sh -x -c nc -zv -t -w 2 10.0.158.72 32103'
Oct 29 22:06:14.324: INFO: stderr: "+ nc -zv -t -w 2 10.0.158.72 32103\nConnection to 10.0.158.72 32103 port [tcp/32103] succeeded!\n"
Oct 29 22:06:14.324: INFO: stdout: ""
Oct 29 22:06:14.324: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:06:14.365: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-9838" for this suite.
[AfterEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786


 [SLOW TEST:10.058 seconds]
[sig-network] Services
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":-1,"completed":44,"skipped":744,"failed":0}

SS
------------------------------
[BeforeEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:06:14.474: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should provide secure master service  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:06:14.711: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-4520" for this suite.
[AfterEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786


------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:06:13.016: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:06:26.450: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "resourcequota-850" for this suite.


 [SLOW TEST:13.534 seconds]
[sig-api-machinery] ResourceQuota
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":-1,"completed":57,"skipped":878,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected configMap
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:06:26.579: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-4308f5cc-cc36-42cb-b701-4617095923f7
STEP: Creating a pod to test consume configMaps
Oct 29 22:06:26.782: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3c0543d4-e1fd-458b-899b-faf0755a1b2c" in namespace "projected-2216" to be "Succeeded or Failed"
Oct 29 22:06:26.807: INFO: Pod "pod-projected-configmaps-3c0543d4-e1fd-458b-899b-faf0755a1b2c": Phase="Pending", Reason="", readiness=false. Elapsed: 25.352398ms
Oct 29 22:06:28.829: INFO: Pod "pod-projected-configmaps-3c0543d4-e1fd-458b-899b-faf0755a1b2c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046534292s
Oct 29 22:06:30.850: INFO: Pod "pod-projected-configmaps-3c0543d4-e1fd-458b-899b-faf0755a1b2c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067868906s
STEP: Saw pod success
Oct 29 22:06:30.850: INFO: Pod "pod-projected-configmaps-3c0543d4-e1fd-458b-899b-faf0755a1b2c" satisfied condition "Succeeded or Failed"
Oct 29 22:06:30.871: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-projected-configmaps-3c0543d4-e1fd-458b-899b-faf0755a1b2c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Oct 29 22:06:30.922: INFO: Waiting for pod pod-projected-configmaps-3c0543d4-e1fd-458b-899b-faf0755a1b2c to disappear
Oct 29 22:06:30.942: INFO: Pod pod-projected-configmaps-3c0543d4-e1fd-458b-899b-faf0755a1b2c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:06:30.942: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-2216" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":58,"skipped":922,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:06:31.070: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Oct 29 22:06:35.954: INFO: Successfully updated pod "labelsupdate702077f4-42a4-4111-a820-6a6ca6755caa"
[AfterEach] [sig-storage] Downward API volume
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:06:40.032: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-5664" for this suite.


 [SLOW TEST:9.065 seconds]
[sig-storage] Downward API volume
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update labels on modification [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":-1,"completed":59,"skipped":952,"failed":0}

SSSSSSSSSS
------------------------------
{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":-1,"completed":45,"skipped":746,"failed":0}
[BeforeEach] [k8s.io] Container Runtime
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:06:14.777: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:06:43.155: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-runtime-6864" for this suite.


 [SLOW TEST:28.483 seconds]
[k8s.io] Container Runtime
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":-1,"completed":46,"skipped":746,"failed":0}

SSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-node] ConfigMap
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:06:40.142: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-6678/configmap-test-000e15c3-facd-41cb-83d7-7914e5fb9a05
STEP: Creating a pod to test consume configMaps
Oct 29 22:06:40.369: INFO: Waiting up to 5m0s for pod "pod-configmaps-bdf5e8ea-ea91-4dbf-ad3d-6c11df6196fb" in namespace "configmap-6678" to be "Succeeded or Failed"
Oct 29 22:06:40.392: INFO: Pod "pod-configmaps-bdf5e8ea-ea91-4dbf-ad3d-6c11df6196fb": Phase="Pending", Reason="", readiness=false. Elapsed: 22.786661ms
Oct 29 22:06:42.413: INFO: Pod "pod-configmaps-bdf5e8ea-ea91-4dbf-ad3d-6c11df6196fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043813975s
Oct 29 22:06:44.439: INFO: Pod "pod-configmaps-bdf5e8ea-ea91-4dbf-ad3d-6c11df6196fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.070194318s
STEP: Saw pod success
Oct 29 22:06:44.440: INFO: Pod "pod-configmaps-bdf5e8ea-ea91-4dbf-ad3d-6c11df6196fb" satisfied condition "Succeeded or Failed"
Oct 29 22:06:44.472: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-configmaps-bdf5e8ea-ea91-4dbf-ad3d-6c11df6196fb container env-test: <nil>
STEP: delete the pod
Oct 29 22:06:44.552: INFO: Waiting for pod pod-configmaps-bdf5e8ea-ea91-4dbf-ad3d-6c11df6196fb to disappear
Oct 29 22:06:44.574: INFO: Pod pod-configmaps-bdf5e8ea-ea91-4dbf-ad3d-6c11df6196fb no longer exists
[AfterEach] [sig-node] ConfigMap
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:06:44.574: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-6678" for this suite.


------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":-1,"completed":60,"skipped":962,"failed":0}

SSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Events
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:06:44.706: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:06:45.072: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "events-9161" for this suite.


------------------------------
{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":-1,"completed":61,"skipped":967,"failed":0}

S
------------------------------
[BeforeEach] [sig-storage] Projected combined
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:06:43.270: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-projected-all-test-volume-fcf10e57-1731-4db9-9773-0ed2ef918353
STEP: Creating secret with name secret-projected-all-test-volume-53e57618-ebf1-4e62-92d2-fbdc6326c7cc
STEP: Creating a pod to test Check all projections for projected volume plugin
Oct 29 22:06:43.525: INFO: Waiting up to 5m0s for pod "projected-volume-67b579f0-3c88-43db-8c02-d67a6a3b7bc3" in namespace "projected-5064" to be "Succeeded or Failed"
Oct 29 22:06:43.547: INFO: Pod "projected-volume-67b579f0-3c88-43db-8c02-d67a6a3b7bc3": Phase="Pending", Reason="", readiness=false. Elapsed: 21.904467ms
Oct 29 22:06:45.569: INFO: Pod "projected-volume-67b579f0-3c88-43db-8c02-d67a6a3b7bc3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04443736s
Oct 29 22:06:47.592: INFO: Pod "projected-volume-67b579f0-3c88-43db-8c02-d67a6a3b7bc3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067029113s
STEP: Saw pod success
Oct 29 22:06:47.592: INFO: Pod "projected-volume-67b579f0-3c88-43db-8c02-d67a6a3b7bc3" satisfied condition "Succeeded or Failed"
Oct 29 22:06:47.613: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod projected-volume-67b579f0-3c88-43db-8c02-d67a6a3b7bc3 container projected-all-volume-test: <nil>
STEP: delete the pod
Oct 29 22:06:47.743: INFO: Waiting for pod projected-volume-67b579f0-3c88-43db-8c02-d67a6a3b7bc3 to disappear
Oct 29 22:06:47.765: INFO: Pod projected-volume-67b579f0-3c88-43db-8c02-d67a6a3b7bc3 no longer exists
[AfterEach] [sig-storage] Projected combined
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:06:47.765: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-5064" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":-1,"completed":47,"skipped":759,"failed":0}

SSS
------------------------------
[BeforeEach] [sig-apps] Deployment
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:06:47.874: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 22:06:48.015: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Oct 29 22:06:48.064: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Oct 29 22:06:52.110: INFO: Creating deployment "test-rolling-update-deployment"
Oct 29 22:06:52.137: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Oct 29 22:06:52.182: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Oct 29 22:06:52.209: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605992, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605992, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605992, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605992, loc:(*time.Location)(0x77098a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-c4cb8d6d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 29 22:06:54.231: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605992, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605992, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605992, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605992, loc:(*time.Location)(0x77098a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-c4cb8d6d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 29 22:06:56.232: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Oct 29 22:06:56.299: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-416 /apis/apps/v1/namespaces/deployment-416/deployments/test-rolling-update-deployment e1e555f4-b9d8-47a4-a454-2e1d33d22ed2 257221 1 2020-10-29 22:06:32 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2020-10-29 22:06:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-10-29 22:06:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0030d7628 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-10-29 22:06:32 +0000 UTC,LastTransitionTime:2020-10-29 22:06:32 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" has successfully progressed.,LastUpdateTime:2020-10-29 22:06:36 +0000 UTC,LastTransitionTime:2020-10-29 22:06:32 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Oct 29 22:06:56.326: INFO: New ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9  deployment-416 /apis/apps/v1/namespaces/deployment-416/replicasets/test-rolling-update-deployment-c4cb8d6d9 c6943e32-cfd6-4aed-9e52-3a14b23f501b 257210 1 2020-10-29 22:06:32 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment e1e555f4-b9d8-47a4-a454-2e1d33d22ed2 0xc0030d7b70 0xc0030d7b71}] []  [{kube-controller-manager Update apps/v1 2020-10-29 22:06:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e1e555f4-b9d8-47a4-a454-2e1d33d22ed2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: c4cb8d6d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0030d7be8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Oct 29 22:06:56.326: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Oct 29 22:06:56.326: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-416 /apis/apps/v1/namespaces/deployment-416/replicasets/test-rolling-update-controller 3b0522fc-b216-4a16-bd6d-603728fe8725 257220 2 2020-10-29 22:06:28 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment e1e555f4-b9d8-47a4-a454-2e1d33d22ed2 0xc0030d7a67 0xc0030d7a68}] []  [{e2e.test Update apps/v1 2020-10-29 22:06:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-10-29 22:06:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e1e555f4-b9d8-47a4-a454-2e1d33d22ed2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0030d7b08 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Oct 29 22:06:56.349: INFO: Pod "test-rolling-update-deployment-c4cb8d6d9-74bn9" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9-74bn9 test-rolling-update-deployment-c4cb8d6d9- deployment-416 /api/v1/namespaces/deployment-416/pods/test-rolling-update-deployment-c4cb8d6d9-74bn9 dd21264b-73fb-45ee-ba74-7ba2dd392759 257209 0 2020-10-29 22:06:32 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.131.0.139"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "interface": "eth0",
    "ips": [
        "10.131.0.139"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rolling-update-deployment-c4cb8d6d9 c6943e32-cfd6-4aed-9e52-3a14b23f501b 0xc0013860a7 0xc0013860a8}] []  [{kube-controller-manager Update v1 2020-10-29 22:06:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c6943e32-cfd6-4aed-9e52-3a14b23f501b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {multus Update v1 2020-10-29 22:06:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2020-10-29 22:06:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.139\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hr9xn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hr9xn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hr9xn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-142-212.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c98,c37,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-5tqhx,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 22:06:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 22:06:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 22:06:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-29 22:06:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.142.212,PodIP:10.131.0.139,StartTime:2020-10-29 22:06:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-10-29 22:06:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:cri-o://a2bc182f673a6f3f95aba4bea9ce2c25ab9015314d1b486fd71d761b8d4e1845,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.139,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:06:56.349: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "deployment-416" for this suite.


 [SLOW TEST:8.592 seconds]
[sig-apps] Deployment
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":-1,"completed":48,"skipped":762,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:06:13.216: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Oct 29 22:06:13.433: INFO: PodSpec: initContainers in spec.initContainers
Oct 29 22:06:57.750: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-cc0c9a95-1139-4c14-9381-03150e1b7c29", GenerateName:"", Namespace:"init-container-8283", SelfLink:"/api/v1/namespaces/init-container-8283/pods/pod-init-cc0c9a95-1139-4c14-9381-03150e1b7c29", UID:"ce5ea8b9-2e2b-4eac-8a62-7e5792ee3055", ResourceVersion:"257277", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63739605953, loc:(*time.Location)(0x77098a0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"433057731"}, Annotations:map[string]string{"k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.131.0.128\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.131.0.128\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc00368a700), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00368a720)}, v1.ManagedFieldsEntry{Manager:"multus", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc00368a740), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00368a7a0)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc00368a7c0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00368a7e0)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-q6ws4", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001156700), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-q6ws4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc002cf32c0), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-q6ws4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc002cf3440), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-q6ws4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc002cf3140), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc003900a20), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-0-142-212.us-east-2.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0030a0690), ImagePullSecrets:[]v1.LocalObjectReference{v1.LocalObjectReference{Name:"default-dockercfg-7kvtf"}}, Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003900ad0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003900af0)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc003900b0c), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc003900b10), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000a7f740), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605953, loc:(*time.Location)(0x77098a0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605953, loc:(*time.Location)(0x77098a0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605953, loc:(*time.Location)(0x77098a0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739605953, loc:(*time.Location)(0x77098a0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.142.212", PodIP:"10.131.0.128", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.131.0.128"}}, StartTime:(*v1.Time)(0xc00368a800), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0030a0770)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0030a07e0)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"cri-o://92cddbe56846c597a35793e17bbf2b37b6df581b0ab5361cc79c0d771d71e40e", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00368a840), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00368a820), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc003900b8f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:06:57.750: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "init-container-8283" for this suite.


 [SLOW TEST:44.641 seconds]
[k8s.io] InitContainer [NodeConformance]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":-1,"completed":69,"skipped":1235,"failed":0}

SSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Secrets
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:06:56.483: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-04821b43-76bb-4928-abfd-75b220b995f4
STEP: Creating a pod to test consume secrets
Oct 29 22:06:56.694: INFO: Waiting up to 5m0s for pod "pod-secrets-62062c5e-a8ca-4ae0-b024-3bc0dbb71d84" in namespace "secrets-7365" to be "Succeeded or Failed"
Oct 29 22:06:56.717: INFO: Pod "pod-secrets-62062c5e-a8ca-4ae0-b024-3bc0dbb71d84": Phase="Pending", Reason="", readiness=false. Elapsed: 22.754821ms
Oct 29 22:06:58.760: INFO: Pod "pod-secrets-62062c5e-a8ca-4ae0-b024-3bc0dbb71d84": Phase="Pending", Reason="", readiness=false. Elapsed: 2.065294716s
Oct 29 22:07:00.783: INFO: Pod "pod-secrets-62062c5e-a8ca-4ae0-b024-3bc0dbb71d84": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.088295955s
STEP: Saw pod success
Oct 29 22:07:00.783: INFO: Pod "pod-secrets-62062c5e-a8ca-4ae0-b024-3bc0dbb71d84" satisfied condition "Succeeded or Failed"
Oct 29 22:07:00.804: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-secrets-62062c5e-a8ca-4ae0-b024-3bc0dbb71d84 container secret-volume-test: <nil>
STEP: delete the pod
Oct 29 22:07:00.858: INFO: Waiting for pod pod-secrets-62062c5e-a8ca-4ae0-b024-3bc0dbb71d84 to disappear
Oct 29 22:07:00.880: INFO: Pod pod-secrets-62062c5e-a8ca-4ae0-b024-3bc0dbb71d84 no longer exists
[AfterEach] [sig-storage] Secrets
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:07:00.880: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-7365" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":-1,"completed":49,"skipped":785,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Docker Containers
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:06:57.866: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Docker Containers
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:07:02.134: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "containers-4135" for this suite.


------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":-1,"completed":70,"skipped":1245,"failed":0}

SSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Discovery
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:07:02.260: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 22:07:03.077: INFO: Checking APIGroup: apiregistration.k8s.io
Oct 29 22:07:03.097: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Oct 29 22:07:03.097: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Oct 29 22:07:03.097: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Oct 29 22:07:03.097: INFO: Checking APIGroup: extensions
Oct 29 22:07:03.117: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Oct 29 22:07:03.117: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Oct 29 22:07:03.117: INFO: extensions/v1beta1 matches extensions/v1beta1
Oct 29 22:07:03.117: INFO: Checking APIGroup: apps
Oct 29 22:07:03.138: INFO: PreferredVersion.GroupVersion: apps/v1
Oct 29 22:07:03.138: INFO: Versions found [{apps/v1 v1}]
Oct 29 22:07:03.138: INFO: apps/v1 matches apps/v1
Oct 29 22:07:03.138: INFO: Checking APIGroup: events.k8s.io
Oct 29 22:07:03.158: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Oct 29 22:07:03.158: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Oct 29 22:07:03.158: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Oct 29 22:07:03.158: INFO: Checking APIGroup: authentication.k8s.io
Oct 29 22:07:03.178: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Oct 29 22:07:03.178: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Oct 29 22:07:03.178: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Oct 29 22:07:03.178: INFO: Checking APIGroup: authorization.k8s.io
Oct 29 22:07:03.198: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Oct 29 22:07:03.198: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Oct 29 22:07:03.198: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Oct 29 22:07:03.198: INFO: Checking APIGroup: autoscaling
Oct 29 22:07:03.218: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Oct 29 22:07:03.218: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Oct 29 22:07:03.218: INFO: autoscaling/v1 matches autoscaling/v1
Oct 29 22:07:03.218: INFO: Checking APIGroup: batch
Oct 29 22:07:03.238: INFO: PreferredVersion.GroupVersion: batch/v1
Oct 29 22:07:03.238: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Oct 29 22:07:03.238: INFO: batch/v1 matches batch/v1
Oct 29 22:07:03.238: INFO: Checking APIGroup: certificates.k8s.io
Oct 29 22:07:03.258: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Oct 29 22:07:03.258: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Oct 29 22:07:03.258: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Oct 29 22:07:03.258: INFO: Checking APIGroup: networking.k8s.io
Oct 29 22:07:03.278: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Oct 29 22:07:03.278: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Oct 29 22:07:03.278: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Oct 29 22:07:03.278: INFO: Checking APIGroup: policy
Oct 29 22:07:03.298: INFO: PreferredVersion.GroupVersion: policy/v1beta1
Oct 29 22:07:03.298: INFO: Versions found [{policy/v1beta1 v1beta1}]
Oct 29 22:07:03.298: INFO: policy/v1beta1 matches policy/v1beta1
Oct 29 22:07:03.298: INFO: Checking APIGroup: rbac.authorization.k8s.io
Oct 29 22:07:03.318: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Oct 29 22:07:03.318: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Oct 29 22:07:03.318: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Oct 29 22:07:03.318: INFO: Checking APIGroup: storage.k8s.io
Oct 29 22:07:03.338: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Oct 29 22:07:03.338: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Oct 29 22:07:03.338: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Oct 29 22:07:03.338: INFO: Checking APIGroup: admissionregistration.k8s.io
Oct 29 22:07:03.358: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Oct 29 22:07:03.358: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Oct 29 22:07:03.358: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Oct 29 22:07:03.358: INFO: Checking APIGroup: apiextensions.k8s.io
Oct 29 22:07:03.378: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Oct 29 22:07:03.378: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Oct 29 22:07:03.378: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Oct 29 22:07:03.378: INFO: Checking APIGroup: scheduling.k8s.io
Oct 29 22:07:03.398: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Oct 29 22:07:03.398: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
Oct 29 22:07:03.398: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Oct 29 22:07:03.398: INFO: Checking APIGroup: coordination.k8s.io
Oct 29 22:07:03.418: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Oct 29 22:07:03.418: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Oct 29 22:07:03.418: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Oct 29 22:07:03.418: INFO: Checking APIGroup: node.k8s.io
Oct 29 22:07:03.438: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1beta1
Oct 29 22:07:03.438: INFO: Versions found [{node.k8s.io/v1beta1 v1beta1}]
Oct 29 22:07:03.438: INFO: node.k8s.io/v1beta1 matches node.k8s.io/v1beta1
Oct 29 22:07:03.438: INFO: Checking APIGroup: discovery.k8s.io
Oct 29 22:07:03.458: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
Oct 29 22:07:03.458: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
Oct 29 22:07:03.458: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
Oct 29 22:07:03.458: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Oct 29 22:07:03.478: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1alpha1
Oct 29 22:07:03.478: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1alpha1 v1alpha1}]
Oct 29 22:07:03.478: INFO: flowcontrol.apiserver.k8s.io/v1alpha1 matches flowcontrol.apiserver.k8s.io/v1alpha1
Oct 29 22:07:03.478: INFO: Checking APIGroup: apps.openshift.io
Oct 29 22:07:03.498: INFO: PreferredVersion.GroupVersion: apps.openshift.io/v1
Oct 29 22:07:03.498: INFO: Versions found [{apps.openshift.io/v1 v1}]
Oct 29 22:07:03.498: INFO: apps.openshift.io/v1 matches apps.openshift.io/v1
Oct 29 22:07:03.498: INFO: Checking APIGroup: authorization.openshift.io
Oct 29 22:07:03.518: INFO: PreferredVersion.GroupVersion: authorization.openshift.io/v1
Oct 29 22:07:03.518: INFO: Versions found [{authorization.openshift.io/v1 v1}]
Oct 29 22:07:03.518: INFO: authorization.openshift.io/v1 matches authorization.openshift.io/v1
Oct 29 22:07:03.518: INFO: Checking APIGroup: build.openshift.io
Oct 29 22:07:03.538: INFO: PreferredVersion.GroupVersion: build.openshift.io/v1
Oct 29 22:07:03.538: INFO: Versions found [{build.openshift.io/v1 v1}]
Oct 29 22:07:03.538: INFO: build.openshift.io/v1 matches build.openshift.io/v1
Oct 29 22:07:03.538: INFO: Checking APIGroup: image.openshift.io
Oct 29 22:07:03.558: INFO: PreferredVersion.GroupVersion: image.openshift.io/v1
Oct 29 22:07:03.558: INFO: Versions found [{image.openshift.io/v1 v1}]
Oct 29 22:07:03.558: INFO: image.openshift.io/v1 matches image.openshift.io/v1
Oct 29 22:07:03.558: INFO: Checking APIGroup: oauth.openshift.io
Oct 29 22:07:03.578: INFO: PreferredVersion.GroupVersion: oauth.openshift.io/v1
Oct 29 22:07:03.578: INFO: Versions found [{oauth.openshift.io/v1 v1}]
Oct 29 22:07:03.578: INFO: oauth.openshift.io/v1 matches oauth.openshift.io/v1
Oct 29 22:07:03.578: INFO: Checking APIGroup: project.openshift.io
Oct 29 22:07:03.598: INFO: PreferredVersion.GroupVersion: project.openshift.io/v1
Oct 29 22:07:03.598: INFO: Versions found [{project.openshift.io/v1 v1}]
Oct 29 22:07:03.598: INFO: project.openshift.io/v1 matches project.openshift.io/v1
Oct 29 22:07:03.598: INFO: Checking APIGroup: quota.openshift.io
Oct 29 22:07:03.618: INFO: PreferredVersion.GroupVersion: quota.openshift.io/v1
Oct 29 22:07:03.618: INFO: Versions found [{quota.openshift.io/v1 v1}]
Oct 29 22:07:03.618: INFO: quota.openshift.io/v1 matches quota.openshift.io/v1
Oct 29 22:07:03.618: INFO: Checking APIGroup: route.openshift.io
Oct 29 22:07:03.639: INFO: PreferredVersion.GroupVersion: route.openshift.io/v1
Oct 29 22:07:03.639: INFO: Versions found [{route.openshift.io/v1 v1}]
Oct 29 22:07:03.639: INFO: route.openshift.io/v1 matches route.openshift.io/v1
Oct 29 22:07:03.639: INFO: Checking APIGroup: security.openshift.io
Oct 29 22:07:03.658: INFO: PreferredVersion.GroupVersion: security.openshift.io/v1
Oct 29 22:07:03.658: INFO: Versions found [{security.openshift.io/v1 v1}]
Oct 29 22:07:03.658: INFO: security.openshift.io/v1 matches security.openshift.io/v1
Oct 29 22:07:03.658: INFO: Checking APIGroup: template.openshift.io
Oct 29 22:07:03.678: INFO: PreferredVersion.GroupVersion: template.openshift.io/v1
Oct 29 22:07:03.678: INFO: Versions found [{template.openshift.io/v1 v1}]
Oct 29 22:07:03.678: INFO: template.openshift.io/v1 matches template.openshift.io/v1
Oct 29 22:07:03.678: INFO: Checking APIGroup: user.openshift.io
Oct 29 22:07:03.698: INFO: PreferredVersion.GroupVersion: user.openshift.io/v1
Oct 29 22:07:03.698: INFO: Versions found [{user.openshift.io/v1 v1}]
Oct 29 22:07:03.698: INFO: user.openshift.io/v1 matches user.openshift.io/v1
Oct 29 22:07:03.698: INFO: Checking APIGroup: packages.operators.coreos.com
Oct 29 22:07:03.718: INFO: PreferredVersion.GroupVersion: packages.operators.coreos.com/v1
Oct 29 22:07:03.718: INFO: Versions found [{packages.operators.coreos.com/v1 v1}]
Oct 29 22:07:03.718: INFO: packages.operators.coreos.com/v1 matches packages.operators.coreos.com/v1
Oct 29 22:07:03.718: INFO: Checking APIGroup: config.openshift.io
Oct 29 22:07:03.738: INFO: PreferredVersion.GroupVersion: config.openshift.io/v1
Oct 29 22:07:03.738: INFO: Versions found [{config.openshift.io/v1 v1}]
Oct 29 22:07:03.738: INFO: config.openshift.io/v1 matches config.openshift.io/v1
Oct 29 22:07:03.738: INFO: Checking APIGroup: operator.openshift.io
Oct 29 22:07:03.758: INFO: PreferredVersion.GroupVersion: operator.openshift.io/v1
Oct 29 22:07:03.758: INFO: Versions found [{operator.openshift.io/v1 v1} {operator.openshift.io/v1alpha1 v1alpha1}]
Oct 29 22:07:03.758: INFO: operator.openshift.io/v1 matches operator.openshift.io/v1
Oct 29 22:07:03.758: INFO: Checking APIGroup: autoscaling.openshift.io
Oct 29 22:07:03.778: INFO: PreferredVersion.GroupVersion: autoscaling.openshift.io/v1
Oct 29 22:07:03.778: INFO: Versions found [{autoscaling.openshift.io/v1 v1} {autoscaling.openshift.io/v1beta1 v1beta1}]
Oct 29 22:07:03.778: INFO: autoscaling.openshift.io/v1 matches autoscaling.openshift.io/v1
Oct 29 22:07:03.778: INFO: Checking APIGroup: cloudcredential.openshift.io
Oct 29 22:07:03.798: INFO: PreferredVersion.GroupVersion: cloudcredential.openshift.io/v1
Oct 29 22:07:03.798: INFO: Versions found [{cloudcredential.openshift.io/v1 v1}]
Oct 29 22:07:03.798: INFO: cloudcredential.openshift.io/v1 matches cloudcredential.openshift.io/v1
Oct 29 22:07:03.798: INFO: Checking APIGroup: console.openshift.io
Oct 29 22:07:03.818: INFO: PreferredVersion.GroupVersion: console.openshift.io/v1
Oct 29 22:07:03.818: INFO: Versions found [{console.openshift.io/v1 v1}]
Oct 29 22:07:03.818: INFO: console.openshift.io/v1 matches console.openshift.io/v1
Oct 29 22:07:03.818: INFO: Checking APIGroup: imageregistry.operator.openshift.io
Oct 29 22:07:03.838: INFO: PreferredVersion.GroupVersion: imageregistry.operator.openshift.io/v1
Oct 29 22:07:03.838: INFO: Versions found [{imageregistry.operator.openshift.io/v1 v1}]
Oct 29 22:07:03.838: INFO: imageregistry.operator.openshift.io/v1 matches imageregistry.operator.openshift.io/v1
Oct 29 22:07:03.838: INFO: Checking APIGroup: ingress.operator.openshift.io
Oct 29 22:07:03.857: INFO: PreferredVersion.GroupVersion: ingress.operator.openshift.io/v1
Oct 29 22:07:03.857: INFO: Versions found [{ingress.operator.openshift.io/v1 v1}]
Oct 29 22:07:03.858: INFO: ingress.operator.openshift.io/v1 matches ingress.operator.openshift.io/v1
Oct 29 22:07:03.858: INFO: Checking APIGroup: k8s.cni.cncf.io
Oct 29 22:07:03.877: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
Oct 29 22:07:03.877: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
Oct 29 22:07:03.878: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
Oct 29 22:07:03.878: INFO: Checking APIGroup: machineconfiguration.openshift.io
Oct 29 22:07:03.897: INFO: PreferredVersion.GroupVersion: machineconfiguration.openshift.io/v1
Oct 29 22:07:03.897: INFO: Versions found [{machineconfiguration.openshift.io/v1 v1}]
Oct 29 22:07:03.897: INFO: machineconfiguration.openshift.io/v1 matches machineconfiguration.openshift.io/v1
Oct 29 22:07:03.897: INFO: Checking APIGroup: monitoring.coreos.com
Oct 29 22:07:03.918: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
Oct 29 22:07:03.918: INFO: Versions found [{monitoring.coreos.com/v1 v1}]
Oct 29 22:07:03.918: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
Oct 29 22:07:03.918: INFO: Checking APIGroup: network.openshift.io
Oct 29 22:07:03.938: INFO: PreferredVersion.GroupVersion: network.openshift.io/v1
Oct 29 22:07:03.938: INFO: Versions found [{network.openshift.io/v1 v1}]
Oct 29 22:07:03.938: INFO: network.openshift.io/v1 matches network.openshift.io/v1
Oct 29 22:07:03.938: INFO: Checking APIGroup: network.operator.openshift.io
Oct 29 22:07:03.958: INFO: PreferredVersion.GroupVersion: network.operator.openshift.io/v1
Oct 29 22:07:03.958: INFO: Versions found [{network.operator.openshift.io/v1 v1}]
Oct 29 22:07:03.958: INFO: network.operator.openshift.io/v1 matches network.operator.openshift.io/v1
Oct 29 22:07:03.958: INFO: Checking APIGroup: operators.coreos.com
Oct 29 22:07:03.978: INFO: PreferredVersion.GroupVersion: operators.coreos.com/v1
Oct 29 22:07:03.978: INFO: Versions found [{operators.coreos.com/v1 v1} {operators.coreos.com/v1alpha2 v1alpha2} {operators.coreos.com/v1alpha1 v1alpha1}]
Oct 29 22:07:03.978: INFO: operators.coreos.com/v1 matches operators.coreos.com/v1
Oct 29 22:07:03.978: INFO: Checking APIGroup: samples.operator.openshift.io
Oct 29 22:07:03.997: INFO: PreferredVersion.GroupVersion: samples.operator.openshift.io/v1
Oct 29 22:07:03.997: INFO: Versions found [{samples.operator.openshift.io/v1 v1}]
Oct 29 22:07:03.997: INFO: samples.operator.openshift.io/v1 matches samples.operator.openshift.io/v1
Oct 29 22:07:03.997: INFO: Checking APIGroup: security.internal.openshift.io
Oct 29 22:07:04.017: INFO: PreferredVersion.GroupVersion: security.internal.openshift.io/v1
Oct 29 22:07:04.017: INFO: Versions found [{security.internal.openshift.io/v1 v1}]
Oct 29 22:07:04.017: INFO: security.internal.openshift.io/v1 matches security.internal.openshift.io/v1
Oct 29 22:07:04.017: INFO: Checking APIGroup: tuned.openshift.io
Oct 29 22:07:04.038: INFO: PreferredVersion.GroupVersion: tuned.openshift.io/v1
Oct 29 22:07:04.038: INFO: Versions found [{tuned.openshift.io/v1 v1}]
Oct 29 22:07:04.038: INFO: tuned.openshift.io/v1 matches tuned.openshift.io/v1
Oct 29 22:07:04.038: INFO: Checking APIGroup: velero.io
Oct 29 22:07:04.058: INFO: PreferredVersion.GroupVersion: velero.io/v1
Oct 29 22:07:04.058: INFO: Versions found [{velero.io/v1 v1}]
Oct 29 22:07:04.058: INFO: velero.io/v1 matches velero.io/v1
Oct 29 22:07:04.058: INFO: Checking APIGroup: cloudingress.managed.openshift.io
Oct 29 22:07:04.078: INFO: PreferredVersion.GroupVersion: cloudingress.managed.openshift.io/v1alpha1
Oct 29 22:07:04.078: INFO: Versions found [{cloudingress.managed.openshift.io/v1alpha1 v1alpha1}]
Oct 29 22:07:04.078: INFO: cloudingress.managed.openshift.io/v1alpha1 matches cloudingress.managed.openshift.io/v1alpha1
Oct 29 22:07:04.078: INFO: Checking APIGroup: managed.openshift.io
Oct 29 22:07:04.097: INFO: PreferredVersion.GroupVersion: managed.openshift.io/v1alpha2
Oct 29 22:07:04.097: INFO: Versions found [{managed.openshift.io/v1alpha2 v1alpha2} {managed.openshift.io/v1alpha1 v1alpha1}]
Oct 29 22:07:04.098: INFO: managed.openshift.io/v1alpha2 matches managed.openshift.io/v1alpha2
Oct 29 22:07:04.098: INFO: Checking APIGroup: metal3.io
Oct 29 22:07:04.117: INFO: PreferredVersion.GroupVersion: metal3.io/v1alpha1
Oct 29 22:07:04.117: INFO: Versions found [{metal3.io/v1alpha1 v1alpha1}]
Oct 29 22:07:04.118: INFO: metal3.io/v1alpha1 matches metal3.io/v1alpha1
Oct 29 22:07:04.118: INFO: Checking APIGroup: migration.k8s.io
Oct 29 22:07:04.137: INFO: PreferredVersion.GroupVersion: migration.k8s.io/v1alpha1
Oct 29 22:07:04.138: INFO: Versions found [{migration.k8s.io/v1alpha1 v1alpha1}]
Oct 29 22:07:04.138: INFO: migration.k8s.io/v1alpha1 matches migration.k8s.io/v1alpha1
Oct 29 22:07:04.138: INFO: Checking APIGroup: splunkforwarder.managed.openshift.io
Oct 29 22:07:04.157: INFO: PreferredVersion.GroupVersion: splunkforwarder.managed.openshift.io/v1alpha1
Oct 29 22:07:04.157: INFO: Versions found [{splunkforwarder.managed.openshift.io/v1alpha1 v1alpha1}]
Oct 29 22:07:04.157: INFO: splunkforwarder.managed.openshift.io/v1alpha1 matches splunkforwarder.managed.openshift.io/v1alpha1
Oct 29 22:07:04.157: INFO: Checking APIGroup: upgrade.managed.openshift.io
Oct 29 22:07:04.177: INFO: PreferredVersion.GroupVersion: upgrade.managed.openshift.io/v1alpha1
Oct 29 22:07:04.177: INFO: Versions found [{upgrade.managed.openshift.io/v1alpha1 v1alpha1}]
Oct 29 22:07:04.177: INFO: upgrade.managed.openshift.io/v1alpha1 matches upgrade.managed.openshift.io/v1alpha1
Oct 29 22:07:04.177: INFO: Checking APIGroup: whereabouts.cni.cncf.io
Oct 29 22:07:04.197: INFO: PreferredVersion.GroupVersion: whereabouts.cni.cncf.io/v1alpha1
Oct 29 22:07:04.197: INFO: Versions found [{whereabouts.cni.cncf.io/v1alpha1 v1alpha1}]
Oct 29 22:07:04.197: INFO: whereabouts.cni.cncf.io/v1alpha1 matches whereabouts.cni.cncf.io/v1alpha1
Oct 29 22:07:04.197: INFO: Checking APIGroup: helm.openshift.io
Oct 29 22:07:04.217: INFO: PreferredVersion.GroupVersion: helm.openshift.io/v1beta1
Oct 29 22:07:04.217: INFO: Versions found [{helm.openshift.io/v1beta1 v1beta1}]
Oct 29 22:07:04.217: INFO: helm.openshift.io/v1beta1 matches helm.openshift.io/v1beta1
Oct 29 22:07:04.217: INFO: Checking APIGroup: machine.openshift.io
Oct 29 22:07:04.237: INFO: PreferredVersion.GroupVersion: machine.openshift.io/v1beta1
Oct 29 22:07:04.237: INFO: Versions found [{machine.openshift.io/v1beta1 v1beta1}]
Oct 29 22:07:04.237: INFO: machine.openshift.io/v1beta1 matches machine.openshift.io/v1beta1
Oct 29 22:07:04.237: INFO: Checking APIGroup: snapshot.storage.k8s.io
Oct 29 22:07:04.257: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1beta1
Oct 29 22:07:04.257: INFO: Versions found [{snapshot.storage.k8s.io/v1beta1 v1beta1}]
Oct 29 22:07:04.257: INFO: snapshot.storage.k8s.io/v1beta1 matches snapshot.storage.k8s.io/v1beta1
Oct 29 22:07:04.257: INFO: Checking APIGroup: metrics.k8s.io
Oct 29 22:07:04.277: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Oct 29 22:07:04.277: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Oct 29 22:07:04.277: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:07:04.277: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "discovery-7548" for this suite.


------------------------------
{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":-1,"completed":71,"skipped":1258,"failed":0}

SSSSSS
------------------------------
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:07:04.370: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check is all data is printed  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 22:07:04.528: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig version'
Oct 29 22:07:04.768: INFO: stderr: ""
Oct 29 22:07:04.768: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"19+\", GitVersion:\"v1.19.4-rc.0.27+9dfb4c876bfca7\", GitCommit:\"9dfb4c876bfca7a5ae84259fae2bc337ed90c2d7\", GitTreeState:\"clean\", BuildDate:\"2020-10-29T21:09:36Z\", GoVersion:\"go1.15.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.0+d59ce34\", GitCommit:\"d59ce3486ae3ca3a0c36e5498e56f51594076596\", GitTreeState:\"clean\", BuildDate:\"2020-10-08T15:58:07Z\", GoVersion:\"go1.15.0\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:07:04.768: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-937" for this suite.


------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":-1,"completed":72,"skipped":1264,"failed":0}

SSSSSS
------------------------------
[BeforeEach] [sig-node] Downward API
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:07:04.842: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Oct 29 22:07:05.040: INFO: Waiting up to 5m0s for pod "downward-api-5bc37193-f2bb-4ad3-8b93-e8b0e8ae648c" in namespace "downward-api-8874" to be "Succeeded or Failed"
Oct 29 22:07:05.062: INFO: Pod "downward-api-5bc37193-f2bb-4ad3-8b93-e8b0e8ae648c": Phase="Pending", Reason="", readiness=false. Elapsed: 22.273687ms
Oct 29 22:07:07.085: INFO: Pod "downward-api-5bc37193-f2bb-4ad3-8b93-e8b0e8ae648c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044844586s
Oct 29 22:07:09.110: INFO: Pod "downward-api-5bc37193-f2bb-4ad3-8b93-e8b0e8ae648c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.06970833s
STEP: Saw pod success
Oct 29 22:07:09.110: INFO: Pod "downward-api-5bc37193-f2bb-4ad3-8b93-e8b0e8ae648c" satisfied condition "Succeeded or Failed"
Oct 29 22:07:09.132: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod downward-api-5bc37193-f2bb-4ad3-8b93-e8b0e8ae648c container dapi-container: <nil>
STEP: delete the pod
Oct 29 22:07:09.195: INFO: Waiting for pod downward-api-5bc37193-f2bb-4ad3-8b93-e8b0e8ae648c to disappear
Oct 29 22:07:09.216: INFO: Pod downward-api-5bc37193-f2bb-4ad3-8b93-e8b0e8ae648c no longer exists
[AfterEach] [sig-node] Downward API
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:07:09.216: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-8874" for this suite.


------------------------------
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":-1,"completed":73,"skipped":1270,"failed":0}

SSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:07:09.331: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 22:07:09.493: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:07:09.667: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5869" for this suite.


------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":-1,"completed":74,"skipped":1274,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:07:01.022: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve a basic endpoint from pods  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service endpoint-test2 in namespace services-2841
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2841 to expose endpoints map[]
Oct 29 22:07:01.379: INFO: successfully validated that service endpoint-test2 in namespace services-2841 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-2841
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2841 to expose endpoints map[pod1:[80]]
Oct 29 22:07:05.534: INFO: successfully validated that service endpoint-test2 in namespace services-2841 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-2841
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2841 to expose endpoints map[pod1:[80] pod2:[80]]
Oct 29 22:07:09.723: INFO: successfully validated that service endpoint-test2 in namespace services-2841 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-2841
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2841 to expose endpoints map[pod2:[80]]
Oct 29 22:07:09.847: INFO: successfully validated that service endpoint-test2 in namespace services-2841 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-2841
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2841 to expose endpoints map[]
Oct 29 22:07:11.004: INFO: successfully validated that service endpoint-test2 in namespace services-2841 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:07:11.055: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-2841" for this suite.
[AfterEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786


 [SLOW TEST:10.151 seconds]
[sig-network] Services
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":-1,"completed":50,"skipped":834,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Variable Expansion
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:05:15.069: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 22:07:15.341: INFO: Deleting pod "var-expansion-b7fa1568-d90f-4caf-afbd-e3f772e1db1a" in namespace "var-expansion-4783"
Oct 29 22:07:15.370: INFO: Wait up to 5m0s for pod "var-expansion-b7fa1568-d90f-4caf-afbd-e3f772e1db1a" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:07:21.420: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "var-expansion-4783" for this suite.


 [SLOW TEST:126.465 seconds]
[k8s.io] Variable Expansion
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":-1,"completed":66,"skipped":1162,"failed":0}

SSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-auth] ServiceAccounts
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:07:21.548: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:07:21.915: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "svcaccounts-3503" for this suite.


------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":-1,"completed":67,"skipped":1179,"failed":0}

SSS
------------------------------
[BeforeEach] [sig-apps] Job
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:06:45.122: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-4495, will wait for the garbage collector to delete the pods
Oct 29 22:06:49.412: INFO: Deleting Job.batch foo took: 25.873583ms
Oct 29 22:06:49.512: INFO: Terminating Job.batch foo pods took: 100.112991ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:07:22.940: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "job-4495" for this suite.


 [SLOW TEST:37.906 seconds]
[sig-apps] Job
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":-1,"completed":62,"skipped":968,"failed":0}

SSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] ResourceQuota
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:07:21.971: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:07:29.272: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "resourcequota-7717" for this suite.


 [SLOW TEST:7.408 seconds]
[sig-api-machinery] ResourceQuota
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":-1,"completed":68,"skipped":1182,"failed":0}

SSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-apps] StatefulSet
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:07:11.190: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-5103
[It] Should recreate evicted statefulset [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-5103
STEP: Creating statefulset with conflicting port in namespace statefulset-5103
STEP: Waiting until pod test-pod will start running in namespace statefulset-5103
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5103
Oct 29 22:07:15.854: INFO: Observed stateful pod in namespace: statefulset-5103, name: ss-0, uid: 40df6762-52ea-4edf-8473-bd1ff2f50223, status phase: Failed. Waiting for statefulset controller to delete.
Oct 29 22:07:15.892: INFO: Observed stateful pod in namespace: statefulset-5103, name: ss-0, uid: 40df6762-52ea-4edf-8473-bd1ff2f50223, status phase: Failed. Waiting for statefulset controller to delete.
Oct 29 22:07:15.897: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5103
STEP: Removing pod with conflicting port in namespace statefulset-5103
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5103 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Oct 29 22:07:19.995: INFO: Deleting all statefulset in ns statefulset-5103
Oct 29 22:07:20.017: INFO: Scaling statefulset ss to 0
Oct 29 22:07:30.107: INFO: Waiting for statefulset status.replicas updated to 0
Oct 29 22:07:30.128: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:07:30.212: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "statefulset-5103" for this suite.


 [SLOW TEST:19.119 seconds]
[sig-apps] StatefulSet
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Should recreate evicted statefulset [Conformance]
    /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":-1,"completed":51,"skipped":858,"failed":0}

SSSSSSS
------------------------------
[BeforeEach] [k8s.io] Pods
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:07:30.317: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should get a host IP [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating pod
Oct 29 22:07:34.647: INFO: Pod pod-hostip-d74827cb-e60e-45ea-9eb4-68e85e3334f7 has hostIP: 10.0.142.212
[AfterEach] [k8s.io] Pods
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:07:34.648: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-4603" for this suite.


------------------------------
{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":-1,"completed":52,"skipped":865,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:07:23.039: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Oct 29 22:07:31.475: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Oct 29 22:07:31.500: INFO: Pod pod-with-prestop-exec-hook still exists
Oct 29 22:07:33.500: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Oct 29 22:07:33.522: INFO: Pod pod-with-prestop-exec-hook still exists
Oct 29 22:07:35.500: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Oct 29 22:07:35.528: INFO: Pod pod-with-prestop-exec-hook still exists
Oct 29 22:07:37.500: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Oct 29 22:07:37.521: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:07:37.543: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3258" for this suite.


 [SLOW TEST:14.605 seconds]
[k8s.io] Container Lifecycle Hook
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":-1,"completed":63,"skipped":980,"failed":0}

SSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-node] Downward API
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:07:34.788: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Oct 29 22:07:35.003: INFO: Waiting up to 5m0s for pod "downward-api-4aa5daff-876d-4434-9846-a6aec90932ec" in namespace "downward-api-6199" to be "Succeeded or Failed"
Oct 29 22:07:35.026: INFO: Pod "downward-api-4aa5daff-876d-4434-9846-a6aec90932ec": Phase="Pending", Reason="", readiness=false. Elapsed: 22.638022ms
Oct 29 22:07:37.051: INFO: Pod "downward-api-4aa5daff-876d-4434-9846-a6aec90932ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048281379s
Oct 29 22:07:39.074: INFO: Pod "downward-api-4aa5daff-876d-4434-9846-a6aec90932ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.070339163s
STEP: Saw pod success
Oct 29 22:07:39.074: INFO: Pod "downward-api-4aa5daff-876d-4434-9846-a6aec90932ec" satisfied condition "Succeeded or Failed"
Oct 29 22:07:39.095: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod downward-api-4aa5daff-876d-4434-9846-a6aec90932ec container dapi-container: <nil>
STEP: delete the pod
Oct 29 22:07:39.167: INFO: Waiting for pod downward-api-4aa5daff-876d-4434-9846-a6aec90932ec to disappear
Oct 29 22:07:39.187: INFO: Pod downward-api-4aa5daff-876d-4434-9846-a6aec90932ec no longer exists
[AfterEach] [sig-node] Downward API
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:07:39.187: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-6199" for this suite.


------------------------------
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":-1,"completed":53,"skipped":907,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:07:37.662: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Oct 29 22:07:37.864: INFO: Waiting up to 5m0s for pod "pod-b076f135-fcc8-405b-b4db-f2255d459bb7" in namespace "emptydir-6269" to be "Succeeded or Failed"
Oct 29 22:07:37.887: INFO: Pod "pod-b076f135-fcc8-405b-b4db-f2255d459bb7": Phase="Pending", Reason="", readiness=false. Elapsed: 22.78994ms
Oct 29 22:07:39.910: INFO: Pod "pod-b076f135-fcc8-405b-b4db-f2255d459bb7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045483209s
Oct 29 22:07:41.931: INFO: Pod "pod-b076f135-fcc8-405b-b4db-f2255d459bb7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067179707s
STEP: Saw pod success
Oct 29 22:07:41.931: INFO: Pod "pod-b076f135-fcc8-405b-b4db-f2255d459bb7" satisfied condition "Succeeded or Failed"
Oct 29 22:07:41.952: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-b076f135-fcc8-405b-b4db-f2255d459bb7 container test-container: <nil>
STEP: delete the pod
Oct 29 22:07:42.009: INFO: Waiting for pod pod-b076f135-fcc8-405b-b4db-f2255d459bb7 to disappear
Oct 29 22:07:42.030: INFO: Pod pod-b076f135-fcc8-405b-b4db-f2255d459bb7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:07:42.030: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-6269" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":64,"skipped":999,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:07:39.329: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 29 22:07:40.148: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 29 22:07:42.243: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739606040, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739606040, loc:(*time.Location)(0x77098a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739606040, loc:(*time.Location)(0x77098a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739606040, loc:(*time.Location)(0x77098a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 29 22:07:45.324: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
Oct 29 22:07:45.722: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:45.885: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:45.990: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:46.091: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:46.187: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:46.302: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:46.393: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:46.486: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:46.591: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:46.710: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:46.818: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:46.890: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:46.989: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:47.134: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:47.321: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:47.393: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:47.490: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:47.593: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:47.699: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:47.833: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:48.009: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:48.095: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:48.193: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:48.296: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:48.392: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:48.513: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:48.588: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:48.693: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:48.794: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:48.910: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:48.997: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:49.099: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:49.205: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:49.301: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:49.389: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:49.486: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:49.587: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:49.699: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:49.811: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:49.888: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:49.992: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:50.091: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:50.198: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:50.295: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:50.395: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:50.489: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:50.603: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:50.692: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:50.790: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:50.892: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:51.209: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:51.305: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:51.400: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:51.536: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:51.695: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:51.802: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:51.909: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:52.000: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:52.116: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:52.188: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:52.291: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:52.386: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:52.495: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:52.591: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:52.691: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:52.785: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:52.887: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:52.984: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:53.083: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:53.183: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:53.288: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:53.383: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:53.484: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:53.585: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:53.684: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:53.785: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:53.893: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:53.985: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:54.086: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:54.186: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:54.284: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:54.391: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:54.485: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:54.585: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:54.688: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:54.786: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:54.885: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:54.987: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:55.085: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:55.189: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:55.290: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:55.385: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:55.486: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:55.584: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:55.686: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:55.785: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:55.885: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:55.987: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:56.085: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:56.185: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:56.287: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:56.389: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:56.491: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:56.589: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:56.688: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:56.787: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:56.885: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:56.986: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:57.085: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:57.184: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:57.284: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:57.385: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:57.486: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:57.585: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:57.684: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:57.786: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:57.887: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:57.985: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:58.084: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:58.184: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:58.285: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:58.385: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:58.484: INFO: Waiting for webhook configuration to be ready...
Oct 29 22:07:58.587: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:07:58.833: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "webhook-6564" for this suite.
STEP: Destroying namespace "webhook-6564-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102


 [SLOW TEST:19.734 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":-1,"completed":54,"skipped":937,"failed":0}

SSS
------------------------------
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:07:59.067: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:07:59.266: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "tables-9217" for this suite.


------------------------------
{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":-1,"completed":55,"skipped":940,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Garbage collector
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:07:59.338: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W1029 22:07:59.877876   53211 metrics_grabber.go:83] Can't find any pods in namespace kube-system to grab metrics from
W1029 22:07:59.877891   53211 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1029 22:07:59.877898   53211 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1029 22:07:59.877905   53211 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Oct 29 22:07:59.877: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:07:59.877: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "gc-2139" for this suite.


------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":-1,"completed":56,"skipped":973,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Secrets
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:07:59.944: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-632f2f14-7084-4373-b305-efacb1ab6817
STEP: Creating a pod to test consume secrets
Oct 29 22:08:00.148: INFO: Waiting up to 5m0s for pod "pod-secrets-fee6b2d2-13b1-4629-8449-7176e2ed7b60" in namespace "secrets-569" to be "Succeeded or Failed"
Oct 29 22:08:00.175: INFO: Pod "pod-secrets-fee6b2d2-13b1-4629-8449-7176e2ed7b60": Phase="Pending", Reason="", readiness=false. Elapsed: 26.862881ms
Oct 29 22:08:02.197: INFO: Pod "pod-secrets-fee6b2d2-13b1-4629-8449-7176e2ed7b60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049176156s
Oct 29 22:08:04.219: INFO: Pod "pod-secrets-fee6b2d2-13b1-4629-8449-7176e2ed7b60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.071063591s
STEP: Saw pod success
Oct 29 22:08:04.219: INFO: Pod "pod-secrets-fee6b2d2-13b1-4629-8449-7176e2ed7b60" satisfied condition "Succeeded or Failed"
Oct 29 22:08:04.244: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-secrets-fee6b2d2-13b1-4629-8449-7176e2ed7b60 container secret-volume-test: <nil>
STEP: delete the pod
Oct 29 22:08:04.320: INFO: Waiting for pod pod-secrets-fee6b2d2-13b1-4629-8449-7176e2ed7b60 to disappear
Oct 29 22:08:04.357: INFO: Pod pod-secrets-fee6b2d2-13b1-4629-8449-7176e2ed7b60 no longer exists
[AfterEach] [sig-storage] Secrets
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:08:04.357: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-569" for this suite.


------------------------------
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:07:09.783: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Oct 29 22:07:09.975: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Oct 29 22:07:39.223: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
Oct 29 22:07:46.574: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:08:13.062: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5546" for this suite.


 [SLOW TEST:63.386 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":-1,"completed":75,"skipped":1331,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Container Runtime
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:08:13.188: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Oct 29 22:08:16.517: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:08:16.570: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-runtime-1323" for this suite.


------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":-1,"completed":76,"skipped":1356,"failed":0}

SSSSSSSSSSSSSS
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":57,"skipped":996,"failed":0}
[BeforeEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:08:04.468: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-917
STEP: creating service affinity-clusterip in namespace services-917
STEP: creating replication controller affinity-clusterip in namespace services-917
I1029 22:08:04.694323   53211 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-917, replica count: 3
I1029 22:08:07.744746   53211 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1029 22:08:10.745115   53211 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 29 22:08:10.787: INFO: Creating new exec pod
Oct 29 22:08:15.890: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=services-917 execpod-affinityzhjg7 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Oct 29 22:08:16.256: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Oct 29 22:08:16.256: INFO: stdout: ""
Oct 29 22:08:16.257: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=services-917 execpod-affinityzhjg7 -- /bin/sh -x -c nc -zv -t -w 2 172.30.23.90 80'
Oct 29 22:08:16.592: INFO: stderr: "+ nc -zv -t -w 2 172.30.23.90 80\nConnection to 172.30.23.90 80 port [tcp/http] succeeded!\n"
Oct 29 22:08:16.592: INFO: stdout: ""
Oct 29 22:08:16.592: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=services-917 execpod-affinityzhjg7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.23.90:80/ ; done'
Oct 29 22:08:16.991: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.23.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.23.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.23.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.23.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.23.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.23.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.23.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.23.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.23.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.23.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.23.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.23.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.23.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.23.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.23.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.23.90:80/\n"
Oct 29 22:08:16.992: INFO: stdout: "\naffinity-clusterip-q6zkh\naffinity-clusterip-q6zkh\naffinity-clusterip-q6zkh\naffinity-clusterip-q6zkh\naffinity-clusterip-q6zkh\naffinity-clusterip-q6zkh\naffinity-clusterip-q6zkh\naffinity-clusterip-q6zkh\naffinity-clusterip-q6zkh\naffinity-clusterip-q6zkh\naffinity-clusterip-q6zkh\naffinity-clusterip-q6zkh\naffinity-clusterip-q6zkh\naffinity-clusterip-q6zkh\naffinity-clusterip-q6zkh\naffinity-clusterip-q6zkh"
Oct 29 22:08:16.992: INFO: Received response from host: affinity-clusterip-q6zkh
Oct 29 22:08:16.992: INFO: Received response from host: affinity-clusterip-q6zkh
Oct 29 22:08:16.992: INFO: Received response from host: affinity-clusterip-q6zkh
Oct 29 22:08:16.992: INFO: Received response from host: affinity-clusterip-q6zkh
Oct 29 22:08:16.992: INFO: Received response from host: affinity-clusterip-q6zkh
Oct 29 22:08:16.992: INFO: Received response from host: affinity-clusterip-q6zkh
Oct 29 22:08:16.992: INFO: Received response from host: affinity-clusterip-q6zkh
Oct 29 22:08:16.992: INFO: Received response from host: affinity-clusterip-q6zkh
Oct 29 22:08:16.992: INFO: Received response from host: affinity-clusterip-q6zkh
Oct 29 22:08:16.992: INFO: Received response from host: affinity-clusterip-q6zkh
Oct 29 22:08:16.992: INFO: Received response from host: affinity-clusterip-q6zkh
Oct 29 22:08:16.992: INFO: Received response from host: affinity-clusterip-q6zkh
Oct 29 22:08:16.992: INFO: Received response from host: affinity-clusterip-q6zkh
Oct 29 22:08:16.992: INFO: Received response from host: affinity-clusterip-q6zkh
Oct 29 22:08:16.992: INFO: Received response from host: affinity-clusterip-q6zkh
Oct 29 22:08:16.992: INFO: Received response from host: affinity-clusterip-q6zkh
Oct 29 22:08:16.992: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-917, will wait for the garbage collector to delete the pods
Oct 29 22:08:17.132: INFO: Deleting ReplicationController affinity-clusterip took: 25.049653ms
Oct 29 22:08:17.332: INFO: Terminating ReplicationController affinity-clusterip pods took: 200.261197ms
[AfterEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:08:27.175: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-917" for this suite.
[AfterEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786


 [SLOW TEST:22.795 seconds]
[sig-network] Services
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":-1,"completed":58,"skipped":996,"failed":0}

SSSSSSSSSSS
------------------------------
[BeforeEach] [sig-instrumentation] Events API
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:08:27.271: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Oct 29 22:08:27.542: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:08:27.612: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "events-8955" for this suite.


------------------------------
{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":-1,"completed":59,"skipped":1007,"failed":0}

SSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-network] IngressClass API
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:08:27.673: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Oct 29 22:08:27.996: INFO: starting watch
STEP: patching
STEP: updating
Oct 29 22:08:28.062: INFO: waiting for watch events with expected annotations
Oct 29 22:08:28.062: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:08:28.180: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "ingressclass-1827" for this suite.


------------------------------
{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":-1,"completed":60,"skipped":1025,"failed":0}

SSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Probing container
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:07:29.391: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Probing container
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:08:29.618: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-probe-5157" for this suite.


 [SLOW TEST:60.338 seconds]
[k8s.io] Probing container
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":-1,"completed":69,"skipped":1198,"failed":0}

SSSSSS
------------------------------
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:08:28.240: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Oct 29 22:08:28.434: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bf8c1f18-b43f-42dd-8120-72feb7b91f6b" in namespace "downward-api-8738" to be "Succeeded or Failed"
Oct 29 22:08:28.466: INFO: Pod "downwardapi-volume-bf8c1f18-b43f-42dd-8120-72feb7b91f6b": Phase="Pending", Reason="", readiness=false. Elapsed: 32.01191ms
Oct 29 22:08:30.489: INFO: Pod "downwardapi-volume-bf8c1f18-b43f-42dd-8120-72feb7b91f6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054490869s
Oct 29 22:08:32.511: INFO: Pod "downwardapi-volume-bf8c1f18-b43f-42dd-8120-72feb7b91f6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.07653125s
STEP: Saw pod success
Oct 29 22:08:32.511: INFO: Pod "downwardapi-volume-bf8c1f18-b43f-42dd-8120-72feb7b91f6b" satisfied condition "Succeeded or Failed"
Oct 29 22:08:32.533: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod downwardapi-volume-bf8c1f18-b43f-42dd-8120-72feb7b91f6b container client-container: <nil>
STEP: delete the pod
Oct 29 22:08:32.587: INFO: Waiting for pod downwardapi-volume-bf8c1f18-b43f-42dd-8120-72feb7b91f6b to disappear
Oct 29 22:08:32.609: INFO: Pod downwardapi-volume-bf8c1f18-b43f-42dd-8120-72feb7b91f6b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:08:32.609: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-8738" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":-1,"completed":61,"skipped":1043,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:08:32.736: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:163
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:08:33.014: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-3536" for this suite.


------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":-1,"completed":62,"skipped":1071,"failed":0}

SSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] Secrets
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:08:29.736: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-4feed546-6f34-4b4b-9bce-9ea8fe1cfc8b
STEP: Creating a pod to test consume secrets
Oct 29 22:08:29.942: INFO: Waiting up to 5m0s for pod "pod-secrets-331bcb5c-c35f-4dd8-a89e-635d92ea0183" in namespace "secrets-5906" to be "Succeeded or Failed"
Oct 29 22:08:29.969: INFO: Pod "pod-secrets-331bcb5c-c35f-4dd8-a89e-635d92ea0183": Phase="Pending", Reason="", readiness=false. Elapsed: 27.279298ms
Oct 29 22:08:31.991: INFO: Pod "pod-secrets-331bcb5c-c35f-4dd8-a89e-635d92ea0183": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049771583s
Oct 29 22:08:34.013: INFO: Pod "pod-secrets-331bcb5c-c35f-4dd8-a89e-635d92ea0183": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.071470503s
STEP: Saw pod success
Oct 29 22:08:34.013: INFO: Pod "pod-secrets-331bcb5c-c35f-4dd8-a89e-635d92ea0183" satisfied condition "Succeeded or Failed"
Oct 29 22:08:34.035: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod pod-secrets-331bcb5c-c35f-4dd8-a89e-635d92ea0183 container secret-env-test: <nil>
STEP: delete the pod
Oct 29 22:08:34.090: INFO: Waiting for pod pod-secrets-331bcb5c-c35f-4dd8-a89e-635d92ea0183 to disappear
Oct 29 22:08:34.111: INFO: Pod pod-secrets-331bcb5c-c35f-4dd8-a89e-635d92ea0183 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:08:34.111: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-5906" for this suite.


------------------------------
{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":-1,"completed":70,"skipped":1204,"failed":0}

SSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:08:34.230: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Oct 29 22:08:34.383: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a713e82e-0bca-416e-8888-5052ba48bb40" in namespace "projected-5301" to be "Succeeded or Failed"
Oct 29 22:08:34.405: INFO: Pod "downwardapi-volume-a713e82e-0bca-416e-8888-5052ba48bb40": Phase="Pending", Reason="", readiness=false. Elapsed: 22.315085ms
Oct 29 22:08:36.427: INFO: Pod "downwardapi-volume-a713e82e-0bca-416e-8888-5052ba48bb40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044343813s
Oct 29 22:08:38.449: INFO: Pod "downwardapi-volume-a713e82e-0bca-416e-8888-5052ba48bb40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066634775s
STEP: Saw pod success
Oct 29 22:08:38.450: INFO: Pod "downwardapi-volume-a713e82e-0bca-416e-8888-5052ba48bb40" satisfied condition "Succeeded or Failed"
Oct 29 22:08:38.473: INFO: Trying to get logs from node ip-10-0-142-212.us-east-2.compute.internal pod downwardapi-volume-a713e82e-0bca-416e-8888-5052ba48bb40 container client-container: <nil>
STEP: delete the pod
Oct 29 22:08:38.531: INFO: Waiting for pod downwardapi-volume-a713e82e-0bca-416e-8888-5052ba48bb40 to disappear
Oct 29 22:08:38.553: INFO: Pod downwardapi-volume-a713e82e-0bca-416e-8888-5052ba48bb40 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:08:38.553: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-5301" for this suite.


------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":-1,"completed":71,"skipped":1222,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Probing container
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:07:42.163: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-2b2072a8-e364-48b4-9b43-2208245bde5f in namespace container-probe-2060
Oct 29 22:07:46.435: INFO: Started pod busybox-2b2072a8-e364-48b4-9b43-2208245bde5f in namespace container-probe-2060
STEP: checking the pod's current state and verifying that restartCount is present
Oct 29 22:07:46.456: INFO: Initial restart count of pod busybox-2b2072a8-e364-48b4-9b43-2208245bde5f is 0
Oct 29 22:08:39.043: INFO: Restart count of pod container-probe-2060/busybox-2b2072a8-e364-48b4-9b43-2208245bde5f is now 1 (52.587092284s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:08:39.072: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-probe-2060" for this suite.


 [SLOW TEST:57.012 seconds]
[k8s.io] Probing container
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
[BeforeEach] [sig-network] Networking
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:08:16.691: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-7329
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Oct 29 22:08:16.840: INFO: Waiting up to 10m0s for all (but 3) nodes to be schedulable
Oct 29 22:08:17.107: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Oct 29 22:08:19.131: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Oct 29 22:08:21.129: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 29 22:08:23.129: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 29 22:08:25.129: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 29 22:08:27.129: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 29 22:08:29.129: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 29 22:08:31.129: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 29 22:08:33.137: INFO: The status of Pod netserver-0 is Running (Ready = true)
Oct 29 22:08:33.203: INFO: The status of Pod netserver-1 is Running (Ready = false)
Oct 29 22:08:35.225: INFO: The status of Pod netserver-1 is Running (Ready = false)
Oct 29 22:08:37.225: INFO: The status of Pod netserver-1 is Running (Ready = false)
Oct 29 22:08:39.233: INFO: The status of Pod netserver-1 is Running (Ready = true)
Oct 29 22:08:39.291: INFO: The status of Pod netserver-2 is Running (Ready = true)
Oct 29 22:08:39.341: INFO: The status of Pod netserver-3 is Running (Ready = true)
STEP: Creating test pods
Oct 29 22:08:43.496: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.0.165:8080/dial?request=hostname&protocol=udp&host=10.131.0.158&port=8081&tries=1'] Namespace:pod-network-test-7329 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 29 22:08:43.496: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
Oct 29 22:08:43.699: INFO: Waiting for responses: map[]
Oct 29 22:08:43.721: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.0.165:8080/dial?request=hostname&protocol=udp&host=10.129.2.88&port=8081&tries=1'] Namespace:pod-network-test-7329 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 29 22:08:43.721: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
Oct 29 22:08:43.932: INFO: Waiting for responses: map[]
Oct 29 22:08:43.954: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.0.165:8080/dial?request=hostname&protocol=udp&host=10.130.2.87&port=8081&tries=1'] Namespace:pod-network-test-7329 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 29 22:08:43.954: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
Oct 29 22:08:44.162: INFO: Waiting for responses: map[]
Oct 29 22:08:44.191: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.0.165:8080/dial?request=hostname&protocol=udp&host=10.128.3.19&port=8081&tries=1'] Namespace:pod-network-test-7329 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 29 22:08:44.191: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
Oct 29 22:08:44.402: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:08:44.402: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pod-network-test-7329" for this suite.


 [SLOW TEST:27.842 seconds]
[sig-network] Networking
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":-1,"completed":77,"skipped":1370,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":-1,"completed":65,"skipped":1037,"failed":0}
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:08:39.176: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 22:08:39.406: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Oct 29 22:08:46.775: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig --namespace=crd-publish-openapi-8829 create -f -'
Oct 29 22:08:51.644: INFO: stderr: ""
Oct 29 22:08:51.644: INFO: stdout: "e2e-test-crd-publish-openapi-8311-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Oct 29 22:08:51.644: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig --namespace=crd-publish-openapi-8829 delete e2e-test-crd-publish-openapi-8311-crds test-cr'
Oct 29 22:08:51.836: INFO: stderr: ""
Oct 29 22:08:51.836: INFO: stdout: "e2e-test-crd-publish-openapi-8311-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Oct 29 22:08:51.836: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig --namespace=crd-publish-openapi-8829 apply -f -'
Oct 29 22:08:52.588: INFO: stderr: ""
Oct 29 22:08:52.588: INFO: stdout: "e2e-test-crd-publish-openapi-8311-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Oct 29 22:08:52.588: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig --namespace=crd-publish-openapi-8829 delete e2e-test-crd-publish-openapi-8311-crds test-cr'
Oct 29 22:08:52.752: INFO: stderr: ""
Oct 29 22:08:52.752: INFO: stdout: "e2e-test-crd-publish-openapi-8311-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Oct 29 22:08:52.753: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig explain e2e-test-crd-publish-openapi-8311-crds'
Oct 29 22:08:53.438: INFO: stderr: ""
Oct 29 22:08:53.438: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8311-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:09:00.744: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8829" for this suite.


 [SLOW TEST:21.669 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":-1,"completed":66,"skipped":1037,"failed":0}

SSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:08:38.681: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Oct 29 22:08:49.129: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Oct 29 22:08:49.151: INFO: Pod pod-with-poststart-http-hook still exists
Oct 29 22:08:51.151: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Oct 29 22:08:51.173: INFO: Pod pod-with-poststart-http-hook still exists
Oct 29 22:08:53.151: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Oct 29 22:08:53.173: INFO: Pod pod-with-poststart-http-hook still exists
Oct 29 22:08:55.151: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Oct 29 22:08:55.174: INFO: Pod pod-with-poststart-http-hook still exists
Oct 29 22:08:57.151: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Oct 29 22:08:57.174: INFO: Pod pod-with-poststart-http-hook still exists
Oct 29 22:08:59.151: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Oct 29 22:08:59.173: INFO: Pod pod-with-poststart-http-hook still exists
Oct 29 22:09:01.151: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Oct 29 22:09:01.173: INFO: Pod pod-with-poststart-http-hook still exists
Oct 29 22:09:03.151: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Oct 29 22:09:03.173: INFO: Pod pod-with-poststart-http-hook still exists
Oct 29 22:09:05.151: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Oct 29 22:09:05.174: INFO: Pod pod-with-poststart-http-hook still exists
Oct 29 22:09:07.151: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Oct 29 22:09:07.173: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:09:07.173: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1407" for this suite.


 [SLOW TEST:28.596 seconds]
[k8s.io] Container Lifecycle Hook
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":-1,"completed":72,"skipped":1257,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-storage] Secrets
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:09:00.859: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 22:09:01.025: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-3672fec2-f6bf-4e01-a503-6a8306581bfb
STEP: Creating secret with name s-test-opt-upd-510a7222-330b-4907-b6e4-fe15752b5733
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-3672fec2-f6bf-4e01-a503-6a8306581bfb
STEP: Updating secret s-test-opt-upd-510a7222-330b-4907-b6e4-fe15752b5733
STEP: Creating secret with name s-test-opt-create-10dd48ae-9391-42e0-ba43-b6f39119e289
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:09:09.461: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-7987" for this suite.


 [SLOW TEST:8.703 seconds]
[sig-storage] Secrets
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":-1,"completed":67,"skipped":1057,"failed":0}

SSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Pods
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:09:09.572: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should delete a collection of pods [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pods
Oct 29 22:09:09.748: INFO: created test-pod-1
Oct 29 22:09:09.775: INFO: created test-pod-2
Oct 29 22:09:09.804: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:09:09.890: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-267" for this suite.


------------------------------
{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":-1,"completed":68,"skipped":1067,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Kubelet
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:09:09.958: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:09:14.209: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubelet-test-9499" for this suite.


------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":-1,"completed":69,"skipped":1092,"failed":0}

SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Variable Expansion
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:08:33.077: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Oct 29 22:08:37.373: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-4133 PodName:var-expansion-4c7b549a-fcaa-49f7-aa33-3112144ad745 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 29 22:08:37.373: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: test for file in mounted path
Oct 29 22:08:37.592: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-4133 PodName:var-expansion-4c7b549a-fcaa-49f7-aa33-3112144ad745 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 29 22:08:37.592: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: updating the annotation value
Oct 29 22:08:38.356: INFO: Successfully updated pod "var-expansion-4c7b549a-fcaa-49f7-aa33-3112144ad745"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Oct 29 22:08:38.379: INFO: Deleting pod "var-expansion-4c7b549a-fcaa-49f7-aa33-3112144ad745" in namespace "var-expansion-4133"
Oct 29 22:08:38.405: INFO: Wait up to 5m0s for pod "var-expansion-4c7b549a-fcaa-49f7-aa33-3112144ad745" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:09:18.452: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "var-expansion-4133" for this suite.


 [SLOW TEST:45.494 seconds]
[k8s.io] Variable Expansion
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":-1,"completed":63,"skipped":1084,"failed":0}

SSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:09:14.335: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 29 22:09:14.477: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:09:18.750: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5639" for this suite.


------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":-1,"completed":70,"skipped":1121,"failed":0}

SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[BeforeEach] [k8s.io] Lease
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:09:18.918: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Lease
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:09:19.569: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "lease-test-5027" for this suite.


------------------------------
{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":-1,"completed":71,"skipped":1143,"failed":0}

SSSSSSSSSSSSSSSSS
------------------------------
Oct 29 22:09:19.673: INFO: Running AfterSuite actions on all nodes


[BeforeEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:09:07.299: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-416
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-416
I1029 22:09:07.584044   53213 runners.go:190] Created replication controller with name: externalname-service, namespace: services-416, replica count: 2
I1029 22:09:10.634652   53213 runners.go:190] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1029 22:09:13.634986   53213 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 29 22:09:13.635: INFO: Creating new exec pod
Oct 29 22:09:18.741: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=services-416 execpod6hf9z -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Oct 29 22:09:19.177: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Oct 29 22:09:19.177: INFO: stdout: ""
Oct 29 22:09:19.177: INFO: Running '/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/jeder-461-cncf2.kubeconfig exec --namespace=services-416 execpod6hf9z -- /bin/sh -x -c nc -zv -t -w 2 172.30.255.65 80'
Oct 29 22:09:19.563: INFO: stderr: "+ nc -zv -t -w 2 172.30.255.65 80\nConnection to 172.30.255.65 80 port [tcp/http] succeeded!\n"
Oct 29 22:09:19.563: INFO: stdout: ""
Oct 29 22:09:19.563: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:09:19.643: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-416" for this suite.
[AfterEach] [sig-network] Services
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786


 [SLOW TEST:12.468 seconds]
[sig-network] Services
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
[BeforeEach] version v1
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:09:18.590: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-r8qpj in namespace proxy-1537
I1029 22:09:18.891160   53211 runners.go:190] Created replication controller with name: proxy-service-r8qpj, namespace: proxy-1537, replica count: 1
I1029 22:09:19.941780   53211 runners.go:190] proxy-service-r8qpj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1029 22:09:20.942082   53211 runners.go:190] proxy-service-r8qpj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1029 22:09:21.942391   53211 runners.go:190] proxy-service-r8qpj Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1029 22:09:22.942636   53211 runners.go:190] proxy-service-r8qpj Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1029 22:09:23.942950   53211 runners.go:190] proxy-service-r8qpj Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1029 22:09:24.943258   53211 runners.go:190] proxy-service-r8qpj Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1029 22:09:25.943495   53211 runners.go:190] proxy-service-r8qpj Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1029 22:09:26.943585   53211 runners.go:190] proxy-service-r8qpj Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1029 22:09:27.943842   53211 runners.go:190] proxy-service-r8qpj Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1029 22:09:28.944081   53211 runners.go:190] proxy-service-r8qpj Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1029 22:09:29.944283   53211 runners.go:190] proxy-service-r8qpj Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1029 22:09:30.944600   53211 runners.go:190] proxy-service-r8qpj Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1029 22:09:31.944833   53211 runners.go:190] proxy-service-r8qpj Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 29 22:09:31.968: INFO: setup took 13.151860533s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Oct 29 22:09:31.992: INFO: (0) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k/proxy/rewriteme">test</a> (200; 24.487591ms)
Oct 29 22:09:31.993: INFO: (0) /api/v1/namespaces/proxy-1537/services/proxy-service-r8qpj:portname1/proxy/: foo (200; 24.535972ms)
Oct 29 22:09:31.993: INFO: (0) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:162/proxy/: bar (200; 24.817123ms)
Oct 29 22:09:31.993: INFO: (0) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:160/proxy/: foo (200; 24.988917ms)
Oct 29 22:09:31.995: INFO: (0) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:1080/proxy/rewriteme">test<... (200; 26.781876ms)
Oct 29 22:09:32.012: INFO: (0) /api/v1/namespaces/proxy-1537/services/https:proxy-service-r8qpj:tlsportname2/proxy/: tls qux (200; 43.466786ms)
Oct 29 22:09:32.012: INFO: (0) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:443/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:443/proxy/tlsrewritem... (200; 43.476436ms)
Oct 29 22:09:32.012: INFO: (0) /api/v1/namespaces/proxy-1537/services/http:proxy-service-r8qpj:portname1/proxy/: foo (200; 43.438253ms)
Oct 29 22:09:32.012: INFO: (0) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:1080/proxy/rewriteme">... (200; 43.535342ms)
Oct 29 22:09:32.012: INFO: (0) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:160/proxy/: foo (200; 43.477595ms)
Oct 29 22:09:32.012: INFO: (0) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:462/proxy/: tls qux (200; 43.68101ms)
Oct 29 22:09:32.012: INFO: (0) /api/v1/namespaces/proxy-1537/services/https:proxy-service-r8qpj:tlsportname1/proxy/: tls baz (200; 43.629209ms)
Oct 29 22:09:32.012: INFO: (0) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:460/proxy/: tls baz (200; 43.609341ms)
Oct 29 22:09:32.012: INFO: (0) /api/v1/namespaces/proxy-1537/services/http:proxy-service-r8qpj:portname2/proxy/: bar (200; 43.506602ms)
Oct 29 22:09:32.012: INFO: (0) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:162/proxy/: bar (200; 43.640579ms)
Oct 29 22:09:32.017: INFO: (0) /api/v1/namespaces/proxy-1537/services/proxy-service-r8qpj:portname2/proxy/: bar (200; 49.040669ms)
Oct 29 22:09:32.040: INFO: (1) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k/proxy/rewriteme">test</a> (200; 23.335125ms)
Oct 29 22:09:32.041: INFO: (1) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:162/proxy/: bar (200; 23.522249ms)
Oct 29 22:09:32.041: INFO: (1) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:160/proxy/: foo (200; 23.659429ms)
Oct 29 22:09:32.041: INFO: (1) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:1080/proxy/rewriteme">test<... (200; 23.758508ms)
Oct 29 22:09:32.043: INFO: (1) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:160/proxy/: foo (200; 25.330331ms)
Oct 29 22:09:32.043: INFO: (1) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:460/proxy/: tls baz (200; 25.37791ms)
Oct 29 22:09:32.044: INFO: (1) /api/v1/namespaces/proxy-1537/services/https:proxy-service-r8qpj:tlsportname2/proxy/: tls qux (200; 26.614932ms)
Oct 29 22:09:32.044: INFO: (1) /api/v1/namespaces/proxy-1537/services/https:proxy-service-r8qpj:tlsportname1/proxy/: tls baz (200; 26.959265ms)
Oct 29 22:09:32.044: INFO: (1) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:1080/proxy/rewriteme">... (200; 27.048668ms)
Oct 29 22:09:32.044: INFO: (1) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:443/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:443/proxy/tlsrewritem... (200; 27.258026ms)
Oct 29 22:09:32.045: INFO: (1) /api/v1/namespaces/proxy-1537/services/http:proxy-service-r8qpj:portname1/proxy/: foo (200; 27.377419ms)
Oct 29 22:09:32.045: INFO: (1) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:462/proxy/: tls qux (200; 27.193502ms)
Oct 29 22:09:32.045: INFO: (1) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:162/proxy/: bar (200; 27.305805ms)
Oct 29 22:09:32.047: INFO: (1) /api/v1/namespaces/proxy-1537/services/http:proxy-service-r8qpj:portname2/proxy/: bar (200; 30.028012ms)
Oct 29 22:09:32.059: INFO: (1) /api/v1/namespaces/proxy-1537/services/proxy-service-r8qpj:portname1/proxy/: foo (200; 42.284622ms)
Oct 29 22:09:32.059: INFO: (1) /api/v1/namespaces/proxy-1537/services/proxy-service-r8qpj:portname2/proxy/: bar (200; 42.265707ms)
Oct 29 22:09:32.082: INFO: (2) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k/proxy/rewriteme">test</a> (200; 22.557941ms)
Oct 29 22:09:32.083: INFO: (2) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:1080/proxy/rewriteme">test<... (200; 22.81119ms)
Oct 29 22:09:32.083: INFO: (2) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:160/proxy/: foo (200; 23.167885ms)
Oct 29 22:09:32.083: INFO: (2) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:162/proxy/: bar (200; 23.570113ms)
Oct 29 22:09:32.083: INFO: (2) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:1080/proxy/rewriteme">... (200; 23.700192ms)
Oct 29 22:09:32.084: INFO: (2) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:460/proxy/: tls baz (200; 24.517475ms)
Oct 29 22:09:32.085: INFO: (2) /api/v1/namespaces/proxy-1537/services/proxy-service-r8qpj:portname1/proxy/: foo (200; 24.799356ms)
Oct 29 22:09:32.085: INFO: (2) /api/v1/namespaces/proxy-1537/services/https:proxy-service-r8qpj:tlsportname2/proxy/: tls qux (200; 25.367571ms)
Oct 29 22:09:32.086: INFO: (2) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:443/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:443/proxy/tlsrewritem... (200; 26.048764ms)
Oct 29 22:09:32.086: INFO: (2) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:160/proxy/: foo (200; 26.325886ms)
Oct 29 22:09:32.086: INFO: (2) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:162/proxy/: bar (200; 26.321222ms)
Oct 29 22:09:32.086: INFO: (2) /api/v1/namespaces/proxy-1537/services/https:proxy-service-r8qpj:tlsportname1/proxy/: tls baz (200; 26.392195ms)
Oct 29 22:09:32.086: INFO: (2) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:462/proxy/: tls qux (200; 26.679336ms)
Oct 29 22:09:32.087: INFO: (2) /api/v1/namespaces/proxy-1537/services/proxy-service-r8qpj:portname2/proxy/: bar (200; 27.008216ms)
Oct 29 22:09:32.087: INFO: (2) /api/v1/namespaces/proxy-1537/services/http:proxy-service-r8qpj:portname2/proxy/: bar (200; 27.76343ms)
Oct 29 22:09:32.089: INFO: (2) /api/v1/namespaces/proxy-1537/services/http:proxy-service-r8qpj:portname1/proxy/: foo (200; 29.609244ms)
Oct 29 22:09:32.112: INFO: (3) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:443/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:443/proxy/tlsrewritem... (200; 22.542347ms)
Oct 29 22:09:32.112: INFO: (3) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:160/proxy/: foo (200; 22.27655ms)
Oct 29 22:09:32.113: INFO: (3) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:460/proxy/: tls baz (200; 23.142035ms)
Oct 29 22:09:32.114: INFO: (3) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:162/proxy/: bar (200; 24.398814ms)
Oct 29 22:09:32.114: INFO: (3) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:1080/proxy/rewriteme">test<... (200; 24.448928ms)
Oct 29 22:09:32.114: INFO: (3) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k/proxy/rewriteme">test</a> (200; 24.214867ms)
Oct 29 22:09:32.114: INFO: (3) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:1080/proxy/rewriteme">... (200; 24.217806ms)
Oct 29 22:09:32.115: INFO: (3) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:160/proxy/: foo (200; 25.106981ms)
Oct 29 22:09:32.116: INFO: (3) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:162/proxy/: bar (200; 25.802627ms)
Oct 29 22:09:32.118: INFO: (3) /api/v1/namespaces/proxy-1537/services/proxy-service-r8qpj:portname1/proxy/: foo (200; 27.741533ms)
Oct 29 22:09:32.118: INFO: (3) /api/v1/namespaces/proxy-1537/services/proxy-service-r8qpj:portname2/proxy/: bar (200; 27.702101ms)
Oct 29 22:09:32.118: INFO: (3) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:462/proxy/: tls qux (200; 27.921091ms)
Oct 29 22:09:32.118: INFO: (3) /api/v1/namespaces/proxy-1537/services/http:proxy-service-r8qpj:portname1/proxy/: foo (200; 28.280481ms)
Oct 29 22:09:32.118: INFO: (3) /api/v1/namespaces/proxy-1537/services/https:proxy-service-r8qpj:tlsportname1/proxy/: tls baz (200; 27.841481ms)
Oct 29 22:09:32.118: INFO: (3) /api/v1/namespaces/proxy-1537/services/http:proxy-service-r8qpj:portname2/proxy/: bar (200; 28.083034ms)
Oct 29 22:09:32.118: INFO: (3) /api/v1/namespaces/proxy-1537/services/https:proxy-service-r8qpj:tlsportname2/proxy/: tls qux (200; 28.391049ms)
Oct 29 22:09:32.141: INFO: (4) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:443/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:443/proxy/tlsrewritem... (200; 22.482697ms)
Oct 29 22:09:32.141: INFO: (4) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:1080/proxy/rewriteme">test<... (200; 22.395548ms)
Oct 29 22:09:32.142: INFO: (4) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:160/proxy/: foo (200; 23.399884ms)
Oct 29 22:09:32.142: INFO: (4) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k/proxy/rewriteme">test</a> (200; 23.13103ms)
Oct 29 22:09:32.144: INFO: (4) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:162/proxy/: bar (200; 25.071214ms)
Oct 29 22:09:32.146: INFO: (4) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:162/proxy/: bar (200; 27.266015ms)
Oct 29 22:09:32.147: INFO: (4) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:1080/proxy/rewriteme">... (200; 28.071417ms)
Oct 29 22:09:32.147: INFO: (4) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:160/proxy/: foo (200; 28.868042ms)
Oct 29 22:09:32.148: INFO: (4) /api/v1/namespaces/proxy-1537/services/proxy-service-r8qpj:portname1/proxy/: foo (200; 28.833146ms)
Oct 29 22:09:32.148: INFO: (4) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:460/proxy/: tls baz (200; 28.690062ms)
Oct 29 22:09:32.148: INFO: (4) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:462/proxy/: tls qux (200; 29.169493ms)
Oct 29 22:09:32.149: INFO: (4) /api/v1/namespaces/proxy-1537/services/https:proxy-service-r8qpj:tlsportname2/proxy/: tls qux (200; 30.405528ms)
Oct 29 22:09:32.150: INFO: (4) /api/v1/namespaces/proxy-1537/services/https:proxy-service-r8qpj:tlsportname1/proxy/: tls baz (200; 30.598768ms)
Oct 29 22:09:32.150: INFO: (4) /api/v1/namespaces/proxy-1537/services/proxy-service-r8qpj:portname2/proxy/: bar (200; 31.049056ms)
Oct 29 22:09:32.154: INFO: (4) /api/v1/namespaces/proxy-1537/services/http:proxy-service-r8qpj:portname1/proxy/: foo (200; 35.142444ms)
Oct 29 22:09:32.154: INFO: (4) /api/v1/namespaces/proxy-1537/services/http:proxy-service-r8qpj:portname2/proxy/: bar (200; 34.839355ms)
Oct 29 22:09:32.176: INFO: (5) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:162/proxy/: bar (200; 22.6804ms)
Oct 29 22:09:32.179: INFO: (5) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:160/proxy/: foo (200; 24.782065ms)
Oct 29 22:09:32.179: INFO: (5) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:160/proxy/: foo (200; 24.902815ms)
Oct 29 22:09:32.179: INFO: (5) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k/proxy/rewriteme">test</a> (200; 25.072001ms)
Oct 29 22:09:32.180: INFO: (5) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:162/proxy/: bar (200; 25.509512ms)
Oct 29 22:09:32.180: INFO: (5) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:1080/proxy/rewriteme">... (200; 25.631437ms)
Oct 29 22:09:32.181: INFO: (5) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:1080/proxy/rewriteme">test<... (200; 27.006249ms)
Oct 29 22:09:32.181: INFO: (5) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:462/proxy/: tls qux (200; 27.227797ms)
Oct 29 22:09:32.181: INFO: (5) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:460/proxy/: tls baz (200; 27.399543ms)
Oct 29 22:09:32.182: INFO: (5) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:443/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:443/proxy/tlsrewritem... (200; 27.532883ms)
Oct 29 22:09:32.182: INFO: (5) /api/v1/namespaces/proxy-1537/services/https:proxy-service-r8qpj:tlsportname2/proxy/: tls qux (200; 27.985292ms)
Oct 29 22:09:32.184: INFO: (5) /api/v1/namespaces/proxy-1537/services/proxy-service-r8qpj:portname2/proxy/: bar (200; 29.950653ms)
Oct 29 22:09:32.184: INFO: (5) /api/v1/namespaces/proxy-1537/services/https:proxy-service-r8qpj:tlsportname1/proxy/: tls baz (200; 30.262509ms)
Oct 29 22:09:32.185: INFO: (5) /api/v1/namespaces/proxy-1537/services/http:proxy-service-r8qpj:portname2/proxy/: bar (200; 30.708423ms)
Oct 29 22:09:32.185: INFO: (5) /api/v1/namespaces/proxy-1537/services/http:proxy-service-r8qpj:portname1/proxy/: foo (200; 30.742029ms)
Oct 29 22:09:32.185: INFO: (5) /api/v1/namespaces/proxy-1537/services/proxy-service-r8qpj:portname1/proxy/: foo (200; 30.979719ms)
Oct 29 22:09:32.207: INFO: (6) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k/proxy/rewriteme">test</a> (200; 21.920181ms)
Oct 29 22:09:32.207: INFO: (6) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:1080/proxy/rewriteme">test<... (200; 22.106953ms)
Oct 29 22:09:32.208: INFO: (6) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:160/proxy/: foo (200; 22.910573ms)
Oct 29 22:09:32.208: INFO: (6) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:162/proxy/: bar (200; 22.944975ms)
Oct 29 22:09:32.208: INFO: (6) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:460/proxy/: tls baz (200; 23.329308ms)
Oct 29 22:09:32.210: INFO: (6) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:162/proxy/: bar (200; 24.949639ms)
Oct 29 22:09:32.210: INFO: (6) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:1080/proxy/rewriteme">... (200; 25.129575ms)
Oct 29 22:09:32.210: INFO: (6) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:462/proxy/: tls qux (200; 25.371469ms)
Oct 29 22:09:32.212: INFO: (6) /api/v1/namespaces/proxy-1537/services/http:proxy-service-r8qpj:portname2/proxy/: bar (200; 26.662777ms)
Oct 29 22:09:32.212: INFO: (6) /api/v1/namespaces/proxy-1537/services/https:proxy-service-r8qpj:tlsportname2/proxy/: tls qux (200; 26.735288ms)
Oct 29 22:09:32.212: INFO: (6) /api/v1/namespaces/proxy-1537/services/proxy-service-r8qpj:portname1/proxy/: foo (200; 27.182612ms)
Oct 29 22:09:32.212: INFO: (6) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:160/proxy/: foo (200; 27.183806ms)
Oct 29 22:09:32.212: INFO: (6) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:443/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:443/proxy/tlsrewritem... (200; 27.272335ms)
Oct 29 22:09:32.212: INFO: (6) /api/v1/namespaces/proxy-1537/services/proxy-service-r8qpj:portname2/proxy/: bar (200; 27.281668ms)
Oct 29 22:09:32.213: INFO: (6) /api/v1/namespaces/proxy-1537/services/http:proxy-service-r8qpj:portname1/proxy/: foo (200; 28.224381ms)
Oct 29 22:09:32.214: INFO: (6) /api/v1/namespaces/proxy-1537/services/https:proxy-service-r8qpj:tlsportname1/proxy/: tls baz (200; 28.890815ms)
Oct 29 22:09:32.236: INFO: (7) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:1080/proxy/rewriteme">... (200; 22.132255ms)
Oct 29 22:09:32.236: INFO: (7) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k/proxy/rewriteme">test</a> (200; 22.196251ms)
Oct 29 22:09:32.236: INFO: (7) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:443/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:443/proxy/tlsrewritem... (200; 22.322114ms)
Oct 29 22:09:32.237: INFO: (7) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:460/proxy/: tls baz (200; 22.825394ms)
Oct 29 22:09:32.237: INFO: (7) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:162/proxy/: bar (200; 23.143094ms)
Oct 29 22:09:32.238: INFO: (7) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:162/proxy/: bar (200; 23.736888ms)
Oct 29 22:09:32.238: INFO: (7) /api/v1/namespaces/proxy-1537/services/proxy-service-r8qpj:portname2/proxy/: bar (200; 24.053306ms)
Oct 29 22:09:32.239: INFO: (7) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:1080/proxy/rewriteme">test<... (200; 25.473638ms)
Oct 29 22:09:32.239: INFO: (7) /api/v1/namespaces/proxy-1537/services/http:proxy-service-r8qpj:portname1/proxy/: foo (200; 25.57281ms)
Oct 29 22:09:32.240: INFO: (7) /api/v1/namespaces/proxy-1537/services/http:proxy-service-r8qpj:portname2/proxy/: bar (200; 25.558893ms)
Oct 29 22:09:32.240: INFO: (7) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:462/proxy/: tls qux (200; 25.529833ms)
Oct 29 22:09:32.240: INFO: (7) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:160/proxy/: foo (200; 25.627994ms)
Oct 29 22:09:32.240: INFO: (7) /api/v1/namespaces/proxy-1537/services/https:proxy-service-r8qpj:tlsportname1/proxy/: tls baz (200; 25.535316ms)
Oct 29 22:09:32.240: INFO: (7) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:160/proxy/: foo (200; 25.816718ms)
Oct 29 22:09:32.241: INFO: (7) /api/v1/namespaces/proxy-1537/services/proxy-service-r8qpj:portname1/proxy/: foo (200; 26.770442ms)
Oct 29 22:09:32.242: INFO: (7) /api/v1/namespaces/proxy-1537/services/https:proxy-service-r8qpj:tlsportname2/proxy/: tls qux (200; 27.9783ms)
Oct 29 22:09:32.264: INFO: (8) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:443/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:443/proxy/tlsrewritem... (200; 22.037011ms)
Oct 29 22:09:32.265: INFO: (8) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:1080/proxy/rewriteme">test<... (200; 22.557601ms)
Oct 29 22:09:32.267: INFO: (8) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:160/proxy/: foo (200; 25.218264ms)
Oct 29 22:09:32.268: INFO: (8) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:162/proxy/: bar (200; 25.653335ms)
Oct 29 22:09:32.268: INFO: (8) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:1080/proxy/rewriteme">... (200; 25.756633ms)
Oct 29 22:09:32.268: INFO: (8) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:162/proxy/: bar (200; 26.023489ms)
Oct 29 22:09:32.268: INFO: (8) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:460/proxy/: tls baz (200; 26.09015ms)
Oct 29 22:09:32.268: INFO: (8) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k/proxy/rewriteme">test</a> (200; 26.30064ms)
Oct 29 22:09:32.268: INFO: (8) /api/v1/namespaces/proxy-1537/services/http:proxy-service-r8qpj:portname1/proxy/: foo (200; 26.25047ms)
Oct 29 22:09:32.269: INFO: (8) /api/v1/namespaces/proxy-1537/services/proxy-service-r8qpj:portname1/proxy/: foo (200; 26.522421ms)
Oct 29 22:09:32.269: INFO: (8) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:462/proxy/: tls qux (200; 26.62194ms)
Oct 29 22:09:32.269: INFO: (8) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:160/proxy/: foo (200; 26.670974ms)
Oct 29 22:09:32.269: INFO: (8) /api/v1/namespaces/proxy-1537/services/https:proxy-service-r8qpj:tlsportname1/proxy/: tls baz (200; 26.81178ms)
Oct 29 22:09:32.269: INFO: (8) /api/v1/namespaces/proxy-1537/services/http:proxy-service-r8qpj:portname2/proxy/: bar (200; 27.402845ms)
Oct 29 22:09:32.270: INFO: (8) /api/v1/namespaces/proxy-1537/services/proxy-service-r8qpj:portname2/proxy/: bar (200; 27.782749ms)
Oct 29 22:09:32.270: INFO: (8) /api/v1/namespaces/proxy-1537/services/https:proxy-service-r8qpj:tlsportname2/proxy/: tls qux (200; 28.110529ms)
Oct 29 22:09:32.292: INFO: (9) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k/proxy/rewriteme">test</a> (200; 22.063468ms)
Oct 29 22:09:32.292: INFO: (9) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:162/proxy/: bar (200; 22.162532ms)
Oct 29 22:09:32.293: INFO: (9) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:160/proxy/: foo (200; 22.245445ms)
Oct 29 22:09:32.294: INFO: (9) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:162/proxy/: bar (200; 23.378568ms)
Oct 29 22:09:32.294: INFO: (9) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:1080/proxy/rewriteme">... (200; 23.569362ms)
Oct 29 22:09:32.294: INFO: (9) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:443/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:443/proxy/tlsrewritem... (200; 23.74576ms)
Oct 29 22:09:32.295: INFO: (9) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:462/proxy/: tls qux (200; 24.296553ms)
Oct 29 22:09:32.295: INFO: (9) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:460/proxy/: tls baz (200; 24.345766ms)
Oct 29 22:09:32.295: INFO: (9) /api/v1/namespaces/proxy-1537/services/http:proxy-service-r8qpj:portname1/proxy/: foo (200; 24.715077ms)
Oct 29 22:09:32.295: INFO: (9) /api/v1/namespaces/proxy-1537/services/https:proxy-service-r8qpj:tlsportname2/proxy/: tls qux (200; 24.68638ms)
Oct 29 22:09:32.296: INFO: (9) /api/v1/namespaces/proxy-1537/services/http:proxy-service-r8qpj:portname2/proxy/: bar (200; 25.274145ms)
Oct 29 22:09:32.296: INFO: (9) /api/v1/namespaces/proxy-1537/services/proxy-service-r8qpj:portname2/proxy/: bar (200; 25.967664ms)
Oct 29 22:09:32.296: INFO: (9) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:160/proxy/: foo (200; 26.18345ms)
Oct 29 22:09:32.297: INFO: (9) /api/v1/namespaces/proxy-1537/services/https:proxy-service-r8qpj:tlsportname1/proxy/: tls baz (200; 26.196554ms)
Oct 29 22:09:32.297: INFO: (9) /api/v1/namespaces/proxy-1537/services/proxy-service-r8qpj:portname1/proxy/: foo (200; 26.815433ms)
Oct 29 22:09:32.297: INFO: (9) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:1080/proxy/rewriteme">test<... (200; 26.938722ms)
Oct 29 22:09:32.319: INFO: (10) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:462/proxy/: tls qux (200; 22.169964ms)
Oct 29 22:09:32.320: INFO: (10) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:162/proxy/: bar (200; 22.362024ms)
Oct 29 22:09:32.320: INFO: (10) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:162/proxy/: bar (200; 22.311698ms)
Oct 29 22:09:32.320: INFO: (10) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:460/proxy/: tls baz (200; 23.156142ms)
Oct 29 22:09:32.323: INFO: (10) /api/v1/namespaces/proxy-1537/services/proxy-service-r8qpj:portname1/proxy/: foo (200; 25.397531ms)
Oct 29 22:09:32.323: INFO: (10) /api/v1/namespaces/proxy-1537/services/https:proxy-service-r8qpj:tlsportname2/proxy/: tls qux (200; 25.574468ms)
Oct 29 22:09:32.323: INFO: (10) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:443/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:443/proxy/tlsrewritem... (200; 25.564788ms)
Oct 29 22:09:32.325: INFO: (10) /api/v1/namespaces/proxy-1537/services/proxy-service-r8qpj:portname2/proxy/: bar (200; 27.259349ms)
Oct 29 22:09:32.325: INFO: (10) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:160/proxy/: foo (200; 27.723422ms)
Oct 29 22:09:32.325: INFO: (10) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:1080/proxy/rewriteme">test<... (200; 27.758875ms)
Oct 29 22:09:32.325: INFO: (10) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:160/proxy/: foo (200; 27.721132ms)
Oct 29 22:09:32.325: INFO: (10) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:1080/proxy/rewriteme">... (200; 27.849089ms)
Oct 29 22:09:32.325: INFO: (10) /api/v1/namespaces/proxy-1537/services/https:proxy-service-r8qpj:tlsportname1/proxy/: tls baz (200; 27.889575ms)
Oct 29 22:09:32.325: INFO: (10) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k/proxy/rewriteme">test</a> (200; 27.903366ms)
Oct 29 22:09:32.326: INFO: (10) /api/v1/namespaces/proxy-1537/services/http:proxy-service-r8qpj:portname2/proxy/: bar (200; 28.878206ms)
Oct 29 22:09:32.327: INFO: (10) /api/v1/namespaces/proxy-1537/services/http:proxy-service-r8qpj:portname1/proxy/: foo (200; 29.246938ms)
Oct 29 22:09:32.350: INFO: (11) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k/proxy/rewriteme">test</a> (200; 22.944596ms)
Oct 29 22:09:32.350: INFO: (11) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:162/proxy/: bar (200; 23.056751ms)
Oct 29 22:09:32.350: INFO: (11) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:462/proxy/: tls qux (200; 22.953862ms)
Oct 29 22:09:32.350: INFO: (11) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:160/proxy/: foo (200; 23.27424ms)
Oct 29 22:09:32.351: INFO: (11) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:160/proxy/: foo (200; 24.171387ms)
Oct 29 22:09:32.351: INFO: (11) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:1080/proxy/rewriteme">test<... (200; 24.373552ms)
Oct 29 22:09:32.352: INFO: (11) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:1080/proxy/rewriteme">... (200; 25.609622ms)
Oct 29 22:09:32.352: INFO: (11) /api/v1/namespaces/proxy-1537/services/proxy-service-r8qpj:portname2/proxy/: bar (200; 25.738579ms)
Oct 29 22:09:32.352: INFO: (11) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:443/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:443/proxy/tlsrewritem... (200; 25.620821ms)
Oct 29 22:09:32.352: INFO: (11) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:162/proxy/: bar (200; 25.589434ms)
Oct 29 22:09:32.352: INFO: (11) /api/v1/namespaces/proxy-1537/services/proxy-service-r8qpj:portname1/proxy/: foo (200; 25.71503ms)
Oct 29 22:09:32.352: INFO: (11) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:460/proxy/: tls baz (200; 25.739703ms)
Oct 29 22:09:32.352: INFO: (11) /api/v1/namespaces/proxy-1537/services/http:proxy-service-r8qpj:portname1/proxy/: foo (200; 25.657016ms)
Oct 29 22:09:32.353: INFO: (11) /api/v1/namespaces/proxy-1537/services/https:proxy-service-r8qpj:tlsportname1/proxy/: tls baz (200; 26.496984ms)
Oct 29 22:09:32.357: INFO: (11) /api/v1/namespaces/proxy-1537/services/https:proxy-service-r8qpj:tlsportname2/proxy/: tls qux (200; 29.897289ms)
Oct 29 22:09:32.357: INFO: (11) /api/v1/namespaces/proxy-1537/services/http:proxy-service-r8qpj:portname2/proxy/: bar (200; 29.94498ms)
Oct 29 22:09:32.379: INFO: (12) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:460/proxy/: tls baz (200; 22.223655ms)
Oct 29 22:09:32.380: INFO: (12) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:160/proxy/: foo (200; 22.889782ms)
Oct 29 22:09:32.381: INFO: (12) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k/proxy/rewriteme">test</a> (200; 24.65737ms)
Oct 29 22:09:32.382: INFO: (12) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:443/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:443/proxy/tlsrewritem... (200; 25.021006ms)
Oct 29 22:09:32.382: INFO: (12) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:162/proxy/: bar (200; 25.039548ms)
Oct 29 22:09:32.383: INFO: (12) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:162/proxy/: bar (200; 25.781616ms)
Oct 29 22:09:32.383: INFO: (12) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:462/proxy/: tls qux (200; 25.847372ms)
Oct 29 22:09:32.383: INFO: (12) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:1080/proxy/rewriteme">... (200; 26.077603ms)
Oct 29 22:09:32.383: INFO: (12) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:1080/proxy/rewriteme">test<... (200; 25.869513ms)
Oct 29 22:09:32.383: INFO: (12) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:160/proxy/: foo (200; 26.202895ms)
Oct 29 22:09:32.384: INFO: (12) /api/v1/namespaces/proxy-1537/services/proxy-service-r8qpj:portname1/proxy/: foo (200; 26.697376ms)
Oct 29 22:09:32.384: INFO: (12) /api/v1/namespaces/proxy-1537/services/http:proxy-service-r8qpj:portname1/proxy/: foo (200; 26.919936ms)
Oct 29 22:09:32.385: INFO: (12) /api/v1/namespaces/proxy-1537/services/proxy-service-r8qpj:portname2/proxy/: bar (200; 27.587306ms)
Oct 29 22:09:32.386: INFO: (12) /api/v1/namespaces/proxy-1537/services/https:proxy-service-r8qpj:tlsportname1/proxy/: tls baz (200; 28.722099ms)
Oct 29 22:09:32.386: INFO: (12) /api/v1/namespaces/proxy-1537/services/https:proxy-service-r8qpj:tlsportname2/proxy/: tls qux (200; 28.544551ms)
Oct 29 22:09:32.386: INFO: (12) /api/v1/namespaces/proxy-1537/services/http:proxy-service-r8qpj:portname2/proxy/: bar (200; 28.713822ms)
Oct 29 22:09:32.408: INFO: (13) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:443/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:443/proxy/tlsrewritem... (200; 22.143128ms)
Oct 29 22:09:32.409: INFO: (13) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:460/proxy/: tls baz (200; 23.320541ms)
Oct 29 22:09:32.409: INFO: (13) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:462/proxy/: tls qux (200; 23.440766ms)
Oct 29 22:09:32.409: INFO: (13) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:1080/proxy/rewriteme">... (200; 23.467202ms)
Oct 29 22:09:32.410: INFO: (13) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k/proxy/rewriteme">test</a> (200; 23.670146ms)
Oct 29 22:09:32.410: INFO: (13) /api/v1/namespaces/proxy-1537/services/http:proxy-service-r8qpj:portname1/proxy/: foo (200; 24.254199ms)
Oct 29 22:09:32.412: INFO: (13) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:160/proxy/: foo (200; 25.827884ms)
Oct 29 22:09:32.412: INFO: (13) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:1080/proxy/rewriteme">test<... (200; 25.74868ms)
Oct 29 22:09:32.412: INFO: (13) /api/v1/namespaces/proxy-1537/services/https:proxy-service-r8qpj:tlsportname1/proxy/: tls baz (200; 25.978768ms)
Oct 29 22:09:32.412: INFO: (13) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:162/proxy/: bar (200; 25.970497ms)
Oct 29 22:09:32.412: INFO: (13) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:160/proxy/: foo (200; 26.04402ms)
Oct 29 22:09:32.412: INFO: (13) /api/v1/namespaces/proxy-1537/services/proxy-service-r8qpj:portname2/proxy/: bar (200; 26.303154ms)
Oct 29 22:09:32.412: INFO: (13) /api/v1/namespaces/proxy-1537/services/proxy-service-r8qpj:portname1/proxy/: foo (200; 26.296799ms)
Oct 29 22:09:32.412: INFO: (13) /api/v1/namespaces/proxy-1537/services/https:proxy-service-r8qpj:tlsportname2/proxy/: tls qux (200; 26.457941ms)
Oct 29 22:09:32.412: INFO: (13) /api/v1/namespaces/proxy-1537/services/http:proxy-service-r8qpj:portname2/proxy/: bar (200; 26.492133ms)
Oct 29 22:09:32.412: INFO: (13) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:162/proxy/: bar (200; 26.389862ms)
Oct 29 22:09:32.435: INFO: (14) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k/proxy/rewriteme">test</a> (200; 22.634178ms)
Oct 29 22:09:32.436: INFO: (14) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:462/proxy/: tls qux (200; 23.818454ms)
Oct 29 22:09:32.436: INFO: (14) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:160/proxy/: foo (200; 23.840842ms)
Oct 29 22:09:32.437: INFO: (14) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:1080/proxy/rewriteme">... (200; 24.216971ms)
Oct 29 22:09:32.437: INFO: (14) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:162/proxy/: bar (200; 24.820099ms)
Oct 29 22:09:32.437: INFO: (14) /api/v1/namespaces/proxy-1537/services/http:proxy-service-r8qpj:portname2/proxy/: bar (200; 24.711454ms)
Oct 29 22:09:32.437: INFO: (14) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:160/proxy/: foo (200; 24.880732ms)
Oct 29 22:09:32.438: INFO: (14) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:460/proxy/: tls baz (200; 25.326053ms)
Oct 29 22:09:32.438: INFO: (14) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:1080/proxy/rewriteme">test<... (200; 25.317797ms)
Oct 29 22:09:32.438: INFO: (14) /api/v1/namespaces/proxy-1537/services/http:proxy-service-r8qpj:portname1/proxy/: foo (200; 25.362322ms)
Oct 29 22:09:32.438: INFO: (14) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:443/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:443/proxy/tlsrewritem... (200; 25.456057ms)
Oct 29 22:09:32.438: INFO: (14) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:162/proxy/: bar (200; 25.577217ms)
Oct 29 22:09:32.438: INFO: (14) /api/v1/namespaces/proxy-1537/services/proxy-service-r8qpj:portname2/proxy/: bar (200; 25.811723ms)
Oct 29 22:09:32.439: INFO: (14) /api/v1/namespaces/proxy-1537/services/https:proxy-service-r8qpj:tlsportname1/proxy/: tls baz (200; 26.538393ms)
Oct 29 22:09:32.439: INFO: (14) /api/v1/namespaces/proxy-1537/services/proxy-service-r8qpj:portname1/proxy/: foo (200; 26.99075ms)
Oct 29 22:09:32.440: INFO: (14) /api/v1/namespaces/proxy-1537/services/https:proxy-service-r8qpj:tlsportname2/proxy/: tls qux (200; 27.477113ms)
Oct 29 22:09:32.463: INFO: (15) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:1080/proxy/rewriteme">test<... (200; 23.220423ms)
Oct 29 22:09:32.463: INFO: (15) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:160/proxy/: foo (200; 23.322231ms)
Oct 29 22:09:32.464: INFO: (15) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:1080/proxy/rewriteme">... (200; 23.82566ms)
Oct 29 22:09:32.465: INFO: (15) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:462/proxy/: tls qux (200; 24.616634ms)
Oct 29 22:09:32.465: INFO: (15) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:443/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:443/proxy/tlsrewritem... (200; 24.719049ms)
Oct 29 22:09:32.465: INFO: (15) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:460/proxy/: tls baz (200; 24.851036ms)
Oct 29 22:09:32.465: INFO: (15) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:160/proxy/: foo (200; 25.281106ms)
Oct 29 22:09:32.465: INFO: (15) /api/v1/namespaces/proxy-1537/services/https:proxy-service-r8qpj:tlsportname2/proxy/: tls qux (200; 25.277128ms)
Oct 29 22:09:32.466: INFO: (15) /api/v1/namespaces/proxy-1537/services/proxy-service-r8qpj:portname1/proxy/: foo (200; 25.435694ms)
Oct 29 22:09:32.466: INFO: (15) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k/proxy/rewriteme">test</a> (200; 25.408038ms)
Oct 29 22:09:32.466: INFO: (15) /api/v1/namespaces/proxy-1537/services/http:proxy-service-r8qpj:portname2/proxy/: bar (200; 25.731119ms)
Oct 29 22:09:32.466: INFO: (15) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:162/proxy/: bar (200; 25.68738ms)
Oct 29 22:09:32.466: INFO: (15) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:162/proxy/: bar (200; 25.821399ms)
Oct 29 22:09:32.466: INFO: (15) /api/v1/namespaces/proxy-1537/services/https:proxy-service-r8qpj:tlsportname1/proxy/: tls baz (200; 26.290282ms)
Oct 29 22:09:32.467: INFO: (15) /api/v1/namespaces/proxy-1537/services/http:proxy-service-r8qpj:portname1/proxy/: foo (200; 26.460788ms)
Oct 29 22:09:32.467: INFO: (15) /api/v1/namespaces/proxy-1537/services/proxy-service-r8qpj:portname2/proxy/: bar (200; 27.114205ms)
Oct 29 22:09:32.490: INFO: (16) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:443/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:443/proxy/tlsrewritem... (200; 22.227625ms)
Oct 29 22:09:32.490: INFO: (16) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:162/proxy/: bar (200; 22.184ms)
Oct 29 22:09:32.490: INFO: (16) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:460/proxy/: tls baz (200; 23.015843ms)
Oct 29 22:09:32.491: INFO: (16) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:162/proxy/: bar (200; 23.802355ms)
Oct 29 22:09:32.491: INFO: (16) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k/proxy/rewriteme">test</a> (200; 24.097821ms)
Oct 29 22:09:32.491: INFO: (16) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:462/proxy/: tls qux (200; 24.147822ms)
Oct 29 22:09:32.493: INFO: (16) /api/v1/namespaces/proxy-1537/services/https:proxy-service-r8qpj:tlsportname2/proxy/: tls qux (200; 25.140143ms)
Oct 29 22:09:32.493: INFO: (16) /api/v1/namespaces/proxy-1537/services/http:proxy-service-r8qpj:portname1/proxy/: foo (200; 25.25948ms)
Oct 29 22:09:32.493: INFO: (16) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:160/proxy/: foo (200; 25.468705ms)
Oct 29 22:09:32.493: INFO: (16) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:160/proxy/: foo (200; 25.690701ms)
Oct 29 22:09:32.494: INFO: (16) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:1080/proxy/rewriteme">... (200; 26.382188ms)
Oct 29 22:09:32.494: INFO: (16) /api/v1/namespaces/proxy-1537/services/http:proxy-service-r8qpj:portname2/proxy/: bar (200; 26.469343ms)
Oct 29 22:09:32.494: INFO: (16) /api/v1/namespaces/proxy-1537/services/proxy-service-r8qpj:portname2/proxy/: bar (200; 26.486916ms)
Oct 29 22:09:32.494: INFO: (16) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:1080/proxy/rewriteme">test<... (200; 26.737254ms)
Oct 29 22:09:32.494: INFO: (16) /api/v1/namespaces/proxy-1537/services/proxy-service-r8qpj:portname1/proxy/: foo (200; 26.994189ms)
Oct 29 22:09:32.494: INFO: (16) /api/v1/namespaces/proxy-1537/services/https:proxy-service-r8qpj:tlsportname1/proxy/: tls baz (200; 27.062139ms)
Oct 29 22:09:32.517: INFO: (17) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:462/proxy/: tls qux (200; 22.143633ms)
Oct 29 22:09:32.517: INFO: (17) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:162/proxy/: bar (200; 22.463396ms)
Oct 29 22:09:32.518: INFO: (17) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:1080/proxy/rewriteme">... (200; 23.195597ms)
Oct 29 22:09:32.519: INFO: (17) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:162/proxy/: bar (200; 23.221891ms)
Oct 29 22:09:32.519: INFO: (17) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k/proxy/rewriteme">test</a> (200; 23.344026ms)
Oct 29 22:09:32.519: INFO: (17) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:1080/proxy/rewriteme">test<... (200; 24.168059ms)
Oct 29 22:09:32.519: INFO: (17) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:160/proxy/: foo (200; 23.508216ms)
Oct 29 22:09:32.521: INFO: (17) /api/v1/namespaces/proxy-1537/services/proxy-service-r8qpj:portname1/proxy/: foo (200; 25.72851ms)
Oct 29 22:09:32.520: INFO: (17) /api/v1/namespaces/proxy-1537/services/proxy-service-r8qpj:portname2/proxy/: bar (200; 25.601855ms)
Oct 29 22:09:32.521: INFO: (17) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:460/proxy/: tls baz (200; 26.03782ms)
Oct 29 22:09:32.521: INFO: (17) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:160/proxy/: foo (200; 25.235798ms)
Oct 29 22:09:32.521: INFO: (17) /api/v1/namespaces/proxy-1537/services/http:proxy-service-r8qpj:portname2/proxy/: bar (200; 25.857585ms)
Oct 29 22:09:32.521: INFO: (17) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:443/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:443/proxy/tlsrewritem... (200; 25.780311ms)
Oct 29 22:09:32.521: INFO: (17) /api/v1/namespaces/proxy-1537/services/https:proxy-service-r8qpj:tlsportname1/proxy/: tls baz (200; 26.279337ms)
Oct 29 22:09:32.522: INFO: (17) /api/v1/namespaces/proxy-1537/services/https:proxy-service-r8qpj:tlsportname2/proxy/: tls qux (200; 26.563305ms)
Oct 29 22:09:32.522: INFO: (17) /api/v1/namespaces/proxy-1537/services/http:proxy-service-r8qpj:portname1/proxy/: foo (200; 26.558965ms)
Oct 29 22:09:32.545: INFO: (18) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:162/proxy/: bar (200; 22.725381ms)
Oct 29 22:09:32.545: INFO: (18) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:443/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:443/proxy/tlsrewritem... (200; 22.730843ms)
Oct 29 22:09:32.545: INFO: (18) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:462/proxy/: tls qux (200; 23.39069ms)
Oct 29 22:09:32.546: INFO: (18) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:1080/proxy/rewriteme">... (200; 23.524875ms)
Oct 29 22:09:32.546: INFO: (18) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:162/proxy/: bar (200; 24.095623ms)
Oct 29 22:09:32.546: INFO: (18) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:460/proxy/: tls baz (200; 24.320019ms)
Oct 29 22:09:32.547: INFO: (18) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:1080/proxy/rewriteme">test<... (200; 24.716165ms)
Oct 29 22:09:32.547: INFO: (18) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k/proxy/rewriteme">test</a> (200; 24.749513ms)
Oct 29 22:09:32.547: INFO: (18) /api/v1/namespaces/proxy-1537/services/https:proxy-service-r8qpj:tlsportname2/proxy/: tls qux (200; 25.221201ms)
Oct 29 22:09:32.548: INFO: (18) /api/v1/namespaces/proxy-1537/services/https:proxy-service-r8qpj:tlsportname1/proxy/: tls baz (200; 25.513755ms)
Oct 29 22:09:32.549: INFO: (18) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:160/proxy/: foo (200; 26.778363ms)
Oct 29 22:09:32.549: INFO: (18) /api/v1/namespaces/proxy-1537/services/proxy-service-r8qpj:portname1/proxy/: foo (200; 27.148453ms)
Oct 29 22:09:32.549: INFO: (18) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:160/proxy/: foo (200; 27.111605ms)
Oct 29 22:09:32.549: INFO: (18) /api/v1/namespaces/proxy-1537/services/proxy-service-r8qpj:portname2/proxy/: bar (200; 27.424565ms)
Oct 29 22:09:32.550: INFO: (18) /api/v1/namespaces/proxy-1537/services/http:proxy-service-r8qpj:portname1/proxy/: foo (200; 27.579759ms)
Oct 29 22:09:32.550: INFO: (18) /api/v1/namespaces/proxy-1537/services/http:proxy-service-r8qpj:portname2/proxy/: bar (200; 28.214453ms)
Oct 29 22:09:32.572: INFO: (19) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:443/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:443/proxy/tlsrewritem... (200; 21.908496ms)
Oct 29 22:09:32.572: INFO: (19) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:160/proxy/: foo (200; 21.739919ms)
Oct 29 22:09:32.573: INFO: (19) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k/proxy/rewriteme">test</a> (200; 21.90743ms)
Oct 29 22:09:32.574: INFO: (19) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:460/proxy/: tls baz (200; 22.853082ms)
Oct 29 22:09:32.574: INFO: (19) /api/v1/namespaces/proxy-1537/services/http:proxy-service-r8qpj:portname1/proxy/: foo (200; 23.570798ms)
Oct 29 22:09:32.574: INFO: (19) /api/v1/namespaces/proxy-1537/pods/https:proxy-service-r8qpj-7mg8k:462/proxy/: tls qux (200; 23.35019ms)
Oct 29 22:09:32.574: INFO: (19) /api/v1/namespaces/proxy-1537/services/https:proxy-service-r8qpj:tlsportname1/proxy/: tls baz (200; 23.308492ms)
Oct 29 22:09:32.576: INFO: (19) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:162/proxy/: bar (200; 24.581996ms)
Oct 29 22:09:32.576: INFO: (19) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:162/proxy/: bar (200; 25.009703ms)
Oct 29 22:09:32.576: INFO: (19) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:1080/proxy/rewriteme">... (200; 25.25946ms)
Oct 29 22:09:32.576: INFO: (19) /api/v1/namespaces/proxy-1537/pods/http:proxy-service-r8qpj-7mg8k:160/proxy/: foo (200; 25.851018ms)
Oct 29 22:09:32.576: INFO: (19) /api/v1/namespaces/proxy-1537/services/proxy-service-r8qpj:portname2/proxy/: bar (200; 25.492255ms)
Oct 29 22:09:32.577: INFO: (19) /api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1537/pods/proxy-service-r8qpj-7mg8k:1080/proxy/rewriteme">test<... (200; 25.851219ms)
Oct 29 22:09:32.577: INFO: (19) /api/v1/namespaces/proxy-1537/services/proxy-service-r8qpj:portname1/proxy/: foo (200; 25.731984ms)
Oct 29 22:09:32.577: INFO: (19) /api/v1/namespaces/proxy-1537/services/http:proxy-service-r8qpj:portname2/proxy/: bar (200; 25.887395ms)
Oct 29 22:09:32.577: INFO: (19) /api/v1/namespaces/proxy-1537/services/https:proxy-service-r8qpj:tlsportname2/proxy/: tls qux (200; 26.375017ms)
STEP: deleting ReplicationController proxy-service-r8qpj in namespace proxy-1537, will wait for the garbage collector to delete the pods
Oct 29 22:09:32.674: INFO: Deleting ReplicationController proxy-service-r8qpj took: 25.195229ms
Oct 29 22:09:32.775: INFO: Terminating ReplicationController proxy-service-r8qpj pods took: 100.139252ms
[AfterEach] version v1
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:09:34.975: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "proxy-1537" for this suite.


 [SLOW TEST:16.474 seconds]
[sig-network] Proxy
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":-1,"completed":64,"skipped":1103,"failed":0}
Oct 29 22:09:35.067: INFO: Running AfterSuite actions on all nodes


[BeforeEach] [k8s.io] Probing container
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 29 22:08:44.556: INFO: >>> kubeConfig: /tmp/jeder-461-cncf2.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-6e924ed6-17c4-4cf4-85b0-de08b876ae77 in namespace container-probe-3696
Oct 29 22:08:48.856: INFO: Started pod liveness-6e924ed6-17c4-4cf4-85b0-de08b876ae77 in namespace container-probe-3696
STEP: checking the pod's current state and verifying that restartCount is present
Oct 29 22:08:48.878: INFO: Initial restart count of pod liveness-6e924ed6-17c4-4cf4-85b0-de08b876ae77 is 0
Oct 29 22:09:09.145: INFO: Restart count of pod container-probe-3696/liveness-6e924ed6-17c4-4cf4-85b0-de08b876ae77 is now 1 (20.266441974s elapsed)
Oct 29 22:09:27.382: INFO: Restart count of pod container-probe-3696/liveness-6e924ed6-17c4-4cf4-85b0-de08b876ae77 is now 2 (38.503459054s elapsed)
Oct 29 22:09:47.607: INFO: Restart count of pod container-probe-3696/liveness-6e924ed6-17c4-4cf4-85b0-de08b876ae77 is now 3 (58.729118592s elapsed)
Oct 29 22:10:07.835: INFO: Restart count of pod container-probe-3696/liveness-6e924ed6-17c4-4cf4-85b0-de08b876ae77 is now 4 (1m18.957232194s elapsed)
Oct 29 22:11:16.609: INFO: Restart count of pod container-probe-3696/liveness-6e924ed6-17c4-4cf4-85b0-de08b876ae77 is now 5 (2m27.731156864s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 29 22:11:16.643: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-probe-3696" for this suite.


 [SLOW TEST:152.193 seconds]
[k8s.io] Probing container
/home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /home/jeder/osd_46_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":-1,"completed":78,"skipped":1399,"failed":0}
Oct 29 22:11:16.750: INFO: Running AfterSuite actions on all nodes


{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":-1,"completed":73,"skipped":1286,"failed":0}
Oct 29 22:09:19.768: INFO: Running AfterSuite actions on all nodes
Oct 29 22:11:16.795: INFO: Running AfterSuite actions on node 1
Oct 29 22:11:16.795: INFO: Dumping logs locally to: /home/jeder/osd_46_conformance/origin/_output/scripts/conformance-k8s/artifacts
Oct 29 22:11:16.795: INFO: Error running cluster/log-dump/log-dump.sh: fork/exec ../../cluster/log-dump/log-dump.sh: no such file or directory


Ran 286 of 5234 Specs in 1398.078 seconds
SUCCESS! -- 286 Passed | 0 Failed | 0 Pending | 4948 Skipped


Ginkgo ran 1 suite in 23m19.65229007s
Test Suite Passed
