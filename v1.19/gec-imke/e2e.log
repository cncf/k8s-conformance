I0426 13:33:50.152988      21 test_context.go:416] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-610780823
I0426 13:33:50.153015      21 test_context.go:429] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0426 13:33:50.153290      21 e2e.go:129] Starting e2e run "5015cbb9-27fc-4e0a-981d-c5508c00e05b" on Ginkgo node 1
{"msg":"Test Suite starting","total":303,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1619444028 - Will randomize all specs
Will run 303 of 5234 specs

E0426 13:33:50.222391      21 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp 127.0.0.1:8099: connect: connection refused
Apr 26 13:33:50.222: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
Apr 26 13:33:50.225: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Apr 26 13:33:50.269: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Apr 26 13:33:50.329: INFO: 22 / 22 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Apr 26 13:33:50.329: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Apr 26 13:33:50.329: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Apr 26 13:33:50.360: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'canal' (0 seconds elapsed)
Apr 26 13:33:50.360: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'csi-cinder-nodeplugin-centos' (0 seconds elapsed)
Apr 26 13:33:50.360: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'csi-cinder-nodeplugin-coreos' (0 seconds elapsed)
Apr 26 13:33:50.360: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'csi-cinder-nodeplugin-flatcar' (0 seconds elapsed)
Apr 26 13:33:50.360: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'csi-cinder-nodeplugin-ubuntu' (0 seconds elapsed)
Apr 26 13:33:50.360: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Apr 26 13:33:50.360: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'logrotate' (0 seconds elapsed)
Apr 26 13:33:50.360: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
Apr 26 13:33:50.360: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'user-ssh-keys-agent' (0 seconds elapsed)
Apr 26 13:33:50.360: INFO: e2e test version: v1.19.4
Apr 26 13:33:50.363: INFO: kube-apiserver version: v1.19.4
Apr 26 13:33:50.363: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
Apr 26 13:33:50.371: INFO: Cluster IP family: ipv4
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:33:50.371: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename webhook
Apr 26 13:33:50.443: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Apr 26 13:33:50.457: INFO: No PSP annotation exists on dry run pod; assuming PodSecurityPolicy is disabled
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 26 13:33:51.045: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 26 13:33:53.064: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755040831, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755040831, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755040831, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755040831, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 26 13:33:55.070: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755040831, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755040831, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755040831, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755040831, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 26 13:33:57.071: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755040831, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755040831, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755040831, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755040831, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 26 13:34:00.094: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Apr 26 13:34:00.101: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-29-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:34:01.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4183" for this suite.
STEP: Destroying namespace "webhook-4183-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:11.208 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":303,"completed":1,"skipped":9,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:34:01.579: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating api versions
Apr 26 13:34:01.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 api-versions'
Apr 26 13:34:01.793: INFO: stderr: ""
Apr 26 13:34:01.793: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncluster.k8s.io/v1alpha1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nsnapshot.storage.k8s.io/v1alpha1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:34:01.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7862" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":303,"completed":2,"skipped":15,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:34:01.815: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Apr 26 13:34:01.874: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Apr 26 13:34:15.934: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
Apr 26 13:34:19.593: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:34:34.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7358" for this suite.

• [SLOW TEST:32.285 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":303,"completed":3,"skipped":17,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:34:34.100: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Apr 26 13:34:34.919: INFO: starting watch
STEP: patching
STEP: updating
Apr 26 13:34:34.951: INFO: waiting for watch events with expected annotations
Apr 26 13:34:34.951: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:34:35.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-959" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":303,"completed":4,"skipped":28,"failed":0}
SSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:34:35.090: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Apr 26 13:36:35.195: INFO: Deleting pod "var-expansion-d4840ad6-67dc-4b0e-abdc-b995ae4ed0c4" in namespace "var-expansion-1479"
Apr 26 13:36:35.211: INFO: Wait up to 5m0s for pod "var-expansion-d4840ad6-67dc-4b0e-abdc-b995ae4ed0c4" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:36:37.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1479" for this suite.

• [SLOW TEST:122.158 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":303,"completed":5,"skipped":33,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:36:37.250: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-320249a7-bbc2-4031-b275-05dc604eb347
STEP: Creating a pod to test consume secrets
Apr 26 13:36:37.334: INFO: Waiting up to 5m0s for pod "pod-secrets-6cb6b5d4-0275-4ef3-b294-4e27c89fd14e" in namespace "secrets-4965" to be "Succeeded or Failed"
Apr 26 13:36:37.344: INFO: Pod "pod-secrets-6cb6b5d4-0275-4ef3-b294-4e27c89fd14e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.191187ms
Apr 26 13:36:39.351: INFO: Pod "pod-secrets-6cb6b5d4-0275-4ef3-b294-4e27c89fd14e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016724509s
Apr 26 13:36:41.357: INFO: Pod "pod-secrets-6cb6b5d4-0275-4ef3-b294-4e27c89fd14e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023530277s
STEP: Saw pod success
Apr 26 13:36:41.357: INFO: Pod "pod-secrets-6cb6b5d4-0275-4ef3-b294-4e27c89fd14e" satisfied condition "Succeeded or Failed"
Apr 26 13:36:41.365: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod pod-secrets-6cb6b5d4-0275-4ef3-b294-4e27c89fd14e container secret-volume-test: <nil>
STEP: delete the pod
Apr 26 13:36:41.407: INFO: Waiting for pod pod-secrets-6cb6b5d4-0275-4ef3-b294-4e27c89fd14e to disappear
Apr 26 13:36:41.412: INFO: Pod pod-secrets-6cb6b5d4-0275-4ef3-b294-4e27c89fd14e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:36:41.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4965" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":6,"skipped":36,"failed":0}
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:36:41.452: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-projected-8znm
STEP: Creating a pod to test atomic-volume-subpath
Apr 26 13:36:41.541: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-8znm" in namespace "subpath-7414" to be "Succeeded or Failed"
Apr 26 13:36:41.552: INFO: Pod "pod-subpath-test-projected-8znm": Phase="Pending", Reason="", readiness=false. Elapsed: 10.335374ms
Apr 26 13:36:43.563: INFO: Pod "pod-subpath-test-projected-8znm": Phase="Running", Reason="", readiness=true. Elapsed: 2.022164113s
Apr 26 13:36:45.570: INFO: Pod "pod-subpath-test-projected-8znm": Phase="Running", Reason="", readiness=true. Elapsed: 4.02919002s
Apr 26 13:36:47.583: INFO: Pod "pod-subpath-test-projected-8znm": Phase="Running", Reason="", readiness=true. Elapsed: 6.041592903s
Apr 26 13:36:49.589: INFO: Pod "pod-subpath-test-projected-8znm": Phase="Running", Reason="", readiness=true. Elapsed: 8.048267635s
Apr 26 13:36:51.599: INFO: Pod "pod-subpath-test-projected-8znm": Phase="Running", Reason="", readiness=true. Elapsed: 10.057809788s
Apr 26 13:36:53.606: INFO: Pod "pod-subpath-test-projected-8znm": Phase="Running", Reason="", readiness=true. Elapsed: 12.064678099s
Apr 26 13:36:55.613: INFO: Pod "pod-subpath-test-projected-8znm": Phase="Running", Reason="", readiness=true. Elapsed: 14.071428419s
Apr 26 13:36:57.621: INFO: Pod "pod-subpath-test-projected-8znm": Phase="Running", Reason="", readiness=true. Elapsed: 16.07960821s
Apr 26 13:36:59.628: INFO: Pod "pod-subpath-test-projected-8znm": Phase="Running", Reason="", readiness=true. Elapsed: 18.086365715s
Apr 26 13:37:01.636: INFO: Pod "pod-subpath-test-projected-8znm": Phase="Running", Reason="", readiness=true. Elapsed: 20.095154312s
Apr 26 13:37:03.644: INFO: Pod "pod-subpath-test-projected-8znm": Phase="Running", Reason="", readiness=true. Elapsed: 22.10306731s
Apr 26 13:37:05.653: INFO: Pod "pod-subpath-test-projected-8znm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.111477217s
STEP: Saw pod success
Apr 26 13:37:05.653: INFO: Pod "pod-subpath-test-projected-8znm" satisfied condition "Succeeded or Failed"
Apr 26 13:37:05.659: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod pod-subpath-test-projected-8znm container test-container-subpath-projected-8znm: <nil>
STEP: delete the pod
Apr 26 13:37:05.715: INFO: Waiting for pod pod-subpath-test-projected-8znm to disappear
Apr 26 13:37:05.721: INFO: Pod pod-subpath-test-projected-8znm no longer exists
STEP: Deleting pod pod-subpath-test-projected-8znm
Apr 26 13:37:05.721: INFO: Deleting pod "pod-subpath-test-projected-8znm" in namespace "subpath-7414"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:37:05.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7414" for this suite.

• [SLOW TEST:24.298 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":303,"completed":7,"skipped":40,"failed":0}
SS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:37:05.751: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-f28feed4-4abf-4274-9695-a2c3b7ab36c1 in namespace container-probe-3328
Apr 26 13:37:07.858: INFO: Started pod liveness-f28feed4-4abf-4274-9695-a2c3b7ab36c1 in namespace container-probe-3328
STEP: checking the pod's current state and verifying that restartCount is present
Apr 26 13:37:07.866: INFO: Initial restart count of pod liveness-f28feed4-4abf-4274-9695-a2c3b7ab36c1 is 0
Apr 26 13:37:25.940: INFO: Restart count of pod container-probe-3328/liveness-f28feed4-4abf-4274-9695-a2c3b7ab36c1 is now 1 (18.074073326s elapsed)
Apr 26 13:37:46.027: INFO: Restart count of pod container-probe-3328/liveness-f28feed4-4abf-4274-9695-a2c3b7ab36c1 is now 2 (38.160777723s elapsed)
Apr 26 13:38:06.120: INFO: Restart count of pod container-probe-3328/liveness-f28feed4-4abf-4274-9695-a2c3b7ab36c1 is now 3 (58.253959598s elapsed)
Apr 26 13:38:24.192: INFO: Restart count of pod container-probe-3328/liveness-f28feed4-4abf-4274-9695-a2c3b7ab36c1 is now 4 (1m16.325215498s elapsed)
Apr 26 13:39:26.523: INFO: Restart count of pod container-probe-3328/liveness-f28feed4-4abf-4274-9695-a2c3b7ab36c1 is now 5 (2m18.656559497s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:39:26.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3328" for this suite.

• [SLOW TEST:140.815 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":303,"completed":8,"skipped":42,"failed":0}
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:39:26.568: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Apr 26 13:39:26.630: INFO: namespace kubectl-5007
Apr 26 13:39:26.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 create -f - --namespace=kubectl-5007'
Apr 26 13:39:27.405: INFO: stderr: ""
Apr 26 13:39:27.405: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Apr 26 13:39:28.419: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 26 13:39:28.419: INFO: Found 0 / 1
Apr 26 13:39:29.414: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 26 13:39:29.414: INFO: Found 0 / 1
Apr 26 13:39:30.411: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 26 13:39:30.411: INFO: Found 1 / 1
Apr 26 13:39:30.411: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr 26 13:39:30.418: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 26 13:39:30.418: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 26 13:39:30.418: INFO: wait on agnhost-primary startup in kubectl-5007 
Apr 26 13:39:30.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 logs agnhost-primary-mptkz agnhost-primary --namespace=kubectl-5007'
Apr 26 13:39:30.683: INFO: stderr: ""
Apr 26 13:39:30.683: INFO: stdout: "Paused\n"
STEP: exposing RC
Apr 26 13:39:30.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-5007'
Apr 26 13:39:30.823: INFO: stderr: ""
Apr 26 13:39:30.823: INFO: stdout: "service/rm2 exposed\n"
Apr 26 13:39:30.829: INFO: Service rm2 in namespace kubectl-5007 found.
STEP: exposing service
Apr 26 13:39:32.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-5007'
Apr 26 13:39:32.965: INFO: stderr: ""
Apr 26 13:39:32.965: INFO: stdout: "service/rm3 exposed\n"
Apr 26 13:39:32.979: INFO: Service rm3 in namespace kubectl-5007 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:39:34.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5007" for this suite.

• [SLOW TEST:8.448 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1246
    should create services for rc  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":303,"completed":9,"skipped":42,"failed":0}
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:39:35.016: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-8984
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-8984
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8984
Apr 26 13:39:35.123: INFO: Found 0 stateful pods, waiting for 1
Apr 26 13:39:45.129: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Apr 26 13:39:55.130: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Apr 26 13:39:55.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8984 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 26 13:39:55.778: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 26 13:39:55.778: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 26 13:39:55.778: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 26 13:39:55.794: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Apr 26 13:40:05.807: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 26 13:40:05.807: INFO: Waiting for statefulset status.replicas updated to 0
Apr 26 13:40:05.858: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999735s
Apr 26 13:40:06.866: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.977784302s
Apr 26 13:40:07.872: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.970623755s
Apr 26 13:40:08.885: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.964329546s
Apr 26 13:40:09.892: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.951343814s
Apr 26 13:40:10.906: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.943606875s
Apr 26 13:40:11.916: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.928413783s
Apr 26 13:40:12.923: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.91965341s
Apr 26 13:40:13.930: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.912914985s
Apr 26 13:40:14.937: INFO: Verifying statefulset ss doesn't scale past 1 for another 905.737239ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8984
Apr 26 13:40:15.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8984 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:40:16.580: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 26 13:40:16.580: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 26 13:40:16.580: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 26 13:40:16.587: INFO: Found 1 stateful pods, waiting for 3
Apr 26 13:40:26.595: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 26 13:40:26.595: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 26 13:40:26.595: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Apr 26 13:40:36.599: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 26 13:40:36.599: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 26 13:40:36.599: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Apr 26 13:40:46.599: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 26 13:40:46.599: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 26 13:40:46.599: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Apr 26 13:40:46.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8984 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 26 13:40:47.268: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 26 13:40:47.268: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 26 13:40:47.268: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 26 13:40:47.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8984 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 26 13:40:47.915: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 26 13:40:47.915: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 26 13:40:47.915: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 26 13:40:47.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8984 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 26 13:40:48.586: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 26 13:40:48.586: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 26 13:40:48.586: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 26 13:40:48.586: INFO: Waiting for statefulset status.replicas updated to 0
Apr 26 13:40:48.593: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Apr 26 13:40:58.606: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 26 13:40:58.606: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Apr 26 13:40:58.606: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Apr 26 13:40:58.633: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999741s
Apr 26 13:40:59.639: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.989980036s
Apr 26 13:41:00.648: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.982729507s
Apr 26 13:41:01.655: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.975344066s
Apr 26 13:41:02.663: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.967997198s
Apr 26 13:41:03.670: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.960130768s
Apr 26 13:41:04.678: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.952962071s
Apr 26 13:41:05.688: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.945286746s
Apr 26 13:41:06.708: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.934940202s
Apr 26 13:41:07.715: INFO: Verifying statefulset ss doesn't scale past 3 for another 915.041511ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8984
Apr 26 13:41:08.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8984 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:41:09.340: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 26 13:41:09.341: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 26 13:41:09.341: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 26 13:41:09.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8984 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:41:09.981: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 26 13:41:09.981: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 26 13:41:09.981: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 26 13:41:09.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8984 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:41:10.619: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 26 13:41:10.619: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 26 13:41:10.619: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 26 13:41:10.619: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Apr 26 13:41:30.659: INFO: Deleting all statefulset in ns statefulset-8984
Apr 26 13:41:30.664: INFO: Scaling statefulset ss to 0
Apr 26 13:41:30.684: INFO: Waiting for statefulset status.replicas updated to 0
Apr 26 13:41:30.694: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:41:30.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8984" for this suite.

• [SLOW TEST:115.733 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":303,"completed":10,"skipped":44,"failed":0}
SSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:41:30.753: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:41:30.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8285" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":303,"completed":11,"skipped":50,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:41:30.907: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5319 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5319;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5319 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5319;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5319.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5319.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5319.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5319.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5319.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5319.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5319.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5319.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5319.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5319.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5319.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5319.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5319.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 41.26.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.26.41_udp@PTR;check="$$(dig +tcp +noall +answer +search 41.26.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.26.41_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5319 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5319;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5319 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5319;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5319.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5319.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5319.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5319.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5319.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5319.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5319.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5319.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5319.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5319.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5319.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5319.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5319.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 41.26.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.26.41_udp@PTR;check="$$(dig +tcp +noall +answer +search 41.26.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.26.41_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 26 13:41:43.274: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5319/dns-test-99f68d75-6ed0-4731-8d9a-f592337b8763: the server could not find the requested resource (get pods dns-test-99f68d75-6ed0-4731-8d9a-f592337b8763)
Apr 26 13:41:43.318: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5319/dns-test-99f68d75-6ed0-4731-8d9a-f592337b8763: the server could not find the requested resource (get pods dns-test-99f68d75-6ed0-4731-8d9a-f592337b8763)
Apr 26 13:41:43.329: INFO: Unable to read wheezy_udp@dns-test-service.dns-5319 from pod dns-5319/dns-test-99f68d75-6ed0-4731-8d9a-f592337b8763: the server could not find the requested resource (get pods dns-test-99f68d75-6ed0-4731-8d9a-f592337b8763)
Apr 26 13:41:43.338: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5319 from pod dns-5319/dns-test-99f68d75-6ed0-4731-8d9a-f592337b8763: the server could not find the requested resource (get pods dns-test-99f68d75-6ed0-4731-8d9a-f592337b8763)
Apr 26 13:41:43.347: INFO: Unable to read wheezy_udp@dns-test-service.dns-5319.svc from pod dns-5319/dns-test-99f68d75-6ed0-4731-8d9a-f592337b8763: the server could not find the requested resource (get pods dns-test-99f68d75-6ed0-4731-8d9a-f592337b8763)
Apr 26 13:41:43.356: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5319.svc from pod dns-5319/dns-test-99f68d75-6ed0-4731-8d9a-f592337b8763: the server could not find the requested resource (get pods dns-test-99f68d75-6ed0-4731-8d9a-f592337b8763)
Apr 26 13:41:43.366: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5319.svc from pod dns-5319/dns-test-99f68d75-6ed0-4731-8d9a-f592337b8763: the server could not find the requested resource (get pods dns-test-99f68d75-6ed0-4731-8d9a-f592337b8763)
Apr 26 13:41:43.376: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5319.svc from pod dns-5319/dns-test-99f68d75-6ed0-4731-8d9a-f592337b8763: the server could not find the requested resource (get pods dns-test-99f68d75-6ed0-4731-8d9a-f592337b8763)
Apr 26 13:41:43.913: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5319/dns-test-99f68d75-6ed0-4731-8d9a-f592337b8763: the server could not find the requested resource (get pods dns-test-99f68d75-6ed0-4731-8d9a-f592337b8763)
Apr 26 13:41:43.921: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5319/dns-test-99f68d75-6ed0-4731-8d9a-f592337b8763: the server could not find the requested resource (get pods dns-test-99f68d75-6ed0-4731-8d9a-f592337b8763)
Apr 26 13:41:43.932: INFO: Unable to read jessie_udp@dns-test-service.dns-5319 from pod dns-5319/dns-test-99f68d75-6ed0-4731-8d9a-f592337b8763: the server could not find the requested resource (get pods dns-test-99f68d75-6ed0-4731-8d9a-f592337b8763)
Apr 26 13:41:43.944: INFO: Unable to read jessie_tcp@dns-test-service.dns-5319 from pod dns-5319/dns-test-99f68d75-6ed0-4731-8d9a-f592337b8763: the server could not find the requested resource (get pods dns-test-99f68d75-6ed0-4731-8d9a-f592337b8763)
Apr 26 13:41:43.954: INFO: Unable to read jessie_udp@dns-test-service.dns-5319.svc from pod dns-5319/dns-test-99f68d75-6ed0-4731-8d9a-f592337b8763: the server could not find the requested resource (get pods dns-test-99f68d75-6ed0-4731-8d9a-f592337b8763)
Apr 26 13:41:43.971: INFO: Unable to read jessie_tcp@dns-test-service.dns-5319.svc from pod dns-5319/dns-test-99f68d75-6ed0-4731-8d9a-f592337b8763: the server could not find the requested resource (get pods dns-test-99f68d75-6ed0-4731-8d9a-f592337b8763)
Apr 26 13:41:43.979: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5319.svc from pod dns-5319/dns-test-99f68d75-6ed0-4731-8d9a-f592337b8763: the server could not find the requested resource (get pods dns-test-99f68d75-6ed0-4731-8d9a-f592337b8763)
Apr 26 13:41:43.988: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5319.svc from pod dns-5319/dns-test-99f68d75-6ed0-4731-8d9a-f592337b8763: the server could not find the requested resource (get pods dns-test-99f68d75-6ed0-4731-8d9a-f592337b8763)
Apr 26 13:41:44.482: INFO: Lookups using dns-5319/dns-test-99f68d75-6ed0-4731-8d9a-f592337b8763 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5319 wheezy_tcp@dns-test-service.dns-5319 wheezy_udp@dns-test-service.dns-5319.svc wheezy_tcp@dns-test-service.dns-5319.svc wheezy_udp@_http._tcp.dns-test-service.dns-5319.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5319.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5319 jessie_tcp@dns-test-service.dns-5319 jessie_udp@dns-test-service.dns-5319.svc jessie_tcp@dns-test-service.dns-5319.svc jessie_udp@_http._tcp.dns-test-service.dns-5319.svc jessie_tcp@_http._tcp.dns-test-service.dns-5319.svc]

Apr 26 13:41:51.813: INFO: DNS probes using dns-5319/dns-test-99f68d75-6ed0-4731-8d9a-f592337b8763 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:41:51.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5319" for this suite.

• [SLOW TEST:21.050 seconds]
[sig-network] DNS
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":303,"completed":12,"skipped":67,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:41:51.957: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 26 13:41:52.555: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 26 13:41:54.579: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755041312, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755041312, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755041312, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755041312, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 26 13:41:57.617: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:41:57.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8247" for this suite.
STEP: Destroying namespace "webhook-8247-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.080 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":303,"completed":13,"skipped":101,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:41:58.038: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Apr 26 13:41:58.132: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:42:02.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6218" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":303,"completed":14,"skipped":122,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:42:02.401: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:42:06.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-856" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":303,"completed":15,"skipped":143,"failed":0}
SS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:42:06.636: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service nodeport-service with the type=NodePort in namespace services-5365
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-5365
STEP: creating replication controller externalsvc in namespace services-5365
I0426 13:42:06.827265      21 runners.go:190] Created replication controller with name: externalsvc, namespace: services-5365, replica count: 2
I0426 13:42:09.877668      21 runners.go:190] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0426 13:42:12.877987      21 runners.go:190] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0426 13:42:15.878213      21 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Apr 26 13:42:15.940: INFO: Creating new exec pod
Apr 26 13:42:19.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-5365 execpodlvgxf -- /bin/sh -x -c nslookup nodeport-service.services-5365.svc.cluster.local'
Apr 26 13:42:20.600: INFO: stderr: "+ nslookup nodeport-service.services-5365.svc.cluster.local\n"
Apr 26 13:42:20.600: INFO: stdout: "Server:\t\t10.240.16.10\nAddress:\t10.240.16.10#53\n\nnodeport-service.services-5365.svc.cluster.local\tcanonical name = externalsvc.services-5365.svc.cluster.local.\nName:\texternalsvc.services-5365.svc.cluster.local\nAddress: 10.240.31.141\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-5365, will wait for the garbage collector to delete the pods
Apr 26 13:42:20.670: INFO: Deleting ReplicationController externalsvc took: 11.761958ms
Apr 26 13:42:21.271: INFO: Terminating ReplicationController externalsvc pods took: 600.45066ms
Apr 26 13:42:35.816: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:42:35.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5365" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:29.230 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":303,"completed":16,"skipped":145,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:42:35.867: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 26 13:42:36.390: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 26 13:42:38.409: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755041356, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755041356, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755041356, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755041356, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 26 13:42:41.445: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Apr 26 13:42:41.558: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:42:41.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1237" for this suite.
STEP: Destroying namespace "webhook-1237-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.899 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":303,"completed":17,"skipped":175,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:42:41.766: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Apr 26 13:42:48.035: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 26 13:42:48.050: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 26 13:42:50.050: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 26 13:42:50.060: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 26 13:42:52.050: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 26 13:42:52.056: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 26 13:42:54.050: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 26 13:42:54.058: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:42:54.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8229" for this suite.

• [SLOW TEST:12.369 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":303,"completed":18,"skipped":196,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:42:54.136: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Apr 26 13:42:54.221: INFO: Waiting up to 5m0s for pod "downwardapi-volume-de566540-27db-4386-96d9-f7b56c579f64" in namespace "downward-api-4537" to be "Succeeded or Failed"
Apr 26 13:42:54.231: INFO: Pod "downwardapi-volume-de566540-27db-4386-96d9-f7b56c579f64": Phase="Pending", Reason="", readiness=false. Elapsed: 9.45869ms
Apr 26 13:42:56.243: INFO: Pod "downwardapi-volume-de566540-27db-4386-96d9-f7b56c579f64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021095788s
Apr 26 13:42:58.252: INFO: Pod "downwardapi-volume-de566540-27db-4386-96d9-f7b56c579f64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030629643s
STEP: Saw pod success
Apr 26 13:42:58.252: INFO: Pod "downwardapi-volume-de566540-27db-4386-96d9-f7b56c579f64" satisfied condition "Succeeded or Failed"
Apr 26 13:42:58.259: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-w8lg4 pod downwardapi-volume-de566540-27db-4386-96d9-f7b56c579f64 container client-container: <nil>
STEP: delete the pod
Apr 26 13:42:58.305: INFO: Waiting for pod downwardapi-volume-de566540-27db-4386-96d9-f7b56c579f64 to disappear
Apr 26 13:42:58.313: INFO: Pod downwardapi-volume-de566540-27db-4386-96d9-f7b56c579f64 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:42:58.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4537" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":303,"completed":19,"skipped":197,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:42:58.342: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override arguments
Apr 26 13:42:58.421: INFO: Waiting up to 5m0s for pod "client-containers-62dc3e21-4207-4eba-800a-44b17919a96d" in namespace "containers-2667" to be "Succeeded or Failed"
Apr 26 13:42:58.432: INFO: Pod "client-containers-62dc3e21-4207-4eba-800a-44b17919a96d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.14813ms
Apr 26 13:43:00.441: INFO: Pod "client-containers-62dc3e21-4207-4eba-800a-44b17919a96d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020036s
Apr 26 13:43:02.452: INFO: Pod "client-containers-62dc3e21-4207-4eba-800a-44b17919a96d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030929535s
STEP: Saw pod success
Apr 26 13:43:02.452: INFO: Pod "client-containers-62dc3e21-4207-4eba-800a-44b17919a96d" satisfied condition "Succeeded or Failed"
Apr 26 13:43:02.459: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-w8lg4 pod client-containers-62dc3e21-4207-4eba-800a-44b17919a96d container test-container: <nil>
STEP: delete the pod
Apr 26 13:43:02.541: INFO: Waiting for pod client-containers-62dc3e21-4207-4eba-800a-44b17919a96d to disappear
Apr 26 13:43:02.548: INFO: Pod client-containers-62dc3e21-4207-4eba-800a-44b17919a96d no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:43:02.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2667" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":303,"completed":20,"skipped":216,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:43:02.577: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-2782
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 26 13:43:02.642: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr 26 13:43:02.736: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr 26 13:43:04.746: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr 26 13:43:06.743: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 26 13:43:08.744: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 26 13:43:10.743: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 26 13:43:12.746: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 26 13:43:14.744: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 26 13:43:16.743: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 26 13:43:18.744: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 26 13:43:20.749: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 26 13:43:22.743: INFO: The status of Pod netserver-0 is Running (Ready = true)
Apr 26 13:43:22.754: INFO: The status of Pod netserver-1 is Running (Ready = true)
Apr 26 13:43:22.767: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Apr 26 13:43:26.836: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.1.21 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2782 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 26 13:43:26.837: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
Apr 26 13:43:28.366: INFO: Found all expected endpoints: [netserver-0]
Apr 26 13:43:28.374: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.2.6 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2782 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 26 13:43:28.375: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
Apr 26 13:43:29.948: INFO: Found all expected endpoints: [netserver-1]
Apr 26 13:43:29.954: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.0.14 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2782 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 26 13:43:29.954: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
Apr 26 13:43:31.517: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:43:31.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2782" for this suite.

• [SLOW TEST:28.963 seconds]
[sig-network] Networking
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":21,"skipped":239,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:43:31.541: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-47hm
STEP: Creating a pod to test atomic-volume-subpath
Apr 26 13:43:31.662: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-47hm" in namespace "subpath-6967" to be "Succeeded or Failed"
Apr 26 13:43:31.671: INFO: Pod "pod-subpath-test-configmap-47hm": Phase="Pending", Reason="", readiness=false. Elapsed: 8.937996ms
Apr 26 13:43:33.678: INFO: Pod "pod-subpath-test-configmap-47hm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015741365s
Apr 26 13:43:35.686: INFO: Pod "pod-subpath-test-configmap-47hm": Phase="Running", Reason="", readiness=true. Elapsed: 4.023562235s
Apr 26 13:43:37.696: INFO: Pod "pod-subpath-test-configmap-47hm": Phase="Running", Reason="", readiness=true. Elapsed: 6.034041061s
Apr 26 13:43:39.703: INFO: Pod "pod-subpath-test-configmap-47hm": Phase="Running", Reason="", readiness=true. Elapsed: 8.041102525s
Apr 26 13:43:41.711: INFO: Pod "pod-subpath-test-configmap-47hm": Phase="Running", Reason="", readiness=true. Elapsed: 10.048512477s
Apr 26 13:43:43.717: INFO: Pod "pod-subpath-test-configmap-47hm": Phase="Running", Reason="", readiness=true. Elapsed: 12.054420137s
Apr 26 13:43:45.723: INFO: Pod "pod-subpath-test-configmap-47hm": Phase="Running", Reason="", readiness=true. Elapsed: 14.061192992s
Apr 26 13:43:47.730: INFO: Pod "pod-subpath-test-configmap-47hm": Phase="Running", Reason="", readiness=true. Elapsed: 16.068180221s
Apr 26 13:43:49.743: INFO: Pod "pod-subpath-test-configmap-47hm": Phase="Running", Reason="", readiness=true. Elapsed: 18.080519059s
Apr 26 13:43:51.749: INFO: Pod "pod-subpath-test-configmap-47hm": Phase="Running", Reason="", readiness=true. Elapsed: 20.086579584s
Apr 26 13:43:53.758: INFO: Pod "pod-subpath-test-configmap-47hm": Phase="Running", Reason="", readiness=true. Elapsed: 22.095754899s
Apr 26 13:43:55.765: INFO: Pod "pod-subpath-test-configmap-47hm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.103083373s
STEP: Saw pod success
Apr 26 13:43:55.765: INFO: Pod "pod-subpath-test-configmap-47hm" satisfied condition "Succeeded or Failed"
Apr 26 13:43:55.772: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod pod-subpath-test-configmap-47hm container test-container-subpath-configmap-47hm: <nil>
STEP: delete the pod
Apr 26 13:43:55.829: INFO: Waiting for pod pod-subpath-test-configmap-47hm to disappear
Apr 26 13:43:55.841: INFO: Pod pod-subpath-test-configmap-47hm no longer exists
STEP: Deleting pod pod-subpath-test-configmap-47hm
Apr 26 13:43:55.841: INFO: Deleting pod "pod-subpath-test-configmap-47hm" in namespace "subpath-6967"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:43:55.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6967" for this suite.

• [SLOW TEST:24.333 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":303,"completed":22,"skipped":260,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:43:55.881: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0426 13:44:06.013339      21 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0426 13:44:06.013664      21 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0426 13:44:06.013809      21 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Apr 26 13:44:06.013: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:44:06.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3109" for this suite.

• [SLOW TEST:10.151 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":303,"completed":23,"skipped":277,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:44:06.037: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Apr 26 13:44:06.113: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Apr 26 13:44:06.154: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr 26 13:44:10.187: INFO: Creating deployment "test-rolling-update-deployment"
Apr 26 13:44:10.198: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Apr 26 13:44:10.218: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Apr 26 13:44:12.231: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Apr 26 13:44:12.238: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755041450, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755041450, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755041450, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755041450, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-c4cb8d6d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 26 13:44:14.245: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Apr 26 13:44:14.264: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-8609 /apis/apps/v1/namespaces/deployment-8609/deployments/test-rolling-update-deployment 32b6d445-dad6-4195-896a-dd869549a6b6 10039 1 2021-04-26 13:44:10 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-04-26 13:44:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-04-26 13:44:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002855df8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-04-26 13:44:10 +0000 UTC,LastTransitionTime:2021-04-26 13:44:10 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" has successfully progressed.,LastUpdateTime:2021-04-26 13:44:12 +0000 UTC,LastTransitionTime:2021-04-26 13:44:10 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Apr 26 13:44:14.270: INFO: New ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9  deployment-8609 /apis/apps/v1/namespaces/deployment-8609/replicasets/test-rolling-update-deployment-c4cb8d6d9 a79030c0-8f15-4732-ae58-01510bb6938a 10025 1 2021-04-26 13:44:10 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 32b6d445-dad6-4195-896a-dd869549a6b6 0xc0037024e0 0xc0037024e1}] []  [{kube-controller-manager Update apps/v1 2021-04-26 13:44:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"32b6d445-dad6-4195-896a-dd869549a6b6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: c4cb8d6d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003702558 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr 26 13:44:14.270: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Apr 26 13:44:14.270: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-8609 /apis/apps/v1/namespaces/deployment-8609/replicasets/test-rolling-update-controller 5891e0f5-3050-432e-8fba-fcbaa55e7669 10038 2 2021-04-26 13:44:06 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 32b6d445-dad6-4195-896a-dd869549a6b6 0xc0037023d7 0xc0037023d8}] []  [{e2e.test Update apps/v1 2021-04-26 13:44:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-04-26 13:44:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"32b6d445-dad6-4195-896a-dd869549a6b6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003702478 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 26 13:44:14.277: INFO: Pod "test-rolling-update-deployment-c4cb8d6d9-xrj6k" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9-xrj6k test-rolling-update-deployment-c4cb8d6d9- deployment-8609 /api/v1/namespaces/deployment-8609/pods/test-rolling-update-deployment-c4cb8d6d9-xrj6k 02304212-52d1-46f8-9096-1841907016ff 10024 0 2021-04-26 13:44:10 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[cni.projectcalico.org/podIP:172.25.0.16/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-c4cb8d6d9 a79030c0-8f15-4732-ae58-01510bb6938a 0xc003702a50 0xc003702a51}] []  [{kube-controller-manager Update v1 2021-04-26 13:44:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a79030c0-8f15-4732-ae58-01510bb6938a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-04-26 13:44:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2021-04-26 13:44:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.0.16\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hlhh2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hlhh2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hlhh2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:elastic-hugle-5f9d9b5555-w8lg4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 13:44:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 13:44:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 13:44:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 13:44:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.9,PodIP:172.25.0.16,StartTime:2021-04-26 13:44:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-26 13:44:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:docker://598e8b2ffbf1cb4dbbb3197695c17aab77019152ca65df5717dfb4701b491398,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.16,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:44:14.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8609" for this suite.

• [SLOW TEST:8.265 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":303,"completed":24,"skipped":284,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:44:14.302: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
Apr 26 13:44:14.921: INFO: created pod pod-service-account-defaultsa
Apr 26 13:44:14.921: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Apr 26 13:44:14.936: INFO: created pod pod-service-account-mountsa
Apr 26 13:44:14.936: INFO: pod pod-service-account-mountsa service account token volume mount: true
Apr 26 13:44:14.953: INFO: created pod pod-service-account-nomountsa
Apr 26 13:44:14.953: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Apr 26 13:44:14.964: INFO: created pod pod-service-account-defaultsa-mountspec
Apr 26 13:44:14.964: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Apr 26 13:44:14.982: INFO: created pod pod-service-account-mountsa-mountspec
Apr 26 13:44:14.982: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Apr 26 13:44:15.001: INFO: created pod pod-service-account-nomountsa-mountspec
Apr 26 13:44:15.001: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Apr 26 13:44:15.011: INFO: created pod pod-service-account-defaultsa-nomountspec
Apr 26 13:44:15.011: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Apr 26 13:44:15.020: INFO: created pod pod-service-account-mountsa-nomountspec
Apr 26 13:44:15.020: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Apr 26 13:44:15.032: INFO: created pod pod-service-account-nomountsa-nomountspec
Apr 26 13:44:15.033: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:44:15.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9533" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":303,"completed":25,"skipped":329,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:44:15.064: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-52361643-8f8e-4a5e-a18b-5f860ffca8f3
STEP: Creating a pod to test consume configMaps
Apr 26 13:44:15.162: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f3a31322-a04b-4a90-88a0-bdca7665fb7a" in namespace "projected-5371" to be "Succeeded or Failed"
Apr 26 13:44:15.171: INFO: Pod "pod-projected-configmaps-f3a31322-a04b-4a90-88a0-bdca7665fb7a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.331145ms
Apr 26 13:44:17.178: INFO: Pod "pod-projected-configmaps-f3a31322-a04b-4a90-88a0-bdca7665fb7a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015763638s
Apr 26 13:44:19.187: INFO: Pod "pod-projected-configmaps-f3a31322-a04b-4a90-88a0-bdca7665fb7a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024520304s
Apr 26 13:44:21.201: INFO: Pod "pod-projected-configmaps-f3a31322-a04b-4a90-88a0-bdca7665fb7a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038252634s
STEP: Saw pod success
Apr 26 13:44:21.201: INFO: Pod "pod-projected-configmaps-f3a31322-a04b-4a90-88a0-bdca7665fb7a" satisfied condition "Succeeded or Failed"
Apr 26 13:44:21.208: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-w8lg4 pod pod-projected-configmaps-f3a31322-a04b-4a90-88a0-bdca7665fb7a container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 26 13:44:21.246: INFO: Waiting for pod pod-projected-configmaps-f3a31322-a04b-4a90-88a0-bdca7665fb7a to disappear
Apr 26 13:44:21.253: INFO: Pod pod-projected-configmaps-f3a31322-a04b-4a90-88a0-bdca7665fb7a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:44:21.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5371" for this suite.

• [SLOW TEST:6.214 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":26,"skipped":337,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:44:21.279: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:44:32.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5631" for this suite.

• [SLOW TEST:11.184 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":303,"completed":27,"skipped":339,"failed":0}
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:44:32.463: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name secret-emptykey-test-404bcead-fc27-4abe-b472-a06c6aab6af7
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:44:32.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9645" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":303,"completed":28,"skipped":339,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:44:32.556: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-secret-fd4v
STEP: Creating a pod to test atomic-volume-subpath
Apr 26 13:44:32.669: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-fd4v" in namespace "subpath-9034" to be "Succeeded or Failed"
Apr 26 13:44:32.681: INFO: Pod "pod-subpath-test-secret-fd4v": Phase="Pending", Reason="", readiness=false. Elapsed: 12.026908ms
Apr 26 13:44:34.687: INFO: Pod "pod-subpath-test-secret-fd4v": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018042753s
Apr 26 13:44:36.696: INFO: Pod "pod-subpath-test-secret-fd4v": Phase="Running", Reason="", readiness=true. Elapsed: 4.026993853s
Apr 26 13:44:38.703: INFO: Pod "pod-subpath-test-secret-fd4v": Phase="Running", Reason="", readiness=true. Elapsed: 6.033459682s
Apr 26 13:44:40.710: INFO: Pod "pod-subpath-test-secret-fd4v": Phase="Running", Reason="", readiness=true. Elapsed: 8.04029833s
Apr 26 13:44:42.716: INFO: Pod "pod-subpath-test-secret-fd4v": Phase="Running", Reason="", readiness=true. Elapsed: 10.046948694s
Apr 26 13:44:44.723: INFO: Pod "pod-subpath-test-secret-fd4v": Phase="Running", Reason="", readiness=true. Elapsed: 12.05329809s
Apr 26 13:44:46.735: INFO: Pod "pod-subpath-test-secret-fd4v": Phase="Running", Reason="", readiness=true. Elapsed: 14.065172247s
Apr 26 13:44:48.744: INFO: Pod "pod-subpath-test-secret-fd4v": Phase="Running", Reason="", readiness=true. Elapsed: 16.074793417s
Apr 26 13:44:50.751: INFO: Pod "pod-subpath-test-secret-fd4v": Phase="Running", Reason="", readiness=true. Elapsed: 18.081454182s
Apr 26 13:44:52.761: INFO: Pod "pod-subpath-test-secret-fd4v": Phase="Running", Reason="", readiness=true. Elapsed: 20.091968641s
Apr 26 13:44:54.768: INFO: Pod "pod-subpath-test-secret-fd4v": Phase="Running", Reason="", readiness=true. Elapsed: 22.098424112s
Apr 26 13:44:56.775: INFO: Pod "pod-subpath-test-secret-fd4v": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.105738708s
STEP: Saw pod success
Apr 26 13:44:56.775: INFO: Pod "pod-subpath-test-secret-fd4v" satisfied condition "Succeeded or Failed"
Apr 26 13:44:56.786: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-w8lg4 pod pod-subpath-test-secret-fd4v container test-container-subpath-secret-fd4v: <nil>
STEP: delete the pod
Apr 26 13:44:56.833: INFO: Waiting for pod pod-subpath-test-secret-fd4v to disappear
Apr 26 13:44:56.838: INFO: Pod pod-subpath-test-secret-fd4v no longer exists
STEP: Deleting pod pod-subpath-test-secret-fd4v
Apr 26 13:44:56.838: INFO: Deleting pod "pod-subpath-test-secret-fd4v" in namespace "subpath-9034"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:44:56.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9034" for this suite.

• [SLOW TEST:24.313 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":303,"completed":29,"skipped":357,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:44:56.870: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-35744acd-3524-4e51-b719-574ad8947d6f
STEP: Creating a pod to test consume configMaps
Apr 26 13:44:56.982: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-725c252c-5e4b-4225-bb6e-46dc70f03836" in namespace "projected-8863" to be "Succeeded or Failed"
Apr 26 13:44:56.993: INFO: Pod "pod-projected-configmaps-725c252c-5e4b-4225-bb6e-46dc70f03836": Phase="Pending", Reason="", readiness=false. Elapsed: 11.127173ms
Apr 26 13:44:59.002: INFO: Pod "pod-projected-configmaps-725c252c-5e4b-4225-bb6e-46dc70f03836": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020396023s
Apr 26 13:45:01.008: INFO: Pod "pod-projected-configmaps-725c252c-5e4b-4225-bb6e-46dc70f03836": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02638176s
STEP: Saw pod success
Apr 26 13:45:01.008: INFO: Pod "pod-projected-configmaps-725c252c-5e4b-4225-bb6e-46dc70f03836" satisfied condition "Succeeded or Failed"
Apr 26 13:45:01.014: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod pod-projected-configmaps-725c252c-5e4b-4225-bb6e-46dc70f03836 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 26 13:45:01.099: INFO: Waiting for pod pod-projected-configmaps-725c252c-5e4b-4225-bb6e-46dc70f03836 to disappear
Apr 26 13:45:01.104: INFO: Pod pod-projected-configmaps-725c252c-5e4b-4225-bb6e-46dc70f03836 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:45:01.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8863" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":30,"skipped":358,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:45:01.125: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on tmpfs
Apr 26 13:45:01.203: INFO: Waiting up to 5m0s for pod "pod-f99baa8d-b21e-4c28-8b3f-193ef1696a16" in namespace "emptydir-3019" to be "Succeeded or Failed"
Apr 26 13:45:01.210: INFO: Pod "pod-f99baa8d-b21e-4c28-8b3f-193ef1696a16": Phase="Pending", Reason="", readiness=false. Elapsed: 7.147981ms
Apr 26 13:45:03.219: INFO: Pod "pod-f99baa8d-b21e-4c28-8b3f-193ef1696a16": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016110831s
Apr 26 13:45:05.227: INFO: Pod "pod-f99baa8d-b21e-4c28-8b3f-193ef1696a16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024104603s
STEP: Saw pod success
Apr 26 13:45:05.227: INFO: Pod "pod-f99baa8d-b21e-4c28-8b3f-193ef1696a16" satisfied condition "Succeeded or Failed"
Apr 26 13:45:05.233: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod pod-f99baa8d-b21e-4c28-8b3f-193ef1696a16 container test-container: <nil>
STEP: delete the pod
Apr 26 13:45:05.271: INFO: Waiting for pod pod-f99baa8d-b21e-4c28-8b3f-193ef1696a16 to disappear
Apr 26 13:45:05.281: INFO: Pod pod-f99baa8d-b21e-4c28-8b3f-193ef1696a16 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:45:05.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3019" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":31,"skipped":364,"failed":0}

------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:45:05.303: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-3429b3a9-9729-4299-8c23-a2c87ec2f133
STEP: Creating a pod to test consume secrets
Apr 26 13:45:05.411: INFO: Waiting up to 5m0s for pod "pod-secrets-acf72718-b2ae-4834-8266-f276f0da0cf3" in namespace "secrets-8223" to be "Succeeded or Failed"
Apr 26 13:45:05.418: INFO: Pod "pod-secrets-acf72718-b2ae-4834-8266-f276f0da0cf3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.070174ms
Apr 26 13:45:07.433: INFO: Pod "pod-secrets-acf72718-b2ae-4834-8266-f276f0da0cf3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021449922s
STEP: Saw pod success
Apr 26 13:45:07.433: INFO: Pod "pod-secrets-acf72718-b2ae-4834-8266-f276f0da0cf3" satisfied condition "Succeeded or Failed"
Apr 26 13:45:07.442: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod pod-secrets-acf72718-b2ae-4834-8266-f276f0da0cf3 container secret-volume-test: <nil>
STEP: delete the pod
Apr 26 13:45:07.482: INFO: Waiting for pod pod-secrets-acf72718-b2ae-4834-8266-f276f0da0cf3 to disappear
Apr 26 13:45:07.490: INFO: Pod pod-secrets-acf72718-b2ae-4834-8266-f276f0da0cf3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:45:07.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8223" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":32,"skipped":364,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:45:07.523: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-ae42b595-b74d-4c69-8768-5188f226c056
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-ae42b595-b74d-4c69-8768-5188f226c056
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:46:38.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9094" for this suite.

• [SLOW TEST:91.125 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":303,"completed":33,"skipped":377,"failed":0}
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:46:38.649: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Apr 26 13:46:38.709: INFO: Creating deployment "test-recreate-deployment"
Apr 26 13:46:38.722: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Apr 26 13:46:38.741: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Apr 26 13:46:40.762: INFO: Waiting deployment "test-recreate-deployment" to complete
Apr 26 13:46:40.767: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755041598, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755041598, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755041598, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755041598, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 26 13:46:42.775: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Apr 26 13:46:42.794: INFO: Updating deployment test-recreate-deployment
Apr 26 13:46:42.794: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Apr 26 13:46:42.914: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-5889 /apis/apps/v1/namespaces/deployment-5889/deployments/test-recreate-deployment b58ca27a-59a2-4704-8560-a14c3f1418f0 11142 2 2021-04-26 13:46:38 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-04-26 13:46:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-04-26 13:46:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00301c9a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-04-26 13:46:42 +0000 UTC,LastTransitionTime:2021-04-26 13:46:42 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f79dd4667" is progressing.,LastUpdateTime:2021-04-26 13:46:42 +0000 UTC,LastTransitionTime:2021-04-26 13:46:38 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Apr 26 13:46:42.921: INFO: New ReplicaSet "test-recreate-deployment-f79dd4667" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f79dd4667  deployment-5889 /apis/apps/v1/namespaces/deployment-5889/replicasets/test-recreate-deployment-f79dd4667 72d66f45-84c6-47e4-b975-3334fb7d1657 11140 1 2021-04-26 13:46:42 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment b58ca27a-59a2-4704-8560-a14c3f1418f0 0xc00301ceb0 0xc00301ceb1}] []  [{kube-controller-manager Update apps/v1 2021-04-26 13:46:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b58ca27a-59a2-4704-8560-a14c3f1418f0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f79dd4667,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00301cf28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 26 13:46:42.921: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Apr 26 13:46:42.921: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-c96cf48f  deployment-5889 /apis/apps/v1/namespaces/deployment-5889/replicasets/test-recreate-deployment-c96cf48f b2ddf3b9-04f2-4ac8-8654-6dd00d39763c 11131 2 2021-04-26 13:46:38 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment b58ca27a-59a2-4704-8560-a14c3f1418f0 0xc00301cdbf 0xc00301cdd0}] []  [{kube-controller-manager Update apps/v1 2021-04-26 13:46:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b58ca27a-59a2-4704-8560-a14c3f1418f0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c96cf48f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00301ce48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 26 13:46:42.926: INFO: Pod "test-recreate-deployment-f79dd4667-sb8sx" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f79dd4667-sb8sx test-recreate-deployment-f79dd4667- deployment-5889 /api/v1/namespaces/deployment-5889/pods/test-recreate-deployment-f79dd4667-sb8sx 4075cdb8-1fe6-439a-bbb5-1a56669cb983 11144 0 2021-04-26 13:46:42 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [{apps/v1 ReplicaSet test-recreate-deployment-f79dd4667 72d66f45-84c6-47e4-b975-3334fb7d1657 0xc00301d3f0 0xc00301d3f1}] []  [{kube-controller-manager Update v1 2021-04-26 13:46:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72d66f45-84c6-47e4-b975-3334fb7d1657\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-26 13:46:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7nksr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7nksr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7nksr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:elastic-hugle-5f9d9b5555-8444w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 13:46:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 13:46:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 13:46:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 13:46:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.11,PodIP:,StartTime:2021-04-26 13:46:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:46:42.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5889" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":303,"completed":34,"skipped":377,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:46:42.950: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test env composition
Apr 26 13:46:43.020: INFO: Waiting up to 5m0s for pod "var-expansion-ec189b4f-cfa7-4fad-80e6-9efdba5f7a90" in namespace "var-expansion-513" to be "Succeeded or Failed"
Apr 26 13:46:43.027: INFO: Pod "var-expansion-ec189b4f-cfa7-4fad-80e6-9efdba5f7a90": Phase="Pending", Reason="", readiness=false. Elapsed: 7.132058ms
Apr 26 13:46:45.037: INFO: Pod "var-expansion-ec189b4f-cfa7-4fad-80e6-9efdba5f7a90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01699072s
Apr 26 13:46:47.044: INFO: Pod "var-expansion-ec189b4f-cfa7-4fad-80e6-9efdba5f7a90": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023312139s
Apr 26 13:46:49.051: INFO: Pod "var-expansion-ec189b4f-cfa7-4fad-80e6-9efdba5f7a90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030657357s
STEP: Saw pod success
Apr 26 13:46:49.051: INFO: Pod "var-expansion-ec189b4f-cfa7-4fad-80e6-9efdba5f7a90" satisfied condition "Succeeded or Failed"
Apr 26 13:46:49.056: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-w8lg4 pod var-expansion-ec189b4f-cfa7-4fad-80e6-9efdba5f7a90 container dapi-container: <nil>
STEP: delete the pod
Apr 26 13:46:49.116: INFO: Waiting for pod var-expansion-ec189b4f-cfa7-4fad-80e6-9efdba5f7a90 to disappear
Apr 26 13:46:49.121: INFO: Pod var-expansion-ec189b4f-cfa7-4fad-80e6-9efdba5f7a90 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:46:49.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-513" for this suite.

• [SLOW TEST:6.191 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":303,"completed":35,"skipped":399,"failed":0}
S
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:46:49.148: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-4193
STEP: creating service affinity-nodeport-transition in namespace services-4193
STEP: creating replication controller affinity-nodeport-transition in namespace services-4193
I0426 13:46:49.272349      21 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-4193, replica count: 3
I0426 13:46:52.323873      21 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 26 13:46:52.372: INFO: Creating new exec pod
Apr 26 13:46:57.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-4193 execpod-affinitysslxn -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
Apr 26 13:46:58.074: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Apr 26 13:46:58.074: INFO: stdout: ""
Apr 26 13:46:58.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-4193 execpod-affinitysslxn -- /bin/sh -x -c nc -zv -t -w 2 10.240.28.200 80'
Apr 26 13:46:58.687: INFO: stderr: "+ nc -zv -t -w 2 10.240.28.200 80\nConnection to 10.240.28.200 80 port [tcp/http] succeeded!\n"
Apr 26 13:46:58.687: INFO: stdout: ""
Apr 26 13:46:58.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-4193 execpod-affinitysslxn -- /bin/sh -x -c nc -zv -t -w 2 192.168.1.7 32634'
Apr 26 13:46:59.302: INFO: stderr: "+ nc -zv -t -w 2 192.168.1.7 32634\nConnection to 192.168.1.7 32634 port [tcp/32634] succeeded!\n"
Apr 26 13:46:59.303: INFO: stdout: ""
Apr 26 13:46:59.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-4193 execpod-affinitysslxn -- /bin/sh -x -c nc -zv -t -w 2 192.168.1.11 32634'
Apr 26 13:46:59.952: INFO: stderr: "+ nc -zv -t -w 2 192.168.1.11 32634\nConnection to 192.168.1.11 32634 port [tcp/32634] succeeded!\n"
Apr 26 13:46:59.952: INFO: stdout: ""
Apr 26 13:46:59.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-4193 execpod-affinitysslxn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.1.11:32634/ ; done'
Apr 26 13:47:00.701: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32634/\n"
Apr 26 13:47:00.701: INFO: stdout: "\naffinity-nodeport-transition-qvmhh\naffinity-nodeport-transition-rwwtb\naffinity-nodeport-transition-phjjn\naffinity-nodeport-transition-qvmhh\naffinity-nodeport-transition-rwwtb\naffinity-nodeport-transition-phjjn\naffinity-nodeport-transition-qvmhh\naffinity-nodeport-transition-rwwtb\naffinity-nodeport-transition-phjjn\naffinity-nodeport-transition-qvmhh\naffinity-nodeport-transition-rwwtb\naffinity-nodeport-transition-phjjn\naffinity-nodeport-transition-qvmhh\naffinity-nodeport-transition-rwwtb\naffinity-nodeport-transition-phjjn\naffinity-nodeport-transition-qvmhh"
Apr 26 13:47:00.701: INFO: Received response from host: affinity-nodeport-transition-qvmhh
Apr 26 13:47:00.701: INFO: Received response from host: affinity-nodeport-transition-rwwtb
Apr 26 13:47:00.701: INFO: Received response from host: affinity-nodeport-transition-phjjn
Apr 26 13:47:00.701: INFO: Received response from host: affinity-nodeport-transition-qvmhh
Apr 26 13:47:00.701: INFO: Received response from host: affinity-nodeport-transition-rwwtb
Apr 26 13:47:00.701: INFO: Received response from host: affinity-nodeport-transition-phjjn
Apr 26 13:47:00.701: INFO: Received response from host: affinity-nodeport-transition-qvmhh
Apr 26 13:47:00.701: INFO: Received response from host: affinity-nodeport-transition-rwwtb
Apr 26 13:47:00.701: INFO: Received response from host: affinity-nodeport-transition-phjjn
Apr 26 13:47:00.701: INFO: Received response from host: affinity-nodeport-transition-qvmhh
Apr 26 13:47:00.701: INFO: Received response from host: affinity-nodeport-transition-rwwtb
Apr 26 13:47:00.701: INFO: Received response from host: affinity-nodeport-transition-phjjn
Apr 26 13:47:00.701: INFO: Received response from host: affinity-nodeport-transition-qvmhh
Apr 26 13:47:00.701: INFO: Received response from host: affinity-nodeport-transition-rwwtb
Apr 26 13:47:00.701: INFO: Received response from host: affinity-nodeport-transition-phjjn
Apr 26 13:47:00.701: INFO: Received response from host: affinity-nodeport-transition-qvmhh
Apr 26 13:47:00.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-4193 execpod-affinitysslxn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.1.11:32634/ ; done'
Apr 26 13:47:01.464: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32634/\n"
Apr 26 13:47:01.464: INFO: stdout: "\naffinity-nodeport-transition-phjjn\naffinity-nodeport-transition-phjjn\naffinity-nodeport-transition-phjjn\naffinity-nodeport-transition-phjjn\naffinity-nodeport-transition-phjjn\naffinity-nodeport-transition-phjjn\naffinity-nodeport-transition-phjjn\naffinity-nodeport-transition-phjjn\naffinity-nodeport-transition-phjjn\naffinity-nodeport-transition-phjjn\naffinity-nodeport-transition-phjjn\naffinity-nodeport-transition-phjjn\naffinity-nodeport-transition-phjjn\naffinity-nodeport-transition-phjjn\naffinity-nodeport-transition-phjjn\naffinity-nodeport-transition-phjjn"
Apr 26 13:47:01.464: INFO: Received response from host: affinity-nodeport-transition-phjjn
Apr 26 13:47:01.464: INFO: Received response from host: affinity-nodeport-transition-phjjn
Apr 26 13:47:01.464: INFO: Received response from host: affinity-nodeport-transition-phjjn
Apr 26 13:47:01.464: INFO: Received response from host: affinity-nodeport-transition-phjjn
Apr 26 13:47:01.464: INFO: Received response from host: affinity-nodeport-transition-phjjn
Apr 26 13:47:01.464: INFO: Received response from host: affinity-nodeport-transition-phjjn
Apr 26 13:47:01.464: INFO: Received response from host: affinity-nodeport-transition-phjjn
Apr 26 13:47:01.464: INFO: Received response from host: affinity-nodeport-transition-phjjn
Apr 26 13:47:01.465: INFO: Received response from host: affinity-nodeport-transition-phjjn
Apr 26 13:47:01.465: INFO: Received response from host: affinity-nodeport-transition-phjjn
Apr 26 13:47:01.465: INFO: Received response from host: affinity-nodeport-transition-phjjn
Apr 26 13:47:01.465: INFO: Received response from host: affinity-nodeport-transition-phjjn
Apr 26 13:47:01.465: INFO: Received response from host: affinity-nodeport-transition-phjjn
Apr 26 13:47:01.465: INFO: Received response from host: affinity-nodeport-transition-phjjn
Apr 26 13:47:01.465: INFO: Received response from host: affinity-nodeport-transition-phjjn
Apr 26 13:47:01.465: INFO: Received response from host: affinity-nodeport-transition-phjjn
Apr 26 13:47:01.465: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-4193, will wait for the garbage collector to delete the pods
Apr 26 13:47:01.563: INFO: Deleting ReplicationController affinity-nodeport-transition took: 12.578169ms
Apr 26 13:47:01.664: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.752484ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:47:15.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4193" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:26.719 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":303,"completed":36,"skipped":400,"failed":0}
SS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:47:15.868: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:171
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating server pod server in namespace prestop-9213
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-9213
STEP: Deleting pre-stop pod
Apr 26 13:47:29.114: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:47:29.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-9213" for this suite.

• [SLOW TEST:13.287 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":303,"completed":37,"skipped":402,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:47:29.160: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Apr 26 13:47:33.838: INFO: Successfully updated pod "annotationupdateaab2a3e7-b365-46ad-9759-99dbbda579b2"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:47:35.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8776" for this suite.

• [SLOW TEST:6.759 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":303,"completed":38,"skipped":460,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:47:35.920: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Apr 26 13:49:36.014: INFO: Deleting pod "var-expansion-9fb72ebc-7f02-42c0-9dfd-a805ce7994e7" in namespace "var-expansion-6535"
Apr 26 13:49:36.028: INFO: Wait up to 5m0s for pod "var-expansion-9fb72ebc-7f02-42c0-9dfd-a805ce7994e7" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:49:40.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6535" for this suite.

• [SLOW TEST:124.150 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":303,"completed":39,"skipped":475,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:49:40.073: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl logs
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1415
STEP: creating an pod
Apr 26 13:49:40.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.20 --namespace=kubectl-9731 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Apr 26 13:49:40.623: INFO: stderr: ""
Apr 26 13:49:40.623: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Waiting for log generator to start.
Apr 26 13:49:40.623: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Apr 26 13:49:40.630: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-9731" to be "running and ready, or succeeded"
Apr 26 13:49:40.642: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 12.532336ms
Apr 26 13:49:42.650: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020158115s
Apr 26 13:49:44.661: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.030636376s
Apr 26 13:49:44.661: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Apr 26 13:49:44.661: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Apr 26 13:49:44.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 logs logs-generator logs-generator --namespace=kubectl-9731'
Apr 26 13:49:44.809: INFO: stderr: ""
Apr 26 13:49:44.809: INFO: stdout: "I0426 13:49:42.071744       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/bpmf 243\nI0426 13:49:42.271837       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/nrq 221\nI0426 13:49:42.471866       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/wqjw 589\nI0426 13:49:42.671851       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/wb57 300\nI0426 13:49:42.871844       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/h72 294\nI0426 13:49:43.071870       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/dxd 572\nI0426 13:49:43.271842       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/4jkm 515\nI0426 13:49:43.471845       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/q557 460\nI0426 13:49:43.671856       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/ztm 473\nI0426 13:49:43.871847       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/68t 498\nI0426 13:49:44.071864       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/82dr 566\nI0426 13:49:44.271910       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/nwg 449\nI0426 13:49:44.471849       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/vpjz 375\nI0426 13:49:44.672189       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/f5t 277\n"
STEP: limiting log lines
Apr 26 13:49:44.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 logs logs-generator logs-generator --namespace=kubectl-9731 --tail=1'
Apr 26 13:49:44.944: INFO: stderr: ""
Apr 26 13:49:44.944: INFO: stdout: "I0426 13:49:44.871884       1 logs_generator.go:76] 14 GET /api/v1/namespaces/ns/pods/9rn4 328\n"
Apr 26 13:49:44.945: INFO: got output "I0426 13:49:44.871884       1 logs_generator.go:76] 14 GET /api/v1/namespaces/ns/pods/9rn4 328\n"
STEP: limiting log bytes
Apr 26 13:49:44.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 logs logs-generator logs-generator --namespace=kubectl-9731 --limit-bytes=1'
Apr 26 13:49:45.215: INFO: stderr: ""
Apr 26 13:49:45.215: INFO: stdout: "I"
Apr 26 13:49:45.215: INFO: got output "I"
STEP: exposing timestamps
Apr 26 13:49:45.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 logs logs-generator logs-generator --namespace=kubectl-9731 --tail=1 --timestamps'
Apr 26 13:49:45.357: INFO: stderr: ""
Apr 26 13:49:45.357: INFO: stdout: "2021-04-26T13:49:45.272068643Z I0426 13:49:45.271914       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/9wn 286\n"
Apr 26 13:49:45.357: INFO: got output "2021-04-26T13:49:45.272068643Z I0426 13:49:45.271914       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/9wn 286\n"
STEP: restricting to a time range
Apr 26 13:49:47.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 logs logs-generator logs-generator --namespace=kubectl-9731 --since=1s'
Apr 26 13:49:47.970: INFO: stderr: ""
Apr 26 13:49:47.970: INFO: stdout: "I0426 13:49:47.071872       1 logs_generator.go:76] 25 POST /api/v1/namespaces/kube-system/pods/vww8 245\nI0426 13:49:47.271837       1 logs_generator.go:76] 26 GET /api/v1/namespaces/kube-system/pods/cpll 357\nI0426 13:49:47.471918       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/69h5 499\nI0426 13:49:47.671984       1 logs_generator.go:76] 28 GET /api/v1/namespaces/kube-system/pods/zdg 353\nI0426 13:49:47.871882       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/kube-system/pods/xmvn 504\n"
Apr 26 13:49:47.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 logs logs-generator logs-generator --namespace=kubectl-9731 --since=24h'
Apr 26 13:49:48.088: INFO: stderr: ""
Apr 26 13:49:48.088: INFO: stdout: "I0426 13:49:42.071744       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/bpmf 243\nI0426 13:49:42.271837       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/nrq 221\nI0426 13:49:42.471866       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/wqjw 589\nI0426 13:49:42.671851       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/wb57 300\nI0426 13:49:42.871844       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/h72 294\nI0426 13:49:43.071870       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/dxd 572\nI0426 13:49:43.271842       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/4jkm 515\nI0426 13:49:43.471845       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/q557 460\nI0426 13:49:43.671856       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/ztm 473\nI0426 13:49:43.871847       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/68t 498\nI0426 13:49:44.071864       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/82dr 566\nI0426 13:49:44.271910       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/nwg 449\nI0426 13:49:44.471849       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/vpjz 375\nI0426 13:49:44.672189       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/f5t 277\nI0426 13:49:44.871884       1 logs_generator.go:76] 14 GET /api/v1/namespaces/ns/pods/9rn4 328\nI0426 13:49:45.071903       1 logs_generator.go:76] 15 GET /api/v1/namespaces/kube-system/pods/tgj2 501\nI0426 13:49:45.271914       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/9wn 286\nI0426 13:49:45.471855       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/qrt 501\nI0426 13:49:45.672238       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/9867 575\nI0426 13:49:45.871853       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/bv75 328\nI0426 13:49:46.071993       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/q6bd 438\nI0426 13:49:46.271860       1 logs_generator.go:76] 21 GET /api/v1/namespaces/ns/pods/s7bd 444\nI0426 13:49:46.472101       1 logs_generator.go:76] 22 GET /api/v1/namespaces/kube-system/pods/4bd 390\nI0426 13:49:46.671896       1 logs_generator.go:76] 23 GET /api/v1/namespaces/ns/pods/4mb 405\nI0426 13:49:46.871853       1 logs_generator.go:76] 24 GET /api/v1/namespaces/default/pods/6lk2 213\nI0426 13:49:47.071872       1 logs_generator.go:76] 25 POST /api/v1/namespaces/kube-system/pods/vww8 245\nI0426 13:49:47.271837       1 logs_generator.go:76] 26 GET /api/v1/namespaces/kube-system/pods/cpll 357\nI0426 13:49:47.471918       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/69h5 499\nI0426 13:49:47.671984       1 logs_generator.go:76] 28 GET /api/v1/namespaces/kube-system/pods/zdg 353\nI0426 13:49:47.871882       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/kube-system/pods/xmvn 504\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
Apr 26 13:49:48.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 delete pod logs-generator --namespace=kubectl-9731'
Apr 26 13:49:55.747: INFO: stderr: ""
Apr 26 13:49:55.747: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:49:55.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9731" for this suite.

• [SLOW TEST:15.694 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1411
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":303,"completed":40,"skipped":519,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:49:55.768: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should delete a collection of pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pods
Apr 26 13:49:55.846: INFO: created test-pod-1
Apr 26 13:49:55.861: INFO: created test-pod-2
Apr 26 13:49:55.872: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:49:56.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4270" for this suite.
•{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":303,"completed":41,"skipped":529,"failed":0}
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:49:56.044: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-8114
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating stateful set ss in namespace statefulset-8114
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8114
Apr 26 13:49:56.141: INFO: Found 0 stateful pods, waiting for 1
Apr 26 13:50:06.148: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Apr 26 13:50:06.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 26 13:50:06.793: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 26 13:50:06.793: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 26 13:50:06.793: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 26 13:50:06.799: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Apr 26 13:50:16.807: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 26 13:50:16.807: INFO: Waiting for statefulset status.replicas updated to 0
Apr 26 13:50:16.841: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Apr 26 13:50:16.841: INFO: ss-0  elastic-hugle-5f9d9b5555-8444w  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:49:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:49:56 +0000 UTC  }]
Apr 26 13:50:16.841: INFO: 
Apr 26 13:50:16.841: INFO: StatefulSet ss has not reached scale 3, at 1
Apr 26 13:50:17.849: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993210056s
Apr 26 13:50:18.857: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.985215092s
Apr 26 13:50:19.865: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.977285511s
Apr 26 13:50:20.872: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.969803929s
Apr 26 13:50:21.879: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.96227028s
Apr 26 13:50:22.887: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.955371895s
Apr 26 13:50:23.894: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.947038161s
Apr 26 13:50:24.902: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.940391089s
Apr 26 13:50:25.910: INFO: Verifying statefulset ss doesn't scale past 3 for another 932.184543ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8114
Apr 26 13:50:26.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:50:27.549: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 26 13:50:27.549: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 26 13:50:27.549: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 26 13:50:27.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:50:28.195: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Apr 26 13:50:28.195: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 26 13:50:28.195: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 26 13:50:28.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:50:28.909: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Apr 26 13:50:28.909: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 26 13:50:28.909: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 26 13:50:28.915: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 26 13:50:28.915: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 26 13:50:28.915: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Apr 26 13:50:28.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 26 13:50:29.360: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 26 13:50:29.360: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 26 13:50:29.360: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 26 13:50:29.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 26 13:50:30.014: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 26 13:50:30.014: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 26 13:50:30.014: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 26 13:50:30.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 26 13:50:30.705: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 26 13:50:30.705: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 26 13:50:30.705: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 26 13:50:30.705: INFO: Waiting for statefulset status.replicas updated to 0
Apr 26 13:50:30.714: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Apr 26 13:50:40.731: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 26 13:50:40.731: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Apr 26 13:50:40.731: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Apr 26 13:50:40.774: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Apr 26 13:50:40.774: INFO: ss-0  elastic-hugle-5f9d9b5555-8444w  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:49:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:49:56 +0000 UTC  }]
Apr 26 13:50:40.774: INFO: ss-1  elastic-hugle-5f9d9b5555-w8lg4  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:16 +0000 UTC  }]
Apr 26 13:50:40.774: INFO: ss-2  elastic-hugle-5f9d9b5555-mj2j9  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:16 +0000 UTC  }]
Apr 26 13:50:40.774: INFO: 
Apr 26 13:50:40.774: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 26 13:50:41.783: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Apr 26 13:50:41.783: INFO: ss-0  elastic-hugle-5f9d9b5555-8444w  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:49:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:49:56 +0000 UTC  }]
Apr 26 13:50:41.783: INFO: ss-1  elastic-hugle-5f9d9b5555-w8lg4  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:16 +0000 UTC  }]
Apr 26 13:50:41.783: INFO: ss-2  elastic-hugle-5f9d9b5555-mj2j9  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:16 +0000 UTC  }]
Apr 26 13:50:41.783: INFO: 
Apr 26 13:50:41.783: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 26 13:50:42.790: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Apr 26 13:50:42.790: INFO: ss-0  elastic-hugle-5f9d9b5555-8444w  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:49:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:49:56 +0000 UTC  }]
Apr 26 13:50:42.790: INFO: ss-1  elastic-hugle-5f9d9b5555-w8lg4  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:16 +0000 UTC  }]
Apr 26 13:50:42.790: INFO: ss-2  elastic-hugle-5f9d9b5555-mj2j9  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:16 +0000 UTC  }]
Apr 26 13:50:42.790: INFO: 
Apr 26 13:50:42.790: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 26 13:50:43.797: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Apr 26 13:50:43.797: INFO: ss-0  elastic-hugle-5f9d9b5555-8444w  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:49:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:49:56 +0000 UTC  }]
Apr 26 13:50:43.797: INFO: ss-1  elastic-hugle-5f9d9b5555-w8lg4  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:16 +0000 UTC  }]
Apr 26 13:50:43.797: INFO: ss-2  elastic-hugle-5f9d9b5555-mj2j9  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:16 +0000 UTC  }]
Apr 26 13:50:43.797: INFO: 
Apr 26 13:50:43.797: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 26 13:50:44.808: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Apr 26 13:50:44.808: INFO: ss-0  elastic-hugle-5f9d9b5555-8444w  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:49:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:49:56 +0000 UTC  }]
Apr 26 13:50:44.808: INFO: ss-1  elastic-hugle-5f9d9b5555-w8lg4  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:16 +0000 UTC  }]
Apr 26 13:50:44.809: INFO: ss-2  elastic-hugle-5f9d9b5555-mj2j9  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:16 +0000 UTC  }]
Apr 26 13:50:44.809: INFO: 
Apr 26 13:50:44.809: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 26 13:50:45.816: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Apr 26 13:50:45.816: INFO: ss-1  elastic-hugle-5f9d9b5555-w8lg4  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:16 +0000 UTC  }]
Apr 26 13:50:45.816: INFO: ss-2  elastic-hugle-5f9d9b5555-mj2j9  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:16 +0000 UTC  }]
Apr 26 13:50:45.816: INFO: 
Apr 26 13:50:45.816: INFO: StatefulSet ss has not reached scale 0, at 2
Apr 26 13:50:46.823: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Apr 26 13:50:46.823: INFO: ss-1  elastic-hugle-5f9d9b5555-w8lg4  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:16 +0000 UTC  }]
Apr 26 13:50:46.823: INFO: ss-2  elastic-hugle-5f9d9b5555-mj2j9  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:16 +0000 UTC  }]
Apr 26 13:50:46.823: INFO: 
Apr 26 13:50:46.823: INFO: StatefulSet ss has not reached scale 0, at 2
Apr 26 13:50:47.830: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Apr 26 13:50:47.830: INFO: ss-1  elastic-hugle-5f9d9b5555-w8lg4  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:16 +0000 UTC  }]
Apr 26 13:50:47.830: INFO: 
Apr 26 13:50:47.830: INFO: StatefulSet ss has not reached scale 0, at 1
Apr 26 13:50:48.838: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Apr 26 13:50:48.838: INFO: ss-1  elastic-hugle-5f9d9b5555-w8lg4  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:16 +0000 UTC  }]
Apr 26 13:50:48.838: INFO: 
Apr 26 13:50:48.838: INFO: StatefulSet ss has not reached scale 0, at 1
Apr 26 13:50:49.846: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Apr 26 13:50:49.846: INFO: ss-1  elastic-hugle-5f9d9b5555-w8lg4  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-26 13:50:16 +0000 UTC  }]
Apr 26 13:50:49.846: INFO: 
Apr 26 13:50:49.846: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8114
Apr 26 13:50:50.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:50:51.120: INFO: rc: 1
Apr 26 13:50:51.120: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Apr 26 13:51:01.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:51:01.219: INFO: rc: 1
Apr 26 13:51:01.219: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Apr 26 13:51:11.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:51:11.327: INFO: rc: 1
Apr 26 13:51:11.327: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Apr 26 13:51:21.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:51:21.426: INFO: rc: 1
Apr 26 13:51:21.426: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Apr 26 13:51:31.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:51:31.553: INFO: rc: 1
Apr 26 13:51:31.553: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Apr 26 13:51:41.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:51:41.667: INFO: rc: 1
Apr 26 13:51:41.667: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Apr 26 13:51:51.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:51:51.764: INFO: rc: 1
Apr 26 13:51:51.764: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Apr 26 13:52:01.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:52:01.860: INFO: rc: 1
Apr 26 13:52:01.860: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Apr 26 13:52:11.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:52:11.971: INFO: rc: 1
Apr 26 13:52:11.971: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Apr 26 13:52:21.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:52:22.069: INFO: rc: 1
Apr 26 13:52:22.069: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Apr 26 13:52:32.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:52:32.169: INFO: rc: 1
Apr 26 13:52:32.169: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Apr 26 13:52:42.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:52:42.274: INFO: rc: 1
Apr 26 13:52:42.274: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Apr 26 13:52:52.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:52:52.388: INFO: rc: 1
Apr 26 13:52:52.388: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Apr 26 13:53:02.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:53:02.498: INFO: rc: 1
Apr 26 13:53:02.498: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Apr 26 13:53:12.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:53:12.600: INFO: rc: 1
Apr 26 13:53:12.600: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Apr 26 13:53:22.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:53:22.697: INFO: rc: 1
Apr 26 13:53:22.697: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Apr 26 13:53:32.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:53:32.799: INFO: rc: 1
Apr 26 13:53:32.799: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Apr 26 13:53:42.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:53:42.890: INFO: rc: 1
Apr 26 13:53:42.890: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Apr 26 13:53:52.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:53:52.995: INFO: rc: 1
Apr 26 13:53:52.996: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Apr 26 13:54:02.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:54:03.088: INFO: rc: 1
Apr 26 13:54:03.088: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Apr 26 13:54:13.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:54:13.197: INFO: rc: 1
Apr 26 13:54:13.197: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Apr 26 13:54:23.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:54:23.296: INFO: rc: 1
Apr 26 13:54:23.296: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Apr 26 13:54:33.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:54:33.392: INFO: rc: 1
Apr 26 13:54:33.392: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Apr 26 13:54:43.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:54:43.504: INFO: rc: 1
Apr 26 13:54:43.504: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Apr 26 13:54:53.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:54:53.600: INFO: rc: 1
Apr 26 13:54:53.600: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Apr 26 13:55:03.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:55:03.709: INFO: rc: 1
Apr 26 13:55:03.709: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Apr 26 13:55:13.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:55:13.817: INFO: rc: 1
Apr 26 13:55:13.817: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Apr 26 13:55:23.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:55:23.921: INFO: rc: 1
Apr 26 13:55:23.921: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Apr 26 13:55:33.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:55:34.027: INFO: rc: 1
Apr 26 13:55:34.027: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Apr 26 13:55:44.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:55:44.126: INFO: rc: 1
Apr 26 13:55:44.126: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Apr 26 13:55:54.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-8114 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 13:55:54.236: INFO: rc: 1
Apr 26 13:55:54.236: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: 
Apr 26 13:55:54.236: INFO: Scaling statefulset ss to 0
Apr 26 13:55:54.271: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Apr 26 13:55:54.278: INFO: Deleting all statefulset in ns statefulset-8114
Apr 26 13:55:54.284: INFO: Scaling statefulset ss to 0
Apr 26 13:55:54.301: INFO: Waiting for statefulset status.replicas updated to 0
Apr 26 13:55:54.306: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:55:54.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8114" for this suite.

• [SLOW TEST:358.315 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":303,"completed":42,"skipped":532,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:55:54.359: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Apr 26 13:55:54.446: INFO: Created pod &Pod{ObjectMeta:{dns-6886  dns-6886 /api/v1/namespaces/dns-6886/pods/dns-6886 611c7293-b70f-48a7-9489-fae93a68eecd 14216 0 2021-04-26 13:55:54 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2021-04-26 13:55:54 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6x4b7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6x4b7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6x4b7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 13:55:54.460: INFO: The status of Pod dns-6886 is Pending, waiting for it to be Running (with Ready = true)
Apr 26 13:55:56.468: INFO: The status of Pod dns-6886 is Pending, waiting for it to be Running (with Ready = true)
Apr 26 13:55:58.474: INFO: The status of Pod dns-6886 is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Apr 26 13:55:58.474: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-6886 PodName:dns-6886 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 26 13:55:58.474: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Verifying customized DNS server is configured on pod...
Apr 26 13:55:59.008: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-6886 PodName:dns-6886 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 26 13:55:59.009: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
Apr 26 13:55:59.513: INFO: Deleting pod dns-6886...
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:55:59.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6886" for this suite.

• [SLOW TEST:5.215 seconds]
[sig-network] DNS
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":303,"completed":43,"skipped":547,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:55:59.578: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Apr 26 13:55:59.699: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr 26 13:56:03.728: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Apr 26 13:56:03.775: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-5776 /apis/apps/v1/namespaces/deployment-5776/deployments/test-cleanup-deployment f35ff3e1-5bd2-4287-b144-b0e92bd72a2a 14330 1 2021-04-26 13:56:03 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2021-04-26 13:56:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0008e4548 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Apr 26 13:56:03.784: INFO: New ReplicaSet "test-cleanup-deployment-5d446bdd47" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-5d446bdd47  deployment-5776 /apis/apps/v1/namespaces/deployment-5776/replicasets/test-cleanup-deployment-5d446bdd47 e5122819-a3a4-475b-9ac6-198251a7ca70 14332 1 2021-04-26 13:56:03 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment f35ff3e1-5bd2-4287-b144-b0e92bd72a2a 0xc00301cd07 0xc00301cd08}] []  [{kube-controller-manager Update apps/v1 2021-04-26 13:56:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f35ff3e1-5bd2-4287-b144-b0e92bd72a2a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 5d446bdd47,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00301cd98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 26 13:56:03.784: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Apr 26 13:56:03.785: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-5776 /apis/apps/v1/namespaces/deployment-5776/replicasets/test-cleanup-controller 560ddba9-147f-48b1-8555-76e06a149cb4 14331 1 2021-04-26 13:55:59 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment f35ff3e1-5bd2-4287-b144-b0e92bd72a2a 0xc00301cbf7 0xc00301cbf8}] []  [{e2e.test Update apps/v1 2021-04-26 13:55:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-04-26 13:56:03 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"f35ff3e1-5bd2-4287-b144-b0e92bd72a2a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00301cc98 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr 26 13:56:03.798: INFO: Pod "test-cleanup-controller-djp27" is available:
&Pod{ObjectMeta:{test-cleanup-controller-djp27 test-cleanup-controller- deployment-5776 /api/v1/namespaces/deployment-5776/pods/test-cleanup-controller-djp27 b140c967-babd-425b-b856-3c094a9e1e49 14322 0 2021-04-26 13:55:59 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/podIP:172.25.1.45/32] [{apps/v1 ReplicaSet test-cleanup-controller 560ddba9-147f-48b1-8555-76e06a149cb4 0xc00301d267 0xc00301d268}] []  [{kube-controller-manager Update v1 2021-04-26 13:55:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"560ddba9-147f-48b1-8555-76e06a149cb4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-04-26 13:56:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2021-04-26 13:56:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.45\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5qkq6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5qkq6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5qkq6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:elastic-hugle-5f9d9b5555-8444w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 13:55:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 13:56:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 13:56:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 13:55:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.11,PodIP:172.25.1.45,StartTime:2021-04-26 13:55:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-26 13:56:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://188c13bb23d8f22801330d1ec55d577e41f4cd0bcfca9934728be07233bfb2cc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.45,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 13:56:03.798: INFO: Pod "test-cleanup-deployment-5d446bdd47-vxdzr" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-5d446bdd47-vxdzr test-cleanup-deployment-5d446bdd47- deployment-5776 /api/v1/namespaces/deployment-5776/pods/test-cleanup-deployment-5d446bdd47-vxdzr 5942d00c-83c6-41ea-9b8c-9a32e1bd4852 14336 0 2021-04-26 13:56:03 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-5d446bdd47 e5122819-a3a4-475b-9ac6-198251a7ca70 0xc00301d417 0xc00301d418}] []  [{kube-controller-manager Update v1 2021-04-26 13:56:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e5122819-a3a4-475b-9ac6-198251a7ca70\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5qkq6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5qkq6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5qkq6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:elastic-hugle-5f9d9b5555-8444w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 13:56:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:56:03.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5776" for this suite.
•{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":303,"completed":44,"skipped":555,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:56:03.840: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-projected-all-test-volume-c4c78732-83a0-4c37-b133-045dbd5f55e1
STEP: Creating secret with name secret-projected-all-test-volume-151b1898-3372-4061-a519-449bc72e6e72
STEP: Creating a pod to test Check all projections for projected volume plugin
Apr 26 13:56:03.925: INFO: Waiting up to 5m0s for pod "projected-volume-03ad2b13-e6e4-4337-af2f-fb7dc138c7a8" in namespace "projected-2811" to be "Succeeded or Failed"
Apr 26 13:56:03.943: INFO: Pod "projected-volume-03ad2b13-e6e4-4337-af2f-fb7dc138c7a8": Phase="Pending", Reason="", readiness=false. Elapsed: 18.58099ms
Apr 26 13:56:05.964: INFO: Pod "projected-volume-03ad2b13-e6e4-4337-af2f-fb7dc138c7a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039646934s
Apr 26 13:56:07.974: INFO: Pod "projected-volume-03ad2b13-e6e4-4337-af2f-fb7dc138c7a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049390974s
STEP: Saw pod success
Apr 26 13:56:07.974: INFO: Pod "projected-volume-03ad2b13-e6e4-4337-af2f-fb7dc138c7a8" satisfied condition "Succeeded or Failed"
Apr 26 13:56:07.989: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-w8lg4 pod projected-volume-03ad2b13-e6e4-4337-af2f-fb7dc138c7a8 container projected-all-volume-test: <nil>
STEP: delete the pod
Apr 26 13:56:08.071: INFO: Waiting for pod projected-volume-03ad2b13-e6e4-4337-af2f-fb7dc138c7a8 to disappear
Apr 26 13:56:08.077: INFO: Pod projected-volume-03ad2b13-e6e4-4337-af2f-fb7dc138c7a8 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:56:08.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2811" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":303,"completed":45,"skipped":560,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:56:08.111: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's command
Apr 26 13:56:08.202: INFO: Waiting up to 5m0s for pod "var-expansion-aa871822-40f6-4b21-b2c6-b3ab590c34b4" in namespace "var-expansion-1960" to be "Succeeded or Failed"
Apr 26 13:56:08.216: INFO: Pod "var-expansion-aa871822-40f6-4b21-b2c6-b3ab590c34b4": Phase="Pending", Reason="", readiness=false. Elapsed: 13.198799ms
Apr 26 13:56:10.222: INFO: Pod "var-expansion-aa871822-40f6-4b21-b2c6-b3ab590c34b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019757616s
Apr 26 13:56:12.229: INFO: Pod "var-expansion-aa871822-40f6-4b21-b2c6-b3ab590c34b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02681726s
STEP: Saw pod success
Apr 26 13:56:12.229: INFO: Pod "var-expansion-aa871822-40f6-4b21-b2c6-b3ab590c34b4" satisfied condition "Succeeded or Failed"
Apr 26 13:56:12.235: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod var-expansion-aa871822-40f6-4b21-b2c6-b3ab590c34b4 container dapi-container: <nil>
STEP: delete the pod
Apr 26 13:56:12.284: INFO: Waiting for pod var-expansion-aa871822-40f6-4b21-b2c6-b3ab590c34b4 to disappear
Apr 26 13:56:12.291: INFO: Pod var-expansion-aa871822-40f6-4b21-b2c6-b3ab590c34b4 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:56:12.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1960" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":303,"completed":46,"skipped":580,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] server version
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:56:12.332: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Request ServerVersion
STEP: Confirm major version
Apr 26 13:56:12.411: INFO: Major version: 1
STEP: Confirm minor version
Apr 26 13:56:12.411: INFO: cleanMinorVersion: 19
Apr 26 13:56:12.411: INFO: Minor version: 19
[AfterEach] [sig-api-machinery] server version
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:56:12.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-2627" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":303,"completed":47,"skipped":587,"failed":0}
SSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:56:12.436: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Apr 26 13:56:12.512: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:56:17.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8342" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":303,"completed":48,"skipped":592,"failed":0}

------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:56:17.337: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Apr 26 13:56:18.126: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Apr 26 13:56:20.150: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755042178, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755042178, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755042178, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755042178, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 26 13:56:23.174: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Apr 26 13:56:23.180: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:56:24.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-4002" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:7.385 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":303,"completed":49,"skipped":592,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:56:24.724: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Apr 26 13:56:24.808: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fa3b9708-2308-405e-8950-afa15968ec90" in namespace "projected-5454" to be "Succeeded or Failed"
Apr 26 13:56:24.817: INFO: Pod "downwardapi-volume-fa3b9708-2308-405e-8950-afa15968ec90": Phase="Pending", Reason="", readiness=false. Elapsed: 9.307617ms
Apr 26 13:56:26.823: INFO: Pod "downwardapi-volume-fa3b9708-2308-405e-8950-afa15968ec90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015491282s
Apr 26 13:56:28.830: INFO: Pod "downwardapi-volume-fa3b9708-2308-405e-8950-afa15968ec90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022273148s
STEP: Saw pod success
Apr 26 13:56:28.830: INFO: Pod "downwardapi-volume-fa3b9708-2308-405e-8950-afa15968ec90" satisfied condition "Succeeded or Failed"
Apr 26 13:56:28.836: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod downwardapi-volume-fa3b9708-2308-405e-8950-afa15968ec90 container client-container: <nil>
STEP: delete the pod
Apr 26 13:56:28.913: INFO: Waiting for pod downwardapi-volume-fa3b9708-2308-405e-8950-afa15968ec90 to disappear
Apr 26 13:56:28.919: INFO: Pod downwardapi-volume-fa3b9708-2308-405e-8950-afa15968ec90 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:56:28.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5454" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":50,"skipped":623,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:56:28.951: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:56:29.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-371" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":303,"completed":51,"skipped":627,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:56:29.084: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name projected-secret-test-a6ee4dbf-9d4f-4dd4-a267-5e93edcd1280
STEP: Creating a pod to test consume secrets
Apr 26 13:56:29.159: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2d0d336b-6cb0-424f-b920-eabb39c6feae" in namespace "projected-4666" to be "Succeeded or Failed"
Apr 26 13:56:29.168: INFO: Pod "pod-projected-secrets-2d0d336b-6cb0-424f-b920-eabb39c6feae": Phase="Pending", Reason="", readiness=false. Elapsed: 8.868802ms
Apr 26 13:56:31.178: INFO: Pod "pod-projected-secrets-2d0d336b-6cb0-424f-b920-eabb39c6feae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018339407s
Apr 26 13:56:33.185: INFO: Pod "pod-projected-secrets-2d0d336b-6cb0-424f-b920-eabb39c6feae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025878655s
STEP: Saw pod success
Apr 26 13:56:33.186: INFO: Pod "pod-projected-secrets-2d0d336b-6cb0-424f-b920-eabb39c6feae" satisfied condition "Succeeded or Failed"
Apr 26 13:56:33.192: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod pod-projected-secrets-2d0d336b-6cb0-424f-b920-eabb39c6feae container secret-volume-test: <nil>
STEP: delete the pod
Apr 26 13:56:33.228: INFO: Waiting for pod pod-projected-secrets-2d0d336b-6cb0-424f-b920-eabb39c6feae to disappear
Apr 26 13:56:33.233: INFO: Pod pod-projected-secrets-2d0d336b-6cb0-424f-b920-eabb39c6feae no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:56:33.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4666" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":303,"completed":52,"skipped":639,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:56:33.257: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Apr 26 13:56:33.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 version'
Apr 26 13:56:33.415: INFO: stderr: ""
Apr 26 13:56:33.415: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.4\", GitCommit:\"d360454c9bcd1634cf4cc52d1867af5491dc9c5f\", GitTreeState:\"clean\", BuildDate:\"2020-11-11T13:17:17Z\", GoVersion:\"go1.15.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.4\", GitCommit:\"d360454c9bcd1634cf4cc52d1867af5491dc9c5f\", GitTreeState:\"clean\", BuildDate:\"2020-11-11T13:09:17Z\", GoVersion:\"go1.15.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:56:33.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7758" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":303,"completed":53,"skipped":645,"failed":0}
SSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:56:33.438: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's args
Apr 26 13:56:33.507: INFO: Waiting up to 5m0s for pod "var-expansion-17406f9a-22c3-43e5-a8ab-8ae62c51afa0" in namespace "var-expansion-5557" to be "Succeeded or Failed"
Apr 26 13:56:33.514: INFO: Pod "var-expansion-17406f9a-22c3-43e5-a8ab-8ae62c51afa0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.341219ms
Apr 26 13:56:35.523: INFO: Pod "var-expansion-17406f9a-22c3-43e5-a8ab-8ae62c51afa0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015839547s
Apr 26 13:56:37.530: INFO: Pod "var-expansion-17406f9a-22c3-43e5-a8ab-8ae62c51afa0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023767515s
STEP: Saw pod success
Apr 26 13:56:37.530: INFO: Pod "var-expansion-17406f9a-22c3-43e5-a8ab-8ae62c51afa0" satisfied condition "Succeeded or Failed"
Apr 26 13:56:37.536: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod var-expansion-17406f9a-22c3-43e5-a8ab-8ae62c51afa0 container dapi-container: <nil>
STEP: delete the pod
Apr 26 13:56:37.573: INFO: Waiting for pod var-expansion-17406f9a-22c3-43e5-a8ab-8ae62c51afa0 to disappear
Apr 26 13:56:37.578: INFO: Pod var-expansion-17406f9a-22c3-43e5-a8ab-8ae62c51afa0 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:56:37.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5557" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":303,"completed":54,"skipped":651,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:56:37.605: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-d30cac72-c38e-44da-bf9f-8781dbe3a121
STEP: Creating a pod to test consume secrets
Apr 26 13:56:37.686: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e2ca8e39-5ddd-4d29-8e81-577008528a66" in namespace "projected-3986" to be "Succeeded or Failed"
Apr 26 13:56:37.694: INFO: Pod "pod-projected-secrets-e2ca8e39-5ddd-4d29-8e81-577008528a66": Phase="Pending", Reason="", readiness=false. Elapsed: 8.238516ms
Apr 26 13:56:39.708: INFO: Pod "pod-projected-secrets-e2ca8e39-5ddd-4d29-8e81-577008528a66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022491788s
STEP: Saw pod success
Apr 26 13:56:39.708: INFO: Pod "pod-projected-secrets-e2ca8e39-5ddd-4d29-8e81-577008528a66" satisfied condition "Succeeded or Failed"
Apr 26 13:56:39.714: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod pod-projected-secrets-e2ca8e39-5ddd-4d29-8e81-577008528a66 container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 26 13:56:39.767: INFO: Waiting for pod pod-projected-secrets-e2ca8e39-5ddd-4d29-8e81-577008528a66 to disappear
Apr 26 13:56:39.775: INFO: Pod pod-projected-secrets-e2ca8e39-5ddd-4d29-8e81-577008528a66 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:56:39.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3986" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":55,"skipped":675,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:56:39.810: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Apr 26 13:56:43.925: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-6132 PodName:var-expansion-b848f003-0ca7-4daf-9941-5d5eaa3e870f ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 26 13:56:43.925: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: test for file in mounted path
Apr 26 13:56:44.447: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-6132 PodName:var-expansion-b848f003-0ca7-4daf-9941-5d5eaa3e870f ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 26 13:56:44.447: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: updating the annotation value
Apr 26 13:56:45.494: INFO: Successfully updated pod "var-expansion-b848f003-0ca7-4daf-9941-5d5eaa3e870f"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Apr 26 13:56:45.500: INFO: Deleting pod "var-expansion-b848f003-0ca7-4daf-9941-5d5eaa3e870f" in namespace "var-expansion-6132"
Apr 26 13:56:45.515: INFO: Wait up to 5m0s for pod "var-expansion-b848f003-0ca7-4daf-9941-5d5eaa3e870f" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:57:19.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6132" for this suite.

• [SLOW TEST:39.746 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":303,"completed":56,"skipped":688,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:57:19.559: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 26 13:57:20.547: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 26 13:57:22.571: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755042240, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755042240, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755042240, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755042240, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 26 13:57:25.594: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:57:38.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8221" for this suite.
STEP: Destroying namespace "webhook-8221-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:18.660 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":303,"completed":57,"skipped":794,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:57:38.220: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:57:54.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4616" for this suite.

• [SLOW TEST:16.270 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":303,"completed":58,"skipped":797,"failed":0}
SSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:57:54.495: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override command
Apr 26 13:57:54.576: INFO: Waiting up to 5m0s for pod "client-containers-881ccb99-8552-4211-827c-dd3596268749" in namespace "containers-9575" to be "Succeeded or Failed"
Apr 26 13:57:54.583: INFO: Pod "client-containers-881ccb99-8552-4211-827c-dd3596268749": Phase="Pending", Reason="", readiness=false. Elapsed: 7.422969ms
Apr 26 13:57:56.591: INFO: Pod "client-containers-881ccb99-8552-4211-827c-dd3596268749": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015191467s
Apr 26 13:57:58.599: INFO: Pod "client-containers-881ccb99-8552-4211-827c-dd3596268749": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023315071s
STEP: Saw pod success
Apr 26 13:57:58.599: INFO: Pod "client-containers-881ccb99-8552-4211-827c-dd3596268749" satisfied condition "Succeeded or Failed"
Apr 26 13:57:58.604: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod client-containers-881ccb99-8552-4211-827c-dd3596268749 container test-container: <nil>
STEP: delete the pod
Apr 26 13:57:58.688: INFO: Waiting for pod client-containers-881ccb99-8552-4211-827c-dd3596268749 to disappear
Apr 26 13:57:58.695: INFO: Pod client-containers-881ccb99-8552-4211-827c-dd3596268749 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:57:58.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9575" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":303,"completed":59,"skipped":801,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:57:58.718: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Update Demo
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:308
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Apr 26 13:57:58.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 create -f - --namespace=kubectl-2002'
Apr 26 13:57:59.137: INFO: stderr: ""
Apr 26 13:57:59.137: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 26 13:57:59.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2002'
Apr 26 13:57:59.251: INFO: stderr: ""
Apr 26 13:57:59.251: INFO: stdout: "update-demo-nautilus-m59tc update-demo-nautilus-xgqc2 "
Apr 26 13:57:59.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 get pods update-demo-nautilus-m59tc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2002'
Apr 26 13:57:59.344: INFO: stderr: ""
Apr 26 13:57:59.344: INFO: stdout: ""
Apr 26 13:57:59.344: INFO: update-demo-nautilus-m59tc is created but not running
Apr 26 13:58:04.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2002'
Apr 26 13:58:04.441: INFO: stderr: ""
Apr 26 13:58:04.441: INFO: stdout: "update-demo-nautilus-m59tc update-demo-nautilus-xgqc2 "
Apr 26 13:58:04.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 get pods update-demo-nautilus-m59tc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2002'
Apr 26 13:58:04.529: INFO: stderr: ""
Apr 26 13:58:04.529: INFO: stdout: "true"
Apr 26 13:58:04.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 get pods update-demo-nautilus-m59tc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2002'
Apr 26 13:58:04.620: INFO: stderr: ""
Apr 26 13:58:04.620: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 26 13:58:04.620: INFO: validating pod update-demo-nautilus-m59tc
Apr 26 13:58:04.715: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 26 13:58:04.715: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 26 13:58:04.715: INFO: update-demo-nautilus-m59tc is verified up and running
Apr 26 13:58:04.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 get pods update-demo-nautilus-xgqc2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2002'
Apr 26 13:58:04.809: INFO: stderr: ""
Apr 26 13:58:04.809: INFO: stdout: "true"
Apr 26 13:58:04.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 get pods update-demo-nautilus-xgqc2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2002'
Apr 26 13:58:04.897: INFO: stderr: ""
Apr 26 13:58:04.897: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 26 13:58:04.897: INFO: validating pod update-demo-nautilus-xgqc2
Apr 26 13:58:04.996: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 26 13:58:04.996: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 26 13:58:04.996: INFO: update-demo-nautilus-xgqc2 is verified up and running
STEP: using delete to clean up resources
Apr 26 13:58:04.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 delete --grace-period=0 --force -f - --namespace=kubectl-2002'
Apr 26 13:58:05.095: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 26 13:58:05.095: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Apr 26 13:58:05.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-2002'
Apr 26 13:58:05.187: INFO: stderr: "No resources found in kubectl-2002 namespace.\n"
Apr 26 13:58:05.187: INFO: stdout: ""
Apr 26 13:58:05.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 get pods -l name=update-demo --namespace=kubectl-2002 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 26 13:58:05.280: INFO: stderr: ""
Apr 26 13:58:05.280: INFO: stdout: "update-demo-nautilus-m59tc\nupdate-demo-nautilus-xgqc2\n"
Apr 26 13:58:05.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-2002'
Apr 26 13:58:05.895: INFO: stderr: "No resources found in kubectl-2002 namespace.\n"
Apr 26 13:58:05.895: INFO: stdout: ""
Apr 26 13:58:05.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 get pods -l name=update-demo --namespace=kubectl-2002 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 26 13:58:06.007: INFO: stderr: ""
Apr 26 13:58:06.007: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:58:06.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2002" for this suite.

• [SLOW TEST:7.315 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:306
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":303,"completed":60,"skipped":812,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:58:06.034: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Apr 26 13:58:06.115: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0a6e67a7-cfa9-4160-8464-e2a3cfe72ac8" in namespace "projected-5056" to be "Succeeded or Failed"
Apr 26 13:58:06.161: INFO: Pod "downwardapi-volume-0a6e67a7-cfa9-4160-8464-e2a3cfe72ac8": Phase="Pending", Reason="", readiness=false. Elapsed: 45.905044ms
Apr 26 13:58:08.167: INFO: Pod "downwardapi-volume-0a6e67a7-cfa9-4160-8464-e2a3cfe72ac8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052400535s
Apr 26 13:58:10.175: INFO: Pod "downwardapi-volume-0a6e67a7-cfa9-4160-8464-e2a3cfe72ac8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060351388s
STEP: Saw pod success
Apr 26 13:58:10.175: INFO: Pod "downwardapi-volume-0a6e67a7-cfa9-4160-8464-e2a3cfe72ac8" satisfied condition "Succeeded or Failed"
Apr 26 13:58:10.182: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod downwardapi-volume-0a6e67a7-cfa9-4160-8464-e2a3cfe72ac8 container client-container: <nil>
STEP: delete the pod
Apr 26 13:58:10.228: INFO: Waiting for pod downwardapi-volume-0a6e67a7-cfa9-4160-8464-e2a3cfe72ac8 to disappear
Apr 26 13:58:10.234: INFO: Pod downwardapi-volume-0a6e67a7-cfa9-4160-8464-e2a3cfe72ac8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:58:10.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5056" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":303,"completed":61,"skipped":829,"failed":0}
SSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:58:10.274: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Apr 26 13:58:14.412: INFO: Waiting up to 5m0s for pod "client-envvars-3029bc85-2826-480f-889e-b32b5e810dd1" in namespace "pods-6569" to be "Succeeded or Failed"
Apr 26 13:58:14.421: INFO: Pod "client-envvars-3029bc85-2826-480f-889e-b32b5e810dd1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.134738ms
Apr 26 13:58:16.427: INFO: Pod "client-envvars-3029bc85-2826-480f-889e-b32b5e810dd1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014958745s
Apr 26 13:58:18.433: INFO: Pod "client-envvars-3029bc85-2826-480f-889e-b32b5e810dd1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021055558s
STEP: Saw pod success
Apr 26 13:58:18.433: INFO: Pod "client-envvars-3029bc85-2826-480f-889e-b32b5e810dd1" satisfied condition "Succeeded or Failed"
Apr 26 13:58:18.439: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-w8lg4 pod client-envvars-3029bc85-2826-480f-889e-b32b5e810dd1 container env3cont: <nil>
STEP: delete the pod
Apr 26 13:58:18.476: INFO: Waiting for pod client-envvars-3029bc85-2826-480f-889e-b32b5e810dd1 to disappear
Apr 26 13:58:18.486: INFO: Pod client-envvars-3029bc85-2826-480f-889e-b32b5e810dd1 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:58:18.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6569" for this suite.

• [SLOW TEST:8.240 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":303,"completed":62,"skipped":832,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:58:18.514: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Apr 26 13:58:18.579: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Creating first CR 
Apr 26 13:58:19.182: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-04-26T13:58:19Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-04-26T13:58:19Z]] name:name1 resourceVersion:15706 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:52c70748-b60b-42aa-92ba-5e3eff274128] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Apr 26 13:58:29.193: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-04-26T13:58:29Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-04-26T13:58:29Z]] name:name2 resourceVersion:15778 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:00c7d851-18b3-4935-b43e-1076f0b30fa3] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Apr 26 13:58:39.204: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-04-26T13:58:19Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-04-26T13:58:39Z]] name:name1 resourceVersion:15826 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:52c70748-b60b-42aa-92ba-5e3eff274128] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Apr 26 13:58:49.216: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-04-26T13:58:29Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-04-26T13:58:49Z]] name:name2 resourceVersion:15873 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:00c7d851-18b3-4935-b43e-1076f0b30fa3] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Apr 26 13:58:59.241: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-04-26T13:58:19Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-04-26T13:58:39Z]] name:name1 resourceVersion:15919 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:52c70748-b60b-42aa-92ba-5e3eff274128] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Apr 26 13:59:09.264: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-04-26T13:58:29Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-04-26T13:58:49Z]] name:name2 resourceVersion:15965 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:00c7d851-18b3-4935-b43e-1076f0b30fa3] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:59:19.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-1358" for this suite.

• [SLOW TEST:61.295 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":303,"completed":63,"skipped":847,"failed":0}
SSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:59:19.812: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:59:20.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5876" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":303,"completed":64,"skipped":852,"failed":0}
SSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:59:20.045: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-2115/configmap-test-5454c68d-c7a4-4dcb-8d9c-c96d2709c32e
STEP: Creating a pod to test consume configMaps
Apr 26 13:59:20.145: INFO: Waiting up to 5m0s for pod "pod-configmaps-5c15422d-e971-4eaa-a91d-c29ddb3170ea" in namespace "configmap-2115" to be "Succeeded or Failed"
Apr 26 13:59:20.154: INFO: Pod "pod-configmaps-5c15422d-e971-4eaa-a91d-c29ddb3170ea": Phase="Pending", Reason="", readiness=false. Elapsed: 8.704312ms
Apr 26 13:59:22.162: INFO: Pod "pod-configmaps-5c15422d-e971-4eaa-a91d-c29ddb3170ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016974668s
Apr 26 13:59:24.169: INFO: Pod "pod-configmaps-5c15422d-e971-4eaa-a91d-c29ddb3170ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023995234s
STEP: Saw pod success
Apr 26 13:59:24.169: INFO: Pod "pod-configmaps-5c15422d-e971-4eaa-a91d-c29ddb3170ea" satisfied condition "Succeeded or Failed"
Apr 26 13:59:24.175: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod pod-configmaps-5c15422d-e971-4eaa-a91d-c29ddb3170ea container env-test: <nil>
STEP: delete the pod
Apr 26 13:59:24.258: INFO: Waiting for pod pod-configmaps-5c15422d-e971-4eaa-a91d-c29ddb3170ea to disappear
Apr 26 13:59:24.263: INFO: Pod pod-configmaps-5c15422d-e971-4eaa-a91d-c29ddb3170ea no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:59:24.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2115" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":303,"completed":65,"skipped":855,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:59:24.304: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 26 13:59:24.854: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 26 13:59:26.875: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755042364, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755042364, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755042364, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755042364, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 26 13:59:29.904: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:59:29.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2821" for this suite.
STEP: Destroying namespace "webhook-2821-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.738 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":303,"completed":66,"skipped":863,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:59:30.047: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Apr 26 13:59:30.119: INFO: Waiting up to 5m0s for pod "pod-7712e0ed-86ca-404c-94f2-8ded4ca4119f" in namespace "emptydir-471" to be "Succeeded or Failed"
Apr 26 13:59:30.128: INFO: Pod "pod-7712e0ed-86ca-404c-94f2-8ded4ca4119f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.892926ms
Apr 26 13:59:32.149: INFO: Pod "pod-7712e0ed-86ca-404c-94f2-8ded4ca4119f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029778289s
Apr 26 13:59:34.157: INFO: Pod "pod-7712e0ed-86ca-404c-94f2-8ded4ca4119f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037587168s
STEP: Saw pod success
Apr 26 13:59:34.157: INFO: Pod "pod-7712e0ed-86ca-404c-94f2-8ded4ca4119f" satisfied condition "Succeeded or Failed"
Apr 26 13:59:34.163: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod pod-7712e0ed-86ca-404c-94f2-8ded4ca4119f container test-container: <nil>
STEP: delete the pod
Apr 26 13:59:34.244: INFO: Waiting for pod pod-7712e0ed-86ca-404c-94f2-8ded4ca4119f to disappear
Apr 26 13:59:34.253: INFO: Pod pod-7712e0ed-86ca-404c-94f2-8ded4ca4119f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:59:34.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-471" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":67,"skipped":871,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:59:34.275: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Apr 26 13:59:34.346: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b17bd75a-c358-40f4-b75c-bdb32cccda47" in namespace "projected-4927" to be "Succeeded or Failed"
Apr 26 13:59:34.361: INFO: Pod "downwardapi-volume-b17bd75a-c358-40f4-b75c-bdb32cccda47": Phase="Pending", Reason="", readiness=false. Elapsed: 15.33077ms
Apr 26 13:59:36.367: INFO: Pod "downwardapi-volume-b17bd75a-c358-40f4-b75c-bdb32cccda47": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021687343s
Apr 26 13:59:38.379: INFO: Pod "downwardapi-volume-b17bd75a-c358-40f4-b75c-bdb32cccda47": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03346695s
STEP: Saw pod success
Apr 26 13:59:38.379: INFO: Pod "downwardapi-volume-b17bd75a-c358-40f4-b75c-bdb32cccda47" satisfied condition "Succeeded or Failed"
Apr 26 13:59:38.389: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod downwardapi-volume-b17bd75a-c358-40f4-b75c-bdb32cccda47 container client-container: <nil>
STEP: delete the pod
Apr 26 13:59:38.427: INFO: Waiting for pod downwardapi-volume-b17bd75a-c358-40f4-b75c-bdb32cccda47 to disappear
Apr 26 13:59:38.435: INFO: Pod downwardapi-volume-b17bd75a-c358-40f4-b75c-bdb32cccda47 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:59:38.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4927" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":303,"completed":68,"skipped":877,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:59:38.496: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 26 13:59:39.169: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 26 13:59:41.189: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755042379, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755042379, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755042379, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755042379, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 26 13:59:44.214: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:59:45.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1220" for this suite.
STEP: Destroying namespace "webhook-1220-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.800 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":303,"completed":69,"skipped":895,"failed":0}
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:59:45.299: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-ab658a5c-caf2-4799-8ac7-178da9dda661
STEP: Creating a pod to test consume secrets
Apr 26 13:59:45.394: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f31d36a4-ee70-4f83-9b0e-369eacf94e80" in namespace "projected-9162" to be "Succeeded or Failed"
Apr 26 13:59:45.405: INFO: Pod "pod-projected-secrets-f31d36a4-ee70-4f83-9b0e-369eacf94e80": Phase="Pending", Reason="", readiness=false. Elapsed: 11.013234ms
Apr 26 13:59:47.420: INFO: Pod "pod-projected-secrets-f31d36a4-ee70-4f83-9b0e-369eacf94e80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026062463s
Apr 26 13:59:49.426: INFO: Pod "pod-projected-secrets-f31d36a4-ee70-4f83-9b0e-369eacf94e80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032648598s
STEP: Saw pod success
Apr 26 13:59:49.426: INFO: Pod "pod-projected-secrets-f31d36a4-ee70-4f83-9b0e-369eacf94e80" satisfied condition "Succeeded or Failed"
Apr 26 13:59:49.433: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod pod-projected-secrets-f31d36a4-ee70-4f83-9b0e-369eacf94e80 container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 26 13:59:49.511: INFO: Waiting for pod pod-projected-secrets-f31d36a4-ee70-4f83-9b0e-369eacf94e80 to disappear
Apr 26 13:59:49.518: INFO: Pod pod-projected-secrets-f31d36a4-ee70-4f83-9b0e-369eacf94e80 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:59:49.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9162" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":70,"skipped":895,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:59:49.541: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Apr 26 13:59:49.607: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Apr 26 13:59:51.699: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:59:51.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7665" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":303,"completed":71,"skipped":925,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:59:51.731: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating cluster-info
Apr 26 13:59:51.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 cluster-info'
Apr 26 13:59:52.584: INFO: stderr: ""
Apr 26 13:59:52.584: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.240.16.1:443\x1b[0m\n\x1b[0;32mkube-dns\x1b[0m is running at \x1b[0;33mhttps://10.240.16.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns-tcp/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:59:52.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2620" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":303,"completed":72,"skipped":936,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:59:52.618: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Apr 26 13:59:52.753: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fae152e8-eee8-40b6-9113-045099a01e4d" in namespace "downward-api-5377" to be "Succeeded or Failed"
Apr 26 13:59:52.777: INFO: Pod "downwardapi-volume-fae152e8-eee8-40b6-9113-045099a01e4d": Phase="Pending", Reason="", readiness=false. Elapsed: 24.377505ms
Apr 26 13:59:54.784: INFO: Pod "downwardapi-volume-fae152e8-eee8-40b6-9113-045099a01e4d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031624856s
Apr 26 13:59:56.806: INFO: Pod "downwardapi-volume-fae152e8-eee8-40b6-9113-045099a01e4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053331931s
STEP: Saw pod success
Apr 26 13:59:56.807: INFO: Pod "downwardapi-volume-fae152e8-eee8-40b6-9113-045099a01e4d" satisfied condition "Succeeded or Failed"
Apr 26 13:59:56.826: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod downwardapi-volume-fae152e8-eee8-40b6-9113-045099a01e4d container client-container: <nil>
STEP: delete the pod
Apr 26 13:59:56.908: INFO: Waiting for pod downwardapi-volume-fae152e8-eee8-40b6-9113-045099a01e4d to disappear
Apr 26 13:59:56.914: INFO: Pod downwardapi-volume-fae152e8-eee8-40b6-9113-045099a01e4d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 13:59:56.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5377" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":303,"completed":73,"skipped":941,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 13:59:56.942: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Apr 26 13:59:57.038: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Apr 26 13:59:57.051: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Apr 26 13:59:57.051: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Apr 26 13:59:57.069: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Apr 26 13:59:57.069: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Apr 26 13:59:57.088: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Apr 26 13:59:57.088: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Apr 26 14:00:04.172: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:00:04.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-5378" for this suite.

• [SLOW TEST:7.280 seconds]
[sig-scheduling] LimitRange
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":303,"completed":74,"skipped":1025,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:00:04.223: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-e889345d-48e3-49fc-908d-36d0954acad9
STEP: Creating a pod to test consume configMaps
Apr 26 14:00:04.336: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-adabdd84-f6c8-482f-8c71-a89fc2e1149b" in namespace "projected-6906" to be "Succeeded or Failed"
Apr 26 14:00:04.348: INFO: Pod "pod-projected-configmaps-adabdd84-f6c8-482f-8c71-a89fc2e1149b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.425501ms
Apr 26 14:00:06.354: INFO: Pod "pod-projected-configmaps-adabdd84-f6c8-482f-8c71-a89fc2e1149b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017493433s
Apr 26 14:00:08.363: INFO: Pod "pod-projected-configmaps-adabdd84-f6c8-482f-8c71-a89fc2e1149b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026680044s
STEP: Saw pod success
Apr 26 14:00:08.363: INFO: Pod "pod-projected-configmaps-adabdd84-f6c8-482f-8c71-a89fc2e1149b" satisfied condition "Succeeded or Failed"
Apr 26 14:00:08.373: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod pod-projected-configmaps-adabdd84-f6c8-482f-8c71-a89fc2e1149b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 26 14:00:08.432: INFO: Waiting for pod pod-projected-configmaps-adabdd84-f6c8-482f-8c71-a89fc2e1149b to disappear
Apr 26 14:00:08.437: INFO: Pod pod-projected-configmaps-adabdd84-f6c8-482f-8c71-a89fc2e1149b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:00:08.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6906" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":303,"completed":75,"skipped":1052,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:00:08.462: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Apr 26 14:00:11.121: INFO: Successfully updated pod "pod-update-activedeadlineseconds-a612b442-34b2-4839-af77-0f2039b798b1"
Apr 26 14:00:11.121: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-a612b442-34b2-4839-af77-0f2039b798b1" in namespace "pods-66" to be "terminated due to deadline exceeded"
Apr 26 14:00:11.130: INFO: Pod "pod-update-activedeadlineseconds-a612b442-34b2-4839-af77-0f2039b798b1": Phase="Running", Reason="", readiness=true. Elapsed: 8.492175ms
Apr 26 14:00:13.137: INFO: Pod "pod-update-activedeadlineseconds-a612b442-34b2-4839-af77-0f2039b798b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.015936468s
Apr 26 14:00:15.143: INFO: Pod "pod-update-activedeadlineseconds-a612b442-34b2-4839-af77-0f2039b798b1": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.022033551s
Apr 26 14:00:15.144: INFO: Pod "pod-update-activedeadlineseconds-a612b442-34b2-4839-af77-0f2039b798b1" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:00:15.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-66" for this suite.

• [SLOW TEST:6.703 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":303,"completed":76,"skipped":1067,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:00:15.166: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:00:15.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2016" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":303,"completed":77,"skipped":1094,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:00:15.306: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
Apr 26 14:00:15.407: INFO: created test-event-1
Apr 26 14:00:15.418: INFO: created test-event-2
Apr 26 14:00:15.426: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Apr 26 14:00:15.431: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Apr 26 14:00:15.497: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:00:15.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7104" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":303,"completed":78,"skipped":1122,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:00:15.551: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on node default medium
Apr 26 14:00:15.629: INFO: Waiting up to 5m0s for pod "pod-754ebbf2-77ee-49fd-bbc1-b05041534e56" in namespace "emptydir-9508" to be "Succeeded or Failed"
Apr 26 14:00:15.639: INFO: Pod "pod-754ebbf2-77ee-49fd-bbc1-b05041534e56": Phase="Pending", Reason="", readiness=false. Elapsed: 10.525362ms
Apr 26 14:00:17.646: INFO: Pod "pod-754ebbf2-77ee-49fd-bbc1-b05041534e56": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017590118s
Apr 26 14:00:19.654: INFO: Pod "pod-754ebbf2-77ee-49fd-bbc1-b05041534e56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025168081s
STEP: Saw pod success
Apr 26 14:00:19.654: INFO: Pod "pod-754ebbf2-77ee-49fd-bbc1-b05041534e56" satisfied condition "Succeeded or Failed"
Apr 26 14:00:19.661: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod pod-754ebbf2-77ee-49fd-bbc1-b05041534e56 container test-container: <nil>
STEP: delete the pod
Apr 26 14:00:19.710: INFO: Waiting for pod pod-754ebbf2-77ee-49fd-bbc1-b05041534e56 to disappear
Apr 26 14:00:19.718: INFO: Pod pod-754ebbf2-77ee-49fd-bbc1-b05041534e56 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:00:19.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9508" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":79,"skipped":1126,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:00:19.741: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-28e1da94-d8e6-4557-9b03-d5bb1be640c9
STEP: Creating a pod to test consume configMaps
Apr 26 14:00:19.836: INFO: Waiting up to 5m0s for pod "pod-configmaps-092bfeed-29a5-4b39-99ad-610d86540d37" in namespace "configmap-9688" to be "Succeeded or Failed"
Apr 26 14:00:19.844: INFO: Pod "pod-configmaps-092bfeed-29a5-4b39-99ad-610d86540d37": Phase="Pending", Reason="", readiness=false. Elapsed: 7.063254ms
Apr 26 14:00:21.851: INFO: Pod "pod-configmaps-092bfeed-29a5-4b39-99ad-610d86540d37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013944423s
Apr 26 14:00:23.860: INFO: Pod "pod-configmaps-092bfeed-29a5-4b39-99ad-610d86540d37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023569843s
STEP: Saw pod success
Apr 26 14:00:23.860: INFO: Pod "pod-configmaps-092bfeed-29a5-4b39-99ad-610d86540d37" satisfied condition "Succeeded or Failed"
Apr 26 14:00:23.874: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod pod-configmaps-092bfeed-29a5-4b39-99ad-610d86540d37 container configmap-volume-test: <nil>
STEP: delete the pod
Apr 26 14:00:23.966: INFO: Waiting for pod pod-configmaps-092bfeed-29a5-4b39-99ad-610d86540d37 to disappear
Apr 26 14:00:23.971: INFO: Pod pod-configmaps-092bfeed-29a5-4b39-99ad-610d86540d37 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:00:23.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9688" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":303,"completed":80,"skipped":1135,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:00:23.997: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-1038
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 26 14:00:24.052: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr 26 14:00:24.118: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr 26 14:00:26.137: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 26 14:00:28.129: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 26 14:00:30.125: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 26 14:00:32.127: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 26 14:00:34.126: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 26 14:00:36.130: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 26 14:00:38.142: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 26 14:00:40.127: INFO: The status of Pod netserver-0 is Running (Ready = true)
Apr 26 14:00:40.141: INFO: The status of Pod netserver-1 is Running (Ready = true)
Apr 26 14:00:40.154: INFO: The status of Pod netserver-2 is Running (Ready = false)
Apr 26 14:00:42.161: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Apr 26 14:00:46.215: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.73:8080/dial?request=hostname&protocol=http&host=172.25.1.72&port=8080&tries=1'] Namespace:pod-network-test-1038 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 26 14:00:46.215: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
Apr 26 14:00:46.738: INFO: Waiting for responses: map[]
Apr 26 14:00:46.746: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.73:8080/dial?request=hostname&protocol=http&host=172.25.2.11&port=8080&tries=1'] Namespace:pod-network-test-1038 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 26 14:00:46.746: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
Apr 26 14:00:47.334: INFO: Waiting for responses: map[]
Apr 26 14:00:47.344: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.73:8080/dial?request=hostname&protocol=http&host=172.25.0.30&port=8080&tries=1'] Namespace:pod-network-test-1038 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 26 14:00:47.344: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
Apr 26 14:00:47.897: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:00:47.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1038" for this suite.

• [SLOW TEST:23.930 seconds]
[sig-network] Networking
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":303,"completed":81,"skipped":1152,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:00:47.933: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
STEP: reading a file in the container
Apr 26 14:00:52.566: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8190 pod-service-account-3ededadd-c20d-42ea-b095-cbb526a824a3 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Apr 26 14:00:53.302: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8190 pod-service-account-3ededadd-c20d-42ea-b095-cbb526a824a3 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Apr 26 14:00:54.040: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8190 pod-service-account-3ededadd-c20d-42ea-b095-cbb526a824a3 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:00:54.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8190" for this suite.

• [SLOW TEST:6.732 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":303,"completed":82,"skipped":1174,"failed":0}
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:00:54.665: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with configMap that has name projected-configmap-test-upd-16e7256e-b76b-4647-9ab5-65fd2cf8bef7
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-16e7256e-b76b-4647-9ab5-65fd2cf8bef7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:02:17.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7247" for this suite.

• [SLOW TEST:83.104 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":303,"completed":83,"skipped":1174,"failed":0}
S
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:02:17.769: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Apr 26 14:02:19.877: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:02:19.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5173" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":303,"completed":84,"skipped":1175,"failed":0}
SSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:02:19.936: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in volume subpath
Apr 26 14:02:20.024: INFO: Waiting up to 5m0s for pod "var-expansion-42088d67-7a61-48cb-bc39-945b50953feb" in namespace "var-expansion-6961" to be "Succeeded or Failed"
Apr 26 14:02:20.031: INFO: Pod "var-expansion-42088d67-7a61-48cb-bc39-945b50953feb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.885334ms
Apr 26 14:02:22.037: INFO: Pod "var-expansion-42088d67-7a61-48cb-bc39-945b50953feb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012756589s
Apr 26 14:02:24.045: INFO: Pod "var-expansion-42088d67-7a61-48cb-bc39-945b50953feb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020373265s
STEP: Saw pod success
Apr 26 14:02:24.045: INFO: Pod "var-expansion-42088d67-7a61-48cb-bc39-945b50953feb" satisfied condition "Succeeded or Failed"
Apr 26 14:02:24.053: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod var-expansion-42088d67-7a61-48cb-bc39-945b50953feb container dapi-container: <nil>
STEP: delete the pod
Apr 26 14:02:24.147: INFO: Waiting for pod var-expansion-42088d67-7a61-48cb-bc39-945b50953feb to disappear
Apr 26 14:02:24.153: INFO: Pod var-expansion-42088d67-7a61-48cb-bc39-945b50953feb no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:02:24.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6961" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":303,"completed":85,"skipped":1178,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:02:24.173: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Apr 26 14:02:24.260: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a8d20020-9a5c-41c1-a591-44be787e1978" in namespace "downward-api-755" to be "Succeeded or Failed"
Apr 26 14:02:24.266: INFO: Pod "downwardapi-volume-a8d20020-9a5c-41c1-a591-44be787e1978": Phase="Pending", Reason="", readiness=false. Elapsed: 5.109783ms
Apr 26 14:02:26.271: INFO: Pod "downwardapi-volume-a8d20020-9a5c-41c1-a591-44be787e1978": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010788112s
Apr 26 14:02:28.277: INFO: Pod "downwardapi-volume-a8d20020-9a5c-41c1-a591-44be787e1978": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01678717s
STEP: Saw pod success
Apr 26 14:02:28.278: INFO: Pod "downwardapi-volume-a8d20020-9a5c-41c1-a591-44be787e1978" satisfied condition "Succeeded or Failed"
Apr 26 14:02:28.282: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod downwardapi-volume-a8d20020-9a5c-41c1-a591-44be787e1978 container client-container: <nil>
STEP: delete the pod
Apr 26 14:02:28.330: INFO: Waiting for pod downwardapi-volume-a8d20020-9a5c-41c1-a591-44be787e1978 to disappear
Apr 26 14:02:28.336: INFO: Pod downwardapi-volume-a8d20020-9a5c-41c1-a591-44be787e1978 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:02:28.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-755" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":86,"skipped":1179,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:02:28.368: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Apr 26 14:02:28.426: INFO: Creating deployment "webserver-deployment"
Apr 26 14:02:28.445: INFO: Waiting for observed generation 1
Apr 26 14:02:30.467: INFO: Waiting for all required pods to come up
Apr 26 14:02:30.483: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Apr 26 14:02:34.517: INFO: Waiting for deployment "webserver-deployment" to complete
Apr 26 14:02:34.531: INFO: Updating deployment "webserver-deployment" with a non-existent image
Apr 26 14:02:34.548: INFO: Updating deployment webserver-deployment
Apr 26 14:02:34.548: INFO: Waiting for observed generation 2
Apr 26 14:02:36.559: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Apr 26 14:02:36.566: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Apr 26 14:02:36.571: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Apr 26 14:02:36.593: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Apr 26 14:02:36.593: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Apr 26 14:02:36.601: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Apr 26 14:02:36.616: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Apr 26 14:02:36.616: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Apr 26 14:02:36.632: INFO: Updating deployment webserver-deployment
Apr 26 14:02:36.632: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Apr 26 14:02:36.651: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Apr 26 14:02:38.683: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Apr 26 14:02:38.698: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-4775 /apis/apps/v1/namespaces/deployment-4775/deployments/webserver-deployment c559f757-d28a-46a7-b5e3-af8df26404aa 18107 3 2021-04-26 14:02:28 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-04-26 14:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-04-26 14:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00060a6f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-04-26 14:02:36 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-04-26 14:02:36 +0000 UTC,LastTransitionTime:2021-04-26 14:02:28 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Apr 26 14:02:38.712: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-4775 /apis/apps/v1/namespaces/deployment-4775/replicasets/webserver-deployment-795d758f88 776c4044-ddfc-4286-b149-d523fd08536f 18100 3 2021-04-26 14:02:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment c559f757-d28a-46a7-b5e3-af8df26404aa 0xc00156a427 0xc00156a428}] []  [{kube-controller-manager Update apps/v1 2021-04-26 14:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c559f757-d28a-46a7-b5e3-af8df26404aa\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00156a5a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 26 14:02:38.713: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Apr 26 14:02:38.713: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-dd94f59b7  deployment-4775 /apis/apps/v1/namespaces/deployment-4775/replicasets/webserver-deployment-dd94f59b7 fdc8162b-bdcf-42d2-b949-61cad3956b29 18101 3 2021-04-26 14:02:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment c559f757-d28a-46a7-b5e3-af8df26404aa 0xc00156a607 0xc00156a608}] []  [{kube-controller-manager Update apps/v1 2021-04-26 14:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c559f757-d28a-46a7-b5e3-af8df26404aa\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: dd94f59b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00156a728 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Apr 26 14:02:38.761: INFO: Pod "webserver-deployment-795d758f88-2nnr4" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-2nnr4 webserver-deployment-795d758f88- deployment-4775 /api/v1/namespaces/deployment-4775/pods/webserver-deployment-795d758f88-2nnr4 551d221e-9669-4c9b-a546-6a7f82995261 18018 0 2021-04-26 14:02:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.25.1.82/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 776c4044-ddfc-4286-b149-d523fd08536f 0xc00156b147 0xc00156b148}] []  [{kube-controller-manager Update v1 2021-04-26 14:02:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"776c4044-ddfc-4286-b149-d523fd08536f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-26 14:02:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-04-26 14:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jqg2m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jqg2m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jqg2m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:elastic-hugle-5f9d9b5555-8444w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.11,PodIP:,StartTime:2021-04-26 14:02:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 14:02:38.761: INFO: Pod "webserver-deployment-795d758f88-2ntq4" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-2ntq4 webserver-deployment-795d758f88- deployment-4775 /api/v1/namespaces/deployment-4775/pods/webserver-deployment-795d758f88-2ntq4 b7b94ec6-2f8f-4bcf-973c-e4835b745fa6 18019 0 2021-04-26 14:02:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.25.1.83/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 776c4044-ddfc-4286-b149-d523fd08536f 0xc00156b547 0xc00156b548}] []  [{kube-controller-manager Update v1 2021-04-26 14:02:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"776c4044-ddfc-4286-b149-d523fd08536f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-26 14:02:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-04-26 14:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jqg2m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jqg2m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jqg2m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:elastic-hugle-5f9d9b5555-8444w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.11,PodIP:,StartTime:2021-04-26 14:02:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 14:02:38.762: INFO: Pod "webserver-deployment-795d758f88-646mk" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-646mk webserver-deployment-795d758f88- deployment-4775 /api/v1/namespaces/deployment-4775/pods/webserver-deployment-795d758f88-646mk 97b05bea-d663-47ff-845a-217cc6e74277 18106 0 2021-04-26 14:02:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 776c4044-ddfc-4286-b149-d523fd08536f 0xc00156b707 0xc00156b708}] []  [{kube-controller-manager Update v1 2021-04-26 14:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"776c4044-ddfc-4286-b149-d523fd08536f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-26 14:02:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jqg2m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jqg2m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jqg2m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:elastic-hugle-5f9d9b5555-8444w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.11,PodIP:,StartTime:2021-04-26 14:02:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 14:02:38.762: INFO: Pod "webserver-deployment-795d758f88-9j4tf" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-9j4tf webserver-deployment-795d758f88- deployment-4775 /api/v1/namespaces/deployment-4775/pods/webserver-deployment-795d758f88-9j4tf 221c6baf-f259-4648-a093-230e828b3c52 18086 0 2021-04-26 14:02:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 776c4044-ddfc-4286-b149-d523fd08536f 0xc00156b8a7 0xc00156b8a8}] []  [{kube-controller-manager Update v1 2021-04-26 14:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"776c4044-ddfc-4286-b149-d523fd08536f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-26 14:02:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jqg2m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jqg2m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jqg2m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:elastic-hugle-5f9d9b5555-mj2j9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.7,PodIP:,StartTime:2021-04-26 14:02:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 14:02:38.766: INFO: Pod "webserver-deployment-795d758f88-9wc29" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-9wc29 webserver-deployment-795d758f88- deployment-4775 /api/v1/namespaces/deployment-4775/pods/webserver-deployment-795d758f88-9wc29 794b46b9-d435-4c8a-9f0b-fcd035379c01 18127 0 2021-04-26 14:02:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 776c4044-ddfc-4286-b149-d523fd08536f 0xc00156ba40 0xc00156ba41}] []  [{kube-controller-manager Update v1 2021-04-26 14:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"776c4044-ddfc-4286-b149-d523fd08536f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-26 14:02:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jqg2m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jqg2m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jqg2m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:elastic-hugle-5f9d9b5555-mj2j9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.7,PodIP:,StartTime:2021-04-26 14:02:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 14:02:38.766: INFO: Pod "webserver-deployment-795d758f88-dgc25" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-dgc25 webserver-deployment-795d758f88- deployment-4775 /api/v1/namespaces/deployment-4775/pods/webserver-deployment-795d758f88-dgc25 4f0c4d07-2490-41cb-a952-2768e4964d1a 18119 0 2021-04-26 14:02:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 776c4044-ddfc-4286-b149-d523fd08536f 0xc00156bbd0 0xc00156bbd1}] []  [{kube-controller-manager Update v1 2021-04-26 14:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"776c4044-ddfc-4286-b149-d523fd08536f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-26 14:02:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jqg2m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jqg2m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jqg2m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:elastic-hugle-5f9d9b5555-mj2j9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.7,PodIP:,StartTime:2021-04-26 14:02:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 14:02:38.767: INFO: Pod "webserver-deployment-795d758f88-j4bns" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-j4bns webserver-deployment-795d758f88- deployment-4775 /api/v1/namespaces/deployment-4775/pods/webserver-deployment-795d758f88-j4bns 673e9d24-d9d6-4884-9934-02b6144c21d6 18139 0 2021-04-26 14:02:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 776c4044-ddfc-4286-b149-d523fd08536f 0xc00156bd60 0xc00156bd61}] []  [{kube-controller-manager Update v1 2021-04-26 14:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"776c4044-ddfc-4286-b149-d523fd08536f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-26 14:02:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jqg2m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jqg2m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jqg2m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:elastic-hugle-5f9d9b5555-8444w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.11,PodIP:,StartTime:2021-04-26 14:02:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 14:02:38.767: INFO: Pod "webserver-deployment-795d758f88-jbjb7" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-jbjb7 webserver-deployment-795d758f88- deployment-4775 /api/v1/namespaces/deployment-4775/pods/webserver-deployment-795d758f88-jbjb7 1bb8da76-dedb-431a-a1ed-6389a7f0dc0e 18140 0 2021-04-26 14:02:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 776c4044-ddfc-4286-b149-d523fd08536f 0xc002574017 0xc002574018}] []  [{kube-controller-manager Update v1 2021-04-26 14:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"776c4044-ddfc-4286-b149-d523fd08536f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-26 14:02:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jqg2m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jqg2m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jqg2m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:elastic-hugle-5f9d9b5555-w8lg4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.9,PodIP:,StartTime:2021-04-26 14:02:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 14:02:38.767: INFO: Pod "webserver-deployment-795d758f88-kbgl6" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-kbgl6 webserver-deployment-795d758f88- deployment-4775 /api/v1/namespaces/deployment-4775/pods/webserver-deployment-795d758f88-kbgl6 83f2e051-f28f-43a3-93fb-a208de305e57 18102 0 2021-04-26 14:02:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 776c4044-ddfc-4286-b149-d523fd08536f 0xc0025741b0 0xc0025741b1}] []  [{kube-controller-manager Update v1 2021-04-26 14:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"776c4044-ddfc-4286-b149-d523fd08536f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-26 14:02:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jqg2m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jqg2m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jqg2m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:elastic-hugle-5f9d9b5555-w8lg4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.9,PodIP:,StartTime:2021-04-26 14:02:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 14:02:38.767: INFO: Pod "webserver-deployment-795d758f88-ktjp6" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-ktjp6 webserver-deployment-795d758f88- deployment-4775 /api/v1/namespaces/deployment-4775/pods/webserver-deployment-795d758f88-ktjp6 ab03d331-245d-4b45-b525-dc94403a0004 18015 0 2021-04-26 14:02:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.25.0.36/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 776c4044-ddfc-4286-b149-d523fd08536f 0xc002574350 0xc002574351}] []  [{kube-controller-manager Update v1 2021-04-26 14:02:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"776c4044-ddfc-4286-b149-d523fd08536f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-26 14:02:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-04-26 14:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jqg2m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jqg2m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jqg2m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:elastic-hugle-5f9d9b5555-w8lg4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.9,PodIP:,StartTime:2021-04-26 14:02:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 14:02:38.767: INFO: Pod "webserver-deployment-795d758f88-mf7dh" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-mf7dh webserver-deployment-795d758f88- deployment-4775 /api/v1/namespaces/deployment-4775/pods/webserver-deployment-795d758f88-mf7dh dd789fe4-21b6-4afb-8db5-dac29db4ce52 18006 0 2021-04-26 14:02:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.25.2.15/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 776c4044-ddfc-4286-b149-d523fd08536f 0xc002574a30 0xc002574a31}] []  [{kube-controller-manager Update v1 2021-04-26 14:02:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"776c4044-ddfc-4286-b149-d523fd08536f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-26 14:02:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-04-26 14:02:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jqg2m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jqg2m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jqg2m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:elastic-hugle-5f9d9b5555-mj2j9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.7,PodIP:,StartTime:2021-04-26 14:02:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 14:02:38.768: INFO: Pod "webserver-deployment-795d758f88-mvpqv" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-mvpqv webserver-deployment-795d758f88- deployment-4775 /api/v1/namespaces/deployment-4775/pods/webserver-deployment-795d758f88-mvpqv 06776235-2ca2-4c65-b7c5-663a0cdb840b 18126 0 2021-04-26 14:02:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 776c4044-ddfc-4286-b149-d523fd08536f 0xc0025758d0 0xc0025758d1}] []  [{kube-controller-manager Update v1 2021-04-26 14:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"776c4044-ddfc-4286-b149-d523fd08536f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-26 14:02:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jqg2m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jqg2m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jqg2m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:elastic-hugle-5f9d9b5555-w8lg4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.9,PodIP:,StartTime:2021-04-26 14:02:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 14:02:38.768: INFO: Pod "webserver-deployment-795d758f88-wcwwt" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-wcwwt webserver-deployment-795d758f88- deployment-4775 /api/v1/namespaces/deployment-4775/pods/webserver-deployment-795d758f88-wcwwt dd48e028-6150-4c79-80e2-dc766e4aed70 18011 0 2021-04-26 14:02:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.25.0.35/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 776c4044-ddfc-4286-b149-d523fd08536f 0xc002575ae0 0xc002575ae1}] []  [{kube-controller-manager Update v1 2021-04-26 14:02:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"776c4044-ddfc-4286-b149-d523fd08536f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-26 14:02:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-04-26 14:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jqg2m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jqg2m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jqg2m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:elastic-hugle-5f9d9b5555-w8lg4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.9,PodIP:,StartTime:2021-04-26 14:02:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 14:02:38.772: INFO: Pod "webserver-deployment-dd94f59b7-6cgxq" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-6cgxq webserver-deployment-dd94f59b7- deployment-4775 /api/v1/namespaces/deployment-4775/pods/webserver-deployment-dd94f59b7-6cgxq 5da6ee01-2e69-4eb6-973e-1a07a8d61eb6 17897 0 2021-04-26 14:02:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.25.1.79/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 fdc8162b-bdcf-42d2-b949-61cad3956b29 0xc002575f30 0xc002575f31}] []  [{kube-controller-manager Update v1 2021-04-26 14:02:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fdc8162b-bdcf-42d2-b949-61cad3956b29\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-04-26 14:02:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2021-04-26 14:02:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.79\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jqg2m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jqg2m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jqg2m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:elastic-hugle-5f9d9b5555-8444w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.11,PodIP:172.25.1.79,StartTime:2021-04-26 14:02:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-26 14:02:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://6ef246414b7bc2269ae490d641a3fa8f67680c2dbf875527f43e64247ba11e41,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.79,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 14:02:38.772: INFO: Pod "webserver-deployment-dd94f59b7-9dv6c" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-9dv6c webserver-deployment-dd94f59b7- deployment-4775 /api/v1/namespaces/deployment-4775/pods/webserver-deployment-dd94f59b7-9dv6c 41bdc4a0-71c8-41fc-9335-ad3b020aa56f 18111 0 2021-04-26 14:02:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 fdc8162b-bdcf-42d2-b949-61cad3956b29 0xc00261a3b0 0xc00261a3b1}] []  [{kube-controller-manager Update v1 2021-04-26 14:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fdc8162b-bdcf-42d2-b949-61cad3956b29\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-26 14:02:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jqg2m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jqg2m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jqg2m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:elastic-hugle-5f9d9b5555-8444w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.11,PodIP:,StartTime:2021-04-26 14:02:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 14:02:38.772: INFO: Pod "webserver-deployment-dd94f59b7-9frxq" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-9frxq webserver-deployment-dd94f59b7- deployment-4775 /api/v1/namespaces/deployment-4775/pods/webserver-deployment-dd94f59b7-9frxq 13d57a4d-3e48-4f6b-a07a-4663c2e0d36e 18058 0 2021-04-26 14:02:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 fdc8162b-bdcf-42d2-b949-61cad3956b29 0xc00261a727 0xc00261a728}] []  [{kube-controller-manager Update v1 2021-04-26 14:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fdc8162b-bdcf-42d2-b949-61cad3956b29\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-26 14:02:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jqg2m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jqg2m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jqg2m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:elastic-hugle-5f9d9b5555-8444w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.11,PodIP:,StartTime:2021-04-26 14:02:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 14:02:38.776: INFO: Pod "webserver-deployment-dd94f59b7-9hw9w" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-9hw9w webserver-deployment-dd94f59b7- deployment-4775 /api/v1/namespaces/deployment-4775/pods/webserver-deployment-dd94f59b7-9hw9w 94193883-9a65-4180-a200-ff4a01bc396e 17889 0 2021-04-26 14:02:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.25.0.33/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 fdc8162b-bdcf-42d2-b949-61cad3956b29 0xc00261a8c7 0xc00261a8c8}] []  [{kube-controller-manager Update v1 2021-04-26 14:02:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fdc8162b-bdcf-42d2-b949-61cad3956b29\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-04-26 14:02:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2021-04-26 14:02:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.0.33\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jqg2m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jqg2m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jqg2m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:elastic-hugle-5f9d9b5555-w8lg4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.9,PodIP:172.25.0.33,StartTime:2021-04-26 14:02:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-26 14:02:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://607e4d54477d68c03a643d9acb5cee1424a6aa71c072adae41107cb735ca8fbd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.33,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 14:02:38.777: INFO: Pod "webserver-deployment-dd94f59b7-9ndt8" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-9ndt8 webserver-deployment-dd94f59b7- deployment-4775 /api/v1/namespaces/deployment-4775/pods/webserver-deployment-dd94f59b7-9ndt8 67efd08c-0ce8-4105-9a31-ac1ca651e469 17880 0 2021-04-26 14:02:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.25.2.13/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 fdc8162b-bdcf-42d2-b949-61cad3956b29 0xc00261ae40 0xc00261ae41}] []  [{kube-controller-manager Update v1 2021-04-26 14:02:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fdc8162b-bdcf-42d2-b949-61cad3956b29\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-04-26 14:02:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2021-04-26 14:02:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.13\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jqg2m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jqg2m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jqg2m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:elastic-hugle-5f9d9b5555-mj2j9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.7,PodIP:172.25.2.13,StartTime:2021-04-26 14:02:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-26 14:02:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://46b4a654b70ed6b49fa52ceb5744f0ff3872d2f368551b54a650f077b99d06e0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.13,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 14:02:38.780: INFO: Pod "webserver-deployment-dd94f59b7-c5c2v" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-c5c2v webserver-deployment-dd94f59b7- deployment-4775 /api/v1/namespaces/deployment-4775/pods/webserver-deployment-dd94f59b7-c5c2v ee67b8c2-0181-4cec-bc6a-39b3b6f00118 18073 0 2021-04-26 14:02:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 fdc8162b-bdcf-42d2-b949-61cad3956b29 0xc00261b0b0 0xc00261b0b1}] []  [{kube-controller-manager Update v1 2021-04-26 14:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fdc8162b-bdcf-42d2-b949-61cad3956b29\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-26 14:02:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jqg2m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jqg2m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jqg2m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:elastic-hugle-5f9d9b5555-w8lg4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.9,PodIP:,StartTime:2021-04-26 14:02:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 14:02:38.782: INFO: Pod "webserver-deployment-dd94f59b7-h49x6" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-h49x6 webserver-deployment-dd94f59b7- deployment-4775 /api/v1/namespaces/deployment-4775/pods/webserver-deployment-dd94f59b7-h49x6 77cdf2cc-84a4-4724-857a-dce6443fa348 18093 0 2021-04-26 14:02:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 fdc8162b-bdcf-42d2-b949-61cad3956b29 0xc00261b477 0xc00261b478}] []  [{kube-controller-manager Update v1 2021-04-26 14:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fdc8162b-bdcf-42d2-b949-61cad3956b29\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-26 14:02:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jqg2m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jqg2m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jqg2m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:elastic-hugle-5f9d9b5555-w8lg4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.9,PodIP:,StartTime:2021-04-26 14:02:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 14:02:38.782: INFO: Pod "webserver-deployment-dd94f59b7-hn8w6" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-hn8w6 webserver-deployment-dd94f59b7- deployment-4775 /api/v1/namespaces/deployment-4775/pods/webserver-deployment-dd94f59b7-hn8w6 ac0e5072-8847-4651-a0e1-f87c0497048e 17912 0 2021-04-26 14:02:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.25.2.14/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 fdc8162b-bdcf-42d2-b949-61cad3956b29 0xc00261b727 0xc00261b728}] []  [{kube-controller-manager Update v1 2021-04-26 14:02:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fdc8162b-bdcf-42d2-b949-61cad3956b29\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-04-26 14:02:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2021-04-26 14:02:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.14\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jqg2m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jqg2m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jqg2m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:elastic-hugle-5f9d9b5555-mj2j9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.7,PodIP:172.25.2.14,StartTime:2021-04-26 14:02:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-26 14:02:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://ed059d14d34611c3298ba831d2c3ea7ab225eb490eb3e0f5c07b40c539e28c22,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.14,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 14:02:38.782: INFO: Pod "webserver-deployment-dd94f59b7-klk5z" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-klk5z webserver-deployment-dd94f59b7- deployment-4775 /api/v1/namespaces/deployment-4775/pods/webserver-deployment-dd94f59b7-klk5z 79737aa3-76ef-4a5d-8d20-9ad77daf53bf 18115 0 2021-04-26 14:02:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 fdc8162b-bdcf-42d2-b949-61cad3956b29 0xc00261b9b0 0xc00261b9b1}] []  [{kube-controller-manager Update v1 2021-04-26 14:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fdc8162b-bdcf-42d2-b949-61cad3956b29\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-26 14:02:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jqg2m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jqg2m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jqg2m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:elastic-hugle-5f9d9b5555-mj2j9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.7,PodIP:,StartTime:2021-04-26 14:02:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 14:02:38.783: INFO: Pod "webserver-deployment-dd94f59b7-lpdz9" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-lpdz9 webserver-deployment-dd94f59b7- deployment-4775 /api/v1/namespaces/deployment-4775/pods/webserver-deployment-dd94f59b7-lpdz9 ef56bbea-0823-4126-be17-34213a04c737 17884 0 2021-04-26 14:02:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.25.2.12/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 fdc8162b-bdcf-42d2-b949-61cad3956b29 0xc00261bb37 0xc00261bb38}] []  [{kube-controller-manager Update v1 2021-04-26 14:02:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fdc8162b-bdcf-42d2-b949-61cad3956b29\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-04-26 14:02:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2021-04-26 14:02:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.12\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jqg2m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jqg2m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jqg2m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:elastic-hugle-5f9d9b5555-mj2j9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.7,PodIP:172.25.2.12,StartTime:2021-04-26 14:02:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-26 14:02:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://755e4fc2d43d8b0ca8ea0a60b951c64ab4b94a8e6441415d6becc78117d9a8d6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.12,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 14:02:38.784: INFO: Pod "webserver-deployment-dd94f59b7-m2nrq" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-m2nrq webserver-deployment-dd94f59b7- deployment-4775 /api/v1/namespaces/deployment-4775/pods/webserver-deployment-dd94f59b7-m2nrq 8b325ffd-0024-4053-b416-432b0552a06c 17919 0 2021-04-26 14:02:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.25.0.32/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 fdc8162b-bdcf-42d2-b949-61cad3956b29 0xc00261bcf0 0xc00261bcf1}] []  [{kube-controller-manager Update v1 2021-04-26 14:02:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fdc8162b-bdcf-42d2-b949-61cad3956b29\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-04-26 14:02:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2021-04-26 14:02:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.0.32\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jqg2m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jqg2m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jqg2m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:elastic-hugle-5f9d9b5555-w8lg4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.9,PodIP:172.25.0.32,StartTime:2021-04-26 14:02:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-26 14:02:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://36faba41e679863ba0f7a3d3a6fbd4ace1dfcef002f04c452ef10e677f6bb44a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.32,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 14:02:38.784: INFO: Pod "webserver-deployment-dd94f59b7-mg4fd" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-mg4fd webserver-deployment-dd94f59b7- deployment-4775 /api/v1/namespaces/deployment-4775/pods/webserver-deployment-dd94f59b7-mg4fd 57e7c5b6-ed3b-42cd-bd65-dc49bec63a88 18113 0 2021-04-26 14:02:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 fdc8162b-bdcf-42d2-b949-61cad3956b29 0xc00261bea0 0xc00261bea1}] []  [{kube-controller-manager Update v1 2021-04-26 14:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fdc8162b-bdcf-42d2-b949-61cad3956b29\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-26 14:02:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jqg2m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jqg2m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jqg2m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:elastic-hugle-5f9d9b5555-mj2j9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.7,PodIP:,StartTime:2021-04-26 14:02:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 14:02:38.787: INFO: Pod "webserver-deployment-dd94f59b7-n6p7f" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-n6p7f webserver-deployment-dd94f59b7- deployment-4775 /api/v1/namespaces/deployment-4775/pods/webserver-deployment-dd94f59b7-n6p7f 5956f42c-bcec-4ecc-9649-dc6d14b4ee69 18084 0 2021-04-26 14:02:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 fdc8162b-bdcf-42d2-b949-61cad3956b29 0xc003174037 0xc003174038}] []  [{kube-controller-manager Update v1 2021-04-26 14:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fdc8162b-bdcf-42d2-b949-61cad3956b29\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-26 14:02:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jqg2m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jqg2m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jqg2m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:elastic-hugle-5f9d9b5555-8444w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.11,PodIP:,StartTime:2021-04-26 14:02:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 14:02:38.787: INFO: Pod "webserver-deployment-dd94f59b7-p77pj" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-p77pj webserver-deployment-dd94f59b7- deployment-4775 /api/v1/namespaces/deployment-4775/pods/webserver-deployment-dd94f59b7-p77pj d01e5501-a2cb-4512-aa09-51cda48624bb 18099 0 2021-04-26 14:02:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 fdc8162b-bdcf-42d2-b949-61cad3956b29 0xc0031741d7 0xc0031741d8}] []  [{kube-controller-manager Update v1 2021-04-26 14:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fdc8162b-bdcf-42d2-b949-61cad3956b29\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-26 14:02:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jqg2m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jqg2m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jqg2m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:elastic-hugle-5f9d9b5555-8444w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.11,PodIP:,StartTime:2021-04-26 14:02:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 14:02:38.788: INFO: Pod "webserver-deployment-dd94f59b7-pxkhd" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-pxkhd webserver-deployment-dd94f59b7- deployment-4775 /api/v1/namespaces/deployment-4775/pods/webserver-deployment-dd94f59b7-pxkhd 6c4806bc-8790-43ba-86e8-1041a40c7488 18110 0 2021-04-26 14:02:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 fdc8162b-bdcf-42d2-b949-61cad3956b29 0xc0031744e7 0xc0031744e8}] []  [{kube-controller-manager Update v1 2021-04-26 14:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fdc8162b-bdcf-42d2-b949-61cad3956b29\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-26 14:02:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jqg2m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jqg2m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jqg2m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:elastic-hugle-5f9d9b5555-w8lg4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.9,PodIP:,StartTime:2021-04-26 14:02:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 14:02:38.788: INFO: Pod "webserver-deployment-dd94f59b7-t8s7z" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-t8s7z webserver-deployment-dd94f59b7- deployment-4775 /api/v1/namespaces/deployment-4775/pods/webserver-deployment-dd94f59b7-t8s7z 7aef61c9-139d-4d35-aac9-f15d6ce319e1 18108 0 2021-04-26 14:02:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 fdc8162b-bdcf-42d2-b949-61cad3956b29 0xc003174757 0xc003174758}] []  [{kube-controller-manager Update v1 2021-04-26 14:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fdc8162b-bdcf-42d2-b949-61cad3956b29\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-26 14:02:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jqg2m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jqg2m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jqg2m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:elastic-hugle-5f9d9b5555-mj2j9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.7,PodIP:,StartTime:2021-04-26 14:02:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 14:02:38.788: INFO: Pod "webserver-deployment-dd94f59b7-tx6s5" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-tx6s5 webserver-deployment-dd94f59b7- deployment-4775 /api/v1/namespaces/deployment-4775/pods/webserver-deployment-dd94f59b7-tx6s5 7adbbe2e-2e50-4b9c-9598-87a0f7f05eb8 18124 0 2021-04-26 14:02:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 fdc8162b-bdcf-42d2-b949-61cad3956b29 0xc003174a17 0xc003174a18}] []  [{kube-controller-manager Update v1 2021-04-26 14:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fdc8162b-bdcf-42d2-b949-61cad3956b29\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-26 14:02:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jqg2m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jqg2m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jqg2m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:elastic-hugle-5f9d9b5555-8444w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.11,PodIP:,StartTime:2021-04-26 14:02:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 14:02:38.788: INFO: Pod "webserver-deployment-dd94f59b7-v9mhx" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-v9mhx webserver-deployment-dd94f59b7- deployment-4775 /api/v1/namespaces/deployment-4775/pods/webserver-deployment-dd94f59b7-v9mhx c1ea8850-d76c-4735-9bea-302423e7fe01 17906 0 2021-04-26 14:02:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.25.1.78/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 fdc8162b-bdcf-42d2-b949-61cad3956b29 0xc003174c47 0xc003174c48}] []  [{kube-controller-manager Update v1 2021-04-26 14:02:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fdc8162b-bdcf-42d2-b949-61cad3956b29\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-04-26 14:02:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2021-04-26 14:02:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.78\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jqg2m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jqg2m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jqg2m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:elastic-hugle-5f9d9b5555-8444w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.11,PodIP:172.25.1.78,StartTime:2021-04-26 14:02:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-26 14:02:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://ba5f6677c9b2c0fa459f07c6ffe957b0c9b54fb74b80d149e56f2893807be32f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.78,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 14:02:38.788: INFO: Pod "webserver-deployment-dd94f59b7-z52db" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-z52db webserver-deployment-dd94f59b7- deployment-4775 /api/v1/namespaces/deployment-4775/pods/webserver-deployment-dd94f59b7-z52db bb1871ac-1605-47c1-ab75-93315a369e01 18117 0 2021-04-26 14:02:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 fdc8162b-bdcf-42d2-b949-61cad3956b29 0xc003174ec0 0xc003174ec1}] []  [{kube-controller-manager Update v1 2021-04-26 14:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fdc8162b-bdcf-42d2-b949-61cad3956b29\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-26 14:02:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jqg2m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jqg2m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jqg2m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:elastic-hugle-5f9d9b5555-w8lg4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.9,PodIP:,StartTime:2021-04-26 14:02:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 26 14:02:38.788: INFO: Pod "webserver-deployment-dd94f59b7-zswd8" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-zswd8 webserver-deployment-dd94f59b7- deployment-4775 /api/v1/namespaces/deployment-4775/pods/webserver-deployment-dd94f59b7-zswd8 b3a48a13-a73d-4d98-9bd7-5e728e641166 17916 0 2021-04-26 14:02:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.25.0.34/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 fdc8162b-bdcf-42d2-b949-61cad3956b29 0xc0031752d7 0xc0031752d8}] []  [{kube-controller-manager Update v1 2021-04-26 14:02:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fdc8162b-bdcf-42d2-b949-61cad3956b29\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-04-26 14:02:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2021-04-26 14:02:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.0.34\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jqg2m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jqg2m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jqg2m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:elastic-hugle-5f9d9b5555-w8lg4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:02:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.9,PodIP:172.25.0.34,StartTime:2021-04-26 14:02:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-26 14:02:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://6847ee4033b43844c50d9b8c4d8b567a8c750e5c3ca17e48621e28134c61f313,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.34,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:02:38.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4775" for this suite.

• [SLOW TEST:10.442 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":303,"completed":87,"skipped":1187,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:02:38.810: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:02:38.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9158" for this suite.
STEP: Destroying namespace "nspatchtest-a2d5289c-296d-4fac-80c0-e247d99be4d6-8477" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":303,"completed":88,"skipped":1195,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:02:39.006: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Apr 26 14:02:39.085: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 26 14:02:39.103: INFO: Waiting for terminating namespaces to be deleted...
Apr 26 14:02:39.109: INFO: 
Logging pods the apiserver thinks is on node elastic-hugle-5f9d9b5555-8444w before test
Apr 26 14:02:39.127: INFO: webserver-deployment-795d758f88-2nnr4 from deployment-4775 started at 2021-04-26 14:02:34 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.129: INFO: 	Container httpd ready: false, restart count 0
Apr 26 14:02:39.129: INFO: webserver-deployment-795d758f88-2ntq4 from deployment-4775 started at 2021-04-26 14:02:34 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.129: INFO: 	Container httpd ready: false, restart count 0
Apr 26 14:02:39.129: INFO: webserver-deployment-795d758f88-646mk from deployment-4775 started at 2021-04-26 14:02:36 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.129: INFO: 	Container httpd ready: false, restart count 0
Apr 26 14:02:39.129: INFO: webserver-deployment-795d758f88-j4bns from deployment-4775 started at 2021-04-26 14:02:36 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.129: INFO: 	Container httpd ready: false, restart count 0
Apr 26 14:02:39.129: INFO: webserver-deployment-dd94f59b7-6cgxq from deployment-4775 started at 2021-04-26 14:02:28 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.129: INFO: 	Container httpd ready: true, restart count 0
Apr 26 14:02:39.129: INFO: webserver-deployment-dd94f59b7-9dv6c from deployment-4775 started at 2021-04-26 14:02:36 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.129: INFO: 	Container httpd ready: false, restart count 0
Apr 26 14:02:39.129: INFO: webserver-deployment-dd94f59b7-9frxq from deployment-4775 started at 2021-04-26 14:02:36 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.132: INFO: 	Container httpd ready: false, restart count 0
Apr 26 14:02:39.132: INFO: webserver-deployment-dd94f59b7-n6p7f from deployment-4775 started at 2021-04-26 14:02:36 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.132: INFO: 	Container httpd ready: false, restart count 0
Apr 26 14:02:39.132: INFO: webserver-deployment-dd94f59b7-p77pj from deployment-4775 started at 2021-04-26 14:02:36 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.132: INFO: 	Container httpd ready: false, restart count 0
Apr 26 14:02:39.132: INFO: webserver-deployment-dd94f59b7-tx6s5 from deployment-4775 started at 2021-04-26 14:02:36 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.132: INFO: 	Container httpd ready: false, restart count 0
Apr 26 14:02:39.132: INFO: webserver-deployment-dd94f59b7-v9mhx from deployment-4775 started at 2021-04-26 14:02:28 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.132: INFO: 	Container httpd ready: true, restart count 0
Apr 26 14:02:39.132: INFO: canal-9hndk from kube-system started at 2021-04-26 13:19:15 +0000 UTC (2 container statuses recorded)
Apr 26 14:02:39.132: INFO: 	Container calico-node ready: true, restart count 0
Apr 26 14:02:39.132: INFO: 	Container kube-flannel ready: true, restart count 0
Apr 26 14:02:39.132: INFO: coredns-85c978995d-bmmb9 from kube-system started at 2021-04-26 13:19:44 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.132: INFO: 	Container coredns ready: true, restart count 0
Apr 26 14:02:39.132: INFO: csi-cinder-nodeplugin-flatcar-nbzzj from kube-system started at 2021-04-26 13:19:37 +0000 UTC (2 container statuses recorded)
Apr 26 14:02:39.132: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Apr 26 14:02:39.132: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 26 14:02:39.132: INFO: kube-proxy-qvhqj from kube-system started at 2021-04-26 13:19:15 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.132: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 26 14:02:39.132: INFO: logrotate-p98vn from kube-system started at 2021-04-26 13:19:37 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.132: INFO: 	Container logrotate ready: true, restart count 0
Apr 26 14:02:39.132: INFO: node-local-dns-pfs8r from kube-system started at 2021-04-26 13:19:37 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.132: INFO: 	Container node-cache ready: true, restart count 0
Apr 26 14:02:39.132: INFO: user-ssh-keys-agent-vck9l from kube-system started at 2021-04-26 13:19:15 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.132: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Apr 26 14:02:39.132: INFO: dashboard-metrics-scraper-975c84c89-fc9kf from kubernetes-dashboard started at 2021-04-26 13:19:45 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.132: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Apr 26 14:02:39.132: INFO: sonobuoy from sonobuoy started at 2021-04-26 13:33:20 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.132: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 26 14:02:39.132: INFO: sonobuoy-systemd-logs-daemon-set-83692f989abe48af-6hhl5 from sonobuoy started at 2021-04-26 13:33:27 +0000 UTC (2 container statuses recorded)
Apr 26 14:02:39.132: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 14:02:39.132: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 26 14:02:39.132: INFO: 
Logging pods the apiserver thinks is on node elastic-hugle-5f9d9b5555-mj2j9 before test
Apr 26 14:02:39.145: INFO: webserver-deployment-795d758f88-9j4tf from deployment-4775 started at 2021-04-26 14:02:36 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.145: INFO: 	Container httpd ready: false, restart count 0
Apr 26 14:02:39.145: INFO: webserver-deployment-795d758f88-9wc29 from deployment-4775 started at 2021-04-26 14:02:36 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.146: INFO: 	Container httpd ready: false, restart count 0
Apr 26 14:02:39.146: INFO: webserver-deployment-795d758f88-dgc25 from deployment-4775 started at 2021-04-26 14:02:36 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.146: INFO: 	Container httpd ready: false, restart count 0
Apr 26 14:02:39.146: INFO: webserver-deployment-795d758f88-mf7dh from deployment-4775 started at 2021-04-26 14:02:34 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.146: INFO: 	Container httpd ready: false, restart count 0
Apr 26 14:02:39.146: INFO: webserver-deployment-dd94f59b7-9ndt8 from deployment-4775 started at 2021-04-26 14:02:28 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.146: INFO: 	Container httpd ready: true, restart count 0
Apr 26 14:02:39.146: INFO: webserver-deployment-dd94f59b7-hn8w6 from deployment-4775 started at 2021-04-26 14:02:28 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.146: INFO: 	Container httpd ready: true, restart count 0
Apr 26 14:02:39.146: INFO: webserver-deployment-dd94f59b7-klk5z from deployment-4775 started at 2021-04-26 14:02:36 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.146: INFO: 	Container httpd ready: false, restart count 0
Apr 26 14:02:39.146: INFO: webserver-deployment-dd94f59b7-lpdz9 from deployment-4775 started at 2021-04-26 14:02:28 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.147: INFO: 	Container httpd ready: true, restart count 0
Apr 26 14:02:39.147: INFO: webserver-deployment-dd94f59b7-mg4fd from deployment-4775 started at 2021-04-26 14:02:36 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.147: INFO: 	Container httpd ready: false, restart count 0
Apr 26 14:02:39.147: INFO: webserver-deployment-dd94f59b7-t8s7z from deployment-4775 started at 2021-04-26 14:02:36 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.147: INFO: 	Container httpd ready: false, restart count 0
Apr 26 14:02:39.147: INFO: canal-vrzgr from kube-system started at 2021-04-26 13:19:17 +0000 UTC (2 container statuses recorded)
Apr 26 14:02:39.147: INFO: 	Container calico-node ready: true, restart count 0
Apr 26 14:02:39.147: INFO: 	Container kube-flannel ready: true, restart count 0
Apr 26 14:02:39.147: INFO: csi-cinder-controllerplugin-0 from kube-system started at 2021-04-26 13:19:45 +0000 UTC (5 container statuses recorded)
Apr 26 14:02:39.148: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Apr 26 14:02:39.148: INFO: 	Container csi-attacher ready: true, restart count 0
Apr 26 14:02:39.148: INFO: 	Container csi-provisioner ready: true, restart count 0
Apr 26 14:02:39.148: INFO: 	Container csi-resizer ready: true, restart count 0
Apr 26 14:02:39.148: INFO: 	Container csi-snapshotter ready: true, restart count 0
Apr 26 14:02:39.148: INFO: csi-cinder-nodeplugin-flatcar-dt2hz from kube-system started at 2021-04-26 13:19:37 +0000 UTC (2 container statuses recorded)
Apr 26 14:02:39.149: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Apr 26 14:02:39.149: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 26 14:02:39.149: INFO: kube-proxy-6gbkb from kube-system started at 2021-04-26 13:19:17 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.149: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 26 14:02:39.149: INFO: logrotate-27s5t from kube-system started at 2021-04-26 13:19:37 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.149: INFO: 	Container logrotate ready: true, restart count 0
Apr 26 14:02:39.149: INFO: node-local-dns-qtvvc from kube-system started at 2021-04-26 13:19:37 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.149: INFO: 	Container node-cache ready: true, restart count 0
Apr 26 14:02:39.160: INFO: user-ssh-keys-agent-46lff from kube-system started at 2021-04-26 13:19:17 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.160: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Apr 26 14:02:39.160: INFO: sonobuoy-systemd-logs-daemon-set-83692f989abe48af-ppqzq from sonobuoy started at 2021-04-26 13:33:27 +0000 UTC (2 container statuses recorded)
Apr 26 14:02:39.160: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 14:02:39.160: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 26 14:02:39.160: INFO: 
Logging pods the apiserver thinks is on node elastic-hugle-5f9d9b5555-w8lg4 before test
Apr 26 14:02:39.177: INFO: webserver-deployment-795d758f88-jbjb7 from deployment-4775 started at 2021-04-26 14:02:36 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.177: INFO: 	Container httpd ready: false, restart count 0
Apr 26 14:02:39.177: INFO: webserver-deployment-795d758f88-kbgl6 from deployment-4775 started at 2021-04-26 14:02:36 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.177: INFO: 	Container httpd ready: false, restart count 0
Apr 26 14:02:39.177: INFO: webserver-deployment-795d758f88-ktjp6 from deployment-4775 started at 2021-04-26 14:02:34 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.177: INFO: 	Container httpd ready: false, restart count 0
Apr 26 14:02:39.178: INFO: webserver-deployment-795d758f88-mvpqv from deployment-4775 started at 2021-04-26 14:02:36 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.178: INFO: 	Container httpd ready: false, restart count 0
Apr 26 14:02:39.178: INFO: webserver-deployment-795d758f88-wcwwt from deployment-4775 started at 2021-04-26 14:02:34 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.178: INFO: 	Container httpd ready: false, restart count 0
Apr 26 14:02:39.178: INFO: webserver-deployment-dd94f59b7-9hw9w from deployment-4775 started at 2021-04-26 14:02:28 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.178: INFO: 	Container httpd ready: true, restart count 0
Apr 26 14:02:39.178: INFO: webserver-deployment-dd94f59b7-c5c2v from deployment-4775 started at 2021-04-26 14:02:36 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.178: INFO: 	Container httpd ready: false, restart count 0
Apr 26 14:02:39.178: INFO: webserver-deployment-dd94f59b7-h49x6 from deployment-4775 started at 2021-04-26 14:02:36 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.178: INFO: 	Container httpd ready: false, restart count 0
Apr 26 14:02:39.179: INFO: webserver-deployment-dd94f59b7-m2nrq from deployment-4775 started at 2021-04-26 14:02:28 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.179: INFO: 	Container httpd ready: true, restart count 0
Apr 26 14:02:39.179: INFO: webserver-deployment-dd94f59b7-pxkhd from deployment-4775 started at 2021-04-26 14:02:36 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.179: INFO: 	Container httpd ready: false, restart count 0
Apr 26 14:02:39.179: INFO: webserver-deployment-dd94f59b7-z52db from deployment-4775 started at 2021-04-26 14:02:36 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.179: INFO: 	Container httpd ready: false, restart count 0
Apr 26 14:02:39.179: INFO: webserver-deployment-dd94f59b7-zswd8 from deployment-4775 started at 2021-04-26 14:02:28 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.179: INFO: 	Container httpd ready: true, restart count 0
Apr 26 14:02:39.179: INFO: canal-xcnxd from kube-system started at 2021-04-26 13:19:12 +0000 UTC (2 container statuses recorded)
Apr 26 14:02:39.179: INFO: 	Container calico-node ready: true, restart count 0
Apr 26 14:02:39.179: INFO: 	Container kube-flannel ready: true, restart count 1
Apr 26 14:02:39.179: INFO: coredns-85c978995d-gzh5v from kube-system started at 2021-04-26 13:19:44 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.179: INFO: 	Container coredns ready: true, restart count 0
Apr 26 14:02:39.179: INFO: csi-cinder-nodeplugin-flatcar-gqw8g from kube-system started at 2021-04-26 13:19:42 +0000 UTC (2 container statuses recorded)
Apr 26 14:02:39.179: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Apr 26 14:02:39.179: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 26 14:02:39.180: INFO: kube-proxy-xkvxw from kube-system started at 2021-04-26 13:19:12 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.180: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 26 14:02:39.180: INFO: logrotate-xhwmz from kube-system started at 2021-04-26 13:19:42 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.180: INFO: 	Container logrotate ready: true, restart count 0
Apr 26 14:02:39.180: INFO: node-local-dns-4jkgn from kube-system started at 2021-04-26 13:19:42 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.181: INFO: 	Container node-cache ready: true, restart count 0
Apr 26 14:02:39.181: INFO: openvpn-client-6c9dd998bc-hzr48 from kube-system started at 2021-04-26 13:19:45 +0000 UTC (2 container statuses recorded)
Apr 26 14:02:39.181: INFO: 	Container dnat-controller ready: true, restart count 0
Apr 26 14:02:39.181: INFO: 	Container openvpn-client ready: true, restart count 0
Apr 26 14:02:39.181: INFO: user-ssh-keys-agent-hr99m from kube-system started at 2021-04-26 13:19:12 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.181: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Apr 26 14:02:39.181: INFO: dashboard-metrics-scraper-975c84c89-drmjh from kubernetes-dashboard started at 2021-04-26 13:19:45 +0000 UTC (1 container statuses recorded)
Apr 26 14:02:39.181: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Apr 26 14:02:39.181: INFO: sonobuoy-e2e-job-c567c19005014a0f from sonobuoy started at 2021-04-26 13:33:27 +0000 UTC (2 container statuses recorded)
Apr 26 14:02:39.183: INFO: 	Container e2e ready: true, restart count 0
Apr 26 14:02:39.183: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 14:02:39.183: INFO: sonobuoy-systemd-logs-daemon-set-83692f989abe48af-rg644 from sonobuoy started at 2021-04-26 13:33:27 +0000 UTC (2 container statuses recorded)
Apr 26 14:02:39.183: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 14:02:39.183: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16796d567b058eba], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16796d567bfd0fa7], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:02:40.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4425" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":303,"completed":89,"skipped":1245,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:02:40.303: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating secret secrets-1727/secret-test-80ef2b17-7060-4777-bd65-53e3c712dd6b
STEP: Creating a pod to test consume secrets
Apr 26 14:02:40.457: INFO: Waiting up to 5m0s for pod "pod-configmaps-3327ca68-ff5f-4427-be03-b5e76521025e" in namespace "secrets-1727" to be "Succeeded or Failed"
Apr 26 14:02:40.478: INFO: Pod "pod-configmaps-3327ca68-ff5f-4427-be03-b5e76521025e": Phase="Pending", Reason="", readiness=false. Elapsed: 20.649751ms
Apr 26 14:02:42.488: INFO: Pod "pod-configmaps-3327ca68-ff5f-4427-be03-b5e76521025e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030725351s
Apr 26 14:02:44.498: INFO: Pod "pod-configmaps-3327ca68-ff5f-4427-be03-b5e76521025e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040854294s
Apr 26 14:02:46.560: INFO: Pod "pod-configmaps-3327ca68-ff5f-4427-be03-b5e76521025e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.102514042s
STEP: Saw pod success
Apr 26 14:02:46.560: INFO: Pod "pod-configmaps-3327ca68-ff5f-4427-be03-b5e76521025e" satisfied condition "Succeeded or Failed"
Apr 26 14:02:46.569: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-mj2j9 pod pod-configmaps-3327ca68-ff5f-4427-be03-b5e76521025e container env-test: <nil>
STEP: delete the pod
Apr 26 14:02:46.694: INFO: Waiting for pod pod-configmaps-3327ca68-ff5f-4427-be03-b5e76521025e to disappear
Apr 26 14:02:46.699: INFO: Pod pod-configmaps-3327ca68-ff5f-4427-be03-b5e76521025e no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:02:46.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1727" for this suite.

• [SLOW TEST:6.421 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:36
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":303,"completed":90,"skipped":1251,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:02:46.725: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 26 14:02:47.766: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 26 14:02:49.791: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755042567, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755042567, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755042567, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755042567, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 26 14:02:52.826: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Apr 26 14:02:52.834: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3338-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:02:54.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1229" for this suite.
STEP: Destroying namespace "webhook-1229-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.766 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":303,"completed":91,"skipped":1255,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:02:54.493: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-1039
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Apr 26 14:02:54.600: INFO: Found 0 stateful pods, waiting for 3
Apr 26 14:03:04.607: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 26 14:03:04.607: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 26 14:03:04.607: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Apr 26 14:03:04.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-1039 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 26 14:03:05.276: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 26 14:03:05.276: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 26 14:03:05.276: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Apr 26 14:03:15.336: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Apr 26 14:03:25.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-1039 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 14:03:26.071: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 26 14:03:26.071: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 26 14:03:26.071: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 26 14:03:26.103: INFO: Waiting for StatefulSet statefulset-1039/ss2 to complete update
Apr 26 14:03:26.103: INFO: Waiting for Pod statefulset-1039/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 26 14:03:26.103: INFO: Waiting for Pod statefulset-1039/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 26 14:03:26.103: INFO: Waiting for Pod statefulset-1039/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 26 14:03:36.119: INFO: Waiting for StatefulSet statefulset-1039/ss2 to complete update
Apr 26 14:03:36.119: INFO: Waiting for Pod statefulset-1039/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 26 14:03:36.119: INFO: Waiting for Pod statefulset-1039/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 26 14:03:36.119: INFO: Waiting for Pod statefulset-1039/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 26 14:03:46.121: INFO: Waiting for StatefulSet statefulset-1039/ss2 to complete update
Apr 26 14:03:46.121: INFO: Waiting for Pod statefulset-1039/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 26 14:03:46.121: INFO: Waiting for Pod statefulset-1039/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 26 14:03:56.116: INFO: Waiting for StatefulSet statefulset-1039/ss2 to complete update
Apr 26 14:03:56.116: INFO: Waiting for Pod statefulset-1039/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 26 14:04:06.121: INFO: Waiting for StatefulSet statefulset-1039/ss2 to complete update
STEP: Rolling back to a previous revision
Apr 26 14:04:16.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-1039 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 26 14:04:16.775: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 26 14:04:16.775: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 26 14:04:16.775: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 26 14:04:26.836: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Apr 26 14:04:36.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=statefulset-1039 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 26 14:04:37.542: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 26 14:04:37.542: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 26 14:04:37.542: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 26 14:04:47.595: INFO: Waiting for StatefulSet statefulset-1039/ss2 to complete update
Apr 26 14:04:47.595: INFO: Waiting for Pod statefulset-1039/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Apr 26 14:04:47.595: INFO: Waiting for Pod statefulset-1039/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Apr 26 14:04:57.610: INFO: Waiting for StatefulSet statefulset-1039/ss2 to complete update
Apr 26 14:04:57.610: INFO: Waiting for Pod statefulset-1039/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Apr 26 14:04:57.610: INFO: Waiting for Pod statefulset-1039/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Apr 26 14:05:07.607: INFO: Waiting for StatefulSet statefulset-1039/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Apr 26 14:05:17.607: INFO: Deleting all statefulset in ns statefulset-1039
Apr 26 14:05:17.615: INFO: Scaling statefulset ss2 to 0
Apr 26 14:05:37.653: INFO: Waiting for statefulset status.replicas updated to 0
Apr 26 14:05:37.669: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:05:37.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1039" for this suite.

• [SLOW TEST:163.260 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":303,"completed":92,"skipped":1299,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:05:37.758: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Apr 26 14:05:37.890: INFO: Pod name pod-release: Found 0 pods out of 1
Apr 26 14:05:42.899: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:05:43.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8494" for this suite.

• [SLOW TEST:6.198 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":303,"completed":93,"skipped":1388,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:05:43.957: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-9900
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Apr 26 14:05:44.042: INFO: Found 0 stateful pods, waiting for 3
Apr 26 14:05:54.050: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 26 14:05:54.050: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 26 14:05:54.050: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Apr 26 14:05:54.094: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Apr 26 14:06:04.161: INFO: Updating stateful set ss2
Apr 26 14:06:04.182: INFO: Waiting for Pod statefulset-9900/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 26 14:06:14.197: INFO: Waiting for Pod statefulset-9900/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Apr 26 14:06:24.314: INFO: Found 2 stateful pods, waiting for 3
Apr 26 14:06:34.323: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 26 14:06:34.323: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 26 14:06:34.323: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Apr 26 14:06:34.363: INFO: Updating stateful set ss2
Apr 26 14:06:34.385: INFO: Waiting for Pod statefulset-9900/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 26 14:06:44.398: INFO: Waiting for Pod statefulset-9900/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 26 14:06:54.428: INFO: Updating stateful set ss2
Apr 26 14:06:54.452: INFO: Waiting for StatefulSet statefulset-9900/ss2 to complete update
Apr 26 14:06:54.456: INFO: Waiting for Pod statefulset-9900/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 26 14:07:04.471: INFO: Waiting for StatefulSet statefulset-9900/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Apr 26 14:07:14.468: INFO: Deleting all statefulset in ns statefulset-9900
Apr 26 14:07:14.474: INFO: Scaling statefulset ss2 to 0
Apr 26 14:07:44.501: INFO: Waiting for statefulset status.replicas updated to 0
Apr 26 14:07:44.507: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:07:44.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9900" for this suite.

• [SLOW TEST:120.598 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":303,"completed":94,"skipped":1400,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:07:44.558: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Apr 26 14:07:44.641: INFO: Waiting up to 5m0s for pod "pod-2787bc6d-8d2e-45c9-8610-79d7007df52b" in namespace "emptydir-4505" to be "Succeeded or Failed"
Apr 26 14:07:44.648: INFO: Pod "pod-2787bc6d-8d2e-45c9-8610-79d7007df52b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.650586ms
Apr 26 14:07:46.655: INFO: Pod "pod-2787bc6d-8d2e-45c9-8610-79d7007df52b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01442794s
Apr 26 14:07:48.661: INFO: Pod "pod-2787bc6d-8d2e-45c9-8610-79d7007df52b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020690766s
STEP: Saw pod success
Apr 26 14:07:48.661: INFO: Pod "pod-2787bc6d-8d2e-45c9-8610-79d7007df52b" satisfied condition "Succeeded or Failed"
Apr 26 14:07:48.668: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod pod-2787bc6d-8d2e-45c9-8610-79d7007df52b container test-container: <nil>
STEP: delete the pod
Apr 26 14:07:48.711: INFO: Waiting for pod pod-2787bc6d-8d2e-45c9-8610-79d7007df52b to disappear
Apr 26 14:07:48.718: INFO: Pod pod-2787bc6d-8d2e-45c9-8610-79d7007df52b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:07:48.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4505" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":95,"skipped":1422,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:07:48.754: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating all guestbook components
Apr 26 14:07:48.823: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Apr 26 14:07:48.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 create -f - --namespace=kubectl-9574'
Apr 26 14:07:49.200: INFO: stderr: ""
Apr 26 14:07:49.200: INFO: stdout: "service/agnhost-replica created\n"
Apr 26 14:07:49.200: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Apr 26 14:07:49.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 create -f - --namespace=kubectl-9574'
Apr 26 14:07:49.737: INFO: stderr: ""
Apr 26 14:07:49.737: INFO: stdout: "service/agnhost-primary created\n"
Apr 26 14:07:49.738: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Apr 26 14:07:49.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 create -f - --namespace=kubectl-9574'
Apr 26 14:07:50.097: INFO: stderr: ""
Apr 26 14:07:50.097: INFO: stdout: "service/frontend created\n"
Apr 26 14:07:50.097: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Apr 26 14:07:50.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 create -f - --namespace=kubectl-9574'
Apr 26 14:07:50.428: INFO: stderr: ""
Apr 26 14:07:50.428: INFO: stdout: "deployment.apps/frontend created\n"
Apr 26 14:07:50.429: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Apr 26 14:07:50.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 create -f - --namespace=kubectl-9574'
Apr 26 14:07:50.746: INFO: stderr: ""
Apr 26 14:07:50.746: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Apr 26 14:07:50.746: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Apr 26 14:07:50.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 create -f - --namespace=kubectl-9574'
Apr 26 14:07:51.111: INFO: stderr: ""
Apr 26 14:07:51.111: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Apr 26 14:07:51.111: INFO: Waiting for all frontend pods to be Running.
Apr 26 14:07:56.162: INFO: Waiting for frontend to serve content.
Apr 26 14:07:56.266: INFO: Trying to add a new entry to the guestbook.
Apr 26 14:07:56.318: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Apr 26 14:07:56.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 delete --grace-period=0 --force -f - --namespace=kubectl-9574'
Apr 26 14:07:56.537: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 26 14:07:56.537: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Apr 26 14:07:56.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 delete --grace-period=0 --force -f - --namespace=kubectl-9574'
Apr 26 14:07:56.686: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 26 14:07:56.686: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Apr 26 14:07:56.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 delete --grace-period=0 --force -f - --namespace=kubectl-9574'
Apr 26 14:07:56.827: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 26 14:07:56.827: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Apr 26 14:07:56.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 delete --grace-period=0 --force -f - --namespace=kubectl-9574'
Apr 26 14:07:56.957: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 26 14:07:56.958: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Apr 26 14:07:56.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 delete --grace-period=0 --force -f - --namespace=kubectl-9574'
Apr 26 14:07:57.158: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 26 14:07:57.158: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Apr 26 14:07:57.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 delete --grace-period=0 --force -f - --namespace=kubectl-9574'
Apr 26 14:07:57.377: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 26 14:07:57.377: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:07:57.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9574" for this suite.

• [SLOW TEST:8.676 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:351
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":303,"completed":96,"skipped":1435,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:07:57.432: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-53e74d9e-f67c-4936-a75c-233bae658845
STEP: Creating a pod to test consume secrets
Apr 26 14:07:57.596: INFO: Waiting up to 5m0s for pod "pod-secrets-f45689d2-f010-49cb-b3e6-bbfea4f0b7a6" in namespace "secrets-7820" to be "Succeeded or Failed"
Apr 26 14:07:57.606: INFO: Pod "pod-secrets-f45689d2-f010-49cb-b3e6-bbfea4f0b7a6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.059692ms
Apr 26 14:07:59.613: INFO: Pod "pod-secrets-f45689d2-f010-49cb-b3e6-bbfea4f0b7a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017127599s
Apr 26 14:08:01.620: INFO: Pod "pod-secrets-f45689d2-f010-49cb-b3e6-bbfea4f0b7a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023452094s
STEP: Saw pod success
Apr 26 14:08:01.620: INFO: Pod "pod-secrets-f45689d2-f010-49cb-b3e6-bbfea4f0b7a6" satisfied condition "Succeeded or Failed"
Apr 26 14:08:01.625: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod pod-secrets-f45689d2-f010-49cb-b3e6-bbfea4f0b7a6 container secret-volume-test: <nil>
STEP: delete the pod
Apr 26 14:08:01.705: INFO: Waiting for pod pod-secrets-f45689d2-f010-49cb-b3e6-bbfea4f0b7a6 to disappear
Apr 26 14:08:01.710: INFO: Pod pod-secrets-f45689d2-f010-49cb-b3e6-bbfea4f0b7a6 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:08:01.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7820" for this suite.
STEP: Destroying namespace "secret-namespace-6542" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":303,"completed":97,"skipped":1447,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:08:01.748: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-5pg7
STEP: Creating a pod to test atomic-volume-subpath
Apr 26 14:08:01.837: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-5pg7" in namespace "subpath-3434" to be "Succeeded or Failed"
Apr 26 14:08:01.844: INFO: Pod "pod-subpath-test-configmap-5pg7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.171364ms
Apr 26 14:08:03.851: INFO: Pod "pod-subpath-test-configmap-5pg7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01370131s
Apr 26 14:08:05.858: INFO: Pod "pod-subpath-test-configmap-5pg7": Phase="Running", Reason="", readiness=true. Elapsed: 4.020693571s
Apr 26 14:08:07.863: INFO: Pod "pod-subpath-test-configmap-5pg7": Phase="Running", Reason="", readiness=true. Elapsed: 6.026540008s
Apr 26 14:08:09.876: INFO: Pod "pod-subpath-test-configmap-5pg7": Phase="Running", Reason="", readiness=true. Elapsed: 8.039512069s
Apr 26 14:08:11.884: INFO: Pod "pod-subpath-test-configmap-5pg7": Phase="Running", Reason="", readiness=true. Elapsed: 10.046797314s
Apr 26 14:08:13.891: INFO: Pod "pod-subpath-test-configmap-5pg7": Phase="Running", Reason="", readiness=true. Elapsed: 12.053866255s
Apr 26 14:08:15.901: INFO: Pod "pod-subpath-test-configmap-5pg7": Phase="Running", Reason="", readiness=true. Elapsed: 14.064092244s
Apr 26 14:08:17.909: INFO: Pod "pod-subpath-test-configmap-5pg7": Phase="Running", Reason="", readiness=true. Elapsed: 16.072012953s
Apr 26 14:08:19.918: INFO: Pod "pod-subpath-test-configmap-5pg7": Phase="Running", Reason="", readiness=true. Elapsed: 18.08131891s
Apr 26 14:08:21.926: INFO: Pod "pod-subpath-test-configmap-5pg7": Phase="Running", Reason="", readiness=true. Elapsed: 20.088972509s
Apr 26 14:08:23.934: INFO: Pod "pod-subpath-test-configmap-5pg7": Phase="Running", Reason="", readiness=true. Elapsed: 22.096729703s
Apr 26 14:08:25.941: INFO: Pod "pod-subpath-test-configmap-5pg7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.10432006s
STEP: Saw pod success
Apr 26 14:08:25.941: INFO: Pod "pod-subpath-test-configmap-5pg7" satisfied condition "Succeeded or Failed"
Apr 26 14:08:25.949: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-w8lg4 pod pod-subpath-test-configmap-5pg7 container test-container-subpath-configmap-5pg7: <nil>
STEP: delete the pod
Apr 26 14:08:25.996: INFO: Waiting for pod pod-subpath-test-configmap-5pg7 to disappear
Apr 26 14:08:26.001: INFO: Pod pod-subpath-test-configmap-5pg7 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-5pg7
Apr 26 14:08:26.001: INFO: Deleting pod "pod-subpath-test-configmap-5pg7" in namespace "subpath-3434"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:08:26.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3434" for this suite.

• [SLOW TEST:24.280 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":303,"completed":98,"skipped":1478,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:08:26.029: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Apr 26 14:08:26.123: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 26 14:09:26.176: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:09:26.182: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:487
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Apr 26 14:09:28.300: INFO: found a healthy node: elastic-hugle-5f9d9b5555-8444w
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Apr 26 14:09:42.489: INFO: pods created so far: [1 1 1]
Apr 26 14:09:42.489: INFO: length of pods created so far: 3
Apr 26 14:10:00.508: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:10:07.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-8911" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:461
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:10:07.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-163" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:101.677 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:450
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":303,"completed":99,"skipped":1484,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:10:07.706: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:10:18.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8251" for this suite.

• [SLOW TEST:11.172 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":303,"completed":100,"skipped":1485,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] version v1
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:10:18.878: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-4jmpl in namespace proxy-6397
I0426 14:10:18.986946      21 runners.go:190] Created replication controller with name: proxy-service-4jmpl, namespace: proxy-6397, replica count: 1
I0426 14:10:20.037877      21 runners.go:190] proxy-service-4jmpl Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0426 14:10:21.038176      21 runners.go:190] proxy-service-4jmpl Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0426 14:10:22.038377      21 runners.go:190] proxy-service-4jmpl Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0426 14:10:23.038616      21 runners.go:190] proxy-service-4jmpl Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0426 14:10:24.038833      21 runners.go:190] proxy-service-4jmpl Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0426 14:10:25.039061      21 runners.go:190] proxy-service-4jmpl Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0426 14:10:26.039563      21 runners.go:190] proxy-service-4jmpl Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0426 14:10:27.039771      21 runners.go:190] proxy-service-4jmpl Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 26 14:10:27.045: INFO: setup took 8.107057325s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Apr 26 14:10:27.069: INFO: (0) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:162/proxy/: bar (200; 24.310366ms)
Apr 26 14:10:27.072: INFO: (0) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:1080/proxy/rewriteme">test<... (200; 26.100761ms)
Apr 26 14:10:27.072: INFO: (0) /api/v1/namespaces/proxy-6397/services/proxy-service-4jmpl:portname1/proxy/: foo (200; 26.587698ms)
Apr 26 14:10:27.072: INFO: (0) /api/v1/namespaces/proxy-6397/services/proxy-service-4jmpl:portname2/proxy/: bar (200; 26.711295ms)
Apr 26 14:10:27.072: INFO: (0) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:1080/proxy/rewriteme">... (200; 26.801256ms)
Apr 26 14:10:27.072: INFO: (0) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:160/proxy/: foo (200; 27.046804ms)
Apr 26 14:10:27.073: INFO: (0) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:160/proxy/: foo (200; 27.015938ms)
Apr 26 14:10:27.073: INFO: (0) /api/v1/namespaces/proxy-6397/services/http:proxy-service-4jmpl:portname2/proxy/: bar (200; 27.251933ms)
Apr 26 14:10:27.073: INFO: (0) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2/proxy/rewriteme">test</a> (200; 27.614341ms)
Apr 26 14:10:27.081: INFO: (0) /api/v1/namespaces/proxy-6397/services/http:proxy-service-4jmpl:portname1/proxy/: foo (200; 35.234708ms)
Apr 26 14:10:27.081: INFO: (0) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:462/proxy/: tls qux (200; 35.736669ms)
Apr 26 14:10:27.081: INFO: (0) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:162/proxy/: bar (200; 35.189318ms)
Apr 26 14:10:27.081: INFO: (0) /api/v1/namespaces/proxy-6397/services/https:proxy-service-4jmpl:tlsportname2/proxy/: tls qux (200; 35.523006ms)
Apr 26 14:10:27.085: INFO: (0) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:443/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:443/proxy/tlsrewritem... (200; 39.667255ms)
Apr 26 14:10:27.087: INFO: (0) /api/v1/namespaces/proxy-6397/services/https:proxy-service-4jmpl:tlsportname1/proxy/: tls baz (200; 41.556577ms)
Apr 26 14:10:27.087: INFO: (0) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:460/proxy/: tls baz (200; 41.453119ms)
Apr 26 14:10:27.100: INFO: (1) /api/v1/namespaces/proxy-6397/services/http:proxy-service-4jmpl:portname1/proxy/: foo (200; 12.204282ms)
Apr 26 14:10:27.102: INFO: (1) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:1080/proxy/rewriteme">test<... (200; 14.793007ms)
Apr 26 14:10:27.103: INFO: (1) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2/proxy/rewriteme">test</a> (200; 15.649245ms)
Apr 26 14:10:27.103: INFO: (1) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:462/proxy/: tls qux (200; 15.744214ms)
Apr 26 14:10:27.103: INFO: (1) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:460/proxy/: tls baz (200; 15.331472ms)
Apr 26 14:10:27.103: INFO: (1) /api/v1/namespaces/proxy-6397/services/proxy-service-4jmpl:portname1/proxy/: foo (200; 15.303682ms)
Apr 26 14:10:27.103: INFO: (1) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:1080/proxy/rewriteme">... (200; 15.298422ms)
Apr 26 14:10:27.103: INFO: (1) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:162/proxy/: bar (200; 15.397561ms)
Apr 26 14:10:27.103: INFO: (1) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:443/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:443/proxy/tlsrewritem... (200; 15.306509ms)
Apr 26 14:10:27.104: INFO: (1) /api/v1/namespaces/proxy-6397/services/https:proxy-service-4jmpl:tlsportname1/proxy/: tls baz (200; 17.112487ms)
Apr 26 14:10:27.104: INFO: (1) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:162/proxy/: bar (200; 16.497136ms)
Apr 26 14:10:27.104: INFO: (1) /api/v1/namespaces/proxy-6397/services/https:proxy-service-4jmpl:tlsportname2/proxy/: tls qux (200; 16.75098ms)
Apr 26 14:10:27.107: INFO: (1) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:160/proxy/: foo (200; 18.921728ms)
Apr 26 14:10:27.111: INFO: (1) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:160/proxy/: foo (200; 23.588268ms)
Apr 26 14:10:27.111: INFO: (1) /api/v1/namespaces/proxy-6397/services/proxy-service-4jmpl:portname2/proxy/: bar (200; 23.545079ms)
Apr 26 14:10:27.112: INFO: (1) /api/v1/namespaces/proxy-6397/services/http:proxy-service-4jmpl:portname2/proxy/: bar (200; 24.309182ms)
Apr 26 14:10:27.129: INFO: (2) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:443/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:443/proxy/tlsrewritem... (200; 15.654964ms)
Apr 26 14:10:27.129: INFO: (2) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:162/proxy/: bar (200; 15.713833ms)
Apr 26 14:10:27.129: INFO: (2) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:160/proxy/: foo (200; 16.37108ms)
Apr 26 14:10:27.129: INFO: (2) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:462/proxy/: tls qux (200; 16.09969ms)
Apr 26 14:10:27.137: INFO: (2) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:460/proxy/: tls baz (200; 24.012941ms)
Apr 26 14:10:27.137: INFO: (2) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:160/proxy/: foo (200; 24.487229ms)
Apr 26 14:10:27.137: INFO: (2) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:1080/proxy/rewriteme">test<... (200; 24.135121ms)
Apr 26 14:10:27.137: INFO: (2) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:162/proxy/: bar (200; 24.106854ms)
Apr 26 14:10:27.137: INFO: (2) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:1080/proxy/rewriteme">... (200; 24.881957ms)
Apr 26 14:10:27.137: INFO: (2) /api/v1/namespaces/proxy-6397/services/https:proxy-service-4jmpl:tlsportname2/proxy/: tls qux (200; 24.222867ms)
Apr 26 14:10:27.137: INFO: (2) /api/v1/namespaces/proxy-6397/services/https:proxy-service-4jmpl:tlsportname1/proxy/: tls baz (200; 24.801496ms)
Apr 26 14:10:27.137: INFO: (2) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2/proxy/rewriteme">test</a> (200; 23.993934ms)
Apr 26 14:10:27.141: INFO: (2) /api/v1/namespaces/proxy-6397/services/http:proxy-service-4jmpl:portname1/proxy/: foo (200; 28.055334ms)
Apr 26 14:10:27.143: INFO: (2) /api/v1/namespaces/proxy-6397/services/http:proxy-service-4jmpl:portname2/proxy/: bar (200; 29.76734ms)
Apr 26 14:10:27.145: INFO: (2) /api/v1/namespaces/proxy-6397/services/proxy-service-4jmpl:portname1/proxy/: foo (200; 32.058479ms)
Apr 26 14:10:27.145: INFO: (2) /api/v1/namespaces/proxy-6397/services/proxy-service-4jmpl:portname2/proxy/: bar (200; 32.024438ms)
Apr 26 14:10:27.161: INFO: (3) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:1080/proxy/rewriteme">test<... (200; 15.333583ms)
Apr 26 14:10:27.161: INFO: (3) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:160/proxy/: foo (200; 15.881798ms)
Apr 26 14:10:27.161: INFO: (3) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:162/proxy/: bar (200; 15.457071ms)
Apr 26 14:10:27.166: INFO: (3) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:1080/proxy/rewriteme">... (200; 20.915435ms)
Apr 26 14:10:27.167: INFO: (3) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:443/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:443/proxy/tlsrewritem... (200; 21.063907ms)
Apr 26 14:10:27.167: INFO: (3) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:460/proxy/: tls baz (200; 21.42015ms)
Apr 26 14:10:27.167: INFO: (3) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2/proxy/rewriteme">test</a> (200; 20.97676ms)
Apr 26 14:10:27.167: INFO: (3) /api/v1/namespaces/proxy-6397/services/proxy-service-4jmpl:portname2/proxy/: bar (200; 21.30606ms)
Apr 26 14:10:27.167: INFO: (3) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:462/proxy/: tls qux (200; 21.690125ms)
Apr 26 14:10:27.168: INFO: (3) /api/v1/namespaces/proxy-6397/services/http:proxy-service-4jmpl:portname1/proxy/: foo (200; 22.216649ms)
Apr 26 14:10:27.168: INFO: (3) /api/v1/namespaces/proxy-6397/services/https:proxy-service-4jmpl:tlsportname2/proxy/: tls qux (200; 22.168039ms)
Apr 26 14:10:27.168: INFO: (3) /api/v1/namespaces/proxy-6397/services/https:proxy-service-4jmpl:tlsportname1/proxy/: tls baz (200; 22.833631ms)
Apr 26 14:10:27.178: INFO: (3) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:162/proxy/: bar (200; 32.226106ms)
Apr 26 14:10:27.178: INFO: (3) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:160/proxy/: foo (200; 32.581657ms)
Apr 26 14:10:27.182: INFO: (3) /api/v1/namespaces/proxy-6397/services/proxy-service-4jmpl:portname1/proxy/: foo (200; 36.765162ms)
Apr 26 14:10:27.183: INFO: (3) /api/v1/namespaces/proxy-6397/services/http:proxy-service-4jmpl:portname2/proxy/: bar (200; 36.598787ms)
Apr 26 14:10:27.198: INFO: (4) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:160/proxy/: foo (200; 15.293999ms)
Apr 26 14:10:27.198: INFO: (4) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:160/proxy/: foo (200; 15.47243ms)
Apr 26 14:10:27.198: INFO: (4) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:1080/proxy/rewriteme">... (200; 15.699754ms)
Apr 26 14:10:27.205: INFO: (4) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:1080/proxy/rewriteme">test<... (200; 21.858023ms)
Apr 26 14:10:27.205: INFO: (4) /api/v1/namespaces/proxy-6397/services/http:proxy-service-4jmpl:portname1/proxy/: foo (200; 22.028824ms)
Apr 26 14:10:27.205: INFO: (4) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:443/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:443/proxy/tlsrewritem... (200; 21.835622ms)
Apr 26 14:10:27.205: INFO: (4) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:460/proxy/: tls baz (200; 21.91429ms)
Apr 26 14:10:27.205: INFO: (4) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2/proxy/rewriteme">test</a> (200; 21.818424ms)
Apr 26 14:10:27.205: INFO: (4) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:162/proxy/: bar (200; 22.024181ms)
Apr 26 14:10:27.205: INFO: (4) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:462/proxy/: tls qux (200; 22.144511ms)
Apr 26 14:10:27.205: INFO: (4) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:162/proxy/: bar (200; 21.996634ms)
Apr 26 14:10:27.205: INFO: (4) /api/v1/namespaces/proxy-6397/services/https:proxy-service-4jmpl:tlsportname1/proxy/: tls baz (200; 22.309885ms)
Apr 26 14:10:27.207: INFO: (4) /api/v1/namespaces/proxy-6397/services/http:proxy-service-4jmpl:portname2/proxy/: bar (200; 23.294814ms)
Apr 26 14:10:27.207: INFO: (4) /api/v1/namespaces/proxy-6397/services/proxy-service-4jmpl:portname2/proxy/: bar (200; 23.473353ms)
Apr 26 14:10:27.207: INFO: (4) /api/v1/namespaces/proxy-6397/services/proxy-service-4jmpl:portname1/proxy/: foo (200; 23.545619ms)
Apr 26 14:10:27.207: INFO: (4) /api/v1/namespaces/proxy-6397/services/https:proxy-service-4jmpl:tlsportname2/proxy/: tls qux (200; 23.778802ms)
Apr 26 14:10:27.219: INFO: (5) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2/proxy/rewriteme">test</a> (200; 11.930532ms)
Apr 26 14:10:27.219: INFO: (5) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:462/proxy/: tls qux (200; 11.820384ms)
Apr 26 14:10:27.219: INFO: (5) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:1080/proxy/rewriteme">... (200; 12.336826ms)
Apr 26 14:10:27.222: INFO: (5) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:460/proxy/: tls baz (200; 14.646493ms)
Apr 26 14:10:27.222: INFO: (5) /api/v1/namespaces/proxy-6397/services/proxy-service-4jmpl:portname1/proxy/: foo (200; 14.471273ms)
Apr 26 14:10:27.222: INFO: (5) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:162/proxy/: bar (200; 14.752911ms)
Apr 26 14:10:27.223: INFO: (5) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:162/proxy/: bar (200; 14.724082ms)
Apr 26 14:10:27.223: INFO: (5) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:160/proxy/: foo (200; 15.541497ms)
Apr 26 14:10:27.224: INFO: (5) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:1080/proxy/rewriteme">test<... (200; 16.013906ms)
Apr 26 14:10:27.227: INFO: (5) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:160/proxy/: foo (200; 19.045646ms)
Apr 26 14:10:27.227: INFO: (5) /api/v1/namespaces/proxy-6397/services/http:proxy-service-4jmpl:portname1/proxy/: foo (200; 18.740839ms)
Apr 26 14:10:27.227: INFO: (5) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:443/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:443/proxy/tlsrewritem... (200; 19.927197ms)
Apr 26 14:10:27.227: INFO: (5) /api/v1/namespaces/proxy-6397/services/http:proxy-service-4jmpl:portname2/proxy/: bar (200; 19.647696ms)
Apr 26 14:10:27.228: INFO: (5) /api/v1/namespaces/proxy-6397/services/proxy-service-4jmpl:portname2/proxy/: bar (200; 21.646277ms)
Apr 26 14:10:27.229: INFO: (5) /api/v1/namespaces/proxy-6397/services/https:proxy-service-4jmpl:tlsportname1/proxy/: tls baz (200; 21.156168ms)
Apr 26 14:10:27.229: INFO: (5) /api/v1/namespaces/proxy-6397/services/https:proxy-service-4jmpl:tlsportname2/proxy/: tls qux (200; 20.932912ms)
Apr 26 14:10:27.240: INFO: (6) /api/v1/namespaces/proxy-6397/services/https:proxy-service-4jmpl:tlsportname2/proxy/: tls qux (200; 11.577898ms)
Apr 26 14:10:27.248: INFO: (6) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:162/proxy/: bar (200; 19.302522ms)
Apr 26 14:10:27.248: INFO: (6) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:162/proxy/: bar (200; 19.391041ms)
Apr 26 14:10:27.248: INFO: (6) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:1080/proxy/rewriteme">test<... (200; 19.543626ms)
Apr 26 14:10:27.252: INFO: (6) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:460/proxy/: tls baz (200; 22.5529ms)
Apr 26 14:10:27.252: INFO: (6) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:1080/proxy/rewriteme">... (200; 22.966952ms)
Apr 26 14:10:27.252: INFO: (6) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:462/proxy/: tls qux (200; 22.558623ms)
Apr 26 14:10:27.252: INFO: (6) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:443/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:443/proxy/tlsrewritem... (200; 22.457561ms)
Apr 26 14:10:27.252: INFO: (6) /api/v1/namespaces/proxy-6397/services/http:proxy-service-4jmpl:portname1/proxy/: foo (200; 22.530659ms)
Apr 26 14:10:27.253: INFO: (6) /api/v1/namespaces/proxy-6397/services/proxy-service-4jmpl:portname1/proxy/: foo (200; 24.346512ms)
Apr 26 14:10:27.258: INFO: (6) /api/v1/namespaces/proxy-6397/services/http:proxy-service-4jmpl:portname2/proxy/: bar (200; 28.766464ms)
Apr 26 14:10:27.258: INFO: (6) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2/proxy/rewriteme">test</a> (200; 28.760762ms)
Apr 26 14:10:27.258: INFO: (6) /api/v1/namespaces/proxy-6397/services/proxy-service-4jmpl:portname2/proxy/: bar (200; 28.827836ms)
Apr 26 14:10:27.258: INFO: (6) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:160/proxy/: foo (200; 29.088538ms)
Apr 26 14:10:27.258: INFO: (6) /api/v1/namespaces/proxy-6397/services/https:proxy-service-4jmpl:tlsportname1/proxy/: tls baz (200; 29.216873ms)
Apr 26 14:10:27.260: INFO: (6) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:160/proxy/: foo (200; 31.319576ms)
Apr 26 14:10:27.279: INFO: (7) /api/v1/namespaces/proxy-6397/services/http:proxy-service-4jmpl:portname2/proxy/: bar (200; 18.890695ms)
Apr 26 14:10:27.279: INFO: (7) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:160/proxy/: foo (200; 18.805092ms)
Apr 26 14:10:27.279: INFO: (7) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:1080/proxy/rewriteme">... (200; 18.880662ms)
Apr 26 14:10:27.279: INFO: (7) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:1080/proxy/rewriteme">test<... (200; 18.631015ms)
Apr 26 14:10:27.280: INFO: (7) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:460/proxy/: tls baz (200; 19.165215ms)
Apr 26 14:10:27.280: INFO: (7) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:162/proxy/: bar (200; 18.712727ms)
Apr 26 14:10:27.280: INFO: (7) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:160/proxy/: foo (200; 19.152692ms)
Apr 26 14:10:27.280: INFO: (7) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2/proxy/rewriteme">test</a> (200; 19.502089ms)
Apr 26 14:10:27.287: INFO: (7) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:162/proxy/: bar (200; 26.748354ms)
Apr 26 14:10:27.287: INFO: (7) /api/v1/namespaces/proxy-6397/services/https:proxy-service-4jmpl:tlsportname1/proxy/: tls baz (200; 26.649217ms)
Apr 26 14:10:27.287: INFO: (7) /api/v1/namespaces/proxy-6397/services/https:proxy-service-4jmpl:tlsportname2/proxy/: tls qux (200; 26.442485ms)
Apr 26 14:10:27.287: INFO: (7) /api/v1/namespaces/proxy-6397/services/http:proxy-service-4jmpl:portname1/proxy/: foo (200; 26.554738ms)
Apr 26 14:10:27.288: INFO: (7) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:462/proxy/: tls qux (200; 26.751067ms)
Apr 26 14:10:27.288: INFO: (7) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:443/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:443/proxy/tlsrewritem... (200; 27.041781ms)
Apr 26 14:10:27.288: INFO: (7) /api/v1/namespaces/proxy-6397/services/proxy-service-4jmpl:portname1/proxy/: foo (200; 26.851858ms)
Apr 26 14:10:27.288: INFO: (7) /api/v1/namespaces/proxy-6397/services/proxy-service-4jmpl:portname2/proxy/: bar (200; 27.200126ms)
Apr 26 14:10:27.301: INFO: (8) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:162/proxy/: bar (200; 12.916445ms)
Apr 26 14:10:27.302: INFO: (8) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:1080/proxy/rewriteme">... (200; 13.513492ms)
Apr 26 14:10:27.302: INFO: (8) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:443/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:443/proxy/tlsrewritem... (200; 13.759353ms)
Apr 26 14:10:27.304: INFO: (8) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:162/proxy/: bar (200; 15.551593ms)
Apr 26 14:10:27.304: INFO: (8) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2/proxy/rewriteme">test</a> (200; 15.634709ms)
Apr 26 14:10:27.304: INFO: (8) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:462/proxy/: tls qux (200; 16.378305ms)
Apr 26 14:10:27.313: INFO: (8) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:1080/proxy/rewriteme">test<... (200; 24.563185ms)
Apr 26 14:10:27.322: INFO: (8) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:460/proxy/: tls baz (200; 33.767802ms)
Apr 26 14:10:27.322: INFO: (8) /api/v1/namespaces/proxy-6397/services/http:proxy-service-4jmpl:portname2/proxy/: bar (200; 33.699551ms)
Apr 26 14:10:27.322: INFO: (8) /api/v1/namespaces/proxy-6397/services/https:proxy-service-4jmpl:tlsportname1/proxy/: tls baz (200; 33.552948ms)
Apr 26 14:10:27.322: INFO: (8) /api/v1/namespaces/proxy-6397/services/proxy-service-4jmpl:portname2/proxy/: bar (200; 33.673686ms)
Apr 26 14:10:27.322: INFO: (8) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:160/proxy/: foo (200; 33.553279ms)
Apr 26 14:10:27.322: INFO: (8) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:160/proxy/: foo (200; 33.512335ms)
Apr 26 14:10:27.348: INFO: (8) /api/v1/namespaces/proxy-6397/services/proxy-service-4jmpl:portname1/proxy/: foo (200; 59.493948ms)
Apr 26 14:10:27.348: INFO: (8) /api/v1/namespaces/proxy-6397/services/https:proxy-service-4jmpl:tlsportname2/proxy/: tls qux (200; 59.423304ms)
Apr 26 14:10:27.348: INFO: (8) /api/v1/namespaces/proxy-6397/services/http:proxy-service-4jmpl:portname1/proxy/: foo (200; 59.669239ms)
Apr 26 14:10:27.364: INFO: (9) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:162/proxy/: bar (200; 15.05969ms)
Apr 26 14:10:27.364: INFO: (9) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:1080/proxy/rewriteme">test<... (200; 14.960194ms)
Apr 26 14:10:27.365: INFO: (9) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:162/proxy/: bar (200; 16.790421ms)
Apr 26 14:10:27.367: INFO: (9) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:160/proxy/: foo (200; 18.119566ms)
Apr 26 14:10:27.367: INFO: (9) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:460/proxy/: tls baz (200; 17.862749ms)
Apr 26 14:10:27.367: INFO: (9) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:1080/proxy/rewriteme">... (200; 17.745797ms)
Apr 26 14:10:27.367: INFO: (9) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:443/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:443/proxy/tlsrewritem... (200; 18.235807ms)
Apr 26 14:10:27.367: INFO: (9) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:160/proxy/: foo (200; 18.186322ms)
Apr 26 14:10:27.367: INFO: (9) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:462/proxy/: tls qux (200; 18.272561ms)
Apr 26 14:10:27.369: INFO: (9) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2/proxy/rewriteme">test</a> (200; 20.279738ms)
Apr 26 14:10:27.371: INFO: (9) /api/v1/namespaces/proxy-6397/services/proxy-service-4jmpl:portname2/proxy/: bar (200; 22.153702ms)
Apr 26 14:10:27.371: INFO: (9) /api/v1/namespaces/proxy-6397/services/https:proxy-service-4jmpl:tlsportname1/proxy/: tls baz (200; 21.955206ms)
Apr 26 14:10:27.411: INFO: (9) /api/v1/namespaces/proxy-6397/services/https:proxy-service-4jmpl:tlsportname2/proxy/: tls qux (200; 62.341547ms)
Apr 26 14:10:27.411: INFO: (9) /api/v1/namespaces/proxy-6397/services/proxy-service-4jmpl:portname1/proxy/: foo (200; 62.256287ms)
Apr 26 14:10:27.416: INFO: (9) /api/v1/namespaces/proxy-6397/services/http:proxy-service-4jmpl:portname1/proxy/: foo (200; 66.771977ms)
Apr 26 14:10:27.455: INFO: (9) /api/v1/namespaces/proxy-6397/services/http:proxy-service-4jmpl:portname2/proxy/: bar (200; 106.205672ms)
Apr 26 14:10:27.480: INFO: (10) /api/v1/namespaces/proxy-6397/services/proxy-service-4jmpl:portname2/proxy/: bar (200; 24.427499ms)
Apr 26 14:10:27.483: INFO: (10) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2/proxy/rewriteme">test</a> (200; 27.27785ms)
Apr 26 14:10:27.483: INFO: (10) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:160/proxy/: foo (200; 27.382694ms)
Apr 26 14:10:27.483: INFO: (10) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:1080/proxy/rewriteme">... (200; 27.323359ms)
Apr 26 14:10:27.483: INFO: (10) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:162/proxy/: bar (200; 27.464548ms)
Apr 26 14:10:27.483: INFO: (10) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:443/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:443/proxy/tlsrewritem... (200; 27.870161ms)
Apr 26 14:10:27.483: INFO: (10) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:160/proxy/: foo (200; 27.194018ms)
Apr 26 14:10:27.491: INFO: (10) /api/v1/namespaces/proxy-6397/services/http:proxy-service-4jmpl:portname1/proxy/: foo (200; 35.009869ms)
Apr 26 14:10:27.491: INFO: (10) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:1080/proxy/rewriteme">test<... (200; 34.934642ms)
Apr 26 14:10:27.491: INFO: (10) /api/v1/namespaces/proxy-6397/services/https:proxy-service-4jmpl:tlsportname1/proxy/: tls baz (200; 34.778197ms)
Apr 26 14:10:27.491: INFO: (10) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:462/proxy/: tls qux (200; 34.856884ms)
Apr 26 14:10:27.491: INFO: (10) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:460/proxy/: tls baz (200; 35.460254ms)
Apr 26 14:10:27.491: INFO: (10) /api/v1/namespaces/proxy-6397/services/http:proxy-service-4jmpl:portname2/proxy/: bar (200; 35.148577ms)
Apr 26 14:10:27.491: INFO: (10) /api/v1/namespaces/proxy-6397/services/https:proxy-service-4jmpl:tlsportname2/proxy/: tls qux (200; 35.325994ms)
Apr 26 14:10:27.491: INFO: (10) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:162/proxy/: bar (200; 35.329492ms)
Apr 26 14:10:27.492: INFO: (10) /api/v1/namespaces/proxy-6397/services/proxy-service-4jmpl:portname1/proxy/: foo (200; 35.953982ms)
Apr 26 14:10:27.504: INFO: (11) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:1080/proxy/rewriteme">... (200; 11.800629ms)
Apr 26 14:10:27.506: INFO: (11) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2/proxy/rewriteme">test</a> (200; 13.334175ms)
Apr 26 14:10:27.510: INFO: (11) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:160/proxy/: foo (200; 18.299362ms)
Apr 26 14:10:27.510: INFO: (11) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:1080/proxy/rewriteme">test<... (200; 18.226922ms)
Apr 26 14:10:27.511: INFO: (11) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:460/proxy/: tls baz (200; 18.354177ms)
Apr 26 14:10:27.511: INFO: (11) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:160/proxy/: foo (200; 17.952778ms)
Apr 26 14:10:27.511: INFO: (11) /api/v1/namespaces/proxy-6397/services/https:proxy-service-4jmpl:tlsportname2/proxy/: tls qux (200; 18.870054ms)
Apr 26 14:10:27.511: INFO: (11) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:443/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:443/proxy/tlsrewritem... (200; 18.641983ms)
Apr 26 14:10:27.511: INFO: (11) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:462/proxy/: tls qux (200; 18.605992ms)
Apr 26 14:10:27.511: INFO: (11) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:162/proxy/: bar (200; 19.090059ms)
Apr 26 14:10:27.511: INFO: (11) /api/v1/namespaces/proxy-6397/services/proxy-service-4jmpl:portname2/proxy/: bar (200; 19.060433ms)
Apr 26 14:10:27.512: INFO: (11) /api/v1/namespaces/proxy-6397/services/https:proxy-service-4jmpl:tlsportname1/proxy/: tls baz (200; 19.534327ms)
Apr 26 14:10:27.513: INFO: (11) /api/v1/namespaces/proxy-6397/services/http:proxy-service-4jmpl:portname1/proxy/: foo (200; 20.48949ms)
Apr 26 14:10:27.520: INFO: (11) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:162/proxy/: bar (200; 27.296327ms)
Apr 26 14:10:27.546: INFO: (11) /api/v1/namespaces/proxy-6397/services/http:proxy-service-4jmpl:portname2/proxy/: bar (200; 53.352689ms)
Apr 26 14:10:27.548: INFO: (11) /api/v1/namespaces/proxy-6397/services/proxy-service-4jmpl:portname1/proxy/: foo (200; 55.622458ms)
Apr 26 14:10:27.560: INFO: (12) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:160/proxy/: foo (200; 11.066541ms)
Apr 26 14:10:27.562: INFO: (12) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:1080/proxy/rewriteme">test<... (200; 13.648132ms)
Apr 26 14:10:27.562: INFO: (12) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:162/proxy/: bar (200; 13.796997ms)
Apr 26 14:10:27.567: INFO: (12) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:162/proxy/: bar (200; 18.308777ms)
Apr 26 14:10:27.567: INFO: (12) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:462/proxy/: tls qux (200; 18.174004ms)
Apr 26 14:10:27.567: INFO: (12) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:460/proxy/: tls baz (200; 18.546578ms)
Apr 26 14:10:27.567: INFO: (12) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2/proxy/rewriteme">test</a> (200; 18.42685ms)
Apr 26 14:10:27.569: INFO: (12) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:443/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:443/proxy/tlsrewritem... (200; 20.608633ms)
Apr 26 14:10:27.569: INFO: (12) /api/v1/namespaces/proxy-6397/services/http:proxy-service-4jmpl:portname1/proxy/: foo (200; 20.406483ms)
Apr 26 14:10:27.569: INFO: (12) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:160/proxy/: foo (200; 20.549751ms)
Apr 26 14:10:27.569: INFO: (12) /api/v1/namespaces/proxy-6397/services/http:proxy-service-4jmpl:portname2/proxy/: bar (200; 20.928859ms)
Apr 26 14:10:27.569: INFO: (12) /api/v1/namespaces/proxy-6397/services/https:proxy-service-4jmpl:tlsportname1/proxy/: tls baz (200; 20.795798ms)
Apr 26 14:10:27.569: INFO: (12) /api/v1/namespaces/proxy-6397/services/https:proxy-service-4jmpl:tlsportname2/proxy/: tls qux (200; 21.220346ms)
Apr 26 14:10:27.569: INFO: (12) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:1080/proxy/rewriteme">... (200; 21.251646ms)
Apr 26 14:10:27.574: INFO: (12) /api/v1/namespaces/proxy-6397/services/proxy-service-4jmpl:portname1/proxy/: foo (200; 25.122328ms)
Apr 26 14:10:27.574: INFO: (12) /api/v1/namespaces/proxy-6397/services/proxy-service-4jmpl:portname2/proxy/: bar (200; 25.338472ms)
Apr 26 14:10:27.589: INFO: (13) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2/proxy/rewriteme">test</a> (200; 14.583164ms)
Apr 26 14:10:27.590: INFO: (13) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:162/proxy/: bar (200; 14.067839ms)
Apr 26 14:10:27.590: INFO: (13) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:443/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:443/proxy/tlsrewritem... (200; 15.344018ms)
Apr 26 14:10:27.590: INFO: (13) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:462/proxy/: tls qux (200; 15.520153ms)
Apr 26 14:10:27.590: INFO: (13) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:162/proxy/: bar (200; 15.266598ms)
Apr 26 14:10:27.590: INFO: (13) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:160/proxy/: foo (200; 14.745499ms)
Apr 26 14:10:27.590: INFO: (13) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:160/proxy/: foo (200; 14.891471ms)
Apr 26 14:10:27.590: INFO: (13) /api/v1/namespaces/proxy-6397/services/https:proxy-service-4jmpl:tlsportname1/proxy/: tls baz (200; 14.708757ms)
Apr 26 14:10:27.590: INFO: (13) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:460/proxy/: tls baz (200; 14.321831ms)
Apr 26 14:10:27.590: INFO: (13) /api/v1/namespaces/proxy-6397/services/https:proxy-service-4jmpl:tlsportname2/proxy/: tls qux (200; 14.897879ms)
Apr 26 14:10:27.591: INFO: (13) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:1080/proxy/rewriteme">... (200; 15.695193ms)
Apr 26 14:10:27.631: INFO: (13) /api/v1/namespaces/proxy-6397/services/proxy-service-4jmpl:portname2/proxy/: bar (200; 54.984319ms)
Apr 26 14:10:27.631: INFO: (13) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:1080/proxy/rewriteme">test<... (200; 54.960445ms)
Apr 26 14:10:27.631: INFO: (13) /api/v1/namespaces/proxy-6397/services/proxy-service-4jmpl:portname1/proxy/: foo (200; 56.801599ms)
Apr 26 14:10:27.631: INFO: (13) /api/v1/namespaces/proxy-6397/services/http:proxy-service-4jmpl:portname2/proxy/: bar (200; 56.110457ms)
Apr 26 14:10:27.631: INFO: (13) /api/v1/namespaces/proxy-6397/services/http:proxy-service-4jmpl:portname1/proxy/: foo (200; 55.829906ms)
Apr 26 14:10:27.646: INFO: (14) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:462/proxy/: tls qux (200; 14.692794ms)
Apr 26 14:10:27.646: INFO: (14) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:162/proxy/: bar (200; 14.367075ms)
Apr 26 14:10:27.646: INFO: (14) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:1080/proxy/rewriteme">... (200; 14.953552ms)
Apr 26 14:10:27.646: INFO: (14) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:460/proxy/: tls baz (200; 14.411225ms)
Apr 26 14:10:27.648: INFO: (14) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:162/proxy/: bar (200; 15.839317ms)
Apr 26 14:10:27.648: INFO: (14) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2/proxy/rewriteme">test</a> (200; 16.538194ms)
Apr 26 14:10:27.650: INFO: (14) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:160/proxy/: foo (200; 18.723585ms)
Apr 26 14:10:27.650: INFO: (14) /api/v1/namespaces/proxy-6397/services/http:proxy-service-4jmpl:portname1/proxy/: foo (200; 18.348293ms)
Apr 26 14:10:27.651: INFO: (14) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:1080/proxy/rewriteme">test<... (200; 18.924192ms)
Apr 26 14:10:27.651: INFO: (14) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:443/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:443/proxy/tlsrewritem... (200; 19.595901ms)
Apr 26 14:10:27.654: INFO: (14) /api/v1/namespaces/proxy-6397/services/http:proxy-service-4jmpl:portname2/proxy/: bar (200; 22.255279ms)
Apr 26 14:10:27.656: INFO: (14) /api/v1/namespaces/proxy-6397/services/https:proxy-service-4jmpl:tlsportname2/proxy/: tls qux (200; 23.675615ms)
Apr 26 14:10:27.656: INFO: (14) /api/v1/namespaces/proxy-6397/services/https:proxy-service-4jmpl:tlsportname1/proxy/: tls baz (200; 23.953769ms)
Apr 26 14:10:27.662: INFO: (14) /api/v1/namespaces/proxy-6397/services/proxy-service-4jmpl:portname2/proxy/: bar (200; 30.024075ms)
Apr 26 14:10:27.662: INFO: (14) /api/v1/namespaces/proxy-6397/services/proxy-service-4jmpl:portname1/proxy/: foo (200; 30.148104ms)
Apr 26 14:10:27.662: INFO: (14) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:160/proxy/: foo (200; 30.744438ms)
Apr 26 14:10:27.674: INFO: (15) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:1080/proxy/rewriteme">... (200; 11.145029ms)
Apr 26 14:10:27.676: INFO: (15) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2/proxy/rewriteme">test</a> (200; 13.158702ms)
Apr 26 14:10:27.677: INFO: (15) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:162/proxy/: bar (200; 12.841781ms)
Apr 26 14:10:27.677: INFO: (15) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:443/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:443/proxy/tlsrewritem... (200; 13.091041ms)
Apr 26 14:10:27.677: INFO: (15) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:460/proxy/: tls baz (200; 14.003878ms)
Apr 26 14:10:27.679: INFO: (15) /api/v1/namespaces/proxy-6397/services/proxy-service-4jmpl:portname2/proxy/: bar (200; 15.379223ms)
Apr 26 14:10:27.681: INFO: (15) /api/v1/namespaces/proxy-6397/services/https:proxy-service-4jmpl:tlsportname1/proxy/: tls baz (200; 17.639235ms)
Apr 26 14:10:27.681: INFO: (15) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:1080/proxy/rewriteme">test<... (200; 17.164891ms)
Apr 26 14:10:27.681: INFO: (15) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:462/proxy/: tls qux (200; 17.368608ms)
Apr 26 14:10:27.681: INFO: (15) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:160/proxy/: foo (200; 17.782949ms)
Apr 26 14:10:27.681: INFO: (15) /api/v1/namespaces/proxy-6397/services/proxy-service-4jmpl:portname1/proxy/: foo (200; 17.594238ms)
Apr 26 14:10:27.682: INFO: (15) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:162/proxy/: bar (200; 18.52598ms)
Apr 26 14:10:27.682: INFO: (15) /api/v1/namespaces/proxy-6397/services/https:proxy-service-4jmpl:tlsportname2/proxy/: tls qux (200; 18.608018ms)
Apr 26 14:10:27.686: INFO: (15) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:160/proxy/: foo (200; 22.588814ms)
Apr 26 14:10:27.686: INFO: (15) /api/v1/namespaces/proxy-6397/services/http:proxy-service-4jmpl:portname2/proxy/: bar (200; 23.378338ms)
Apr 26 14:10:27.688: INFO: (15) /api/v1/namespaces/proxy-6397/services/http:proxy-service-4jmpl:portname1/proxy/: foo (200; 23.986348ms)
Apr 26 14:10:27.701: INFO: (16) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:160/proxy/: foo (200; 12.885879ms)
Apr 26 14:10:27.701: INFO: (16) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:162/proxy/: bar (200; 13.274357ms)
Apr 26 14:10:27.701: INFO: (16) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:462/proxy/: tls qux (200; 13.234074ms)
Apr 26 14:10:27.701: INFO: (16) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:443/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:443/proxy/tlsrewritem... (200; 13.268922ms)
Apr 26 14:10:27.703: INFO: (16) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:160/proxy/: foo (200; 14.582086ms)
Apr 26 14:10:27.704: INFO: (16) /api/v1/namespaces/proxy-6397/services/https:proxy-service-4jmpl:tlsportname1/proxy/: tls baz (200; 16.382106ms)
Apr 26 14:10:27.705: INFO: (16) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:162/proxy/: bar (200; 16.675678ms)
Apr 26 14:10:27.705: INFO: (16) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:1080/proxy/rewriteme">... (200; 17.135183ms)
Apr 26 14:10:27.706: INFO: (16) /api/v1/namespaces/proxy-6397/services/https:proxy-service-4jmpl:tlsportname2/proxy/: tls qux (200; 18.066145ms)
Apr 26 14:10:27.708: INFO: (16) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2/proxy/rewriteme">test</a> (200; 19.77712ms)
Apr 26 14:10:27.708: INFO: (16) /api/v1/namespaces/proxy-6397/services/http:proxy-service-4jmpl:portname1/proxy/: foo (200; 19.701119ms)
Apr 26 14:10:27.710: INFO: (16) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:460/proxy/: tls baz (200; 21.367375ms)
Apr 26 14:10:27.710: INFO: (16) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:1080/proxy/rewriteme">test<... (200; 21.748456ms)
Apr 26 14:10:27.710: INFO: (16) /api/v1/namespaces/proxy-6397/services/http:proxy-service-4jmpl:portname2/proxy/: bar (200; 21.692847ms)
Apr 26 14:10:27.742: INFO: (16) /api/v1/namespaces/proxy-6397/services/proxy-service-4jmpl:portname2/proxy/: bar (200; 54.400359ms)
Apr 26 14:10:27.742: INFO: (16) /api/v1/namespaces/proxy-6397/services/proxy-service-4jmpl:portname1/proxy/: foo (200; 53.988274ms)
Apr 26 14:10:27.755: INFO: (17) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:160/proxy/: foo (200; 12.40013ms)
Apr 26 14:10:27.758: INFO: (17) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:162/proxy/: bar (200; 15.341126ms)
Apr 26 14:10:27.758: INFO: (17) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:443/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:443/proxy/tlsrewritem... (200; 15.116975ms)
Apr 26 14:10:27.758: INFO: (17) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2/proxy/rewriteme">test</a> (200; 15.259418ms)
Apr 26 14:10:27.759: INFO: (17) /api/v1/namespaces/proxy-6397/services/http:proxy-service-4jmpl:portname2/proxy/: bar (200; 15.886251ms)
Apr 26 14:10:27.762: INFO: (17) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:160/proxy/: foo (200; 19.540726ms)
Apr 26 14:10:27.762: INFO: (17) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:1080/proxy/rewriteme">test<... (200; 19.782235ms)
Apr 26 14:10:27.762: INFO: (17) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:1080/proxy/rewriteme">... (200; 19.575162ms)
Apr 26 14:10:27.762: INFO: (17) /api/v1/namespaces/proxy-6397/services/https:proxy-service-4jmpl:tlsportname2/proxy/: tls qux (200; 19.726338ms)
Apr 26 14:10:27.762: INFO: (17) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:460/proxy/: tls baz (200; 19.97509ms)
Apr 26 14:10:27.763: INFO: (17) /api/v1/namespaces/proxy-6397/services/proxy-service-4jmpl:portname1/proxy/: foo (200; 20.404377ms)
Apr 26 14:10:27.764: INFO: (17) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:162/proxy/: bar (200; 21.942032ms)
Apr 26 14:10:27.764: INFO: (17) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:462/proxy/: tls qux (200; 21.654032ms)
Apr 26 14:10:27.766: INFO: (17) /api/v1/namespaces/proxy-6397/services/https:proxy-service-4jmpl:tlsportname1/proxy/: tls baz (200; 23.608609ms)
Apr 26 14:10:27.767: INFO: (17) /api/v1/namespaces/proxy-6397/services/proxy-service-4jmpl:portname2/proxy/: bar (200; 24.385627ms)
Apr 26 14:10:27.767: INFO: (17) /api/v1/namespaces/proxy-6397/services/http:proxy-service-4jmpl:portname1/proxy/: foo (200; 24.7242ms)
Apr 26 14:10:27.781: INFO: (18) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:1080/proxy/rewriteme">test<... (200; 12.853736ms)
Apr 26 14:10:27.783: INFO: (18) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:162/proxy/: bar (200; 14.133242ms)
Apr 26 14:10:27.783: INFO: (18) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:162/proxy/: bar (200; 14.6947ms)
Apr 26 14:10:27.784: INFO: (18) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:160/proxy/: foo (200; 16.049875ms)
Apr 26 14:10:27.784: INFO: (18) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2/proxy/rewriteme">test</a> (200; 14.771972ms)
Apr 26 14:10:27.784: INFO: (18) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:460/proxy/: tls baz (200; 15.481343ms)
Apr 26 14:10:27.784: INFO: (18) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:160/proxy/: foo (200; 16.762268ms)
Apr 26 14:10:27.785: INFO: (18) /api/v1/namespaces/proxy-6397/services/https:proxy-service-4jmpl:tlsportname1/proxy/: tls baz (200; 17.155204ms)
Apr 26 14:10:27.785: INFO: (18) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:1080/proxy/rewriteme">... (200; 17.618586ms)
Apr 26 14:10:27.787: INFO: (18) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:443/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:443/proxy/tlsrewritem... (200; 17.647342ms)
Apr 26 14:10:27.787: INFO: (18) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:462/proxy/: tls qux (200; 19.113139ms)
Apr 26 14:10:27.787: INFO: (18) /api/v1/namespaces/proxy-6397/services/https:proxy-service-4jmpl:tlsportname2/proxy/: tls qux (200; 19.252593ms)
Apr 26 14:10:27.788: INFO: (18) /api/v1/namespaces/proxy-6397/services/proxy-service-4jmpl:portname1/proxy/: foo (200; 19.631215ms)
Apr 26 14:10:27.788: INFO: (18) /api/v1/namespaces/proxy-6397/services/proxy-service-4jmpl:portname2/proxy/: bar (200; 19.455431ms)
Apr 26 14:10:27.788: INFO: (18) /api/v1/namespaces/proxy-6397/services/http:proxy-service-4jmpl:portname2/proxy/: bar (200; 18.550221ms)
Apr 26 14:10:27.789: INFO: (18) /api/v1/namespaces/proxy-6397/services/http:proxy-service-4jmpl:portname1/proxy/: foo (200; 21.054557ms)
Apr 26 14:10:27.809: INFO: (19) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:160/proxy/: foo (200; 19.595975ms)
Apr 26 14:10:27.809: INFO: (19) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2/proxy/rewriteme">test</a> (200; 19.706147ms)
Apr 26 14:10:27.809: INFO: (19) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:1080/proxy/rewriteme">test<... (200; 19.501781ms)
Apr 26 14:10:27.810: INFO: (19) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:1080/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:1080/proxy/rewriteme">... (200; 21.103907ms)
Apr 26 14:10:27.811: INFO: (19) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:460/proxy/: tls baz (200; 21.370999ms)
Apr 26 14:10:27.811: INFO: (19) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:443/proxy/: <a href="/api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:443/proxy/tlsrewritem... (200; 21.297912ms)
Apr 26 14:10:27.811: INFO: (19) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:162/proxy/: bar (200; 21.340835ms)
Apr 26 14:10:27.811: INFO: (19) /api/v1/namespaces/proxy-6397/services/http:proxy-service-4jmpl:portname2/proxy/: bar (200; 21.57844ms)
Apr 26 14:10:27.849: INFO: (19) /api/v1/namespaces/proxy-6397/services/http:proxy-service-4jmpl:portname1/proxy/: foo (200; 59.914213ms)
Apr 26 14:10:27.849: INFO: (19) /api/v1/namespaces/proxy-6397/pods/proxy-service-4jmpl-kx5j2:160/proxy/: foo (200; 60.015444ms)
Apr 26 14:10:27.849: INFO: (19) /api/v1/namespaces/proxy-6397/services/https:proxy-service-4jmpl:tlsportname1/proxy/: tls baz (200; 60.060072ms)
Apr 26 14:10:27.849: INFO: (19) /api/v1/namespaces/proxy-6397/services/https:proxy-service-4jmpl:tlsportname2/proxy/: tls qux (200; 60.00754ms)
Apr 26 14:10:27.849: INFO: (19) /api/v1/namespaces/proxy-6397/pods/https:proxy-service-4jmpl-kx5j2:462/proxy/: tls qux (200; 60.411141ms)
Apr 26 14:10:27.890: INFO: (19) /api/v1/namespaces/proxy-6397/pods/http:proxy-service-4jmpl-kx5j2:162/proxy/: bar (200; 100.6028ms)
Apr 26 14:10:27.890: INFO: (19) /api/v1/namespaces/proxy-6397/services/proxy-service-4jmpl:portname2/proxy/: bar (200; 100.76097ms)
Apr 26 14:10:27.896: INFO: (19) /api/v1/namespaces/proxy-6397/services/proxy-service-4jmpl:portname1/proxy/: foo (200; 106.861267ms)
STEP: deleting ReplicationController proxy-service-4jmpl in namespace proxy-6397, will wait for the garbage collector to delete the pods
Apr 26 14:10:27.969: INFO: Deleting ReplicationController proxy-service-4jmpl took: 15.714181ms
Apr 26 14:10:28.570: INFO: Terminating ReplicationController proxy-service-4jmpl pods took: 600.668916ms
[AfterEach] version v1
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:10:35.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6397" for this suite.

• [SLOW TEST:16.911 seconds]
[sig-network] Proxy
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":303,"completed":101,"skipped":1494,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:10:35.790: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Apr 26 14:10:35.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 create -f - --namespace=kubectl-450'
Apr 26 14:10:36.688: INFO: stderr: ""
Apr 26 14:10:36.688: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Apr 26 14:10:36.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 create -f - --namespace=kubectl-450'
Apr 26 14:10:37.023: INFO: stderr: ""
Apr 26 14:10:37.023: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Apr 26 14:10:38.045: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 26 14:10:38.045: INFO: Found 0 / 1
Apr 26 14:10:39.030: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 26 14:10:39.030: INFO: Found 1 / 1
Apr 26 14:10:39.030: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr 26 14:10:39.037: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 26 14:10:39.037: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 26 14:10:39.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 describe pod agnhost-primary-j9z4k --namespace=kubectl-450'
Apr 26 14:10:39.145: INFO: stderr: ""
Apr 26 14:10:39.145: INFO: stdout: "Name:         agnhost-primary-j9z4k\nNamespace:    kubectl-450\nPriority:     0\nNode:         elastic-hugle-5f9d9b5555-8444w/192.168.1.11\nStart Time:   Mon, 26 Apr 2021 14:10:36 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/podIP: 172.25.1.110/32\nStatus:       Running\nIP:           172.25.1.110\nIPs:\n  IP:           172.25.1.110\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://05cc869be629f67901560dfb5aa5bccbdbaf3f329285780d22f77347a170e3e8\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Image ID:       docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 26 Apr 2021 14:10:38 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-7xkg8 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-7xkg8:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-7xkg8\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-450/agnhost-primary-j9z4k to elastic-hugle-5f9d9b5555-8444w\n  Normal  Pulled     2s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.20\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Apr 26 14:10:39.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 describe rc agnhost-primary --namespace=kubectl-450'
Apr 26 14:10:39.265: INFO: stderr: ""
Apr 26 14:10:39.265: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-450\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-j9z4k\n"
Apr 26 14:10:39.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 describe service agnhost-primary --namespace=kubectl-450'
Apr 26 14:10:39.377: INFO: stderr: ""
Apr 26 14:10:39.377: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-450\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP:                10.240.30.114\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.25.1.110:6379\nSession Affinity:  None\nEvents:            <none>\n"
Apr 26 14:10:39.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 describe node elastic-hugle-5f9d9b5555-8444w'
Apr 26 14:10:39.542: INFO: stderr: ""
Apr 26 14:10:39.543: INFO: stdout: "Name:               elastic-hugle-5f9d9b5555-8444w\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b7c4fa0b-7960-4311-a86b-507dbf58e8ac\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/zone=es1\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=elastic-hugle-5f9d9b5555-8444w\n                    kubernetes.io/os=linux\n                    machine-controller/owned-by=46e5351a-9520-486c-b88c-859c35161168\n                    node.kubernetes.io/instance-type=b7c4fa0b-7960-4311-a86b-507dbf58e8ac\n                    system/cluster=jw8q56b9sv\n                    system/project=dn78vbds2w\n                    topology.cinder.csi.openstack.org/zone=es1\n                    topology.kubernetes.io/zone=es1\n                    x-kubernetes.io/distribution=flatcar\nAnnotations:        alpha.kubernetes.io/provided-node-ip: 192.168.1.11\n                    cluster.k8s.io/machine: kube-system/elastic-hugle-5f9d9b5555-8444w\n                    csi.volume.kubernetes.io/nodeid: {\"cinder.csi.openstack.org\":\"75cd987e-0bdc-44c4-8e42-3a65163a2f07\"}\n                    flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"92:2b:f0:3e:08:7a\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 192.168.1.11\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.25.1.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 26 Apr 2021 13:19:15 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  elastic-hugle-5f9d9b5555-8444w\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 26 Apr 2021 14:10:35 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Mon, 26 Apr 2021 14:09:30 +0000   Mon, 26 Apr 2021 13:19:15 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Mon, 26 Apr 2021 14:09:30 +0000   Mon, 26 Apr 2021 13:19:15 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Mon, 26 Apr 2021 14:09:30 +0000   Mon, 26 Apr 2021 13:19:15 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Mon, 26 Apr 2021 14:09:30 +0000   Mon, 26 Apr 2021 13:19:37 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.1.11\n  Hostname:    elastic-hugle-5f9d9b5555-8444w\nCapacity:\n  cpu:                  2\n  ephemeral-storage:    17897500Ki\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               4039560Ki\n  pods:                 110\nAllocatable:\n  cpu:                  1800m\n  ephemeral-storage:    14346852325\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               3732360Ki\n  pods:                 110\nSystem Info:\n  Machine ID:                 75cd987e0bdc44c48e423a65163a2f07\n  System UUID:                75cd987e-0bdc-44c4-8e42-3a65163a2f07\n  Boot ID:                    818811f2-5b7b-45db-9917-b664465ee4b9\n  Kernel Version:             4.19.124-flatcar\n  OS Image:                   Flatcar Container Linux by Kinvolk 2512.2.0 (Oklo)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://18.6.3\n  Kubelet Version:            v1.19.4\n  Kube-Proxy Version:         v1.19.4\nPodCIDR:                      172.25.1.0/24\nPodCIDRs:                     172.25.1.0/24\nProviderID:                   openstack:///75cd987e-0bdc-44c4-8e42-3a65163a2f07\nNon-terminated Pods:          (11 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 canal-9hndk                                                250m (13%)    0 (0%)      0 (0%)           0 (0%)         51m\n  kube-system                 coredns-85c978995d-bmmb9                                   50m (2%)      100m (5%)   32Mi (0%)        64Mi (1%)      53m\n  kube-system                 csi-cinder-nodeplugin-flatcar-nbzzj                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         51m\n  kube-system                 kube-proxy-qvhqj                                           75m (4%)      250m (13%)  50Mi (1%)        250Mi (6%)     51m\n  kube-system                 logrotate-p98vn                                            75m (4%)      250m (13%)  50Mi (1%)        250Mi (6%)     51m\n  kube-system                 node-local-dns-pfs8r                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         51m\n  kube-system                 user-ssh-keys-agent-vck9l                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         51m\n  kubectl-450                 agnhost-primary-j9z4k                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         3s\n  kubernetes-dashboard        dashboard-metrics-scraper-975c84c89-fc9kf                  50m (2%)      100m (5%)   32Mi (0%)        64Mi (1%)      53m\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         37m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-83692f989abe48af-6hhl5    0 (0%)        0 (0%)      0 (0%)           0 (0%)         37m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource             Requests    Limits\n  --------             --------    ------\n  cpu                  500m (27%)  700m (38%)\n  memory               164Mi (4%)  628Mi (17%)\n  ephemeral-storage    0 (0%)      0 (0%)\n  hugepages-1Gi        0 (0%)      0 (0%)\n  hugepages-2Mi        0 (0%)      0 (0%)\n  example.com/fakecpu  0           0\nEvents:\n  Type    Reason                   Age                From        Message\n  ----    ------                   ----               ----        -------\n  Normal  Starting                 51m                kubelet     Starting kubelet.\n  Normal  NodeHasSufficientMemory  51m (x2 over 51m)  kubelet     Node elastic-hugle-5f9d9b5555-8444w status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    51m (x2 over 51m)  kubelet     Node elastic-hugle-5f9d9b5555-8444w status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     51m (x2 over 51m)  kubelet     Node elastic-hugle-5f9d9b5555-8444w status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  51m                kubelet     Updated Node Allocatable limit across pods\n  Normal  Starting                 51m                kube-proxy  Starting kube-proxy.\n  Normal  NodeReady                51m                kubelet     Node elastic-hugle-5f9d9b5555-8444w status is now: NodeReady\n"
Apr 26 14:10:39.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 describe namespace kubectl-450'
Apr 26 14:10:39.661: INFO: stderr: ""
Apr 26 14:10:39.661: INFO: stdout: "Name:         kubectl-450\nLabels:       e2e-framework=kubectl\n              e2e-run=5015cbb9-27fc-4e0a-981d-c5508c00e05b\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:10:39.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-450" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":303,"completed":102,"skipped":1495,"failed":0}
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:10:39.692: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-c1c574fa-38d3-4a6e-869d-66a2eb527716
STEP: Creating a pod to test consume secrets
Apr 26 14:10:39.790: INFO: Waiting up to 5m0s for pod "pod-secrets-f3c012d5-8502-4443-a38e-a79a791b100f" in namespace "secrets-2495" to be "Succeeded or Failed"
Apr 26 14:10:39.796: INFO: Pod "pod-secrets-f3c012d5-8502-4443-a38e-a79a791b100f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.507648ms
Apr 26 14:10:41.805: INFO: Pod "pod-secrets-f3c012d5-8502-4443-a38e-a79a791b100f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015050348s
Apr 26 14:10:43.811: INFO: Pod "pod-secrets-f3c012d5-8502-4443-a38e-a79a791b100f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021388344s
STEP: Saw pod success
Apr 26 14:10:43.812: INFO: Pod "pod-secrets-f3c012d5-8502-4443-a38e-a79a791b100f" satisfied condition "Succeeded or Failed"
Apr 26 14:10:43.817: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod pod-secrets-f3c012d5-8502-4443-a38e-a79a791b100f container secret-volume-test: <nil>
STEP: delete the pod
Apr 26 14:10:43.854: INFO: Waiting for pod pod-secrets-f3c012d5-8502-4443-a38e-a79a791b100f to disappear
Apr 26 14:10:43.858: INFO: Pod pod-secrets-f3c012d5-8502-4443-a38e-a79a791b100f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:10:43.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2495" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":103,"skipped":1499,"failed":0}

------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:10:43.878: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-32afb592-3477-4e52-9514-e161261a74a2 in namespace container-probe-7968
Apr 26 14:10:47.962: INFO: Started pod liveness-32afb592-3477-4e52-9514-e161261a74a2 in namespace container-probe-7968
STEP: checking the pod's current state and verifying that restartCount is present
Apr 26 14:10:47.967: INFO: Initial restart count of pod liveness-32afb592-3477-4e52-9514-e161261a74a2 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:14:48.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7968" for this suite.

• [SLOW TEST:245.095 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":303,"completed":104,"skipped":1499,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:14:48.975: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Apr 26 14:14:49.037: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:14:55.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1510" for this suite.

• [SLOW TEST:6.577 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":303,"completed":105,"skipped":1508,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:14:55.553: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service endpoint-test2 in namespace services-6105
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6105 to expose endpoints map[]
Apr 26 14:14:55.688: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Apr 26 14:14:56.708: INFO: successfully validated that service endpoint-test2 in namespace services-6105 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-6105
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6105 to expose endpoints map[pod1:[80]]
Apr 26 14:14:59.773: INFO: successfully validated that service endpoint-test2 in namespace services-6105 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-6105
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6105 to expose endpoints map[pod1:[80] pod2:[80]]
Apr 26 14:15:02.845: INFO: successfully validated that service endpoint-test2 in namespace services-6105 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-6105
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6105 to expose endpoints map[pod2:[80]]
Apr 26 14:15:02.895: INFO: successfully validated that service endpoint-test2 in namespace services-6105 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-6105
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6105 to expose endpoints map[]
Apr 26 14:15:02.934: INFO: successfully validated that service endpoint-test2 in namespace services-6105 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:15:02.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6105" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:7.436 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":303,"completed":106,"skipped":1531,"failed":0}
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:15:02.989: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-downwardapi-l9vl
STEP: Creating a pod to test atomic-volume-subpath
Apr 26 14:15:03.077: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-l9vl" in namespace "subpath-6966" to be "Succeeded or Failed"
Apr 26 14:15:03.084: INFO: Pod "pod-subpath-test-downwardapi-l9vl": Phase="Pending", Reason="", readiness=false. Elapsed: 6.965369ms
Apr 26 14:15:05.091: INFO: Pod "pod-subpath-test-downwardapi-l9vl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01415847s
Apr 26 14:15:07.103: INFO: Pod "pod-subpath-test-downwardapi-l9vl": Phase="Running", Reason="", readiness=true. Elapsed: 4.026127215s
Apr 26 14:15:09.110: INFO: Pod "pod-subpath-test-downwardapi-l9vl": Phase="Running", Reason="", readiness=true. Elapsed: 6.032881018s
Apr 26 14:15:11.116: INFO: Pod "pod-subpath-test-downwardapi-l9vl": Phase="Running", Reason="", readiness=true. Elapsed: 8.039266877s
Apr 26 14:15:13.123: INFO: Pod "pod-subpath-test-downwardapi-l9vl": Phase="Running", Reason="", readiness=true. Elapsed: 10.046256099s
Apr 26 14:15:15.131: INFO: Pod "pod-subpath-test-downwardapi-l9vl": Phase="Running", Reason="", readiness=true. Elapsed: 12.053726053s
Apr 26 14:15:17.147: INFO: Pod "pod-subpath-test-downwardapi-l9vl": Phase="Running", Reason="", readiness=true. Elapsed: 14.070210319s
Apr 26 14:15:19.155: INFO: Pod "pod-subpath-test-downwardapi-l9vl": Phase="Running", Reason="", readiness=true. Elapsed: 16.077597547s
Apr 26 14:15:21.161: INFO: Pod "pod-subpath-test-downwardapi-l9vl": Phase="Running", Reason="", readiness=true. Elapsed: 18.084223706s
Apr 26 14:15:23.167: INFO: Pod "pod-subpath-test-downwardapi-l9vl": Phase="Running", Reason="", readiness=true. Elapsed: 20.090198599s
Apr 26 14:15:25.174: INFO: Pod "pod-subpath-test-downwardapi-l9vl": Phase="Running", Reason="", readiness=true. Elapsed: 22.096825714s
Apr 26 14:15:27.181: INFO: Pod "pod-subpath-test-downwardapi-l9vl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.104011793s
STEP: Saw pod success
Apr 26 14:15:27.181: INFO: Pod "pod-subpath-test-downwardapi-l9vl" satisfied condition "Succeeded or Failed"
Apr 26 14:15:27.186: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod pod-subpath-test-downwardapi-l9vl container test-container-subpath-downwardapi-l9vl: <nil>
STEP: delete the pod
Apr 26 14:15:27.231: INFO: Waiting for pod pod-subpath-test-downwardapi-l9vl to disappear
Apr 26 14:15:27.237: INFO: Pod pod-subpath-test-downwardapi-l9vl no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-l9vl
Apr 26 14:15:27.237: INFO: Deleting pod "pod-subpath-test-downwardapi-l9vl" in namespace "subpath-6966"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:15:27.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6966" for this suite.

• [SLOW TEST:24.275 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":303,"completed":107,"skipped":1535,"failed":0}
S
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:15:27.264: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Apr 26 14:15:27.322: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:15:31.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6466" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":303,"completed":108,"skipped":1536,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:15:31.665: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Apr 26 14:15:31.744: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4b775ac5-8ebd-4884-87d7-b6f9174229e8" in namespace "downward-api-9510" to be "Succeeded or Failed"
Apr 26 14:15:31.758: INFO: Pod "downwardapi-volume-4b775ac5-8ebd-4884-87d7-b6f9174229e8": Phase="Pending", Reason="", readiness=false. Elapsed: 13.917559ms
Apr 26 14:15:33.765: INFO: Pod "downwardapi-volume-4b775ac5-8ebd-4884-87d7-b6f9174229e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020447992s
Apr 26 14:15:35.775: INFO: Pod "downwardapi-volume-4b775ac5-8ebd-4884-87d7-b6f9174229e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03130176s
STEP: Saw pod success
Apr 26 14:15:35.776: INFO: Pod "downwardapi-volume-4b775ac5-8ebd-4884-87d7-b6f9174229e8" satisfied condition "Succeeded or Failed"
Apr 26 14:15:35.781: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod downwardapi-volume-4b775ac5-8ebd-4884-87d7-b6f9174229e8 container client-container: <nil>
STEP: delete the pod
Apr 26 14:15:35.820: INFO: Waiting for pod downwardapi-volume-4b775ac5-8ebd-4884-87d7-b6f9174229e8 to disappear
Apr 26 14:15:35.825: INFO: Pod downwardapi-volume-4b775ac5-8ebd-4884-87d7-b6f9174229e8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:15:35.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9510" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":303,"completed":109,"skipped":1561,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:15:35.845: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Apr 26 14:15:35.900: INFO: Creating ReplicaSet my-hostname-basic-8f3ea5b0-e78d-48d4-b81d-9a7aba3ebe13
Apr 26 14:15:35.917: INFO: Pod name my-hostname-basic-8f3ea5b0-e78d-48d4-b81d-9a7aba3ebe13: Found 0 pods out of 1
Apr 26 14:15:40.934: INFO: Pod name my-hostname-basic-8f3ea5b0-e78d-48d4-b81d-9a7aba3ebe13: Found 1 pods out of 1
Apr 26 14:15:40.934: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-8f3ea5b0-e78d-48d4-b81d-9a7aba3ebe13" is running
Apr 26 14:15:40.947: INFO: Pod "my-hostname-basic-8f3ea5b0-e78d-48d4-b81d-9a7aba3ebe13-b4zpr" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-04-26 14:15:35 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-04-26 14:15:37 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-04-26 14:15:37 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-04-26 14:15:35 +0000 UTC Reason: Message:}])
Apr 26 14:15:40.948: INFO: Trying to dial the pod
Apr 26 14:15:46.079: INFO: Controller my-hostname-basic-8f3ea5b0-e78d-48d4-b81d-9a7aba3ebe13: Got expected result from replica 1 [my-hostname-basic-8f3ea5b0-e78d-48d4-b81d-9a7aba3ebe13-b4zpr]: "my-hostname-basic-8f3ea5b0-e78d-48d4-b81d-9a7aba3ebe13-b4zpr", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:15:46.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1562" for this suite.

• [SLOW TEST:10.262 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":303,"completed":110,"skipped":1571,"failed":0}
SSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:15:46.108: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating pod
Apr 26 14:15:50.217: INFO: Pod pod-hostip-d6d34b2e-c148-4fe8-ae17-c3bc0524d33c has hostIP: 192.168.1.9
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:15:50.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-19" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":303,"completed":111,"skipped":1575,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:15:50.254: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Apr 26 14:15:50.320: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Apr 26 14:15:53.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 --namespace=crd-publish-openapi-6419 create -f -'
Apr 26 14:15:54.756: INFO: stderr: ""
Apr 26 14:15:54.756: INFO: stdout: "e2e-test-crd-publish-openapi-5652-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Apr 26 14:15:54.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 --namespace=crd-publish-openapi-6419 delete e2e-test-crd-publish-openapi-5652-crds test-cr'
Apr 26 14:15:54.902: INFO: stderr: ""
Apr 26 14:15:54.902: INFO: stdout: "e2e-test-crd-publish-openapi-5652-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Apr 26 14:15:54.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 --namespace=crd-publish-openapi-6419 apply -f -'
Apr 26 14:15:55.188: INFO: stderr: ""
Apr 26 14:15:55.188: INFO: stdout: "e2e-test-crd-publish-openapi-5652-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Apr 26 14:15:55.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 --namespace=crd-publish-openapi-6419 delete e2e-test-crd-publish-openapi-5652-crds test-cr'
Apr 26 14:15:55.318: INFO: stderr: ""
Apr 26 14:15:55.318: INFO: stdout: "e2e-test-crd-publish-openapi-5652-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Apr 26 14:15:55.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 explain e2e-test-crd-publish-openapi-5652-crds'
Apr 26 14:15:55.790: INFO: stderr: ""
Apr 26 14:15:55.790: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5652-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:15:59.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6419" for this suite.

• [SLOW TEST:9.265 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":303,"completed":112,"skipped":1681,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:15:59.518: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Apr 26 14:15:59.603: INFO: Waiting up to 5m0s for pod "pod-c85b3d6a-56d0-4b13-97c1-7231351a1670" in namespace "emptydir-5912" to be "Succeeded or Failed"
Apr 26 14:15:59.610: INFO: Pod "pod-c85b3d6a-56d0-4b13-97c1-7231351a1670": Phase="Pending", Reason="", readiness=false. Elapsed: 7.468273ms
Apr 26 14:16:01.618: INFO: Pod "pod-c85b3d6a-56d0-4b13-97c1-7231351a1670": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01555915s
STEP: Saw pod success
Apr 26 14:16:01.618: INFO: Pod "pod-c85b3d6a-56d0-4b13-97c1-7231351a1670" satisfied condition "Succeeded or Failed"
Apr 26 14:16:01.629: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod pod-c85b3d6a-56d0-4b13-97c1-7231351a1670 container test-container: <nil>
STEP: delete the pod
Apr 26 14:16:01.708: INFO: Waiting for pod pod-c85b3d6a-56d0-4b13-97c1-7231351a1670 to disappear
Apr 26 14:16:01.714: INFO: Pod pod-c85b3d6a-56d0-4b13-97c1-7231351a1670 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:16:01.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5912" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":113,"skipped":1682,"failed":0}
SSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:16:01.735: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Apr 26 14:16:01.817: INFO: Waiting up to 5m0s for pod "downward-api-0d4f3feb-170b-4133-80f4-13ec57b014b3" in namespace "downward-api-5877" to be "Succeeded or Failed"
Apr 26 14:16:01.826: INFO: Pod "downward-api-0d4f3feb-170b-4133-80f4-13ec57b014b3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.2968ms
Apr 26 14:16:03.833: INFO: Pod "downward-api-0d4f3feb-170b-4133-80f4-13ec57b014b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015390395s
Apr 26 14:16:05.840: INFO: Pod "downward-api-0d4f3feb-170b-4133-80f4-13ec57b014b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022706177s
STEP: Saw pod success
Apr 26 14:16:05.840: INFO: Pod "downward-api-0d4f3feb-170b-4133-80f4-13ec57b014b3" satisfied condition "Succeeded or Failed"
Apr 26 14:16:05.860: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod downward-api-0d4f3feb-170b-4133-80f4-13ec57b014b3 container dapi-container: <nil>
STEP: delete the pod
Apr 26 14:16:05.920: INFO: Waiting for pod downward-api-0d4f3feb-170b-4133-80f4-13ec57b014b3 to disappear
Apr 26 14:16:05.926: INFO: Pod downward-api-0d4f3feb-170b-4133-80f4-13ec57b014b3 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:16:05.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5877" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":303,"completed":114,"skipped":1686,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:16:05.955: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:16:21.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1628" for this suite.
STEP: Destroying namespace "nsdeletetest-8465" for this suite.
Apr 26 14:16:21.240: INFO: Namespace nsdeletetest-8465 was already deleted
STEP: Destroying namespace "nsdeletetest-351" for this suite.

• [SLOW TEST:15.297 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":303,"completed":115,"skipped":1690,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:16:21.255: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Update Demo
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:308
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Apr 26 14:16:21.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 create -f - --namespace=kubectl-3937'
Apr 26 14:16:21.784: INFO: stderr: ""
Apr 26 14:16:21.784: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 26 14:16:21.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3937'
Apr 26 14:16:21.897: INFO: stderr: ""
Apr 26 14:16:21.897: INFO: stdout: "update-demo-nautilus-6gsd4 update-demo-nautilus-qb6pn "
Apr 26 14:16:21.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 get pods update-demo-nautilus-6gsd4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3937'
Apr 26 14:16:21.994: INFO: stderr: ""
Apr 26 14:16:21.994: INFO: stdout: ""
Apr 26 14:16:21.994: INFO: update-demo-nautilus-6gsd4 is created but not running
Apr 26 14:16:26.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3937'
Apr 26 14:16:27.098: INFO: stderr: ""
Apr 26 14:16:27.098: INFO: stdout: "update-demo-nautilus-6gsd4 update-demo-nautilus-qb6pn "
Apr 26 14:16:27.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 get pods update-demo-nautilus-6gsd4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3937'
Apr 26 14:16:27.235: INFO: stderr: ""
Apr 26 14:16:27.235: INFO: stdout: "true"
Apr 26 14:16:27.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 get pods update-demo-nautilus-6gsd4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3937'
Apr 26 14:16:27.343: INFO: stderr: ""
Apr 26 14:16:27.343: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 26 14:16:27.343: INFO: validating pod update-demo-nautilus-6gsd4
Apr 26 14:16:27.481: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 26 14:16:27.481: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 26 14:16:27.481: INFO: update-demo-nautilus-6gsd4 is verified up and running
Apr 26 14:16:27.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 get pods update-demo-nautilus-qb6pn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3937'
Apr 26 14:16:27.576: INFO: stderr: ""
Apr 26 14:16:27.576: INFO: stdout: "true"
Apr 26 14:16:27.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 get pods update-demo-nautilus-qb6pn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3937'
Apr 26 14:16:27.656: INFO: stderr: ""
Apr 26 14:16:27.656: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 26 14:16:27.656: INFO: validating pod update-demo-nautilus-qb6pn
Apr 26 14:16:27.754: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 26 14:16:27.754: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 26 14:16:27.754: INFO: update-demo-nautilus-qb6pn is verified up and running
STEP: scaling down the replication controller
Apr 26 14:16:27.757: INFO: scanned /root for discovery docs: <nil>
Apr 26 14:16:27.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-3937'
Apr 26 14:16:28.893: INFO: stderr: ""
Apr 26 14:16:28.893: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 26 14:16:28.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3937'
Apr 26 14:16:28.996: INFO: stderr: ""
Apr 26 14:16:28.996: INFO: stdout: "update-demo-nautilus-6gsd4 update-demo-nautilus-qb6pn "
STEP: Replicas for name=update-demo: expected=1 actual=2
Apr 26 14:16:33.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3937'
Apr 26 14:16:34.098: INFO: stderr: ""
Apr 26 14:16:34.098: INFO: stdout: "update-demo-nautilus-qb6pn "
Apr 26 14:16:34.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 get pods update-demo-nautilus-qb6pn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3937'
Apr 26 14:16:34.194: INFO: stderr: ""
Apr 26 14:16:34.194: INFO: stdout: "true"
Apr 26 14:16:34.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 get pods update-demo-nautilus-qb6pn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3937'
Apr 26 14:16:34.294: INFO: stderr: ""
Apr 26 14:16:34.294: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 26 14:16:34.294: INFO: validating pod update-demo-nautilus-qb6pn
Apr 26 14:16:34.306: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 26 14:16:34.306: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 26 14:16:34.306: INFO: update-demo-nautilus-qb6pn is verified up and running
STEP: scaling up the replication controller
Apr 26 14:16:34.308: INFO: scanned /root for discovery docs: <nil>
Apr 26 14:16:34.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-3937'
Apr 26 14:16:35.474: INFO: stderr: ""
Apr 26 14:16:35.474: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 26 14:16:35.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3937'
Apr 26 14:16:35.608: INFO: stderr: ""
Apr 26 14:16:35.608: INFO: stdout: "update-demo-nautilus-qb6pn update-demo-nautilus-rslrq "
Apr 26 14:16:35.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 get pods update-demo-nautilus-qb6pn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3937'
Apr 26 14:16:35.786: INFO: stderr: ""
Apr 26 14:16:35.786: INFO: stdout: "true"
Apr 26 14:16:35.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 get pods update-demo-nautilus-qb6pn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3937'
Apr 26 14:16:35.879: INFO: stderr: ""
Apr 26 14:16:35.879: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 26 14:16:35.879: INFO: validating pod update-demo-nautilus-qb6pn
Apr 26 14:16:35.897: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 26 14:16:35.897: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 26 14:16:35.897: INFO: update-demo-nautilus-qb6pn is verified up and running
Apr 26 14:16:35.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 get pods update-demo-nautilus-rslrq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3937'
Apr 26 14:16:36.051: INFO: stderr: ""
Apr 26 14:16:36.051: INFO: stdout: ""
Apr 26 14:16:36.051: INFO: update-demo-nautilus-rslrq is created but not running
Apr 26 14:16:41.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3937'
Apr 26 14:16:41.139: INFO: stderr: ""
Apr 26 14:16:41.139: INFO: stdout: "update-demo-nautilus-qb6pn update-demo-nautilus-rslrq "
Apr 26 14:16:41.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 get pods update-demo-nautilus-qb6pn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3937'
Apr 26 14:16:41.223: INFO: stderr: ""
Apr 26 14:16:41.223: INFO: stdout: "true"
Apr 26 14:16:41.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 get pods update-demo-nautilus-qb6pn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3937'
Apr 26 14:16:41.317: INFO: stderr: ""
Apr 26 14:16:41.317: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 26 14:16:41.317: INFO: validating pod update-demo-nautilus-qb6pn
Apr 26 14:16:41.329: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 26 14:16:41.329: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 26 14:16:41.329: INFO: update-demo-nautilus-qb6pn is verified up and running
Apr 26 14:16:41.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 get pods update-demo-nautilus-rslrq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3937'
Apr 26 14:16:41.420: INFO: stderr: ""
Apr 26 14:16:41.420: INFO: stdout: "true"
Apr 26 14:16:41.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 get pods update-demo-nautilus-rslrq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3937'
Apr 26 14:16:41.597: INFO: stderr: ""
Apr 26 14:16:41.597: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 26 14:16:41.597: INFO: validating pod update-demo-nautilus-rslrq
Apr 26 14:16:41.698: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 26 14:16:41.698: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 26 14:16:41.698: INFO: update-demo-nautilus-rslrq is verified up and running
STEP: using delete to clean up resources
Apr 26 14:16:41.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 delete --grace-period=0 --force -f - --namespace=kubectl-3937'
Apr 26 14:16:41.818: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 26 14:16:41.819: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Apr 26 14:16:41.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-3937'
Apr 26 14:16:41.924: INFO: stderr: "No resources found in kubectl-3937 namespace.\n"
Apr 26 14:16:41.924: INFO: stdout: ""
Apr 26 14:16:41.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 get pods -l name=update-demo --namespace=kubectl-3937 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 26 14:16:42.023: INFO: stderr: ""
Apr 26 14:16:42.023: INFO: stdout: "update-demo-nautilus-qb6pn\nupdate-demo-nautilus-rslrq\n"
Apr 26 14:16:42.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-3937'
Apr 26 14:16:42.656: INFO: stderr: "No resources found in kubectl-3937 namespace.\n"
Apr 26 14:16:42.656: INFO: stdout: ""
Apr 26 14:16:42.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 get pods -l name=update-demo --namespace=kubectl-3937 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 26 14:16:42.753: INFO: stderr: ""
Apr 26 14:16:42.753: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:16:42.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3937" for this suite.

• [SLOW TEST:21.523 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:306
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":303,"completed":116,"skipped":1739,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:16:42.779: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Apr 26 14:16:42.865: INFO: Waiting up to 5m0s for pod "pod-48562c91-3d34-4d85-9b4a-ff85c9c7b2c5" in namespace "emptydir-6601" to be "Succeeded or Failed"
Apr 26 14:16:42.883: INFO: Pod "pod-48562c91-3d34-4d85-9b4a-ff85c9c7b2c5": Phase="Pending", Reason="", readiness=false. Elapsed: 17.344046ms
Apr 26 14:16:44.889: INFO: Pod "pod-48562c91-3d34-4d85-9b4a-ff85c9c7b2c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023837956s
Apr 26 14:16:46.896: INFO: Pod "pod-48562c91-3d34-4d85-9b4a-ff85c9c7b2c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031060593s
STEP: Saw pod success
Apr 26 14:16:46.897: INFO: Pod "pod-48562c91-3d34-4d85-9b4a-ff85c9c7b2c5" satisfied condition "Succeeded or Failed"
Apr 26 14:16:46.903: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod pod-48562c91-3d34-4d85-9b4a-ff85c9c7b2c5 container test-container: <nil>
STEP: delete the pod
Apr 26 14:16:46.947: INFO: Waiting for pod pod-48562c91-3d34-4d85-9b4a-ff85c9c7b2c5 to disappear
Apr 26 14:16:46.958: INFO: Pod pod-48562c91-3d34-4d85-9b4a-ff85c9c7b2c5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:16:46.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6601" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":117,"skipped":1741,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:16:46.989: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Apr 26 14:16:47.050: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: creating replication controller svc-latency-rc in namespace svc-latency-8770
I0426 14:16:47.093618      21 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-8770, replica count: 1
I0426 14:16:48.145441      21 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0426 14:16:49.146048      21 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0426 14:16:50.146183      21 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 26 14:16:50.267: INFO: Created: latency-svc-khfqp
Apr 26 14:16:50.271: INFO: Got endpoints: latency-svc-khfqp [25.398302ms]
Apr 26 14:16:50.291: INFO: Created: latency-svc-n7zz4
Apr 26 14:16:50.303: INFO: Got endpoints: latency-svc-n7zz4 [31.058825ms]
Apr 26 14:16:50.306: INFO: Created: latency-svc-rkw6c
Apr 26 14:16:50.319: INFO: Got endpoints: latency-svc-rkw6c [46.990974ms]
Apr 26 14:16:50.322: INFO: Created: latency-svc-wrzcs
Apr 26 14:16:50.336: INFO: Got endpoints: latency-svc-wrzcs [64.432782ms]
Apr 26 14:16:50.341: INFO: Created: latency-svc-9s2fx
Apr 26 14:16:50.351: INFO: Got endpoints: latency-svc-9s2fx [78.595488ms]
Apr 26 14:16:50.361: INFO: Created: latency-svc-4qcwz
Apr 26 14:16:50.371: INFO: Got endpoints: latency-svc-4qcwz [98.430659ms]
Apr 26 14:16:50.381: INFO: Created: latency-svc-fkzd6
Apr 26 14:16:50.390: INFO: Got endpoints: latency-svc-fkzd6 [116.658799ms]
Apr 26 14:16:50.400: INFO: Created: latency-svc-t8zv4
Apr 26 14:16:50.413: INFO: Got endpoints: latency-svc-t8zv4 [140.434458ms]
Apr 26 14:16:50.423: INFO: Created: latency-svc-q8r8l
Apr 26 14:16:50.436: INFO: Created: latency-svc-jrb7n
Apr 26 14:16:50.436: INFO: Got endpoints: latency-svc-q8r8l [162.562459ms]
Apr 26 14:16:50.450: INFO: Got endpoints: latency-svc-jrb7n [176.625258ms]
Apr 26 14:16:50.454: INFO: Created: latency-svc-l7sqm
Apr 26 14:16:50.463: INFO: Got endpoints: latency-svc-l7sqm [189.479448ms]
Apr 26 14:16:50.476: INFO: Created: latency-svc-wmldd
Apr 26 14:16:50.486: INFO: Got endpoints: latency-svc-wmldd [212.043675ms]
Apr 26 14:16:50.493: INFO: Created: latency-svc-s5gkz
Apr 26 14:16:50.508: INFO: Got endpoints: latency-svc-s5gkz [234.084138ms]
Apr 26 14:16:50.510: INFO: Created: latency-svc-mf76j
Apr 26 14:16:50.517: INFO: Got endpoints: latency-svc-mf76j [242.000753ms]
Apr 26 14:16:50.525: INFO: Created: latency-svc-cttdb
Apr 26 14:16:50.533: INFO: Got endpoints: latency-svc-cttdb [258.975833ms]
Apr 26 14:16:50.533: INFO: Created: latency-svc-4r582
Apr 26 14:16:50.540: INFO: Got endpoints: latency-svc-4r582 [266.213154ms]
Apr 26 14:16:50.554: INFO: Created: latency-svc-dkpf8
Apr 26 14:16:50.571: INFO: Got endpoints: latency-svc-dkpf8 [267.700448ms]
Apr 26 14:16:50.575: INFO: Created: latency-svc-jsdss
Apr 26 14:16:50.583: INFO: Got endpoints: latency-svc-jsdss [264.119374ms]
Apr 26 14:16:50.590: INFO: Created: latency-svc-v5gk9
Apr 26 14:16:50.598: INFO: Got endpoints: latency-svc-v5gk9 [261.006022ms]
Apr 26 14:16:50.606: INFO: Created: latency-svc-p7xvt
Apr 26 14:16:50.617: INFO: Got endpoints: latency-svc-p7xvt [266.102039ms]
Apr 26 14:16:50.618: INFO: Created: latency-svc-mtpf5
Apr 26 14:16:50.629: INFO: Got endpoints: latency-svc-mtpf5 [257.242166ms]
Apr 26 14:16:50.634: INFO: Created: latency-svc-4zbkl
Apr 26 14:16:50.645: INFO: Got endpoints: latency-svc-4zbkl [254.548982ms]
Apr 26 14:16:50.653: INFO: Created: latency-svc-7j4nz
Apr 26 14:16:50.663: INFO: Created: latency-svc-hbmbm
Apr 26 14:16:50.666: INFO: Got endpoints: latency-svc-7j4nz [252.31924ms]
Apr 26 14:16:50.671: INFO: Got endpoints: latency-svc-hbmbm [234.37093ms]
Apr 26 14:16:50.677: INFO: Created: latency-svc-tsqc6
Apr 26 14:16:50.685: INFO: Got endpoints: latency-svc-tsqc6 [234.98499ms]
Apr 26 14:16:50.693: INFO: Created: latency-svc-d7w96
Apr 26 14:16:50.699: INFO: Got endpoints: latency-svc-d7w96 [235.886694ms]
Apr 26 14:16:50.712: INFO: Created: latency-svc-lgq6n
Apr 26 14:16:50.719: INFO: Created: latency-svc-b66gw
Apr 26 14:16:50.725: INFO: Got endpoints: latency-svc-lgq6n [239.64417ms]
Apr 26 14:16:50.729: INFO: Got endpoints: latency-svc-b66gw [220.648696ms]
Apr 26 14:16:50.742: INFO: Created: latency-svc-w8sjw
Apr 26 14:16:50.750: INFO: Created: latency-svc-8cph5
Apr 26 14:16:50.753: INFO: Got endpoints: latency-svc-w8sjw [236.736213ms]
Apr 26 14:16:50.759: INFO: Got endpoints: latency-svc-8cph5 [225.820332ms]
Apr 26 14:16:50.762: INFO: Created: latency-svc-8csjc
Apr 26 14:16:50.772: INFO: Got endpoints: latency-svc-8csjc [231.218944ms]
Apr 26 14:16:50.781: INFO: Created: latency-svc-8lvgc
Apr 26 14:16:50.788: INFO: Got endpoints: latency-svc-8lvgc [217.585274ms]
Apr 26 14:16:50.792: INFO: Created: latency-svc-nmb86
Apr 26 14:16:50.808: INFO: Got endpoints: latency-svc-nmb86 [224.104344ms]
Apr 26 14:16:50.812: INFO: Created: latency-svc-9bdqs
Apr 26 14:16:50.821: INFO: Got endpoints: latency-svc-9bdqs [222.885603ms]
Apr 26 14:16:50.826: INFO: Created: latency-svc-4kr84
Apr 26 14:16:50.835: INFO: Created: latency-svc-bx5q6
Apr 26 14:16:50.843: INFO: Got endpoints: latency-svc-4kr84 [226.051631ms]
Apr 26 14:16:50.847: INFO: Got endpoints: latency-svc-bx5q6 [218.204414ms]
Apr 26 14:16:50.849: INFO: Created: latency-svc-57ltz
Apr 26 14:16:50.856: INFO: Got endpoints: latency-svc-57ltz [211.665284ms]
Apr 26 14:16:50.865: INFO: Created: latency-svc-prqvr
Apr 26 14:16:50.868: INFO: Got endpoints: latency-svc-prqvr [202.003635ms]
Apr 26 14:16:50.877: INFO: Created: latency-svc-4xt74
Apr 26 14:16:50.882: INFO: Got endpoints: latency-svc-4xt74 [211.442945ms]
Apr 26 14:16:50.886: INFO: Created: latency-svc-m9dnp
Apr 26 14:16:50.894: INFO: Got endpoints: latency-svc-m9dnp [209.211005ms]
Apr 26 14:16:50.895: INFO: Created: latency-svc-9kdmn
Apr 26 14:16:50.908: INFO: Created: latency-svc-9x8qb
Apr 26 14:16:50.909: INFO: Got endpoints: latency-svc-9kdmn [210.424657ms]
Apr 26 14:16:50.923: INFO: Got endpoints: latency-svc-9x8qb [198.12366ms]
Apr 26 14:16:50.928: INFO: Created: latency-svc-nwpwl
Apr 26 14:16:50.936: INFO: Got endpoints: latency-svc-nwpwl [207.49243ms]
Apr 26 14:16:50.937: INFO: Created: latency-svc-nd5m5
Apr 26 14:16:50.954: INFO: Created: latency-svc-fg2h9
Apr 26 14:16:50.962: INFO: Created: latency-svc-plgcf
Apr 26 14:16:50.976: INFO: Got endpoints: latency-svc-nd5m5 [223.134894ms]
Apr 26 14:16:50.981: INFO: Created: latency-svc-r4cx7
Apr 26 14:16:50.989: INFO: Created: latency-svc-6c7kn
Apr 26 14:16:51.000: INFO: Created: latency-svc-thxlt
Apr 26 14:16:51.012: INFO: Created: latency-svc-52drt
Apr 26 14:16:51.025: INFO: Got endpoints: latency-svc-fg2h9 [266.215021ms]
Apr 26 14:16:51.025: INFO: Created: latency-svc-jbdhk
Apr 26 14:16:51.038: INFO: Created: latency-svc-wwpdv
Apr 26 14:16:51.050: INFO: Created: latency-svc-wl9lk
Apr 26 14:16:51.058: INFO: Created: latency-svc-9xt77
Apr 26 14:16:51.064: INFO: Created: latency-svc-pc7jz
Apr 26 14:16:51.072: INFO: Got endpoints: latency-svc-plgcf [299.378776ms]
Apr 26 14:16:51.083: INFO: Created: latency-svc-cgpmz
Apr 26 14:16:51.095: INFO: Created: latency-svc-vmkl5
Apr 26 14:16:51.106: INFO: Created: latency-svc-jpl48
Apr 26 14:16:51.118: INFO: Created: latency-svc-bhmjb
Apr 26 14:16:51.125: INFO: Got endpoints: latency-svc-r4cx7 [336.76508ms]
Apr 26 14:16:51.133: INFO: Created: latency-svc-6wtzx
Apr 26 14:16:51.145: INFO: Created: latency-svc-bjq4t
Apr 26 14:16:51.173: INFO: Got endpoints: latency-svc-6c7kn [364.952744ms]
Apr 26 14:16:51.175: INFO: Created: latency-svc-tgxzz
Apr 26 14:16:51.191: INFO: Created: latency-svc-vkld5
Apr 26 14:16:51.227: INFO: Got endpoints: latency-svc-thxlt [405.656303ms]
Apr 26 14:16:51.246: INFO: Created: latency-svc-bh2w7
Apr 26 14:16:51.272: INFO: Got endpoints: latency-svc-52drt [426.46398ms]
Apr 26 14:16:51.300: INFO: Created: latency-svc-fwvtg
Apr 26 14:16:51.326: INFO: Got endpoints: latency-svc-jbdhk [479.128065ms]
Apr 26 14:16:51.345: INFO: Created: latency-svc-22pfk
Apr 26 14:16:51.374: INFO: Got endpoints: latency-svc-wwpdv [517.485264ms]
Apr 26 14:16:51.413: INFO: Created: latency-svc-s8lnt
Apr 26 14:16:51.423: INFO: Got endpoints: latency-svc-wl9lk [555.685656ms]
Apr 26 14:16:51.471: INFO: Created: latency-svc-6qsrg
Apr 26 14:16:51.475: INFO: Got endpoints: latency-svc-9xt77 [593.281797ms]
Apr 26 14:16:51.497: INFO: Created: latency-svc-vkc5q
Apr 26 14:16:51.522: INFO: Got endpoints: latency-svc-pc7jz [627.859272ms]
Apr 26 14:16:51.540: INFO: Created: latency-svc-8f4tg
Apr 26 14:16:51.575: INFO: Got endpoints: latency-svc-cgpmz [665.331261ms]
Apr 26 14:16:51.594: INFO: Created: latency-svc-pgz26
Apr 26 14:16:51.622: INFO: Got endpoints: latency-svc-vmkl5 [698.354497ms]
Apr 26 14:16:51.639: INFO: Created: latency-svc-wh6xz
Apr 26 14:16:51.672: INFO: Got endpoints: latency-svc-jpl48 [735.662609ms]
Apr 26 14:16:51.689: INFO: Created: latency-svc-brs4r
Apr 26 14:16:51.724: INFO: Got endpoints: latency-svc-bhmjb [746.902574ms]
Apr 26 14:16:51.744: INFO: Created: latency-svc-vtxrq
Apr 26 14:16:51.778: INFO: Got endpoints: latency-svc-6wtzx [753.173789ms]
Apr 26 14:16:51.797: INFO: Created: latency-svc-w2vsw
Apr 26 14:16:51.828: INFO: Got endpoints: latency-svc-bjq4t [756.532203ms]
Apr 26 14:16:51.851: INFO: Created: latency-svc-qvsfj
Apr 26 14:16:51.871: INFO: Got endpoints: latency-svc-tgxzz [745.864726ms]
Apr 26 14:16:51.888: INFO: Created: latency-svc-swmhn
Apr 26 14:16:51.924: INFO: Got endpoints: latency-svc-vkld5 [751.1575ms]
Apr 26 14:16:51.944: INFO: Created: latency-svc-tmplc
Apr 26 14:16:51.972: INFO: Got endpoints: latency-svc-bh2w7 [745.112035ms]
Apr 26 14:16:51.991: INFO: Created: latency-svc-x2vr9
Apr 26 14:16:52.024: INFO: Got endpoints: latency-svc-fwvtg [751.946436ms]
Apr 26 14:16:52.053: INFO: Created: latency-svc-2tz59
Apr 26 14:16:52.070: INFO: Got endpoints: latency-svc-22pfk [744.043155ms]
Apr 26 14:16:52.089: INFO: Created: latency-svc-mxv6l
Apr 26 14:16:52.124: INFO: Got endpoints: latency-svc-s8lnt [750.345544ms]
Apr 26 14:16:52.144: INFO: Created: latency-svc-5snw5
Apr 26 14:16:52.175: INFO: Got endpoints: latency-svc-6qsrg [751.122167ms]
Apr 26 14:16:52.192: INFO: Created: latency-svc-tjhcv
Apr 26 14:16:52.223: INFO: Got endpoints: latency-svc-vkc5q [747.606945ms]
Apr 26 14:16:52.242: INFO: Created: latency-svc-zsdzr
Apr 26 14:16:52.274: INFO: Got endpoints: latency-svc-8f4tg [751.696165ms]
Apr 26 14:16:52.298: INFO: Created: latency-svc-5c694
Apr 26 14:16:52.325: INFO: Got endpoints: latency-svc-pgz26 [750.439437ms]
Apr 26 14:16:52.382: INFO: Got endpoints: latency-svc-wh6xz [760.02177ms]
Apr 26 14:16:52.386: INFO: Created: latency-svc-ptbrl
Apr 26 14:16:52.412: INFO: Created: latency-svc-mt75r
Apr 26 14:16:52.469: INFO: Got endpoints: latency-svc-brs4r [796.305612ms]
Apr 26 14:16:52.473: INFO: Got endpoints: latency-svc-vtxrq [748.789831ms]
Apr 26 14:16:52.491: INFO: Created: latency-svc-szktg
Apr 26 14:16:52.501: INFO: Created: latency-svc-s2prs
Apr 26 14:16:52.521: INFO: Got endpoints: latency-svc-w2vsw [743.206758ms]
Apr 26 14:16:52.540: INFO: Created: latency-svc-25dxg
Apr 26 14:16:52.576: INFO: Got endpoints: latency-svc-qvsfj [747.15339ms]
Apr 26 14:16:52.594: INFO: Created: latency-svc-mm5wv
Apr 26 14:16:52.624: INFO: Got endpoints: latency-svc-swmhn [752.13714ms]
Apr 26 14:16:52.644: INFO: Created: latency-svc-6dqpj
Apr 26 14:16:52.681: INFO: Got endpoints: latency-svc-tmplc [756.784192ms]
Apr 26 14:16:52.697: INFO: Created: latency-svc-69pc8
Apr 26 14:16:52.724: INFO: Got endpoints: latency-svc-x2vr9 [752.529082ms]
Apr 26 14:16:52.744: INFO: Created: latency-svc-mwkg9
Apr 26 14:16:52.782: INFO: Got endpoints: latency-svc-2tz59 [758.25269ms]
Apr 26 14:16:52.799: INFO: Created: latency-svc-p8l8n
Apr 26 14:16:52.828: INFO: Got endpoints: latency-svc-mxv6l [758.011474ms]
Apr 26 14:16:52.846: INFO: Created: latency-svc-hvdh2
Apr 26 14:16:52.874: INFO: Got endpoints: latency-svc-5snw5 [749.56172ms]
Apr 26 14:16:52.892: INFO: Created: latency-svc-xspjz
Apr 26 14:16:52.926: INFO: Got endpoints: latency-svc-tjhcv [751.700604ms]
Apr 26 14:16:52.942: INFO: Created: latency-svc-mjtfv
Apr 26 14:16:52.986: INFO: Got endpoints: latency-svc-zsdzr [763.06502ms]
Apr 26 14:16:53.009: INFO: Created: latency-svc-ccv8l
Apr 26 14:16:53.022: INFO: Got endpoints: latency-svc-5c694 [748.000546ms]
Apr 26 14:16:53.043: INFO: Created: latency-svc-jw4wj
Apr 26 14:16:53.072: INFO: Got endpoints: latency-svc-ptbrl [746.182599ms]
Apr 26 14:16:53.091: INFO: Created: latency-svc-45s4d
Apr 26 14:16:53.132: INFO: Got endpoints: latency-svc-mt75r [750.266307ms]
Apr 26 14:16:53.161: INFO: Created: latency-svc-w8c8k
Apr 26 14:16:53.173: INFO: Got endpoints: latency-svc-szktg [703.969461ms]
Apr 26 14:16:53.192: INFO: Created: latency-svc-d579n
Apr 26 14:16:53.229: INFO: Got endpoints: latency-svc-s2prs [756.784308ms]
Apr 26 14:16:53.252: INFO: Created: latency-svc-xlvrf
Apr 26 14:16:53.274: INFO: Got endpoints: latency-svc-25dxg [752.214938ms]
Apr 26 14:16:53.305: INFO: Created: latency-svc-rlfcj
Apr 26 14:16:53.321: INFO: Got endpoints: latency-svc-mm5wv [745.66362ms]
Apr 26 14:16:53.344: INFO: Created: latency-svc-rhdh9
Apr 26 14:16:53.372: INFO: Got endpoints: latency-svc-6dqpj [748.61676ms]
Apr 26 14:16:53.388: INFO: Created: latency-svc-dbwnt
Apr 26 14:16:53.425: INFO: Got endpoints: latency-svc-69pc8 [744.441828ms]
Apr 26 14:16:53.452: INFO: Created: latency-svc-9k2vp
Apr 26 14:16:53.474: INFO: Got endpoints: latency-svc-mwkg9 [749.725889ms]
Apr 26 14:16:53.503: INFO: Created: latency-svc-xc5mc
Apr 26 14:16:53.521: INFO: Got endpoints: latency-svc-p8l8n [738.422736ms]
Apr 26 14:16:53.537: INFO: Created: latency-svc-2nfn2
Apr 26 14:16:53.572: INFO: Got endpoints: latency-svc-hvdh2 [743.190033ms]
Apr 26 14:16:53.590: INFO: Created: latency-svc-7cxfb
Apr 26 14:16:53.622: INFO: Got endpoints: latency-svc-xspjz [747.234138ms]
Apr 26 14:16:53.653: INFO: Created: latency-svc-r6r5t
Apr 26 14:16:53.673: INFO: Got endpoints: latency-svc-mjtfv [746.323468ms]
Apr 26 14:16:53.710: INFO: Created: latency-svc-mszrt
Apr 26 14:16:53.724: INFO: Got endpoints: latency-svc-ccv8l [738.083037ms]
Apr 26 14:16:53.742: INFO: Created: latency-svc-xpccg
Apr 26 14:16:53.774: INFO: Got endpoints: latency-svc-jw4wj [751.285522ms]
Apr 26 14:16:53.792: INFO: Created: latency-svc-w6hnf
Apr 26 14:16:53.833: INFO: Got endpoints: latency-svc-45s4d [761.688229ms]
Apr 26 14:16:53.854: INFO: Created: latency-svc-xmklf
Apr 26 14:16:53.879: INFO: Got endpoints: latency-svc-w8c8k [746.374137ms]
Apr 26 14:16:53.903: INFO: Created: latency-svc-r2hqh
Apr 26 14:16:53.929: INFO: Got endpoints: latency-svc-d579n [755.505424ms]
Apr 26 14:16:53.950: INFO: Created: latency-svc-ktpqj
Apr 26 14:16:53.974: INFO: Got endpoints: latency-svc-xlvrf [744.252913ms]
Apr 26 14:16:54.007: INFO: Created: latency-svc-774gz
Apr 26 14:16:54.026: INFO: Got endpoints: latency-svc-rlfcj [752.404728ms]
Apr 26 14:16:54.046: INFO: Created: latency-svc-hv2cv
Apr 26 14:16:54.074: INFO: Got endpoints: latency-svc-rhdh9 [752.224375ms]
Apr 26 14:16:54.089: INFO: Created: latency-svc-ncfk6
Apr 26 14:16:54.122: INFO: Got endpoints: latency-svc-dbwnt [749.561505ms]
Apr 26 14:16:54.140: INFO: Created: latency-svc-xqv45
Apr 26 14:16:54.175: INFO: Got endpoints: latency-svc-9k2vp [749.798276ms]
Apr 26 14:16:54.190: INFO: Created: latency-svc-q2q6q
Apr 26 14:16:54.222: INFO: Got endpoints: latency-svc-xc5mc [747.559549ms]
Apr 26 14:16:54.244: INFO: Created: latency-svc-xclpt
Apr 26 14:16:54.273: INFO: Got endpoints: latency-svc-2nfn2 [752.872362ms]
Apr 26 14:16:54.292: INFO: Created: latency-svc-cg9rw
Apr 26 14:16:54.320: INFO: Got endpoints: latency-svc-7cxfb [748.279857ms]
Apr 26 14:16:54.336: INFO: Created: latency-svc-dj928
Apr 26 14:16:54.379: INFO: Got endpoints: latency-svc-r6r5t [757.586754ms]
Apr 26 14:16:54.400: INFO: Created: latency-svc-w5dwj
Apr 26 14:16:54.426: INFO: Got endpoints: latency-svc-mszrt [753.257579ms]
Apr 26 14:16:54.445: INFO: Created: latency-svc-nqcrw
Apr 26 14:16:54.472: INFO: Got endpoints: latency-svc-xpccg [746.941877ms]
Apr 26 14:16:54.491: INFO: Created: latency-svc-p2rrf
Apr 26 14:16:54.523: INFO: Got endpoints: latency-svc-w6hnf [748.798861ms]
Apr 26 14:16:54.548: INFO: Created: latency-svc-n9wfv
Apr 26 14:16:54.574: INFO: Got endpoints: latency-svc-xmklf [740.309613ms]
Apr 26 14:16:54.598: INFO: Created: latency-svc-zhslv
Apr 26 14:16:54.625: INFO: Got endpoints: latency-svc-r2hqh [746.060971ms]
Apr 26 14:16:54.643: INFO: Created: latency-svc-6h2lk
Apr 26 14:16:54.674: INFO: Got endpoints: latency-svc-ktpqj [744.94665ms]
Apr 26 14:16:54.696: INFO: Created: latency-svc-pvfpq
Apr 26 14:16:54.721: INFO: Got endpoints: latency-svc-774gz [747.504166ms]
Apr 26 14:16:54.741: INFO: Created: latency-svc-85j7l
Apr 26 14:16:54.786: INFO: Got endpoints: latency-svc-hv2cv [759.937648ms]
Apr 26 14:16:54.815: INFO: Created: latency-svc-dc2vd
Apr 26 14:16:54.829: INFO: Got endpoints: latency-svc-ncfk6 [754.708376ms]
Apr 26 14:16:54.849: INFO: Created: latency-svc-6v6lb
Apr 26 14:16:54.876: INFO: Got endpoints: latency-svc-xqv45 [753.657306ms]
Apr 26 14:16:54.893: INFO: Created: latency-svc-sqs72
Apr 26 14:16:54.923: INFO: Got endpoints: latency-svc-q2q6q [747.724978ms]
Apr 26 14:16:54.942: INFO: Created: latency-svc-56rz9
Apr 26 14:16:54.975: INFO: Got endpoints: latency-svc-xclpt [753.150281ms]
Apr 26 14:16:54.993: INFO: Created: latency-svc-j7rbp
Apr 26 14:16:55.029: INFO: Got endpoints: latency-svc-cg9rw [755.173711ms]
Apr 26 14:16:55.047: INFO: Created: latency-svc-2pb67
Apr 26 14:16:55.074: INFO: Got endpoints: latency-svc-dj928 [754.380314ms]
Apr 26 14:16:55.092: INFO: Created: latency-svc-4gl57
Apr 26 14:16:55.125: INFO: Got endpoints: latency-svc-w5dwj [745.844535ms]
Apr 26 14:16:55.142: INFO: Created: latency-svc-cjvpt
Apr 26 14:16:55.175: INFO: Got endpoints: latency-svc-nqcrw [749.232599ms]
Apr 26 14:16:55.195: INFO: Created: latency-svc-r7j9r
Apr 26 14:16:55.220: INFO: Got endpoints: latency-svc-p2rrf [748.892384ms]
Apr 26 14:16:55.238: INFO: Created: latency-svc-n4wgm
Apr 26 14:16:55.272: INFO: Got endpoints: latency-svc-n9wfv [748.603452ms]
Apr 26 14:16:55.291: INFO: Created: latency-svc-8pjdg
Apr 26 14:16:55.321: INFO: Got endpoints: latency-svc-zhslv [746.659044ms]
Apr 26 14:16:55.344: INFO: Created: latency-svc-4m8lh
Apr 26 14:16:55.372: INFO: Got endpoints: latency-svc-6h2lk [746.339052ms]
Apr 26 14:16:55.388: INFO: Created: latency-svc-hbttk
Apr 26 14:16:55.422: INFO: Got endpoints: latency-svc-pvfpq [748.253333ms]
Apr 26 14:16:55.443: INFO: Created: latency-svc-rtx49
Apr 26 14:16:55.479: INFO: Got endpoints: latency-svc-85j7l [758.03731ms]
Apr 26 14:16:55.503: INFO: Created: latency-svc-l4pfl
Apr 26 14:16:55.526: INFO: Got endpoints: latency-svc-dc2vd [739.605772ms]
Apr 26 14:16:55.544: INFO: Created: latency-svc-86xtb
Apr 26 14:16:55.578: INFO: Got endpoints: latency-svc-6v6lb [749.099099ms]
Apr 26 14:16:55.597: INFO: Created: latency-svc-l2xts
Apr 26 14:16:55.623: INFO: Got endpoints: latency-svc-sqs72 [747.121334ms]
Apr 26 14:16:55.643: INFO: Created: latency-svc-rq8j2
Apr 26 14:16:55.672: INFO: Got endpoints: latency-svc-56rz9 [748.770039ms]
Apr 26 14:16:55.689: INFO: Created: latency-svc-xlwcd
Apr 26 14:16:55.723: INFO: Got endpoints: latency-svc-j7rbp [748.237547ms]
Apr 26 14:16:55.762: INFO: Created: latency-svc-2khlr
Apr 26 14:16:55.773: INFO: Got endpoints: latency-svc-2pb67 [744.259432ms]
Apr 26 14:16:55.794: INFO: Created: latency-svc-x26g8
Apr 26 14:16:55.827: INFO: Got endpoints: latency-svc-4gl57 [752.582303ms]
Apr 26 14:16:55.853: INFO: Created: latency-svc-2tbn8
Apr 26 14:16:55.876: INFO: Got endpoints: latency-svc-cjvpt [750.738949ms]
Apr 26 14:16:55.895: INFO: Created: latency-svc-hsf9j
Apr 26 14:16:55.923: INFO: Got endpoints: latency-svc-r7j9r [747.535114ms]
Apr 26 14:16:55.944: INFO: Created: latency-svc-f5wjn
Apr 26 14:16:55.971: INFO: Got endpoints: latency-svc-n4wgm [750.117271ms]
Apr 26 14:16:55.988: INFO: Created: latency-svc-thcx6
Apr 26 14:16:56.022: INFO: Got endpoints: latency-svc-8pjdg [750.597102ms]
Apr 26 14:16:56.041: INFO: Created: latency-svc-d6hrs
Apr 26 14:16:56.076: INFO: Got endpoints: latency-svc-4m8lh [755.516121ms]
Apr 26 14:16:56.099: INFO: Created: latency-svc-pctbh
Apr 26 14:16:56.126: INFO: Got endpoints: latency-svc-hbttk [754.452858ms]
Apr 26 14:16:56.152: INFO: Created: latency-svc-v7jtw
Apr 26 14:16:56.176: INFO: Got endpoints: latency-svc-rtx49 [753.602424ms]
Apr 26 14:16:56.195: INFO: Created: latency-svc-gzmxs
Apr 26 14:16:56.222: INFO: Got endpoints: latency-svc-l4pfl [742.352397ms]
Apr 26 14:16:56.243: INFO: Created: latency-svc-5btcq
Apr 26 14:16:56.273: INFO: Got endpoints: latency-svc-86xtb [746.530998ms]
Apr 26 14:16:56.291: INFO: Created: latency-svc-7jd6x
Apr 26 14:16:56.323: INFO: Got endpoints: latency-svc-l2xts [744.794413ms]
Apr 26 14:16:56.348: INFO: Created: latency-svc-q9f2t
Apr 26 14:16:56.376: INFO: Got endpoints: latency-svc-rq8j2 [752.814251ms]
Apr 26 14:16:56.394: INFO: Created: latency-svc-8rqgl
Apr 26 14:16:56.426: INFO: Got endpoints: latency-svc-xlwcd [753.547482ms]
Apr 26 14:16:56.452: INFO: Created: latency-svc-m8qcf
Apr 26 14:16:56.478: INFO: Got endpoints: latency-svc-2khlr [754.684779ms]
Apr 26 14:16:56.494: INFO: Created: latency-svc-mnmcj
Apr 26 14:16:56.529: INFO: Got endpoints: latency-svc-x26g8 [755.559706ms]
Apr 26 14:16:56.554: INFO: Created: latency-svc-v42j6
Apr 26 14:16:56.576: INFO: Got endpoints: latency-svc-2tbn8 [748.647383ms]
Apr 26 14:16:56.592: INFO: Created: latency-svc-xz7qp
Apr 26 14:16:56.621: INFO: Got endpoints: latency-svc-hsf9j [744.81432ms]
Apr 26 14:16:56.651: INFO: Created: latency-svc-nj67m
Apr 26 14:16:56.672: INFO: Got endpoints: latency-svc-f5wjn [748.220504ms]
Apr 26 14:16:56.690: INFO: Created: latency-svc-hvf9f
Apr 26 14:16:56.721: INFO: Got endpoints: latency-svc-thcx6 [750.683627ms]
Apr 26 14:16:56.740: INFO: Created: latency-svc-vq6wg
Apr 26 14:16:56.777: INFO: Got endpoints: latency-svc-d6hrs [754.249599ms]
Apr 26 14:16:56.796: INFO: Created: latency-svc-qqcxq
Apr 26 14:16:56.822: INFO: Got endpoints: latency-svc-pctbh [745.856412ms]
Apr 26 14:16:56.850: INFO: Created: latency-svc-rjzt9
Apr 26 14:16:56.875: INFO: Got endpoints: latency-svc-v7jtw [748.31983ms]
Apr 26 14:16:56.895: INFO: Created: latency-svc-s96rr
Apr 26 14:16:56.954: INFO: Got endpoints: latency-svc-gzmxs [777.747208ms]
Apr 26 14:16:56.975: INFO: Got endpoints: latency-svc-5btcq [752.585947ms]
Apr 26 14:16:56.979: INFO: Created: latency-svc-xnwq2
Apr 26 14:16:57.007: INFO: Created: latency-svc-g8mzp
Apr 26 14:16:57.021: INFO: Got endpoints: latency-svc-7jd6x [747.553726ms]
Apr 26 14:16:57.037: INFO: Created: latency-svc-rvvj6
Apr 26 14:16:57.077: INFO: Got endpoints: latency-svc-q9f2t [753.833213ms]
Apr 26 14:16:57.097: INFO: Created: latency-svc-jfp2s
Apr 26 14:16:57.126: INFO: Got endpoints: latency-svc-8rqgl [749.95569ms]
Apr 26 14:16:57.144: INFO: Created: latency-svc-qfpds
Apr 26 14:16:57.174: INFO: Got endpoints: latency-svc-m8qcf [748.20828ms]
Apr 26 14:16:57.192: INFO: Created: latency-svc-4cvzr
Apr 26 14:16:57.224: INFO: Got endpoints: latency-svc-mnmcj [745.966287ms]
Apr 26 14:16:57.239: INFO: Created: latency-svc-mpszz
Apr 26 14:16:57.274: INFO: Got endpoints: latency-svc-v42j6 [745.579357ms]
Apr 26 14:16:57.292: INFO: Created: latency-svc-xzjsj
Apr 26 14:16:57.321: INFO: Got endpoints: latency-svc-xz7qp [745.093133ms]
Apr 26 14:16:57.347: INFO: Created: latency-svc-vj8f7
Apr 26 14:16:57.370: INFO: Got endpoints: latency-svc-nj67m [748.843126ms]
Apr 26 14:16:57.389: INFO: Created: latency-svc-l8qxr
Apr 26 14:16:57.421: INFO: Got endpoints: latency-svc-hvf9f [749.292719ms]
Apr 26 14:16:57.444: INFO: Created: latency-svc-k4hjh
Apr 26 14:16:57.472: INFO: Got endpoints: latency-svc-vq6wg [750.393046ms]
Apr 26 14:16:57.490: INFO: Created: latency-svc-pdfn7
Apr 26 14:16:57.522: INFO: Got endpoints: latency-svc-qqcxq [745.464771ms]
Apr 26 14:16:57.544: INFO: Created: latency-svc-r5zb8
Apr 26 14:16:57.571: INFO: Got endpoints: latency-svc-rjzt9 [748.502424ms]
Apr 26 14:16:57.589: INFO: Created: latency-svc-6xtq6
Apr 26 14:16:57.622: INFO: Got endpoints: latency-svc-s96rr [747.706642ms]
Apr 26 14:16:57.641: INFO: Created: latency-svc-84zbg
Apr 26 14:16:57.677: INFO: Got endpoints: latency-svc-xnwq2 [722.568661ms]
Apr 26 14:16:57.697: INFO: Created: latency-svc-rktd2
Apr 26 14:16:57.721: INFO: Got endpoints: latency-svc-g8mzp [745.758173ms]
Apr 26 14:16:57.740: INFO: Created: latency-svc-lfgtl
Apr 26 14:16:57.773: INFO: Got endpoints: latency-svc-rvvj6 [752.410804ms]
Apr 26 14:16:57.791: INFO: Created: latency-svc-vrkxt
Apr 26 14:16:57.823: INFO: Got endpoints: latency-svc-jfp2s [746.665206ms]
Apr 26 14:16:57.845: INFO: Created: latency-svc-std4p
Apr 26 14:16:57.874: INFO: Got endpoints: latency-svc-qfpds [748.312711ms]
Apr 26 14:16:57.889: INFO: Created: latency-svc-pwch2
Apr 26 14:16:57.921: INFO: Got endpoints: latency-svc-4cvzr [746.902154ms]
Apr 26 14:16:57.945: INFO: Created: latency-svc-jprrc
Apr 26 14:16:57.974: INFO: Got endpoints: latency-svc-mpszz [749.176795ms]
Apr 26 14:16:57.996: INFO: Created: latency-svc-m2r7n
Apr 26 14:16:58.022: INFO: Got endpoints: latency-svc-xzjsj [747.408875ms]
Apr 26 14:16:58.068: INFO: Created: latency-svc-v9sks
Apr 26 14:16:58.072: INFO: Got endpoints: latency-svc-vj8f7 [750.581188ms]
Apr 26 14:16:58.089: INFO: Created: latency-svc-x5knh
Apr 26 14:16:58.121: INFO: Got endpoints: latency-svc-l8qxr [751.535856ms]
Apr 26 14:16:58.176: INFO: Got endpoints: latency-svc-k4hjh [755.110981ms]
Apr 26 14:16:58.225: INFO: Got endpoints: latency-svc-pdfn7 [752.925804ms]
Apr 26 14:16:58.273: INFO: Got endpoints: latency-svc-r5zb8 [750.421918ms]
Apr 26 14:16:58.327: INFO: Got endpoints: latency-svc-6xtq6 [756.449573ms]
Apr 26 14:16:58.377: INFO: Got endpoints: latency-svc-84zbg [754.110032ms]
Apr 26 14:16:58.426: INFO: Got endpoints: latency-svc-rktd2 [748.993212ms]
Apr 26 14:16:58.474: INFO: Got endpoints: latency-svc-lfgtl [753.375734ms]
Apr 26 14:16:58.524: INFO: Got endpoints: latency-svc-vrkxt [750.576249ms]
Apr 26 14:16:58.576: INFO: Got endpoints: latency-svc-std4p [753.00902ms]
Apr 26 14:16:58.621: INFO: Got endpoints: latency-svc-pwch2 [746.745489ms]
Apr 26 14:16:58.676: INFO: Got endpoints: latency-svc-jprrc [754.847126ms]
Apr 26 14:16:58.725: INFO: Got endpoints: latency-svc-m2r7n [751.834389ms]
Apr 26 14:16:58.782: INFO: Got endpoints: latency-svc-v9sks [759.739726ms]
Apr 26 14:16:58.824: INFO: Got endpoints: latency-svc-x5knh [752.085887ms]
Apr 26 14:16:58.825: INFO: Latencies: [31.058825ms 46.990974ms 64.432782ms 78.595488ms 98.430659ms 116.658799ms 140.434458ms 162.562459ms 176.625258ms 189.479448ms 198.12366ms 202.003635ms 207.49243ms 209.211005ms 210.424657ms 211.442945ms 211.665284ms 212.043675ms 217.585274ms 218.204414ms 220.648696ms 222.885603ms 223.134894ms 224.104344ms 225.820332ms 226.051631ms 231.218944ms 234.084138ms 234.37093ms 234.98499ms 235.886694ms 236.736213ms 239.64417ms 242.000753ms 252.31924ms 254.548982ms 257.242166ms 258.975833ms 261.006022ms 264.119374ms 266.102039ms 266.213154ms 266.215021ms 267.700448ms 299.378776ms 336.76508ms 364.952744ms 405.656303ms 426.46398ms 479.128065ms 517.485264ms 555.685656ms 593.281797ms 627.859272ms 665.331261ms 698.354497ms 703.969461ms 722.568661ms 735.662609ms 738.083037ms 738.422736ms 739.605772ms 740.309613ms 742.352397ms 743.190033ms 743.206758ms 744.043155ms 744.252913ms 744.259432ms 744.441828ms 744.794413ms 744.81432ms 744.94665ms 745.093133ms 745.112035ms 745.464771ms 745.579357ms 745.66362ms 745.758173ms 745.844535ms 745.856412ms 745.864726ms 745.966287ms 746.060971ms 746.182599ms 746.323468ms 746.339052ms 746.374137ms 746.530998ms 746.659044ms 746.665206ms 746.745489ms 746.902154ms 746.902574ms 746.941877ms 747.121334ms 747.15339ms 747.234138ms 747.408875ms 747.504166ms 747.535114ms 747.553726ms 747.559549ms 747.606945ms 747.706642ms 747.724978ms 748.000546ms 748.20828ms 748.220504ms 748.237547ms 748.253333ms 748.279857ms 748.312711ms 748.31983ms 748.502424ms 748.603452ms 748.61676ms 748.647383ms 748.770039ms 748.789831ms 748.798861ms 748.843126ms 748.892384ms 748.993212ms 749.099099ms 749.176795ms 749.232599ms 749.292719ms 749.561505ms 749.56172ms 749.725889ms 749.798276ms 749.95569ms 750.117271ms 750.266307ms 750.345544ms 750.393046ms 750.421918ms 750.439437ms 750.576249ms 750.581188ms 750.597102ms 750.683627ms 750.738949ms 751.122167ms 751.1575ms 751.285522ms 751.535856ms 751.696165ms 751.700604ms 751.834389ms 751.946436ms 752.085887ms 752.13714ms 752.214938ms 752.224375ms 752.404728ms 752.410804ms 752.529082ms 752.582303ms 752.585947ms 752.814251ms 752.872362ms 752.925804ms 753.00902ms 753.150281ms 753.173789ms 753.257579ms 753.375734ms 753.547482ms 753.602424ms 753.657306ms 753.833213ms 754.110032ms 754.249599ms 754.380314ms 754.452858ms 754.684779ms 754.708376ms 754.847126ms 755.110981ms 755.173711ms 755.505424ms 755.516121ms 755.559706ms 756.449573ms 756.532203ms 756.784192ms 756.784308ms 757.586754ms 758.011474ms 758.03731ms 758.25269ms 759.739726ms 759.937648ms 760.02177ms 761.688229ms 763.06502ms 777.747208ms 796.305612ms]
Apr 26 14:16:58.825: INFO: 50 %ile: 747.535114ms
Apr 26 14:16:58.825: INFO: 90 %ile: 755.110981ms
Apr 26 14:16:58.825: INFO: 99 %ile: 777.747208ms
Apr 26 14:16:58.825: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:16:58.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-8770" for this suite.

• [SLOW TEST:11.857 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":303,"completed":118,"skipped":1778,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:16:58.851: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:17:03.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3234" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":303,"completed":119,"skipped":1793,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:17:03.031: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Starting the proxy
Apr 26 14:17:03.098: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-610780823 proxy --unix-socket=/tmp/kubectl-proxy-unix565783210/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:17:03.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4744" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":303,"completed":120,"skipped":1798,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:17:03.200: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Apr 26 14:17:03.293: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:17:09.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-825" for this suite.

• [SLOW TEST:6.531 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":303,"completed":121,"skipped":1808,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:17:09.732: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Apr 26 14:17:09.890: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 26 14:18:09.974: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
Apr 26 14:18:10.018: INFO: Created pod: pod0-sched-preemption-low-priority
Apr 26 14:18:10.048: INFO: Created pod: pod1-sched-preemption-medium-priority
Apr 26 14:18:10.084: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:18:30.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7886" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:80.572 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":303,"completed":122,"skipped":1832,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:18:30.306: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Apr 26 14:18:30.388: INFO: Waiting up to 5m0s for pod "pod-d19e6ee9-a638-4aef-9d1c-305ee65ea877" in namespace "emptydir-5266" to be "Succeeded or Failed"
Apr 26 14:18:30.399: INFO: Pod "pod-d19e6ee9-a638-4aef-9d1c-305ee65ea877": Phase="Pending", Reason="", readiness=false. Elapsed: 10.887141ms
Apr 26 14:18:32.406: INFO: Pod "pod-d19e6ee9-a638-4aef-9d1c-305ee65ea877": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017955227s
Apr 26 14:18:34.418: INFO: Pod "pod-d19e6ee9-a638-4aef-9d1c-305ee65ea877": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030371508s
STEP: Saw pod success
Apr 26 14:18:34.418: INFO: Pod "pod-d19e6ee9-a638-4aef-9d1c-305ee65ea877" satisfied condition "Succeeded or Failed"
Apr 26 14:18:34.433: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod pod-d19e6ee9-a638-4aef-9d1c-305ee65ea877 container test-container: <nil>
STEP: delete the pod
Apr 26 14:18:34.528: INFO: Waiting for pod pod-d19e6ee9-a638-4aef-9d1c-305ee65ea877 to disappear
Apr 26 14:18:34.533: INFO: Pod pod-d19e6ee9-a638-4aef-9d1c-305ee65ea877 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:18:34.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5266" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":123,"skipped":1871,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:18:34.555: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Apr 26 14:18:39.703: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:18:40.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2308" for this suite.

• [SLOW TEST:6.207 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":303,"completed":124,"skipped":1880,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:18:40.773: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-d651ad36-9bfe-4e84-b21a-5381b26f984e
STEP: Creating a pod to test consume configMaps
Apr 26 14:18:40.858: INFO: Waiting up to 5m0s for pod "pod-configmaps-12021e8b-8aac-4551-953c-5b99e637134c" in namespace "configmap-7339" to be "Succeeded or Failed"
Apr 26 14:18:40.867: INFO: Pod "pod-configmaps-12021e8b-8aac-4551-953c-5b99e637134c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.229365ms
Apr 26 14:18:42.874: INFO: Pod "pod-configmaps-12021e8b-8aac-4551-953c-5b99e637134c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016351928s
Apr 26 14:18:44.881: INFO: Pod "pod-configmaps-12021e8b-8aac-4551-953c-5b99e637134c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02337289s
STEP: Saw pod success
Apr 26 14:18:44.882: INFO: Pod "pod-configmaps-12021e8b-8aac-4551-953c-5b99e637134c" satisfied condition "Succeeded or Failed"
Apr 26 14:18:44.887: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-w8lg4 pod pod-configmaps-12021e8b-8aac-4551-953c-5b99e637134c container configmap-volume-test: <nil>
STEP: delete the pod
Apr 26 14:18:44.980: INFO: Waiting for pod pod-configmaps-12021e8b-8aac-4551-953c-5b99e637134c to disappear
Apr 26 14:18:44.985: INFO: Pod pod-configmaps-12021e8b-8aac-4551-953c-5b99e637134c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:18:44.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7339" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":125,"skipped":1921,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:18:45.008: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-696b6861-bf7b-43ae-b25d-a4811e449596
STEP: Creating a pod to test consume secrets
Apr 26 14:18:45.097: INFO: Waiting up to 5m0s for pod "pod-secrets-5f393be2-5f82-4bd5-8bff-2f24fe8b08c6" in namespace "secrets-4301" to be "Succeeded or Failed"
Apr 26 14:18:45.103: INFO: Pod "pod-secrets-5f393be2-5f82-4bd5-8bff-2f24fe8b08c6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.916486ms
Apr 26 14:18:47.109: INFO: Pod "pod-secrets-5f393be2-5f82-4bd5-8bff-2f24fe8b08c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012775181s
Apr 26 14:18:49.116: INFO: Pod "pod-secrets-5f393be2-5f82-4bd5-8bff-2f24fe8b08c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019474915s
STEP: Saw pod success
Apr 26 14:18:49.116: INFO: Pod "pod-secrets-5f393be2-5f82-4bd5-8bff-2f24fe8b08c6" satisfied condition "Succeeded or Failed"
Apr 26 14:18:49.122: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-w8lg4 pod pod-secrets-5f393be2-5f82-4bd5-8bff-2f24fe8b08c6 container secret-env-test: <nil>
STEP: delete the pod
Apr 26 14:18:49.166: INFO: Waiting for pod pod-secrets-5f393be2-5f82-4bd5-8bff-2f24fe8b08c6 to disappear
Apr 26 14:18:49.172: INFO: Pod pod-secrets-5f393be2-5f82-4bd5-8bff-2f24fe8b08c6 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:18:49.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4301" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":303,"completed":126,"skipped":1930,"failed":0}
S
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:18:49.194: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Apr 26 14:18:49.255: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:18:53.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-694" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":303,"completed":127,"skipped":1931,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:18:53.338: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-687b022b-c492-4341-9f1b-0ef90ec1a997
STEP: Creating a pod to test consume configMaps
Apr 26 14:18:53.429: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8769b717-08fa-4b75-a5ea-f6cbb79efffc" in namespace "projected-2471" to be "Succeeded or Failed"
Apr 26 14:18:53.462: INFO: Pod "pod-projected-configmaps-8769b717-08fa-4b75-a5ea-f6cbb79efffc": Phase="Pending", Reason="", readiness=false. Elapsed: 32.828976ms
Apr 26 14:18:55.469: INFO: Pod "pod-projected-configmaps-8769b717-08fa-4b75-a5ea-f6cbb79efffc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039878277s
Apr 26 14:18:57.480: INFO: Pod "pod-projected-configmaps-8769b717-08fa-4b75-a5ea-f6cbb79efffc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050729874s
STEP: Saw pod success
Apr 26 14:18:57.480: INFO: Pod "pod-projected-configmaps-8769b717-08fa-4b75-a5ea-f6cbb79efffc" satisfied condition "Succeeded or Failed"
Apr 26 14:18:57.492: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod pod-projected-configmaps-8769b717-08fa-4b75-a5ea-f6cbb79efffc container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 26 14:18:57.575: INFO: Waiting for pod pod-projected-configmaps-8769b717-08fa-4b75-a5ea-f6cbb79efffc to disappear
Apr 26 14:18:57.583: INFO: Pod pod-projected-configmaps-8769b717-08fa-4b75-a5ea-f6cbb79efffc no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:18:57.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2471" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":303,"completed":128,"skipped":1967,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:18:57.610: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 26 14:18:58.304: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 26 14:19:00.325: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755043538, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755043538, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755043538, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755043538, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 26 14:19:03.356: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:19:03.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3960" for this suite.
STEP: Destroying namespace "webhook-3960-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.230 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":303,"completed":129,"skipped":1975,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Ingress API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:19:03.840: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Apr 26 14:19:03.975: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Apr 26 14:19:03.984: INFO: starting watch
STEP: patching
STEP: updating
Apr 26 14:19:04.006: INFO: waiting for watch events with expected annotations
Apr 26 14:19:04.006: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:19:04.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-902" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":303,"completed":130,"skipped":1986,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:19:04.123: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Apr 26 14:19:04.642: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Apr 26 14:19:06.661: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755043544, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755043544, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755043544, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755043544, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 26 14:19:09.697: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Apr 26 14:19:09.703: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:19:11.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-3409" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:7.317 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":303,"completed":131,"skipped":1994,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:19:11.449: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Apr 26 14:19:21.725: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Apr 26 14:19:21.725: INFO: Deleting pod "simpletest-rc-to-be-deleted-2r449" in namespace "gc-7280"
W0426 14:19:21.724943      21 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0426 14:19:21.725016      21 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0426 14:19:21.725022      21 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Apr 26 14:19:21.752: INFO: Deleting pod "simpletest-rc-to-be-deleted-4gtbj" in namespace "gc-7280"
Apr 26 14:19:21.773: INFO: Deleting pod "simpletest-rc-to-be-deleted-8nlwt" in namespace "gc-7280"
Apr 26 14:19:21.818: INFO: Deleting pod "simpletest-rc-to-be-deleted-lf585" in namespace "gc-7280"
Apr 26 14:19:21.856: INFO: Deleting pod "simpletest-rc-to-be-deleted-lxm52" in namespace "gc-7280"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:19:21.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7280" for this suite.

• [SLOW TEST:10.471 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":303,"completed":132,"skipped":2007,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:19:21.919: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Apr 26 14:19:21.998: INFO: Pod name rollover-pod: Found 0 pods out of 1
Apr 26 14:19:27.005: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr 26 14:19:27.006: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Apr 26 14:19:29.012: INFO: Creating deployment "test-rollover-deployment"
Apr 26 14:19:29.028: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Apr 26 14:19:31.051: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Apr 26 14:19:31.061: INFO: Ensure that both replica sets have 1 created replica
Apr 26 14:19:31.071: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Apr 26 14:19:31.088: INFO: Updating deployment test-rollover-deployment
Apr 26 14:19:31.088: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Apr 26 14:19:33.107: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Apr 26 14:19:33.118: INFO: Make sure deployment "test-rollover-deployment" is complete
Apr 26 14:19:33.129: INFO: all replica sets need to contain the pod-template-hash label
Apr 26 14:19:33.129: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755043569, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755043569, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755043571, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755043569, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 26 14:19:35.142: INFO: all replica sets need to contain the pod-template-hash label
Apr 26 14:19:35.142: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755043569, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755043569, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755043573, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755043569, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 26 14:19:37.151: INFO: all replica sets need to contain the pod-template-hash label
Apr 26 14:19:37.151: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755043569, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755043569, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755043573, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755043569, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 26 14:19:39.144: INFO: all replica sets need to contain the pod-template-hash label
Apr 26 14:19:39.144: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755043569, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755043569, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755043573, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755043569, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 26 14:19:41.142: INFO: all replica sets need to contain the pod-template-hash label
Apr 26 14:19:41.143: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755043569, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755043569, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755043573, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755043569, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 26 14:19:43.144: INFO: all replica sets need to contain the pod-template-hash label
Apr 26 14:19:43.144: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755043569, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755043569, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755043573, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755043569, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 26 14:19:45.142: INFO: 
Apr 26 14:19:45.143: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Apr 26 14:19:45.160: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-8836 /apis/apps/v1/namespaces/deployment-8836/deployments/test-rollover-deployment 54ae4d8a-3079-489b-8094-ce2d78a41556 28240 2 2021-04-26 14:19:28 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-04-26 14:19:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-04-26 14:19:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0002bab48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-04-26 14:19:29 +0000 UTC,LastTransitionTime:2021-04-26 14:19:29 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-5797c7764" has successfully progressed.,LastUpdateTime:2021-04-26 14:19:43 +0000 UTC,LastTransitionTime:2021-04-26 14:19:29 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Apr 26 14:19:45.167: INFO: New ReplicaSet "test-rollover-deployment-5797c7764" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-5797c7764  deployment-8836 /apis/apps/v1/namespaces/deployment-8836/replicasets/test-rollover-deployment-5797c7764 1d6737d4-9e9c-4582-bca8-5888d56c9e1d 28229 2 2021-04-26 14:19:31 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 54ae4d8a-3079-489b-8094-ce2d78a41556 0xc0025741f0 0xc0025741f1}] []  [{kube-controller-manager Update apps/v1 2021-04-26 14:19:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"54ae4d8a-3079-489b-8094-ce2d78a41556\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5797c7764,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002574268 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr 26 14:19:45.167: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Apr 26 14:19:45.167: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8836 /apis/apps/v1/namespaces/deployment-8836/replicasets/test-rollover-controller c6a6d05f-d146-427f-8396-057758d010ad 28238 2 2021-04-26 14:19:21 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 54ae4d8a-3079-489b-8094-ce2d78a41556 0xc0025740e7 0xc0025740e8}] []  [{e2e.test Update apps/v1 2021-04-26 14:19:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-04-26 14:19:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"54ae4d8a-3079-489b-8094-ce2d78a41556\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002574188 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 26 14:19:45.167: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-8836 /apis/apps/v1/namespaces/deployment-8836/replicasets/test-rollover-deployment-78bc8b888c d8d27bb2-03ce-4801-8bfd-1f814e2d35bd 28158 2 2021-04-26 14:19:29 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 54ae4d8a-3079-489b-8094-ce2d78a41556 0xc0025742d7 0xc0025742d8}] []  [{kube-controller-manager Update apps/v1 2021-04-26 14:19:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"54ae4d8a-3079-489b-8094-ce2d78a41556\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002574368 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 26 14:19:45.174: INFO: Pod "test-rollover-deployment-5797c7764-lrg8n" is available:
&Pod{ObjectMeta:{test-rollover-deployment-5797c7764-lrg8n test-rollover-deployment-5797c7764- deployment-8836 /api/v1/namespaces/deployment-8836/pods/test-rollover-deployment-5797c7764-lrg8n 31b2728a-fb87-4eae-8a7c-dc917b6784b1 28178 0 2021-04-26 14:19:31 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[cni.projectcalico.org/podIP:172.25.1.139/32] [{apps/v1 ReplicaSet test-rollover-deployment-5797c7764 1d6737d4-9e9c-4582-bca8-5888d56c9e1d 0xc002575be0 0xc002575be1}] []  [{kube-controller-manager Update v1 2021-04-26 14:19:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1d6737d4-9e9c-4582-bca8-5888d56c9e1d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-04-26 14:19:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2021-04-26 14:19:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.139\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tp2wv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tp2wv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tp2wv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:elastic-hugle-5f9d9b5555-8444w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:19:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:19:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:19:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 14:19:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.11,PodIP:172.25.1.139,StartTime:2021-04-26 14:19:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-26 14:19:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:docker://06591ec940e2433ad9f152e6781cccb7ead01f2b2cffd296315ec251b3697088,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.139,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:19:45.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8836" for this suite.

• [SLOW TEST:23.273 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":303,"completed":133,"skipped":2024,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:19:45.197: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 26 14:19:45.875: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 26 14:19:47.894: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755043585, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755043585, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755043585, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755043585, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 26 14:19:50.919: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Apr 26 14:19:50.924: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3189-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:19:52.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8623" for this suite.
STEP: Destroying namespace "webhook-8623-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.305 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":303,"completed":134,"skipped":2049,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:19:52.506: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Apr 26 14:19:56.659: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-5807 PodName:pod-sharedvolume-0fabfd80-d292-46e1-9f19-c3f22db38467 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 26 14:19:56.659: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
Apr 26 14:19:57.178: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:19:57.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5807" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":303,"completed":135,"skipped":2061,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:19:57.198: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:20:57.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8830" for this suite.

• [SLOW TEST:60.115 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":303,"completed":136,"skipped":2078,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:20:57.313: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Apr 26 14:20:57.375: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:20:58.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5538" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":303,"completed":137,"skipped":2087,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:20:58.593: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 26 14:20:59.541: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 26 14:21:01.557: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755043659, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755043659, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755043659, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755043659, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 26 14:21:04.578: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:21:04.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4791" for this suite.
STEP: Destroying namespace "webhook-4791-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.371 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":303,"completed":138,"skipped":2090,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:21:04.964: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:21:15.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5221" for this suite.

• [SLOW TEST:10.116 seconds]
[sig-apps] Job
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":303,"completed":139,"skipped":2111,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:21:15.081: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Apr 26 14:21:55.236: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Apr 26 14:21:55.236: INFO: Deleting pod "simpletest.rc-4zrn8" in namespace "gc-405"
W0426 14:21:55.236705      21 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0426 14:21:55.236748      21 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0426 14:21:55.236754      21 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Apr 26 14:21:55.280: INFO: Deleting pod "simpletest.rc-5sgqs" in namespace "gc-405"
Apr 26 14:21:55.310: INFO: Deleting pod "simpletest.rc-7mbz5" in namespace "gc-405"
Apr 26 14:21:55.331: INFO: Deleting pod "simpletest.rc-8fx8b" in namespace "gc-405"
Apr 26 14:21:55.364: INFO: Deleting pod "simpletest.rc-8p2mc" in namespace "gc-405"
Apr 26 14:21:55.390: INFO: Deleting pod "simpletest.rc-fxpbk" in namespace "gc-405"
Apr 26 14:21:55.428: INFO: Deleting pod "simpletest.rc-hcsmg" in namespace "gc-405"
Apr 26 14:21:55.461: INFO: Deleting pod "simpletest.rc-ml56h" in namespace "gc-405"
Apr 26 14:21:55.488: INFO: Deleting pod "simpletest.rc-wdz2n" in namespace "gc-405"
Apr 26 14:21:55.524: INFO: Deleting pod "simpletest.rc-zpggx" in namespace "gc-405"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:21:55.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-405" for this suite.

• [SLOW TEST:40.498 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":303,"completed":140,"skipped":2117,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:21:55.586: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:22:00.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8505" for this suite.

• [SLOW TEST:5.419 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":303,"completed":141,"skipped":2133,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:22:01.006: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Apr 26 14:22:01.082: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:22:15.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3683" for this suite.

• [SLOW TEST:14.782 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":303,"completed":142,"skipped":2153,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:22:15.794: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:22:19.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7888" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":143,"skipped":2177,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:22:19.946: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create deployment with httpd image
Apr 26 14:22:20.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 create -f -'
Apr 26 14:22:20.571: INFO: stderr: ""
Apr 26 14:22:20.571: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Apr 26 14:22:20.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 diff -f -'
Apr 26 14:22:20.990: INFO: rc: 1
Apr 26 14:22:20.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 delete -f -'
Apr 26 14:22:21.100: INFO: stderr: ""
Apr 26 14:22:21.100: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:22:21.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3704" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":303,"completed":144,"skipped":2201,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:22:21.123: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod test-webserver-aceada03-1251-42b6-969c-a2bba0ec2214 in namespace container-probe-6873
Apr 26 14:22:25.235: INFO: Started pod test-webserver-aceada03-1251-42b6-969c-a2bba0ec2214 in namespace container-probe-6873
STEP: checking the pod's current state and verifying that restartCount is present
Apr 26 14:22:25.240: INFO: Initial restart count of pod test-webserver-aceada03-1251-42b6-969c-a2bba0ec2214 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:26:26.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6873" for this suite.

• [SLOW TEST:245.098 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":303,"completed":145,"skipped":2237,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:26:26.228: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Apr 26 14:26:26.338: INFO: Create a RollingUpdate DaemonSet
Apr 26 14:26:26.353: INFO: Check that daemon pods launch on every node of the cluster
Apr 26 14:26:26.372: INFO: Number of nodes with available pods: 0
Apr 26 14:26:26.372: INFO: Node elastic-hugle-5f9d9b5555-8444w is running more than one daemon pod
Apr 26 14:26:27.395: INFO: Number of nodes with available pods: 0
Apr 26 14:26:27.395: INFO: Node elastic-hugle-5f9d9b5555-8444w is running more than one daemon pod
Apr 26 14:26:28.388: INFO: Number of nodes with available pods: 1
Apr 26 14:26:28.388: INFO: Node elastic-hugle-5f9d9b5555-8444w is running more than one daemon pod
Apr 26 14:26:29.391: INFO: Number of nodes with available pods: 3
Apr 26 14:26:29.391: INFO: Number of running nodes: 3, number of available pods: 3
Apr 26 14:26:29.391: INFO: Update the DaemonSet to trigger a rollout
Apr 26 14:26:29.409: INFO: Updating DaemonSet daemon-set
Apr 26 14:26:42.448: INFO: Roll back the DaemonSet before rollout is complete
Apr 26 14:26:42.465: INFO: Updating DaemonSet daemon-set
Apr 26 14:26:42.465: INFO: Make sure DaemonSet rollback is complete
Apr 26 14:26:42.474: INFO: Wrong image for pod: daemon-set-l6fcg. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Apr 26 14:26:42.474: INFO: Pod daemon-set-l6fcg is not available
Apr 26 14:26:43.495: INFO: Wrong image for pod: daemon-set-l6fcg. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Apr 26 14:26:43.495: INFO: Pod daemon-set-l6fcg is not available
Apr 26 14:26:44.494: INFO: Pod daemon-set-8f2g5 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5870, will wait for the garbage collector to delete the pods
Apr 26 14:26:44.585: INFO: Deleting DaemonSet.extensions daemon-set took: 14.204074ms
Apr 26 14:26:45.185: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.24635ms
Apr 26 14:26:55.791: INFO: Number of nodes with available pods: 0
Apr 26 14:26:55.791: INFO: Number of running nodes: 0, number of available pods: 0
Apr 26 14:26:55.797: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5870/daemonsets","resourceVersion":"31228"},"items":null}

Apr 26 14:26:55.802: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5870/pods","resourceVersion":"31228"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:26:55.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5870" for this suite.

• [SLOW TEST:29.623 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":303,"completed":146,"skipped":2245,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:26:55.853: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Apr 26 14:26:55.908: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 26 14:26:55.930: INFO: Waiting for terminating namespaces to be deleted...
Apr 26 14:26:55.936: INFO: 
Logging pods the apiserver thinks is on node elastic-hugle-5f9d9b5555-8444w before test
Apr 26 14:26:55.946: INFO: canal-9hndk from kube-system started at 2021-04-26 13:19:15 +0000 UTC (2 container statuses recorded)
Apr 26 14:26:55.946: INFO: 	Container calico-node ready: true, restart count 0
Apr 26 14:26:55.947: INFO: 	Container kube-flannel ready: true, restart count 0
Apr 26 14:26:55.947: INFO: coredns-85c978995d-bmmb9 from kube-system started at 2021-04-26 13:19:44 +0000 UTC (1 container statuses recorded)
Apr 26 14:26:55.947: INFO: 	Container coredns ready: true, restart count 0
Apr 26 14:26:55.947: INFO: csi-cinder-nodeplugin-flatcar-nbzzj from kube-system started at 2021-04-26 13:19:37 +0000 UTC (2 container statuses recorded)
Apr 26 14:26:55.947: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Apr 26 14:26:55.947: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 26 14:26:55.947: INFO: kube-proxy-qvhqj from kube-system started at 2021-04-26 13:19:15 +0000 UTC (1 container statuses recorded)
Apr 26 14:26:55.947: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 26 14:26:55.947: INFO: logrotate-p98vn from kube-system started at 2021-04-26 13:19:37 +0000 UTC (1 container statuses recorded)
Apr 26 14:26:55.947: INFO: 	Container logrotate ready: true, restart count 0
Apr 26 14:26:55.947: INFO: node-local-dns-pfs8r from kube-system started at 2021-04-26 13:19:37 +0000 UTC (1 container statuses recorded)
Apr 26 14:26:55.947: INFO: 	Container node-cache ready: true, restart count 0
Apr 26 14:26:55.947: INFO: user-ssh-keys-agent-vck9l from kube-system started at 2021-04-26 13:19:15 +0000 UTC (1 container statuses recorded)
Apr 26 14:26:55.947: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Apr 26 14:26:55.947: INFO: dashboard-metrics-scraper-975c84c89-fc9kf from kubernetes-dashboard started at 2021-04-26 13:19:45 +0000 UTC (1 container statuses recorded)
Apr 26 14:26:55.947: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Apr 26 14:26:55.947: INFO: sonobuoy from sonobuoy started at 2021-04-26 13:33:20 +0000 UTC (1 container statuses recorded)
Apr 26 14:26:55.947: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 26 14:26:55.947: INFO: sonobuoy-systemd-logs-daemon-set-83692f989abe48af-6hhl5 from sonobuoy started at 2021-04-26 13:33:27 +0000 UTC (2 container statuses recorded)
Apr 26 14:26:55.947: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 14:26:55.947: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 26 14:26:55.947: INFO: 
Logging pods the apiserver thinks is on node elastic-hugle-5f9d9b5555-mj2j9 before test
Apr 26 14:26:55.958: INFO: canal-vrzgr from kube-system started at 2021-04-26 13:19:17 +0000 UTC (2 container statuses recorded)
Apr 26 14:26:55.958: INFO: 	Container calico-node ready: true, restart count 0
Apr 26 14:26:55.958: INFO: 	Container kube-flannel ready: true, restart count 0
Apr 26 14:26:55.958: INFO: csi-cinder-controllerplugin-0 from kube-system started at 2021-04-26 13:19:45 +0000 UTC (5 container statuses recorded)
Apr 26 14:26:55.958: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Apr 26 14:26:55.958: INFO: 	Container csi-attacher ready: true, restart count 0
Apr 26 14:26:55.958: INFO: 	Container csi-provisioner ready: true, restart count 0
Apr 26 14:26:55.959: INFO: 	Container csi-resizer ready: true, restart count 0
Apr 26 14:26:55.959: INFO: 	Container csi-snapshotter ready: true, restart count 0
Apr 26 14:26:55.959: INFO: csi-cinder-nodeplugin-flatcar-dt2hz from kube-system started at 2021-04-26 13:19:37 +0000 UTC (2 container statuses recorded)
Apr 26 14:26:55.959: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Apr 26 14:26:55.959: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 26 14:26:55.959: INFO: kube-proxy-6gbkb from kube-system started at 2021-04-26 13:19:17 +0000 UTC (1 container statuses recorded)
Apr 26 14:26:55.959: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 26 14:26:55.959: INFO: logrotate-27s5t from kube-system started at 2021-04-26 13:19:37 +0000 UTC (1 container statuses recorded)
Apr 26 14:26:55.959: INFO: 	Container logrotate ready: true, restart count 0
Apr 26 14:26:55.959: INFO: node-local-dns-qtvvc from kube-system started at 2021-04-26 13:19:37 +0000 UTC (1 container statuses recorded)
Apr 26 14:26:55.960: INFO: 	Container node-cache ready: true, restart count 0
Apr 26 14:26:55.960: INFO: user-ssh-keys-agent-46lff from kube-system started at 2021-04-26 13:19:17 +0000 UTC (1 container statuses recorded)
Apr 26 14:26:55.960: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Apr 26 14:26:55.960: INFO: sonobuoy-systemd-logs-daemon-set-83692f989abe48af-ppqzq from sonobuoy started at 2021-04-26 13:33:27 +0000 UTC (2 container statuses recorded)
Apr 26 14:26:55.960: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 14:26:55.960: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 26 14:26:55.960: INFO: 
Logging pods the apiserver thinks is on node elastic-hugle-5f9d9b5555-w8lg4 before test
Apr 26 14:26:55.976: INFO: canal-xcnxd from kube-system started at 2021-04-26 13:19:12 +0000 UTC (2 container statuses recorded)
Apr 26 14:26:55.976: INFO: 	Container calico-node ready: true, restart count 0
Apr 26 14:26:55.976: INFO: 	Container kube-flannel ready: true, restart count 1
Apr 26 14:26:55.976: INFO: coredns-85c978995d-gzh5v from kube-system started at 2021-04-26 13:19:44 +0000 UTC (1 container statuses recorded)
Apr 26 14:26:55.977: INFO: 	Container coredns ready: true, restart count 0
Apr 26 14:26:55.977: INFO: csi-cinder-nodeplugin-flatcar-gqw8g from kube-system started at 2021-04-26 13:19:42 +0000 UTC (2 container statuses recorded)
Apr 26 14:26:55.977: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Apr 26 14:26:55.977: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 26 14:26:55.977: INFO: kube-proxy-xkvxw from kube-system started at 2021-04-26 13:19:12 +0000 UTC (1 container statuses recorded)
Apr 26 14:26:55.977: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 26 14:26:55.977: INFO: logrotate-xhwmz from kube-system started at 2021-04-26 13:19:42 +0000 UTC (1 container statuses recorded)
Apr 26 14:26:55.977: INFO: 	Container logrotate ready: true, restart count 0
Apr 26 14:26:55.977: INFO: node-local-dns-4jkgn from kube-system started at 2021-04-26 13:19:42 +0000 UTC (1 container statuses recorded)
Apr 26 14:26:55.978: INFO: 	Container node-cache ready: true, restart count 0
Apr 26 14:26:55.978: INFO: openvpn-client-6c9dd998bc-hzr48 from kube-system started at 2021-04-26 13:19:45 +0000 UTC (2 container statuses recorded)
Apr 26 14:26:55.978: INFO: 	Container dnat-controller ready: true, restart count 0
Apr 26 14:26:55.978: INFO: 	Container openvpn-client ready: true, restart count 0
Apr 26 14:26:55.978: INFO: user-ssh-keys-agent-hr99m from kube-system started at 2021-04-26 13:19:12 +0000 UTC (1 container statuses recorded)
Apr 26 14:26:55.978: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Apr 26 14:26:55.978: INFO: dashboard-metrics-scraper-975c84c89-drmjh from kubernetes-dashboard started at 2021-04-26 13:19:45 +0000 UTC (1 container statuses recorded)
Apr 26 14:26:55.978: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Apr 26 14:26:55.978: INFO: sonobuoy-e2e-job-c567c19005014a0f from sonobuoy started at 2021-04-26 13:33:27 +0000 UTC (2 container statuses recorded)
Apr 26 14:26:55.979: INFO: 	Container e2e ready: true, restart count 0
Apr 26 14:26:55.979: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 14:26:55.979: INFO: sonobuoy-systemd-logs-daemon-set-83692f989abe48af-rg644 from sonobuoy started at 2021-04-26 13:33:27 +0000 UTC (2 container statuses recorded)
Apr 26 14:26:55.979: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 14:26:55.979: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-0d3b2eee-233d-4560-8283-ceef2699d0e3 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-0d3b2eee-233d-4560-8283-ceef2699d0e3 off the node elastic-hugle-5f9d9b5555-8444w
STEP: verifying the node doesn't have the label kubernetes.io/e2e-0d3b2eee-233d-4560-8283-ceef2699d0e3
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:27:12.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1832" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:16.374 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":303,"completed":147,"skipped":2267,"failed":0}
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:27:12.227: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Apr 26 14:27:12.301: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 26 14:27:12.319: INFO: Waiting for terminating namespaces to be deleted...
Apr 26 14:27:12.325: INFO: 
Logging pods the apiserver thinks is on node elastic-hugle-5f9d9b5555-8444w before test
Apr 26 14:27:12.344: INFO: canal-9hndk from kube-system started at 2021-04-26 13:19:15 +0000 UTC (2 container statuses recorded)
Apr 26 14:27:12.344: INFO: 	Container calico-node ready: true, restart count 0
Apr 26 14:27:12.344: INFO: 	Container kube-flannel ready: true, restart count 0
Apr 26 14:27:12.344: INFO: coredns-85c978995d-bmmb9 from kube-system started at 2021-04-26 13:19:44 +0000 UTC (1 container statuses recorded)
Apr 26 14:27:12.344: INFO: 	Container coredns ready: true, restart count 0
Apr 26 14:27:12.344: INFO: csi-cinder-nodeplugin-flatcar-nbzzj from kube-system started at 2021-04-26 13:19:37 +0000 UTC (2 container statuses recorded)
Apr 26 14:27:12.344: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Apr 26 14:27:12.344: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 26 14:27:12.344: INFO: kube-proxy-qvhqj from kube-system started at 2021-04-26 13:19:15 +0000 UTC (1 container statuses recorded)
Apr 26 14:27:12.344: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 26 14:27:12.344: INFO: logrotate-p98vn from kube-system started at 2021-04-26 13:19:37 +0000 UTC (1 container statuses recorded)
Apr 26 14:27:12.344: INFO: 	Container logrotate ready: true, restart count 0
Apr 26 14:27:12.344: INFO: node-local-dns-pfs8r from kube-system started at 2021-04-26 13:19:37 +0000 UTC (1 container statuses recorded)
Apr 26 14:27:12.344: INFO: 	Container node-cache ready: true, restart count 0
Apr 26 14:27:12.344: INFO: user-ssh-keys-agent-vck9l from kube-system started at 2021-04-26 13:19:15 +0000 UTC (1 container statuses recorded)
Apr 26 14:27:12.345: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Apr 26 14:27:12.345: INFO: dashboard-metrics-scraper-975c84c89-fc9kf from kubernetes-dashboard started at 2021-04-26 13:19:45 +0000 UTC (1 container statuses recorded)
Apr 26 14:27:12.345: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Apr 26 14:27:12.345: INFO: pod1 from sched-pred-1832 started at 2021-04-26 14:27:00 +0000 UTC (1 container statuses recorded)
Apr 26 14:27:12.345: INFO: 	Container pod1 ready: true, restart count 0
Apr 26 14:27:12.345: INFO: pod2 from sched-pred-1832 started at 2021-04-26 14:27:04 +0000 UTC (1 container statuses recorded)
Apr 26 14:27:12.345: INFO: 	Container pod2 ready: true, restart count 0
Apr 26 14:27:12.345: INFO: pod3 from sched-pred-1832 started at 2021-04-26 14:27:08 +0000 UTC (1 container statuses recorded)
Apr 26 14:27:12.345: INFO: 	Container pod3 ready: true, restart count 0
Apr 26 14:27:12.345: INFO: sonobuoy from sonobuoy started at 2021-04-26 13:33:20 +0000 UTC (1 container statuses recorded)
Apr 26 14:27:12.345: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 26 14:27:12.345: INFO: sonobuoy-systemd-logs-daemon-set-83692f989abe48af-6hhl5 from sonobuoy started at 2021-04-26 13:33:27 +0000 UTC (2 container statuses recorded)
Apr 26 14:27:12.345: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 14:27:12.345: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 26 14:27:12.345: INFO: 
Logging pods the apiserver thinks is on node elastic-hugle-5f9d9b5555-mj2j9 before test
Apr 26 14:27:12.358: INFO: canal-vrzgr from kube-system started at 2021-04-26 13:19:17 +0000 UTC (2 container statuses recorded)
Apr 26 14:27:12.358: INFO: 	Container calico-node ready: true, restart count 0
Apr 26 14:27:12.358: INFO: 	Container kube-flannel ready: true, restart count 0
Apr 26 14:27:12.358: INFO: csi-cinder-controllerplugin-0 from kube-system started at 2021-04-26 13:19:45 +0000 UTC (5 container statuses recorded)
Apr 26 14:27:12.358: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Apr 26 14:27:12.359: INFO: 	Container csi-attacher ready: true, restart count 0
Apr 26 14:27:12.359: INFO: 	Container csi-provisioner ready: true, restart count 0
Apr 26 14:27:12.359: INFO: 	Container csi-resizer ready: true, restart count 0
Apr 26 14:27:12.359: INFO: 	Container csi-snapshotter ready: true, restart count 0
Apr 26 14:27:12.359: INFO: csi-cinder-nodeplugin-flatcar-dt2hz from kube-system started at 2021-04-26 13:19:37 +0000 UTC (2 container statuses recorded)
Apr 26 14:27:12.359: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Apr 26 14:27:12.359: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 26 14:27:12.359: INFO: kube-proxy-6gbkb from kube-system started at 2021-04-26 13:19:17 +0000 UTC (1 container statuses recorded)
Apr 26 14:27:12.359: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 26 14:27:12.359: INFO: logrotate-27s5t from kube-system started at 2021-04-26 13:19:37 +0000 UTC (1 container statuses recorded)
Apr 26 14:27:12.360: INFO: 	Container logrotate ready: true, restart count 0
Apr 26 14:27:12.360: INFO: node-local-dns-qtvvc from kube-system started at 2021-04-26 13:19:37 +0000 UTC (1 container statuses recorded)
Apr 26 14:27:12.360: INFO: 	Container node-cache ready: true, restart count 0
Apr 26 14:27:12.360: INFO: user-ssh-keys-agent-46lff from kube-system started at 2021-04-26 13:19:17 +0000 UTC (1 container statuses recorded)
Apr 26 14:27:12.360: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Apr 26 14:27:12.360: INFO: sonobuoy-systemd-logs-daemon-set-83692f989abe48af-ppqzq from sonobuoy started at 2021-04-26 13:33:27 +0000 UTC (2 container statuses recorded)
Apr 26 14:27:12.360: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 14:27:12.360: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 26 14:27:12.360: INFO: 
Logging pods the apiserver thinks is on node elastic-hugle-5f9d9b5555-w8lg4 before test
Apr 26 14:27:12.372: INFO: canal-xcnxd from kube-system started at 2021-04-26 13:19:12 +0000 UTC (2 container statuses recorded)
Apr 26 14:27:12.373: INFO: 	Container calico-node ready: true, restart count 0
Apr 26 14:27:12.373: INFO: 	Container kube-flannel ready: true, restart count 1
Apr 26 14:27:12.373: INFO: coredns-85c978995d-gzh5v from kube-system started at 2021-04-26 13:19:44 +0000 UTC (1 container statuses recorded)
Apr 26 14:27:12.373: INFO: 	Container coredns ready: true, restart count 0
Apr 26 14:27:12.373: INFO: csi-cinder-nodeplugin-flatcar-gqw8g from kube-system started at 2021-04-26 13:19:42 +0000 UTC (2 container statuses recorded)
Apr 26 14:27:12.373: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Apr 26 14:27:12.373: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 26 14:27:12.374: INFO: kube-proxy-xkvxw from kube-system started at 2021-04-26 13:19:12 +0000 UTC (1 container statuses recorded)
Apr 26 14:27:12.374: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 26 14:27:12.374: INFO: logrotate-xhwmz from kube-system started at 2021-04-26 13:19:42 +0000 UTC (1 container statuses recorded)
Apr 26 14:27:12.374: INFO: 	Container logrotate ready: true, restart count 0
Apr 26 14:27:12.374: INFO: node-local-dns-4jkgn from kube-system started at 2021-04-26 13:19:42 +0000 UTC (1 container statuses recorded)
Apr 26 14:27:12.374: INFO: 	Container node-cache ready: true, restart count 0
Apr 26 14:27:12.374: INFO: openvpn-client-6c9dd998bc-hzr48 from kube-system started at 2021-04-26 13:19:45 +0000 UTC (2 container statuses recorded)
Apr 26 14:27:12.374: INFO: 	Container dnat-controller ready: true, restart count 0
Apr 26 14:27:12.375: INFO: 	Container openvpn-client ready: true, restart count 0
Apr 26 14:27:12.375: INFO: user-ssh-keys-agent-hr99m from kube-system started at 2021-04-26 13:19:12 +0000 UTC (1 container statuses recorded)
Apr 26 14:27:12.375: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Apr 26 14:27:12.375: INFO: dashboard-metrics-scraper-975c84c89-drmjh from kubernetes-dashboard started at 2021-04-26 13:19:45 +0000 UTC (1 container statuses recorded)
Apr 26 14:27:12.375: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Apr 26 14:27:12.375: INFO: sonobuoy-e2e-job-c567c19005014a0f from sonobuoy started at 2021-04-26 13:33:27 +0000 UTC (2 container statuses recorded)
Apr 26 14:27:12.375: INFO: 	Container e2e ready: true, restart count 0
Apr 26 14:27:12.375: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 14:27:12.376: INFO: sonobuoy-systemd-logs-daemon-set-83692f989abe48af-rg644 from sonobuoy started at 2021-04-26 13:33:27 +0000 UTC (2 container statuses recorded)
Apr 26 14:27:12.376: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 14:27:12.376: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: verifying the node has the label node elastic-hugle-5f9d9b5555-8444w
STEP: verifying the node has the label node elastic-hugle-5f9d9b5555-mj2j9
STEP: verifying the node has the label node elastic-hugle-5f9d9b5555-w8lg4
Apr 26 14:27:12.522: INFO: Pod canal-9hndk requesting resource cpu=250m on Node elastic-hugle-5f9d9b5555-8444w
Apr 26 14:27:12.522: INFO: Pod canal-vrzgr requesting resource cpu=250m on Node elastic-hugle-5f9d9b5555-mj2j9
Apr 26 14:27:12.522: INFO: Pod canal-xcnxd requesting resource cpu=250m on Node elastic-hugle-5f9d9b5555-w8lg4
Apr 26 14:27:12.522: INFO: Pod coredns-85c978995d-bmmb9 requesting resource cpu=50m on Node elastic-hugle-5f9d9b5555-8444w
Apr 26 14:27:12.522: INFO: Pod coredns-85c978995d-gzh5v requesting resource cpu=50m on Node elastic-hugle-5f9d9b5555-w8lg4
Apr 26 14:27:12.522: INFO: Pod csi-cinder-controllerplugin-0 requesting resource cpu=0m on Node elastic-hugle-5f9d9b5555-mj2j9
Apr 26 14:27:12.522: INFO: Pod csi-cinder-nodeplugin-flatcar-dt2hz requesting resource cpu=0m on Node elastic-hugle-5f9d9b5555-mj2j9
Apr 26 14:27:12.522: INFO: Pod csi-cinder-nodeplugin-flatcar-gqw8g requesting resource cpu=0m on Node elastic-hugle-5f9d9b5555-w8lg4
Apr 26 14:27:12.522: INFO: Pod csi-cinder-nodeplugin-flatcar-nbzzj requesting resource cpu=0m on Node elastic-hugle-5f9d9b5555-8444w
Apr 26 14:27:12.522: INFO: Pod kube-proxy-6gbkb requesting resource cpu=75m on Node elastic-hugle-5f9d9b5555-mj2j9
Apr 26 14:27:12.522: INFO: Pod kube-proxy-qvhqj requesting resource cpu=75m on Node elastic-hugle-5f9d9b5555-8444w
Apr 26 14:27:12.522: INFO: Pod kube-proxy-xkvxw requesting resource cpu=75m on Node elastic-hugle-5f9d9b5555-w8lg4
Apr 26 14:27:12.522: INFO: Pod logrotate-27s5t requesting resource cpu=75m on Node elastic-hugle-5f9d9b5555-mj2j9
Apr 26 14:27:12.522: INFO: Pod logrotate-p98vn requesting resource cpu=75m on Node elastic-hugle-5f9d9b5555-8444w
Apr 26 14:27:12.522: INFO: Pod logrotate-xhwmz requesting resource cpu=75m on Node elastic-hugle-5f9d9b5555-w8lg4
Apr 26 14:27:12.522: INFO: Pod node-local-dns-4jkgn requesting resource cpu=0m on Node elastic-hugle-5f9d9b5555-w8lg4
Apr 26 14:27:12.522: INFO: Pod node-local-dns-pfs8r requesting resource cpu=0m on Node elastic-hugle-5f9d9b5555-8444w
Apr 26 14:27:12.522: INFO: Pod node-local-dns-qtvvc requesting resource cpu=0m on Node elastic-hugle-5f9d9b5555-mj2j9
Apr 26 14:27:12.522: INFO: Pod openvpn-client-6c9dd998bc-hzr48 requesting resource cpu=30m on Node elastic-hugle-5f9d9b5555-w8lg4
Apr 26 14:27:12.522: INFO: Pod user-ssh-keys-agent-46lff requesting resource cpu=0m on Node elastic-hugle-5f9d9b5555-mj2j9
Apr 26 14:27:12.522: INFO: Pod user-ssh-keys-agent-hr99m requesting resource cpu=0m on Node elastic-hugle-5f9d9b5555-w8lg4
Apr 26 14:27:12.522: INFO: Pod user-ssh-keys-agent-vck9l requesting resource cpu=0m on Node elastic-hugle-5f9d9b5555-8444w
Apr 26 14:27:12.522: INFO: Pod dashboard-metrics-scraper-975c84c89-drmjh requesting resource cpu=50m on Node elastic-hugle-5f9d9b5555-w8lg4
Apr 26 14:27:12.522: INFO: Pod dashboard-metrics-scraper-975c84c89-fc9kf requesting resource cpu=50m on Node elastic-hugle-5f9d9b5555-8444w
Apr 26 14:27:12.522: INFO: Pod pod1 requesting resource cpu=0m on Node elastic-hugle-5f9d9b5555-8444w
Apr 26 14:27:12.522: INFO: Pod pod2 requesting resource cpu=0m on Node elastic-hugle-5f9d9b5555-8444w
Apr 26 14:27:12.522: INFO: Pod pod3 requesting resource cpu=0m on Node elastic-hugle-5f9d9b5555-8444w
Apr 26 14:27:12.522: INFO: Pod sonobuoy requesting resource cpu=0m on Node elastic-hugle-5f9d9b5555-8444w
Apr 26 14:27:12.522: INFO: Pod sonobuoy-e2e-job-c567c19005014a0f requesting resource cpu=0m on Node elastic-hugle-5f9d9b5555-w8lg4
Apr 26 14:27:12.522: INFO: Pod sonobuoy-systemd-logs-daemon-set-83692f989abe48af-6hhl5 requesting resource cpu=0m on Node elastic-hugle-5f9d9b5555-8444w
Apr 26 14:27:12.522: INFO: Pod sonobuoy-systemd-logs-daemon-set-83692f989abe48af-ppqzq requesting resource cpu=0m on Node elastic-hugle-5f9d9b5555-mj2j9
Apr 26 14:27:12.522: INFO: Pod sonobuoy-systemd-logs-daemon-set-83692f989abe48af-rg644 requesting resource cpu=0m on Node elastic-hugle-5f9d9b5555-w8lg4
STEP: Starting Pods to consume most of the cluster CPU.
Apr 26 14:27:12.523: INFO: Creating a pod which consumes cpu=910m on Node elastic-hugle-5f9d9b5555-8444w
Apr 26 14:27:12.550: INFO: Creating a pod which consumes cpu=980m on Node elastic-hugle-5f9d9b5555-mj2j9
Apr 26 14:27:12.569: INFO: Creating a pod which consumes cpu=889m on Node elastic-hugle-5f9d9b5555-w8lg4
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0cd2f3ba-892f-4afc-9cf3-504e2616baf6.16796ead85b2fa3b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1155/filler-pod-0cd2f3ba-892f-4afc-9cf3-504e2616baf6 to elastic-hugle-5f9d9b5555-w8lg4]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0cd2f3ba-892f-4afc-9cf3-504e2616baf6.16796eade151b99d], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0cd2f3ba-892f-4afc-9cf3-504e2616baf6.16796eaded44544b], Reason = [Created], Message = [Created container filler-pod-0cd2f3ba-892f-4afc-9cf3-504e2616baf6]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0cd2f3ba-892f-4afc-9cf3-504e2616baf6.16796eadfa301da0], Reason = [Started], Message = [Started container filler-pod-0cd2f3ba-892f-4afc-9cf3-504e2616baf6]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-456b4a4f-2647-406e-8f01-a5ccf144f74e.16796ead83c1cd6f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1155/filler-pod-456b4a4f-2647-406e-8f01-a5ccf144f74e to elastic-hugle-5f9d9b5555-mj2j9]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-456b4a4f-2647-406e-8f01-a5ccf144f74e.16796eadc766dded], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-456b4a4f-2647-406e-8f01-a5ccf144f74e.16796eadd19ba911], Reason = [Created], Message = [Created container filler-pod-456b4a4f-2647-406e-8f01-a5ccf144f74e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-456b4a4f-2647-406e-8f01-a5ccf144f74e.16796eadddbc06ab], Reason = [Started], Message = [Started container filler-pod-456b4a4f-2647-406e-8f01-a5ccf144f74e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e0061bd0-8562-4a52-ba6f-ebd5f18bc92e.16796ead82dff229], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1155/filler-pod-e0061bd0-8562-4a52-ba6f-ebd5f18bc92e to elastic-hugle-5f9d9b5555-8444w]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e0061bd0-8562-4a52-ba6f-ebd5f18bc92e.16796eadcc44ae16], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e0061bd0-8562-4a52-ba6f-ebd5f18bc92e.16796eadda5c88c8], Reason = [Created], Message = [Created container filler-pod-e0061bd0-8562-4a52-ba6f-ebd5f18bc92e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e0061bd0-8562-4a52-ba6f-ebd5f18bc92e.16796eade6fd24ac], Reason = [Started], Message = [Started container filler-pod-e0061bd0-8562-4a52-ba6f-ebd5f18bc92e]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16796eae76e3dd81], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16796eae77b99f14], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node elastic-hugle-5f9d9b5555-8444w
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node elastic-hugle-5f9d9b5555-mj2j9
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node elastic-hugle-5f9d9b5555-w8lg4
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:27:17.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1155" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:5.539 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":303,"completed":148,"skipped":2276,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:27:17.771: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Apr 26 14:27:17.864: INFO: Waiting up to 5m0s for pod "downwardapi-volume-24f00c02-b3b6-4e2b-8e89-e51d726c742e" in namespace "downward-api-2701" to be "Succeeded or Failed"
Apr 26 14:27:17.888: INFO: Pod "downwardapi-volume-24f00c02-b3b6-4e2b-8e89-e51d726c742e": Phase="Pending", Reason="", readiness=false. Elapsed: 23.911517ms
Apr 26 14:27:19.899: INFO: Pod "downwardapi-volume-24f00c02-b3b6-4e2b-8e89-e51d726c742e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035150068s
Apr 26 14:27:21.906: INFO: Pod "downwardapi-volume-24f00c02-b3b6-4e2b-8e89-e51d726c742e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04153589s
STEP: Saw pod success
Apr 26 14:27:21.906: INFO: Pod "downwardapi-volume-24f00c02-b3b6-4e2b-8e89-e51d726c742e" satisfied condition "Succeeded or Failed"
Apr 26 14:27:21.911: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-w8lg4 pod downwardapi-volume-24f00c02-b3b6-4e2b-8e89-e51d726c742e container client-container: <nil>
STEP: delete the pod
Apr 26 14:27:21.952: INFO: Waiting for pod downwardapi-volume-24f00c02-b3b6-4e2b-8e89-e51d726c742e to disappear
Apr 26 14:27:21.958: INFO: Pod downwardapi-volume-24f00c02-b3b6-4e2b-8e89-e51d726c742e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:27:21.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2701" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":303,"completed":149,"skipped":2284,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:27:21.981: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:27:22.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7640" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":303,"completed":150,"skipped":2291,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:27:22.129: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Apr 26 14:27:22.722: INFO: Pod name wrapped-volume-race-2c62de74-2d64-42f4-9331-eacf518f6395: Found 0 pods out of 5
Apr 26 14:27:27.736: INFO: Pod name wrapped-volume-race-2c62de74-2d64-42f4-9331-eacf518f6395: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-2c62de74-2d64-42f4-9331-eacf518f6395 in namespace emptydir-wrapper-4234, will wait for the garbage collector to delete the pods
Apr 26 14:27:41.898: INFO: Deleting ReplicationController wrapped-volume-race-2c62de74-2d64-42f4-9331-eacf518f6395 took: 15.943798ms
Apr 26 14:27:42.598: INFO: Terminating ReplicationController wrapped-volume-race-2c62de74-2d64-42f4-9331-eacf518f6395 pods took: 700.224171ms
STEP: Creating RC which spawns configmap-volume pods
Apr 26 14:27:56.036: INFO: Pod name wrapped-volume-race-645928fc-4a8a-4748-83a0-7036d5d932c4: Found 0 pods out of 5
Apr 26 14:28:01.049: INFO: Pod name wrapped-volume-race-645928fc-4a8a-4748-83a0-7036d5d932c4: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-645928fc-4a8a-4748-83a0-7036d5d932c4 in namespace emptydir-wrapper-4234, will wait for the garbage collector to delete the pods
Apr 26 14:28:15.185: INFO: Deleting ReplicationController wrapped-volume-race-645928fc-4a8a-4748-83a0-7036d5d932c4 took: 20.47744ms
Apr 26 14:28:15.885: INFO: Terminating ReplicationController wrapped-volume-race-645928fc-4a8a-4748-83a0-7036d5d932c4 pods took: 700.28192ms
STEP: Creating RC which spawns configmap-volume pods
Apr 26 14:28:26.015: INFO: Pod name wrapped-volume-race-1dcdb9e5-4ea4-4d71-82fc-929b3e6d4f49: Found 0 pods out of 5
Apr 26 14:28:31.028: INFO: Pod name wrapped-volume-race-1dcdb9e5-4ea4-4d71-82fc-929b3e6d4f49: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-1dcdb9e5-4ea4-4d71-82fc-929b3e6d4f49 in namespace emptydir-wrapper-4234, will wait for the garbage collector to delete the pods
Apr 26 14:28:45.157: INFO: Deleting ReplicationController wrapped-volume-race-1dcdb9e5-4ea4-4d71-82fc-929b3e6d4f49 took: 23.104004ms
Apr 26 14:28:45.858: INFO: Terminating ReplicationController wrapped-volume-race-1dcdb9e5-4ea4-4d71-82fc-929b3e6d4f49 pods took: 700.5917ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:29:02.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4234" for this suite.

• [SLOW TEST:100.558 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":303,"completed":151,"skipped":2319,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:29:02.689: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl replace
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1581
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Apr 26 14:29:02.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-3801'
Apr 26 14:29:03.276: INFO: stderr: ""
Apr 26 14:29:03.276: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Apr 26 14:29:08.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 get pod e2e-test-httpd-pod --namespace=kubectl-3801 -o json'
Apr 26 14:29:09.418: INFO: stderr: ""
Apr 26 14:29:09.418: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"172.25.1.170/32\"\n        },\n        \"creationTimestamp\": \"2021-04-26T14:29:03Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-04-26T14:29:03Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:annotations\": {\n                            \".\": {},\n                            \"f:cni.projectcalico.org/podIP\": {}\n                        }\n                    }\n                },\n                \"manager\": \"calico\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-04-26T14:29:04Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"172.25.1.170\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-04-26T14:29:05Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-3801\",\n        \"resourceVersion\": \"32755\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-3801/pods/e2e-test-httpd-pod\",\n        \"uid\": \"17ab6d7d-15a3-4d65-bd06-19195799d167\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-srf6g\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"elastic-hugle-5f9d9b5555-8444w\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-srf6g\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-srf6g\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-04-26T14:29:03Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-04-26T14:29:05Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-04-26T14:29:05Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-04-26T14:29:03Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://971b732d1b9580db5ea945aef8c41232132cbde26a094c78230fb623d10e0cce\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-04-26T14:29:04Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.1.11\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.25.1.170\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.25.1.170\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-04-26T14:29:03Z\"\n    }\n}\n"
STEP: replace the image in the pod
Apr 26 14:29:09.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 replace -f - --namespace=kubectl-3801'
Apr 26 14:29:10.116: INFO: stderr: ""
Apr 26 14:29:10.117: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1586
Apr 26 14:29:10.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 delete pods e2e-test-httpd-pod --namespace=kubectl-3801'
Apr 26 14:29:12.371: INFO: stderr: ""
Apr 26 14:29:12.371: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:29:12.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3801" for this suite.

• [SLOW TEST:9.704 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1577
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":303,"completed":152,"skipped":2327,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:29:12.393: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:29:19.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8442" for this suite.

• [SLOW TEST:7.121 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":303,"completed":153,"skipped":2381,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:29:19.515: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-ff3612ca-4968-4d75-b1b7-02ec33701239
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:29:23.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6509" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":303,"completed":154,"skipped":2385,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:29:23.810: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-20d8f758-3888-49ac-9c01-60c44136bbac in namespace container-probe-3413
Apr 26 14:29:27.923: INFO: Started pod busybox-20d8f758-3888-49ac-9c01-60c44136bbac in namespace container-probe-3413
STEP: checking the pod's current state and verifying that restartCount is present
Apr 26 14:29:27.929: INFO: Initial restart count of pod busybox-20d8f758-3888-49ac-9c01-60c44136bbac is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:33:28.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3413" for this suite.

• [SLOW TEST:245.164 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":303,"completed":155,"skipped":2410,"failed":0}
SSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:33:28.975: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Apr 26 14:33:29.083: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3356 /api/v1/namespaces/watch-3356/configmaps/e2e-watch-test-configmap-a 574cc560-8f32-4f0d-9b03-bbfa0dcbc136 34241 0 2021-04-26 14:33:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-04-26 14:33:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 26 14:33:29.083: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3356 /api/v1/namespaces/watch-3356/configmaps/e2e-watch-test-configmap-a 574cc560-8f32-4f0d-9b03-bbfa0dcbc136 34241 0 2021-04-26 14:33:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-04-26 14:33:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Apr 26 14:33:39.100: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3356 /api/v1/namespaces/watch-3356/configmaps/e2e-watch-test-configmap-a 574cc560-8f32-4f0d-9b03-bbfa0dcbc136 34300 0 2021-04-26 14:33:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-04-26 14:33:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 26 14:33:39.100: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3356 /api/v1/namespaces/watch-3356/configmaps/e2e-watch-test-configmap-a 574cc560-8f32-4f0d-9b03-bbfa0dcbc136 34300 0 2021-04-26 14:33:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-04-26 14:33:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Apr 26 14:33:49.117: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3356 /api/v1/namespaces/watch-3356/configmaps/e2e-watch-test-configmap-a 574cc560-8f32-4f0d-9b03-bbfa0dcbc136 34359 0 2021-04-26 14:33:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-04-26 14:33:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 26 14:33:49.117: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3356 /api/v1/namespaces/watch-3356/configmaps/e2e-watch-test-configmap-a 574cc560-8f32-4f0d-9b03-bbfa0dcbc136 34359 0 2021-04-26 14:33:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-04-26 14:33:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Apr 26 14:33:59.134: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3356 /api/v1/namespaces/watch-3356/configmaps/e2e-watch-test-configmap-a 574cc560-8f32-4f0d-9b03-bbfa0dcbc136 34412 0 2021-04-26 14:33:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-04-26 14:33:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 26 14:33:59.135: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3356 /api/v1/namespaces/watch-3356/configmaps/e2e-watch-test-configmap-a 574cc560-8f32-4f0d-9b03-bbfa0dcbc136 34412 0 2021-04-26 14:33:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-04-26 14:33:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Apr 26 14:34:09.149: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3356 /api/v1/namespaces/watch-3356/configmaps/e2e-watch-test-configmap-b 6e2f805c-d2f8-4339-b4bb-7847162c8eb7 34458 0 2021-04-26 14:34:09 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-04-26 14:34:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 26 14:34:09.149: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3356 /api/v1/namespaces/watch-3356/configmaps/e2e-watch-test-configmap-b 6e2f805c-d2f8-4339-b4bb-7847162c8eb7 34458 0 2021-04-26 14:34:09 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-04-26 14:34:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Apr 26 14:34:19.163: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3356 /api/v1/namespaces/watch-3356/configmaps/e2e-watch-test-configmap-b 6e2f805c-d2f8-4339-b4bb-7847162c8eb7 34504 0 2021-04-26 14:34:09 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-04-26 14:34:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 26 14:34:19.163: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3356 /api/v1/namespaces/watch-3356/configmaps/e2e-watch-test-configmap-b 6e2f805c-d2f8-4339-b4bb-7847162c8eb7 34504 0 2021-04-26 14:34:09 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-04-26 14:34:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:34:29.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3356" for this suite.

• [SLOW TEST:60.216 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":303,"completed":156,"skipped":2413,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:34:29.195: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Apr 26 14:34:32.310: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:34:32.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6604" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":303,"completed":157,"skipped":2421,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:34:32.377: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-06ea811c-f12e-49c6-bbed-fd2d6f75c7b1
STEP: Creating a pod to test consume secrets
Apr 26 14:34:32.512: INFO: Waiting up to 5m0s for pod "pod-secrets-e02ece0e-a06a-4e27-86fc-a3946d9b5f3b" in namespace "secrets-3828" to be "Succeeded or Failed"
Apr 26 14:34:32.521: INFO: Pod "pod-secrets-e02ece0e-a06a-4e27-86fc-a3946d9b5f3b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.651506ms
Apr 26 14:34:34.535: INFO: Pod "pod-secrets-e02ece0e-a06a-4e27-86fc-a3946d9b5f3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023074711s
STEP: Saw pod success
Apr 26 14:34:34.535: INFO: Pod "pod-secrets-e02ece0e-a06a-4e27-86fc-a3946d9b5f3b" satisfied condition "Succeeded or Failed"
Apr 26 14:34:34.543: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod pod-secrets-e02ece0e-a06a-4e27-86fc-a3946d9b5f3b container secret-volume-test: <nil>
STEP: delete the pod
Apr 26 14:34:34.628: INFO: Waiting for pod pod-secrets-e02ece0e-a06a-4e27-86fc-a3946d9b5f3b to disappear
Apr 26 14:34:34.634: INFO: Pod pod-secrets-e02ece0e-a06a-4e27-86fc-a3946d9b5f3b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:34:34.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3828" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":303,"completed":158,"skipped":2461,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:34:34.660: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-4620, will wait for the garbage collector to delete the pods
Apr 26 14:34:38.812: INFO: Deleting Job.batch foo took: 15.252935ms
Apr 26 14:34:39.412: INFO: Terminating Job.batch foo pods took: 600.304604ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:35:15.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4620" for this suite.

• [SLOW TEST:41.184 seconds]
[sig-apps] Job
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":303,"completed":159,"skipped":2497,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:35:15.845: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:35:19.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9219" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":303,"completed":160,"skipped":2525,"failed":0}

------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:35:19.959: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Apr 26 14:35:20.040: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-cb6baeb8-5803-4b47-9bdf-12b4d39ec951" in namespace "security-context-test-2085" to be "Succeeded or Failed"
Apr 26 14:35:20.049: INFO: Pod "busybox-privileged-false-cb6baeb8-5803-4b47-9bdf-12b4d39ec951": Phase="Pending", Reason="", readiness=false. Elapsed: 9.408571ms
Apr 26 14:35:22.056: INFO: Pod "busybox-privileged-false-cb6baeb8-5803-4b47-9bdf-12b4d39ec951": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015649959s
Apr 26 14:35:24.062: INFO: Pod "busybox-privileged-false-cb6baeb8-5803-4b47-9bdf-12b4d39ec951": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022440034s
Apr 26 14:35:24.063: INFO: Pod "busybox-privileged-false-cb6baeb8-5803-4b47-9bdf-12b4d39ec951" satisfied condition "Succeeded or Failed"
Apr 26 14:35:24.119: INFO: Got logs for pod "busybox-privileged-false-cb6baeb8-5803-4b47-9bdf-12b4d39ec951": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:35:24.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2085" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":161,"skipped":2525,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:35:24.146: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-9887/configmap-test-f716556b-9858-42bf-862f-4872831a1254
STEP: Creating a pod to test consume configMaps
Apr 26 14:35:24.256: INFO: Waiting up to 5m0s for pod "pod-configmaps-30f95afd-a706-405d-b746-26b119a50c4d" in namespace "configmap-9887" to be "Succeeded or Failed"
Apr 26 14:35:24.262: INFO: Pod "pod-configmaps-30f95afd-a706-405d-b746-26b119a50c4d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.099461ms
Apr 26 14:35:26.269: INFO: Pod "pod-configmaps-30f95afd-a706-405d-b746-26b119a50c4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012751052s
STEP: Saw pod success
Apr 26 14:35:26.269: INFO: Pod "pod-configmaps-30f95afd-a706-405d-b746-26b119a50c4d" satisfied condition "Succeeded or Failed"
Apr 26 14:35:26.275: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod pod-configmaps-30f95afd-a706-405d-b746-26b119a50c4d container env-test: <nil>
STEP: delete the pod
Apr 26 14:35:26.315: INFO: Waiting for pod pod-configmaps-30f95afd-a706-405d-b746-26b119a50c4d to disappear
Apr 26 14:35:26.320: INFO: Pod pod-configmaps-30f95afd-a706-405d-b746-26b119a50c4d no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:35:26.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9887" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":303,"completed":162,"skipped":2552,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:35:26.339: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:35:37.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6247" for this suite.

• [SLOW TEST:11.207 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":303,"completed":163,"skipped":2598,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:35:37.547: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-17a8785a-6eee-433a-8a66-00eba96474b0
STEP: Creating a pod to test consume secrets
Apr 26 14:35:37.726: INFO: Waiting up to 5m0s for pod "pod-secrets-5b87f91d-7572-4e8d-a920-ebd0ddf34e15" in namespace "secrets-5154" to be "Succeeded or Failed"
Apr 26 14:35:37.746: INFO: Pod "pod-secrets-5b87f91d-7572-4e8d-a920-ebd0ddf34e15": Phase="Pending", Reason="", readiness=false. Elapsed: 20.210043ms
Apr 26 14:35:39.753: INFO: Pod "pod-secrets-5b87f91d-7572-4e8d-a920-ebd0ddf34e15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026506707s
STEP: Saw pod success
Apr 26 14:35:39.753: INFO: Pod "pod-secrets-5b87f91d-7572-4e8d-a920-ebd0ddf34e15" satisfied condition "Succeeded or Failed"
Apr 26 14:35:39.760: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod pod-secrets-5b87f91d-7572-4e8d-a920-ebd0ddf34e15 container secret-volume-test: <nil>
STEP: delete the pod
Apr 26 14:35:39.823: INFO: Waiting for pod pod-secrets-5b87f91d-7572-4e8d-a920-ebd0ddf34e15 to disappear
Apr 26 14:35:39.832: INFO: Pod pod-secrets-5b87f91d-7572-4e8d-a920-ebd0ddf34e15 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:35:39.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5154" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":303,"completed":164,"skipped":2601,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:35:39.866: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-398f70b1-c804-412f-befb-2321ebc58787
STEP: Creating secret with name s-test-opt-upd-f08543aa-8a63-405e-911c-81e8f4e29551
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-398f70b1-c804-412f-befb-2321ebc58787
STEP: Updating secret s-test-opt-upd-f08543aa-8a63-405e-911c-81e8f4e29551
STEP: Creating secret with name s-test-opt-create-cd70afb9-3304-446b-b4fb-42bee81bc477
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:35:46.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4798" for this suite.

• [SLOW TEST:6.639 seconds]
[sig-storage] Secrets
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":303,"completed":165,"skipped":2603,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:35:46.506: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Apr 26 14:35:46.602: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Apr 26 14:35:50.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 --namespace=crd-publish-openapi-5148 create -f -'
Apr 26 14:35:51.008: INFO: stderr: ""
Apr 26 14:35:51.008: INFO: stdout: "e2e-test-crd-publish-openapi-3284-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Apr 26 14:35:51.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 --namespace=crd-publish-openapi-5148 delete e2e-test-crd-publish-openapi-3284-crds test-foo'
Apr 26 14:35:51.137: INFO: stderr: ""
Apr 26 14:35:51.137: INFO: stdout: "e2e-test-crd-publish-openapi-3284-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Apr 26 14:35:51.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 --namespace=crd-publish-openapi-5148 apply -f -'
Apr 26 14:35:51.504: INFO: stderr: ""
Apr 26 14:35:51.504: INFO: stdout: "e2e-test-crd-publish-openapi-3284-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Apr 26 14:35:51.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 --namespace=crd-publish-openapi-5148 delete e2e-test-crd-publish-openapi-3284-crds test-foo'
Apr 26 14:35:51.662: INFO: stderr: ""
Apr 26 14:35:51.662: INFO: stdout: "e2e-test-crd-publish-openapi-3284-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Apr 26 14:35:51.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 --namespace=crd-publish-openapi-5148 create -f -'
Apr 26 14:35:51.938: INFO: rc: 1
Apr 26 14:35:51.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 --namespace=crd-publish-openapi-5148 apply -f -'
Apr 26 14:35:52.238: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Apr 26 14:35:52.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 --namespace=crd-publish-openapi-5148 create -f -'
Apr 26 14:35:52.613: INFO: rc: 1
Apr 26 14:35:52.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 --namespace=crd-publish-openapi-5148 apply -f -'
Apr 26 14:35:52.842: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Apr 26 14:35:52.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 explain e2e-test-crd-publish-openapi-3284-crds'
Apr 26 14:35:53.132: INFO: stderr: ""
Apr 26 14:35:53.132: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3284-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Apr 26 14:35:53.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 explain e2e-test-crd-publish-openapi-3284-crds.metadata'
Apr 26 14:35:53.436: INFO: stderr: ""
Apr 26 14:35:53.436: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3284-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Apr 26 14:35:53.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 explain e2e-test-crd-publish-openapi-3284-crds.spec'
Apr 26 14:35:53.750: INFO: stderr: ""
Apr 26 14:35:53.750: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3284-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Apr 26 14:35:53.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 explain e2e-test-crd-publish-openapi-3284-crds.spec.bars'
Apr 26 14:35:54.040: INFO: stderr: ""
Apr 26 14:35:54.040: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3284-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Apr 26 14:35:54.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 explain e2e-test-crd-publish-openapi-3284-crds.spec.bars2'
Apr 26 14:35:54.334: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:35:58.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5148" for this suite.

• [SLOW TEST:11.565 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":303,"completed":166,"skipped":2654,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:35:58.080: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Apr 26 14:35:58.235: INFO: Number of nodes with available pods: 0
Apr 26 14:35:58.235: INFO: Node elastic-hugle-5f9d9b5555-8444w is running more than one daemon pod
Apr 26 14:35:59.251: INFO: Number of nodes with available pods: 0
Apr 26 14:35:59.252: INFO: Node elastic-hugle-5f9d9b5555-8444w is running more than one daemon pod
Apr 26 14:36:00.267: INFO: Number of nodes with available pods: 0
Apr 26 14:36:00.268: INFO: Node elastic-hugle-5f9d9b5555-8444w is running more than one daemon pod
Apr 26 14:36:01.253: INFO: Number of nodes with available pods: 3
Apr 26 14:36:01.253: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Apr 26 14:36:01.319: INFO: Number of nodes with available pods: 2
Apr 26 14:36:01.319: INFO: Node elastic-hugle-5f9d9b5555-w8lg4 is running more than one daemon pod
Apr 26 14:36:02.350: INFO: Number of nodes with available pods: 2
Apr 26 14:36:02.350: INFO: Node elastic-hugle-5f9d9b5555-w8lg4 is running more than one daemon pod
Apr 26 14:36:03.347: INFO: Number of nodes with available pods: 2
Apr 26 14:36:03.347: INFO: Node elastic-hugle-5f9d9b5555-w8lg4 is running more than one daemon pod
Apr 26 14:36:04.335: INFO: Number of nodes with available pods: 3
Apr 26 14:36:04.336: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4901, will wait for the garbage collector to delete the pods
Apr 26 14:36:04.417: INFO: Deleting DaemonSet.extensions daemon-set took: 12.84514ms
Apr 26 14:36:05.017: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.27297ms
Apr 26 14:36:17.524: INFO: Number of nodes with available pods: 0
Apr 26 14:36:17.524: INFO: Number of running nodes: 0, number of available pods: 0
Apr 26 14:36:17.530: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4901/daemonsets","resourceVersion":"35488"},"items":null}

Apr 26 14:36:17.540: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4901/pods","resourceVersion":"35489"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:36:17.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4901" for this suite.

• [SLOW TEST:19.511 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":303,"completed":167,"skipped":2687,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:36:17.594: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Apr 26 14:36:17.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-5'
Apr 26 14:36:17.811: INFO: stderr: ""
Apr 26 14:36:17.811: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1550
Apr 26 14:36:17.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 delete pods e2e-test-httpd-pod --namespace=kubectl-5'
Apr 26 14:36:35.760: INFO: stderr: ""
Apr 26 14:36:35.760: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:36:35.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5" for this suite.

• [SLOW TEST:18.197 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1541
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":303,"completed":168,"skipped":2691,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:36:35.792: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service nodeport-test with type=NodePort in namespace services-6396
STEP: creating replication controller nodeport-test in namespace services-6396
I0426 14:36:35.968264      21 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-6396, replica count: 2
I0426 14:36:39.018743      21 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 26 14:36:39.019: INFO: Creating new exec pod
Apr 26 14:36:44.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-6396 execpodmrjdz -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Apr 26 14:36:44.664: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Apr 26 14:36:44.664: INFO: stdout: ""
Apr 26 14:36:44.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-6396 execpodmrjdz -- /bin/sh -x -c nc -zv -t -w 2 10.240.22.232 80'
Apr 26 14:36:45.217: INFO: stderr: "+ nc -zv -t -w 2 10.240.22.232 80\nConnection to 10.240.22.232 80 port [tcp/http] succeeded!\n"
Apr 26 14:36:45.217: INFO: stdout: ""
Apr 26 14:36:45.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-6396 execpodmrjdz -- /bin/sh -x -c nc -zv -t -w 2 192.168.1.9 31330'
Apr 26 14:36:45.827: INFO: stderr: "+ nc -zv -t -w 2 192.168.1.9 31330\nConnection to 192.168.1.9 31330 port [tcp/31330] succeeded!\n"
Apr 26 14:36:45.827: INFO: stdout: ""
Apr 26 14:36:45.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-6396 execpodmrjdz -- /bin/sh -x -c nc -zv -t -w 2 192.168.1.7 31330'
Apr 26 14:36:46.460: INFO: stderr: "+ nc -zv -t -w 2 192.168.1.7 31330\nConnection to 192.168.1.7 31330 port [tcp/31330] succeeded!\n"
Apr 26 14:36:46.460: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:36:46.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6396" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:10.689 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":303,"completed":169,"skipped":2698,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:36:46.482: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-24beb2b8-6bff-47cf-8c36-51da737d12ae
STEP: Creating a pod to test consume secrets
Apr 26 14:36:46.594: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1e9064f3-b62e-49ea-b176-93483932b6d5" in namespace "projected-5373" to be "Succeeded or Failed"
Apr 26 14:36:46.600: INFO: Pod "pod-projected-secrets-1e9064f3-b62e-49ea-b176-93483932b6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.283831ms
Apr 26 14:36:48.606: INFO: Pod "pod-projected-secrets-1e9064f3-b62e-49ea-b176-93483932b6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011752672s
Apr 26 14:36:50.613: INFO: Pod "pod-projected-secrets-1e9064f3-b62e-49ea-b176-93483932b6d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018529215s
STEP: Saw pod success
Apr 26 14:36:50.613: INFO: Pod "pod-projected-secrets-1e9064f3-b62e-49ea-b176-93483932b6d5" satisfied condition "Succeeded or Failed"
Apr 26 14:36:50.618: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod pod-projected-secrets-1e9064f3-b62e-49ea-b176-93483932b6d5 container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 26 14:36:50.671: INFO: Waiting for pod pod-projected-secrets-1e9064f3-b62e-49ea-b176-93483932b6d5 to disappear
Apr 26 14:36:50.684: INFO: Pod pod-projected-secrets-1e9064f3-b62e-49ea-b176-93483932b6d5 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:36:50.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5373" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":303,"completed":170,"skipped":2710,"failed":0}
SSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:36:50.741: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-a0ca551f-b1b0-407d-8932-e227962df8fd in namespace container-probe-1841
Apr 26 14:36:54.827: INFO: Started pod liveness-a0ca551f-b1b0-407d-8932-e227962df8fd in namespace container-probe-1841
STEP: checking the pod's current state and verifying that restartCount is present
Apr 26 14:36:54.832: INFO: Initial restart count of pod liveness-a0ca551f-b1b0-407d-8932-e227962df8fd is 0
Apr 26 14:37:16.925: INFO: Restart count of pod container-probe-1841/liveness-a0ca551f-b1b0-407d-8932-e227962df8fd is now 1 (22.092544178s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:37:16.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1841" for this suite.

• [SLOW TEST:26.241 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":303,"completed":171,"skipped":2713,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:37:16.983: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Apr 26 14:37:17.082: INFO: Waiting up to 5m0s for pod "busybox-user-65534-a8167e3a-393e-42f6-97bd-18115bd1b124" in namespace "security-context-test-2817" to be "Succeeded or Failed"
Apr 26 14:37:17.094: INFO: Pod "busybox-user-65534-a8167e3a-393e-42f6-97bd-18115bd1b124": Phase="Pending", Reason="", readiness=false. Elapsed: 12.24493ms
Apr 26 14:37:19.100: INFO: Pod "busybox-user-65534-a8167e3a-393e-42f6-97bd-18115bd1b124": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018276964s
Apr 26 14:37:21.110: INFO: Pod "busybox-user-65534-a8167e3a-393e-42f6-97bd-18115bd1b124": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02871077s
Apr 26 14:37:21.110: INFO: Pod "busybox-user-65534-a8167e3a-393e-42f6-97bd-18115bd1b124" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:37:21.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2817" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":172,"skipped":2745,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:37:21.144: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7041.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7041.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7041.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7041.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7041.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7041.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7041.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7041.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7041.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7041.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7041.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7041.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7041.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 154.21.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.21.154_udp@PTR;check="$$(dig +tcp +noall +answer +search 154.21.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.21.154_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7041.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7041.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7041.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7041.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7041.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7041.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7041.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7041.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7041.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7041.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7041.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7041.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7041.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 154.21.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.21.154_udp@PTR;check="$$(dig +tcp +noall +answer +search 154.21.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.21.154_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 26 14:37:25.426: INFO: Unable to read wheezy_udp@dns-test-service.dns-7041.svc.cluster.local from pod dns-7041/dns-test-064ed317-8854-4c8d-be80-9758671ccec1: the server could not find the requested resource (get pods dns-test-064ed317-8854-4c8d-be80-9758671ccec1)
Apr 26 14:37:25.475: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7041.svc.cluster.local from pod dns-7041/dns-test-064ed317-8854-4c8d-be80-9758671ccec1: the server could not find the requested resource (get pods dns-test-064ed317-8854-4c8d-be80-9758671ccec1)
Apr 26 14:37:25.484: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7041.svc.cluster.local from pod dns-7041/dns-test-064ed317-8854-4c8d-be80-9758671ccec1: the server could not find the requested resource (get pods dns-test-064ed317-8854-4c8d-be80-9758671ccec1)
Apr 26 14:37:25.495: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7041.svc.cluster.local from pod dns-7041/dns-test-064ed317-8854-4c8d-be80-9758671ccec1: the server could not find the requested resource (get pods dns-test-064ed317-8854-4c8d-be80-9758671ccec1)
Apr 26 14:37:26.054: INFO: Unable to read jessie_udp@dns-test-service.dns-7041.svc.cluster.local from pod dns-7041/dns-test-064ed317-8854-4c8d-be80-9758671ccec1: the server could not find the requested resource (get pods dns-test-064ed317-8854-4c8d-be80-9758671ccec1)
Apr 26 14:37:26.064: INFO: Unable to read jessie_tcp@dns-test-service.dns-7041.svc.cluster.local from pod dns-7041/dns-test-064ed317-8854-4c8d-be80-9758671ccec1: the server could not find the requested resource (get pods dns-test-064ed317-8854-4c8d-be80-9758671ccec1)
Apr 26 14:37:26.078: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7041.svc.cluster.local from pod dns-7041/dns-test-064ed317-8854-4c8d-be80-9758671ccec1: the server could not find the requested resource (get pods dns-test-064ed317-8854-4c8d-be80-9758671ccec1)
Apr 26 14:37:26.087: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7041.svc.cluster.local from pod dns-7041/dns-test-064ed317-8854-4c8d-be80-9758671ccec1: the server could not find the requested resource (get pods dns-test-064ed317-8854-4c8d-be80-9758671ccec1)
Apr 26 14:37:26.578: INFO: Lookups using dns-7041/dns-test-064ed317-8854-4c8d-be80-9758671ccec1 failed for: [wheezy_udp@dns-test-service.dns-7041.svc.cluster.local wheezy_tcp@dns-test-service.dns-7041.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7041.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7041.svc.cluster.local jessie_udp@dns-test-service.dns-7041.svc.cluster.local jessie_tcp@dns-test-service.dns-7041.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7041.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7041.svc.cluster.local]

Apr 26 14:37:33.321: INFO: DNS probes using dns-7041/dns-test-064ed317-8854-4c8d-be80-9758671ccec1 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:37:33.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7041" for this suite.

• [SLOW TEST:12.350 seconds]
[sig-network] DNS
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":303,"completed":173,"skipped":2770,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:37:33.495: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 26 14:37:34.502: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 26 14:37:36.529: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755044654, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755044654, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755044654, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755044654, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 26 14:37:39.559: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Apr 26 14:37:43.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 attach --namespace=webhook-9461 to-be-attached-pod -i -c=container1'
Apr 26 14:37:43.939: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:37:43.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9461" for this suite.
STEP: Destroying namespace "webhook-9461-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:10.593 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":303,"completed":174,"skipped":2819,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:37:44.089: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:37:44.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-615" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":303,"completed":175,"skipped":2837,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:37:44.304: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Apr 26 14:37:44.431: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Apr 26 14:37:44.464: INFO: Number of nodes with available pods: 0
Apr 26 14:37:44.464: INFO: Node elastic-hugle-5f9d9b5555-8444w is running more than one daemon pod
Apr 26 14:37:45.485: INFO: Number of nodes with available pods: 0
Apr 26 14:37:45.485: INFO: Node elastic-hugle-5f9d9b5555-8444w is running more than one daemon pod
Apr 26 14:37:46.481: INFO: Number of nodes with available pods: 1
Apr 26 14:37:46.481: INFO: Node elastic-hugle-5f9d9b5555-8444w is running more than one daemon pod
Apr 26 14:37:47.485: INFO: Number of nodes with available pods: 3
Apr 26 14:37:47.485: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Apr 26 14:37:47.544: INFO: Wrong image for pod: daemon-set-cn46q. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:37:47.544: INFO: Wrong image for pod: daemon-set-hfx6n. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:37:47.544: INFO: Wrong image for pod: daemon-set-tchzs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:37:48.563: INFO: Wrong image for pod: daemon-set-cn46q. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:37:48.563: INFO: Wrong image for pod: daemon-set-hfx6n. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:37:48.563: INFO: Wrong image for pod: daemon-set-tchzs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:37:49.563: INFO: Wrong image for pod: daemon-set-cn46q. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:37:49.563: INFO: Pod daemon-set-cn46q is not available
Apr 26 14:37:49.563: INFO: Wrong image for pod: daemon-set-hfx6n. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:37:49.563: INFO: Wrong image for pod: daemon-set-tchzs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:37:50.563: INFO: Wrong image for pod: daemon-set-cn46q. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:37:50.563: INFO: Pod daemon-set-cn46q is not available
Apr 26 14:37:50.563: INFO: Wrong image for pod: daemon-set-hfx6n. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:37:50.563: INFO: Wrong image for pod: daemon-set-tchzs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:37:51.563: INFO: Wrong image for pod: daemon-set-cn46q. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:37:51.563: INFO: Pod daemon-set-cn46q is not available
Apr 26 14:37:51.563: INFO: Wrong image for pod: daemon-set-hfx6n. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:37:51.563: INFO: Wrong image for pod: daemon-set-tchzs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:37:52.570: INFO: Wrong image for pod: daemon-set-cn46q. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:37:52.570: INFO: Pod daemon-set-cn46q is not available
Apr 26 14:37:52.570: INFO: Wrong image for pod: daemon-set-hfx6n. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:37:52.570: INFO: Wrong image for pod: daemon-set-tchzs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:37:53.563: INFO: Wrong image for pod: daemon-set-cn46q. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:37:53.563: INFO: Pod daemon-set-cn46q is not available
Apr 26 14:37:53.563: INFO: Wrong image for pod: daemon-set-hfx6n. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:37:53.563: INFO: Wrong image for pod: daemon-set-tchzs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:37:54.563: INFO: Wrong image for pod: daemon-set-cn46q. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:37:54.563: INFO: Pod daemon-set-cn46q is not available
Apr 26 14:37:54.563: INFO: Wrong image for pod: daemon-set-hfx6n. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:37:54.563: INFO: Wrong image for pod: daemon-set-tchzs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:37:55.563: INFO: Wrong image for pod: daemon-set-cn46q. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:37:55.563: INFO: Pod daemon-set-cn46q is not available
Apr 26 14:37:55.563: INFO: Wrong image for pod: daemon-set-hfx6n. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:37:55.563: INFO: Wrong image for pod: daemon-set-tchzs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:37:56.564: INFO: Wrong image for pod: daemon-set-cn46q. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:37:56.564: INFO: Pod daemon-set-cn46q is not available
Apr 26 14:37:56.564: INFO: Wrong image for pod: daemon-set-hfx6n. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:37:56.564: INFO: Wrong image for pod: daemon-set-tchzs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:37:57.562: INFO: Wrong image for pod: daemon-set-hfx6n. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:37:57.562: INFO: Pod daemon-set-q2l6z is not available
Apr 26 14:37:57.562: INFO: Wrong image for pod: daemon-set-tchzs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:37:58.563: INFO: Wrong image for pod: daemon-set-hfx6n. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:37:58.563: INFO: Pod daemon-set-q2l6z is not available
Apr 26 14:37:58.563: INFO: Wrong image for pod: daemon-set-tchzs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:37:59.562: INFO: Wrong image for pod: daemon-set-hfx6n. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:37:59.562: INFO: Pod daemon-set-q2l6z is not available
Apr 26 14:37:59.562: INFO: Wrong image for pod: daemon-set-tchzs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:38:00.571: INFO: Wrong image for pod: daemon-set-hfx6n. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:38:00.571: INFO: Wrong image for pod: daemon-set-tchzs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:38:01.561: INFO: Wrong image for pod: daemon-set-hfx6n. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:38:01.561: INFO: Wrong image for pod: daemon-set-tchzs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:38:01.561: INFO: Pod daemon-set-tchzs is not available
Apr 26 14:38:02.568: INFO: Wrong image for pod: daemon-set-hfx6n. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:38:02.568: INFO: Pod daemon-set-wvgw8 is not available
Apr 26 14:38:03.563: INFO: Wrong image for pod: daemon-set-hfx6n. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:38:03.563: INFO: Pod daemon-set-wvgw8 is not available
Apr 26 14:38:04.561: INFO: Wrong image for pod: daemon-set-hfx6n. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:38:04.561: INFO: Pod daemon-set-wvgw8 is not available
Apr 26 14:38:05.574: INFO: Wrong image for pod: daemon-set-hfx6n. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:38:06.564: INFO: Wrong image for pod: daemon-set-hfx6n. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:38:06.564: INFO: Pod daemon-set-hfx6n is not available
Apr 26 14:38:07.564: INFO: Wrong image for pod: daemon-set-hfx6n. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:38:07.564: INFO: Pod daemon-set-hfx6n is not available
Apr 26 14:38:08.562: INFO: Wrong image for pod: daemon-set-hfx6n. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:38:08.562: INFO: Pod daemon-set-hfx6n is not available
Apr 26 14:38:09.563: INFO: Wrong image for pod: daemon-set-hfx6n. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:38:09.563: INFO: Pod daemon-set-hfx6n is not available
Apr 26 14:38:10.563: INFO: Wrong image for pod: daemon-set-hfx6n. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:38:10.563: INFO: Pod daemon-set-hfx6n is not available
Apr 26 14:38:11.562: INFO: Wrong image for pod: daemon-set-hfx6n. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Apr 26 14:38:11.562: INFO: Pod daemon-set-hfx6n is not available
Apr 26 14:38:12.564: INFO: Pod daemon-set-8mjhr is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Apr 26 14:38:12.590: INFO: Number of nodes with available pods: 2
Apr 26 14:38:12.590: INFO: Node elastic-hugle-5f9d9b5555-w8lg4 is running more than one daemon pod
Apr 26 14:38:13.615: INFO: Number of nodes with available pods: 2
Apr 26 14:38:13.616: INFO: Node elastic-hugle-5f9d9b5555-w8lg4 is running more than one daemon pod
Apr 26 14:38:14.611: INFO: Number of nodes with available pods: 3
Apr 26 14:38:14.612: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7729, will wait for the garbage collector to delete the pods
Apr 26 14:38:14.735: INFO: Deleting DaemonSet.extensions daemon-set took: 19.431192ms
Apr 26 14:38:15.336: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.34802ms
Apr 26 14:38:18.250: INFO: Number of nodes with available pods: 0
Apr 26 14:38:18.250: INFO: Number of running nodes: 0, number of available pods: 0
Apr 26 14:38:18.258: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7729/daemonsets","resourceVersion":"36612"},"items":null}

Apr 26 14:38:18.265: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7729/pods","resourceVersion":"36612"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:38:18.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7729" for this suite.

• [SLOW TEST:34.019 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":303,"completed":176,"skipped":2846,"failed":0}
S
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:38:18.325: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service multi-endpoint-test in namespace services-2036
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2036 to expose endpoints map[]
Apr 26 14:38:18.443: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Apr 26 14:38:19.465: INFO: successfully validated that service multi-endpoint-test in namespace services-2036 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-2036
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2036 to expose endpoints map[pod1:[100]]
Apr 26 14:38:22.538: INFO: successfully validated that service multi-endpoint-test in namespace services-2036 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-2036
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2036 to expose endpoints map[pod1:[100] pod2:[101]]
Apr 26 14:38:25.599: INFO: successfully validated that service multi-endpoint-test in namespace services-2036 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-2036
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2036 to expose endpoints map[pod2:[101]]
Apr 26 14:38:25.645: INFO: successfully validated that service multi-endpoint-test in namespace services-2036 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-2036
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2036 to expose endpoints map[]
Apr 26 14:38:25.682: INFO: successfully validated that service multi-endpoint-test in namespace services-2036 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:38:25.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2036" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:7.441 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":303,"completed":177,"skipped":2847,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:38:25.769: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-1800
STEP: creating service affinity-clusterip-transition in namespace services-1800
STEP: creating replication controller affinity-clusterip-transition in namespace services-1800
I0426 14:38:25.887992      21 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-1800, replica count: 3
I0426 14:38:28.938562      21 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 26 14:38:28.951: INFO: Creating new exec pod
Apr 26 14:38:33.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-1800 execpod-affinity8vr4v -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
Apr 26 14:38:34.609: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Apr 26 14:38:34.609: INFO: stdout: ""
Apr 26 14:38:34.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-1800 execpod-affinity8vr4v -- /bin/sh -x -c nc -zv -t -w 2 10.240.30.164 80'
Apr 26 14:38:35.258: INFO: stderr: "+ nc -zv -t -w 2 10.240.30.164 80\nConnection to 10.240.30.164 80 port [tcp/http] succeeded!\n"
Apr 26 14:38:35.258: INFO: stdout: ""
Apr 26 14:38:35.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-1800 execpod-affinity8vr4v -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.30.164:80/ ; done'
Apr 26 14:38:35.990: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.164:80/\n"
Apr 26 14:38:35.990: INFO: stdout: "\naffinity-clusterip-transition-cv5pt\naffinity-clusterip-transition-m9795\naffinity-clusterip-transition-fws4p\naffinity-clusterip-transition-cv5pt\naffinity-clusterip-transition-m9795\naffinity-clusterip-transition-fws4p\naffinity-clusterip-transition-cv5pt\naffinity-clusterip-transition-m9795\naffinity-clusterip-transition-fws4p\naffinity-clusterip-transition-cv5pt\naffinity-clusterip-transition-m9795\naffinity-clusterip-transition-fws4p\naffinity-clusterip-transition-cv5pt\naffinity-clusterip-transition-m9795\naffinity-clusterip-transition-fws4p\naffinity-clusterip-transition-cv5pt"
Apr 26 14:38:35.990: INFO: Received response from host: affinity-clusterip-transition-cv5pt
Apr 26 14:38:35.990: INFO: Received response from host: affinity-clusterip-transition-m9795
Apr 26 14:38:35.990: INFO: Received response from host: affinity-clusterip-transition-fws4p
Apr 26 14:38:35.990: INFO: Received response from host: affinity-clusterip-transition-cv5pt
Apr 26 14:38:35.990: INFO: Received response from host: affinity-clusterip-transition-m9795
Apr 26 14:38:35.990: INFO: Received response from host: affinity-clusterip-transition-fws4p
Apr 26 14:38:35.990: INFO: Received response from host: affinity-clusterip-transition-cv5pt
Apr 26 14:38:35.990: INFO: Received response from host: affinity-clusterip-transition-m9795
Apr 26 14:38:35.990: INFO: Received response from host: affinity-clusterip-transition-fws4p
Apr 26 14:38:35.990: INFO: Received response from host: affinity-clusterip-transition-cv5pt
Apr 26 14:38:35.990: INFO: Received response from host: affinity-clusterip-transition-m9795
Apr 26 14:38:35.990: INFO: Received response from host: affinity-clusterip-transition-fws4p
Apr 26 14:38:35.991: INFO: Received response from host: affinity-clusterip-transition-cv5pt
Apr 26 14:38:35.991: INFO: Received response from host: affinity-clusterip-transition-m9795
Apr 26 14:38:35.991: INFO: Received response from host: affinity-clusterip-transition-fws4p
Apr 26 14:38:35.991: INFO: Received response from host: affinity-clusterip-transition-cv5pt
Apr 26 14:38:36.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-1800 execpod-affinity8vr4v -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.30.164:80/ ; done'
Apr 26 14:38:36.691: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.164:80/\n"
Apr 26 14:38:36.691: INFO: stdout: "\naffinity-clusterip-transition-fws4p\naffinity-clusterip-transition-fws4p\naffinity-clusterip-transition-fws4p\naffinity-clusterip-transition-fws4p\naffinity-clusterip-transition-fws4p\naffinity-clusterip-transition-fws4p\naffinity-clusterip-transition-fws4p\naffinity-clusterip-transition-fws4p\naffinity-clusterip-transition-fws4p\naffinity-clusterip-transition-fws4p\naffinity-clusterip-transition-fws4p\naffinity-clusterip-transition-fws4p\naffinity-clusterip-transition-fws4p\naffinity-clusterip-transition-fws4p\naffinity-clusterip-transition-fws4p\naffinity-clusterip-transition-fws4p"
Apr 26 14:38:36.691: INFO: Received response from host: affinity-clusterip-transition-fws4p
Apr 26 14:38:36.691: INFO: Received response from host: affinity-clusterip-transition-fws4p
Apr 26 14:38:36.691: INFO: Received response from host: affinity-clusterip-transition-fws4p
Apr 26 14:38:36.691: INFO: Received response from host: affinity-clusterip-transition-fws4p
Apr 26 14:38:36.691: INFO: Received response from host: affinity-clusterip-transition-fws4p
Apr 26 14:38:36.691: INFO: Received response from host: affinity-clusterip-transition-fws4p
Apr 26 14:38:36.691: INFO: Received response from host: affinity-clusterip-transition-fws4p
Apr 26 14:38:36.691: INFO: Received response from host: affinity-clusterip-transition-fws4p
Apr 26 14:38:36.691: INFO: Received response from host: affinity-clusterip-transition-fws4p
Apr 26 14:38:36.691: INFO: Received response from host: affinity-clusterip-transition-fws4p
Apr 26 14:38:36.691: INFO: Received response from host: affinity-clusterip-transition-fws4p
Apr 26 14:38:36.691: INFO: Received response from host: affinity-clusterip-transition-fws4p
Apr 26 14:38:36.691: INFO: Received response from host: affinity-clusterip-transition-fws4p
Apr 26 14:38:36.691: INFO: Received response from host: affinity-clusterip-transition-fws4p
Apr 26 14:38:36.691: INFO: Received response from host: affinity-clusterip-transition-fws4p
Apr 26 14:38:36.691: INFO: Received response from host: affinity-clusterip-transition-fws4p
Apr 26 14:38:36.691: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-1800, will wait for the garbage collector to delete the pods
Apr 26 14:38:36.780: INFO: Deleting ReplicationController affinity-clusterip-transition took: 13.529857ms
Apr 26 14:38:36.880: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.127393ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:38:51.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1800" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:26.079 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":303,"completed":178,"skipped":2852,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:38:51.854: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Apr 26 14:38:51.911: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the sample API server.
Apr 26 14:38:52.538: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Apr 26 14:38:54.617: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755044732, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755044732, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755044732, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755044732, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 26 14:38:56.624: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755044732, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755044732, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755044732, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755044732, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 26 14:38:58.625: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755044732, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755044732, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755044732, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755044732, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 26 14:39:00.624: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755044732, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755044732, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755044732, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755044732, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 26 14:39:02.626: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755044732, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755044732, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755044732, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755044732, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 26 14:39:04.624: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755044732, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755044732, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755044732, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755044732, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 26 14:39:07.674: INFO: Waited 1.02986343s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:39:08.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-4925" for this suite.

• [SLOW TEST:16.976 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":303,"completed":179,"skipped":2868,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:39:08.832: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Apr 26 14:39:08.978: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5fc46da6-d141-4063-9e91-87552c221e78" in namespace "projected-9421" to be "Succeeded or Failed"
Apr 26 14:39:08.983: INFO: Pod "downwardapi-volume-5fc46da6-d141-4063-9e91-87552c221e78": Phase="Pending", Reason="", readiness=false. Elapsed: 5.369602ms
Apr 26 14:39:10.990: INFO: Pod "downwardapi-volume-5fc46da6-d141-4063-9e91-87552c221e78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012156841s
Apr 26 14:39:12.997: INFO: Pod "downwardapi-volume-5fc46da6-d141-4063-9e91-87552c221e78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019263792s
STEP: Saw pod success
Apr 26 14:39:12.997: INFO: Pod "downwardapi-volume-5fc46da6-d141-4063-9e91-87552c221e78" satisfied condition "Succeeded or Failed"
Apr 26 14:39:13.006: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod downwardapi-volume-5fc46da6-d141-4063-9e91-87552c221e78 container client-container: <nil>
STEP: delete the pod
Apr 26 14:39:13.043: INFO: Waiting for pod downwardapi-volume-5fc46da6-d141-4063-9e91-87552c221e78 to disappear
Apr 26 14:39:13.049: INFO: Pod downwardapi-volume-5fc46da6-d141-4063-9e91-87552c221e78 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:39:13.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9421" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":303,"completed":180,"skipped":2921,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:39:13.072: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-7e268ee6-7dbb-41d6-be07-e25f5c936c46
STEP: Creating a pod to test consume secrets
Apr 26 14:39:13.166: INFO: Waiting up to 5m0s for pod "pod-secrets-eb3be09f-bfad-4b94-867d-0eaf1bb0b06e" in namespace "secrets-9505" to be "Succeeded or Failed"
Apr 26 14:39:13.172: INFO: Pod "pod-secrets-eb3be09f-bfad-4b94-867d-0eaf1bb0b06e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.132406ms
Apr 26 14:39:15.180: INFO: Pod "pod-secrets-eb3be09f-bfad-4b94-867d-0eaf1bb0b06e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01402294s
Apr 26 14:39:17.186: INFO: Pod "pod-secrets-eb3be09f-bfad-4b94-867d-0eaf1bb0b06e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020137608s
STEP: Saw pod success
Apr 26 14:39:17.186: INFO: Pod "pod-secrets-eb3be09f-bfad-4b94-867d-0eaf1bb0b06e" satisfied condition "Succeeded or Failed"
Apr 26 14:39:17.196: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod pod-secrets-eb3be09f-bfad-4b94-867d-0eaf1bb0b06e container secret-volume-test: <nil>
STEP: delete the pod
Apr 26 14:39:17.244: INFO: Waiting for pod pod-secrets-eb3be09f-bfad-4b94-867d-0eaf1bb0b06e to disappear
Apr 26 14:39:17.250: INFO: Pod pod-secrets-eb3be09f-bfad-4b94-867d-0eaf1bb0b06e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:39:17.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9505" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":303,"completed":181,"skipped":2943,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:39:17.275: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0426 14:39:18.952591      21 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0426 14:39:18.952856      21 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0426 14:39:18.952932      21 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Apr 26 14:39:18.953: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:39:18.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8442" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":303,"completed":182,"skipped":2952,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:39:18.977: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-fbc18b65-540a-4b15-a252-3873e731fb2d
STEP: Creating a pod to test consume configMaps
Apr 26 14:39:19.093: INFO: Waiting up to 5m0s for pod "pod-configmaps-7ff832dc-b26c-45e2-a7b6-456c6ec72487" in namespace "configmap-2381" to be "Succeeded or Failed"
Apr 26 14:39:19.106: INFO: Pod "pod-configmaps-7ff832dc-b26c-45e2-a7b6-456c6ec72487": Phase="Pending", Reason="", readiness=false. Elapsed: 12.947091ms
Apr 26 14:39:21.113: INFO: Pod "pod-configmaps-7ff832dc-b26c-45e2-a7b6-456c6ec72487": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019511982s
Apr 26 14:39:23.120: INFO: Pod "pod-configmaps-7ff832dc-b26c-45e2-a7b6-456c6ec72487": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026404462s
STEP: Saw pod success
Apr 26 14:39:23.120: INFO: Pod "pod-configmaps-7ff832dc-b26c-45e2-a7b6-456c6ec72487" satisfied condition "Succeeded or Failed"
Apr 26 14:39:23.126: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod pod-configmaps-7ff832dc-b26c-45e2-a7b6-456c6ec72487 container configmap-volume-test: <nil>
STEP: delete the pod
Apr 26 14:39:23.204: INFO: Waiting for pod pod-configmaps-7ff832dc-b26c-45e2-a7b6-456c6ec72487 to disappear
Apr 26 14:39:23.209: INFO: Pod pod-configmaps-7ff832dc-b26c-45e2-a7b6-456c6ec72487 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:39:23.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2381" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":303,"completed":183,"skipped":2985,"failed":0}
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:39:23.231: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-87185398-5c86-4acc-b446-04048cf21aa6
STEP: Creating a pod to test consume secrets
Apr 26 14:39:23.317: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5f0ea737-8e54-40e7-a6c9-bfc4ab9b2150" in namespace "projected-160" to be "Succeeded or Failed"
Apr 26 14:39:23.326: INFO: Pod "pod-projected-secrets-5f0ea737-8e54-40e7-a6c9-bfc4ab9b2150": Phase="Pending", Reason="", readiness=false. Elapsed: 8.871654ms
Apr 26 14:39:25.334: INFO: Pod "pod-projected-secrets-5f0ea737-8e54-40e7-a6c9-bfc4ab9b2150": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017124014s
Apr 26 14:39:27.344: INFO: Pod "pod-projected-secrets-5f0ea737-8e54-40e7-a6c9-bfc4ab9b2150": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027490486s
STEP: Saw pod success
Apr 26 14:39:27.344: INFO: Pod "pod-projected-secrets-5f0ea737-8e54-40e7-a6c9-bfc4ab9b2150" satisfied condition "Succeeded or Failed"
Apr 26 14:39:27.350: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod pod-projected-secrets-5f0ea737-8e54-40e7-a6c9-bfc4ab9b2150 container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 26 14:39:27.396: INFO: Waiting for pod pod-projected-secrets-5f0ea737-8e54-40e7-a6c9-bfc4ab9b2150 to disappear
Apr 26 14:39:27.402: INFO: Pod pod-projected-secrets-5f0ea737-8e54-40e7-a6c9-bfc4ab9b2150 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:39:27.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-160" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":303,"completed":184,"skipped":2986,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:39:27.444: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Apr 26 14:39:27.525: INFO: Waiting up to 5m0s for pod "pod-a07347b1-a056-4a9e-bdac-571439461251" in namespace "emptydir-591" to be "Succeeded or Failed"
Apr 26 14:39:27.531: INFO: Pod "pod-a07347b1-a056-4a9e-bdac-571439461251": Phase="Pending", Reason="", readiness=false. Elapsed: 5.003787ms
Apr 26 14:39:29.537: INFO: Pod "pod-a07347b1-a056-4a9e-bdac-571439461251": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011596399s
Apr 26 14:39:31.549: INFO: Pod "pod-a07347b1-a056-4a9e-bdac-571439461251": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022907633s
STEP: Saw pod success
Apr 26 14:39:31.549: INFO: Pod "pod-a07347b1-a056-4a9e-bdac-571439461251" satisfied condition "Succeeded or Failed"
Apr 26 14:39:31.556: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod pod-a07347b1-a056-4a9e-bdac-571439461251 container test-container: <nil>
STEP: delete the pod
Apr 26 14:39:31.609: INFO: Waiting for pod pod-a07347b1-a056-4a9e-bdac-571439461251 to disappear
Apr 26 14:39:31.614: INFO: Pod pod-a07347b1-a056-4a9e-bdac-571439461251 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:39:31.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-591" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":185,"skipped":2988,"failed":0}
SS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:39:31.634: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating replication controller my-hostname-basic-d44484a0-5069-4aa4-b40e-e2f6348aa713
Apr 26 14:39:31.709: INFO: Pod name my-hostname-basic-d44484a0-5069-4aa4-b40e-e2f6348aa713: Found 0 pods out of 1
Apr 26 14:39:36.715: INFO: Pod name my-hostname-basic-d44484a0-5069-4aa4-b40e-e2f6348aa713: Found 1 pods out of 1
Apr 26 14:39:36.716: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-d44484a0-5069-4aa4-b40e-e2f6348aa713" are running
Apr 26 14:39:36.728: INFO: Pod "my-hostname-basic-d44484a0-5069-4aa4-b40e-e2f6348aa713-znvw2" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-04-26 14:39:31 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-04-26 14:39:33 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-04-26 14:39:33 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-04-26 14:39:31 +0000 UTC Reason: Message:}])
Apr 26 14:39:36.728: INFO: Trying to dial the pod
Apr 26 14:39:41.840: INFO: Controller my-hostname-basic-d44484a0-5069-4aa4-b40e-e2f6348aa713: Got expected result from replica 1 [my-hostname-basic-d44484a0-5069-4aa4-b40e-e2f6348aa713-znvw2]: "my-hostname-basic-d44484a0-5069-4aa4-b40e-e2f6348aa713-znvw2", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:39:41.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-687" for this suite.

• [SLOW TEST:10.226 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":303,"completed":186,"skipped":2990,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:39:41.861: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl label
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1333
STEP: creating the pod
Apr 26 14:39:41.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 create -f - --namespace=kubectl-8222'
Apr 26 14:39:42.265: INFO: stderr: ""
Apr 26 14:39:42.265: INFO: stdout: "pod/pause created\n"
Apr 26 14:39:42.265: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Apr 26 14:39:42.265: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8222" to be "running and ready"
Apr 26 14:39:42.275: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 10.046753ms
Apr 26 14:39:44.283: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018161767s
Apr 26 14:39:46.291: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.025661023s
Apr 26 14:39:46.291: INFO: Pod "pause" satisfied condition "running and ready"
Apr 26 14:39:46.291: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: adding the label testing-label with value testing-label-value to a pod
Apr 26 14:39:46.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 label pods pause testing-label=testing-label-value --namespace=kubectl-8222'
Apr 26 14:39:46.394: INFO: stderr: ""
Apr 26 14:39:46.394: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Apr 26 14:39:46.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 get pod pause -L testing-label --namespace=kubectl-8222'
Apr 26 14:39:46.503: INFO: stderr: ""
Apr 26 14:39:46.503: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Apr 26 14:39:46.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 label pods pause testing-label- --namespace=kubectl-8222'
Apr 26 14:39:46.610: INFO: stderr: ""
Apr 26 14:39:46.611: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Apr 26 14:39:46.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 get pod pause -L testing-label --namespace=kubectl-8222'
Apr 26 14:39:46.706: INFO: stderr: ""
Apr 26 14:39:46.706: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1340
STEP: using delete to clean up resources
Apr 26 14:39:46.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 delete --grace-period=0 --force -f - --namespace=kubectl-8222'
Apr 26 14:39:46.825: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 26 14:39:46.825: INFO: stdout: "pod \"pause\" force deleted\n"
Apr 26 14:39:46.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 get rc,svc -l name=pause --no-headers --namespace=kubectl-8222'
Apr 26 14:39:46.945: INFO: stderr: "No resources found in kubectl-8222 namespace.\n"
Apr 26 14:39:46.945: INFO: stdout: ""
Apr 26 14:39:46.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 get pods -l name=pause --namespace=kubectl-8222 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 26 14:39:47.031: INFO: stderr: ""
Apr 26 14:39:47.031: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:39:47.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8222" for this suite.

• [SLOW TEST:5.201 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1330
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":303,"completed":187,"skipped":3001,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:39:47.062: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-3fa7c8b8-900b-42a3-a42c-20666e159b34
STEP: Creating a pod to test consume configMaps
Apr 26 14:39:47.173: INFO: Waiting up to 5m0s for pod "pod-configmaps-33367b04-3d83-494e-a8fd-318db46e8fbf" in namespace "configmap-3531" to be "Succeeded or Failed"
Apr 26 14:39:47.184: INFO: Pod "pod-configmaps-33367b04-3d83-494e-a8fd-318db46e8fbf": Phase="Pending", Reason="", readiness=false. Elapsed: 10.288086ms
Apr 26 14:39:49.190: INFO: Pod "pod-configmaps-33367b04-3d83-494e-a8fd-318db46e8fbf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017156987s
Apr 26 14:39:51.197: INFO: Pod "pod-configmaps-33367b04-3d83-494e-a8fd-318db46e8fbf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02331776s
STEP: Saw pod success
Apr 26 14:39:51.197: INFO: Pod "pod-configmaps-33367b04-3d83-494e-a8fd-318db46e8fbf" satisfied condition "Succeeded or Failed"
Apr 26 14:39:51.205: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod pod-configmaps-33367b04-3d83-494e-a8fd-318db46e8fbf container configmap-volume-test: <nil>
STEP: delete the pod
Apr 26 14:39:51.246: INFO: Waiting for pod pod-configmaps-33367b04-3d83-494e-a8fd-318db46e8fbf to disappear
Apr 26 14:39:51.252: INFO: Pod pod-configmaps-33367b04-3d83-494e-a8fd-318db46e8fbf no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:39:51.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3531" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":188,"skipped":3014,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:39:51.281: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-d53cd877-debf-44af-8f0a-ce92f4c8893c
STEP: Creating a pod to test consume configMaps
Apr 26 14:39:51.365: INFO: Waiting up to 5m0s for pod "pod-configmaps-1a8cae3a-c8db-4418-a4e6-77c36e46c995" in namespace "configmap-9664" to be "Succeeded or Failed"
Apr 26 14:39:51.370: INFO: Pod "pod-configmaps-1a8cae3a-c8db-4418-a4e6-77c36e46c995": Phase="Pending", Reason="", readiness=false. Elapsed: 4.875206ms
Apr 26 14:39:53.402: INFO: Pod "pod-configmaps-1a8cae3a-c8db-4418-a4e6-77c36e46c995": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.037425676s
STEP: Saw pod success
Apr 26 14:39:53.402: INFO: Pod "pod-configmaps-1a8cae3a-c8db-4418-a4e6-77c36e46c995" satisfied condition "Succeeded or Failed"
Apr 26 14:39:53.419: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod pod-configmaps-1a8cae3a-c8db-4418-a4e6-77c36e46c995 container configmap-volume-test: <nil>
STEP: delete the pod
Apr 26 14:39:53.510: INFO: Waiting for pod pod-configmaps-1a8cae3a-c8db-4418-a4e6-77c36e46c995 to disappear
Apr 26 14:39:53.529: INFO: Pod pod-configmaps-1a8cae3a-c8db-4418-a4e6-77c36e46c995 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:39:53.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9664" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":303,"completed":189,"skipped":3017,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:39:53.553: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting the proxy server
Apr 26 14:39:53.617: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-610780823 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:39:53.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3457" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":303,"completed":190,"skipped":3020,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:39:53.738: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8800.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8800.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8800.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8800.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8800.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8800.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8800.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8800.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8800.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8800.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8800.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8800.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8800.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8800.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8800.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8800.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8800.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8800.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 26 14:39:57.992: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8800.svc.cluster.local from pod dns-8800/dns-test-1f32f20e-79c5-4150-b2fd-72600a83c192: the server could not find the requested resource (get pods dns-test-1f32f20e-79c5-4150-b2fd-72600a83c192)
Apr 26 14:39:58.038: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8800.svc.cluster.local from pod dns-8800/dns-test-1f32f20e-79c5-4150-b2fd-72600a83c192: the server could not find the requested resource (get pods dns-test-1f32f20e-79c5-4150-b2fd-72600a83c192)
Apr 26 14:39:58.055: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8800.svc.cluster.local from pod dns-8800/dns-test-1f32f20e-79c5-4150-b2fd-72600a83c192: the server could not find the requested resource (get pods dns-test-1f32f20e-79c5-4150-b2fd-72600a83c192)
Apr 26 14:39:58.064: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8800.svc.cluster.local from pod dns-8800/dns-test-1f32f20e-79c5-4150-b2fd-72600a83c192: the server could not find the requested resource (get pods dns-test-1f32f20e-79c5-4150-b2fd-72600a83c192)
Apr 26 14:39:58.247: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8800.svc.cluster.local from pod dns-8800/dns-test-1f32f20e-79c5-4150-b2fd-72600a83c192: the server could not find the requested resource (get pods dns-test-1f32f20e-79c5-4150-b2fd-72600a83c192)
Apr 26 14:39:58.258: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8800.svc.cluster.local from pod dns-8800/dns-test-1f32f20e-79c5-4150-b2fd-72600a83c192: the server could not find the requested resource (get pods dns-test-1f32f20e-79c5-4150-b2fd-72600a83c192)
Apr 26 14:39:58.266: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8800.svc.cluster.local from pod dns-8800/dns-test-1f32f20e-79c5-4150-b2fd-72600a83c192: the server could not find the requested resource (get pods dns-test-1f32f20e-79c5-4150-b2fd-72600a83c192)
Apr 26 14:39:58.275: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8800.svc.cluster.local from pod dns-8800/dns-test-1f32f20e-79c5-4150-b2fd-72600a83c192: the server could not find the requested resource (get pods dns-test-1f32f20e-79c5-4150-b2fd-72600a83c192)
Apr 26 14:39:58.414: INFO: Lookups using dns-8800/dns-test-1f32f20e-79c5-4150-b2fd-72600a83c192 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8800.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8800.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8800.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8800.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8800.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8800.svc.cluster.local jessie_udp@dns-test-service-2.dns-8800.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8800.svc.cluster.local]

Apr 26 14:40:04.389: INFO: DNS probes using dns-8800/dns-test-1f32f20e-79c5-4150-b2fd-72600a83c192 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:40:04.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8800" for this suite.

• [SLOW TEST:10.735 seconds]
[sig-network] DNS
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":303,"completed":191,"skipped":3031,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:40:04.475: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Apr 26 14:40:04.549: INFO: Waiting up to 5m0s for pod "downwardapi-volume-216b2ca9-8b9b-4535-808d-d031eddec33c" in namespace "projected-5247" to be "Succeeded or Failed"
Apr 26 14:40:04.562: INFO: Pod "downwardapi-volume-216b2ca9-8b9b-4535-808d-d031eddec33c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.452586ms
Apr 26 14:40:06.568: INFO: Pod "downwardapi-volume-216b2ca9-8b9b-4535-808d-d031eddec33c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019092333s
Apr 26 14:40:08.575: INFO: Pod "downwardapi-volume-216b2ca9-8b9b-4535-808d-d031eddec33c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025761555s
STEP: Saw pod success
Apr 26 14:40:08.575: INFO: Pod "downwardapi-volume-216b2ca9-8b9b-4535-808d-d031eddec33c" satisfied condition "Succeeded or Failed"
Apr 26 14:40:08.581: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod downwardapi-volume-216b2ca9-8b9b-4535-808d-d031eddec33c container client-container: <nil>
STEP: delete the pod
Apr 26 14:40:08.620: INFO: Waiting for pod downwardapi-volume-216b2ca9-8b9b-4535-808d-d031eddec33c to disappear
Apr 26 14:40:08.626: INFO: Pod downwardapi-volume-216b2ca9-8b9b-4535-808d-d031eddec33c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:40:08.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5247" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":303,"completed":192,"skipped":3051,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:40:08.646: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:40:08.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5991" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":303,"completed":193,"skipped":3057,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:40:08.752: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-f52cb7e4-c647-4ab2-8deb-cb231bb44d64
STEP: Creating a pod to test consume configMaps
Apr 26 14:40:08.850: INFO: Waiting up to 5m0s for pod "pod-configmaps-a124e4d4-8748-4426-ac65-b0baceef46da" in namespace "configmap-9783" to be "Succeeded or Failed"
Apr 26 14:40:08.865: INFO: Pod "pod-configmaps-a124e4d4-8748-4426-ac65-b0baceef46da": Phase="Pending", Reason="", readiness=false. Elapsed: 14.507119ms
Apr 26 14:40:10.876: INFO: Pod "pod-configmaps-a124e4d4-8748-4426-ac65-b0baceef46da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025857378s
STEP: Saw pod success
Apr 26 14:40:10.876: INFO: Pod "pod-configmaps-a124e4d4-8748-4426-ac65-b0baceef46da" satisfied condition "Succeeded or Failed"
Apr 26 14:40:10.885: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod pod-configmaps-a124e4d4-8748-4426-ac65-b0baceef46da container configmap-volume-test: <nil>
STEP: delete the pod
Apr 26 14:40:10.926: INFO: Waiting for pod pod-configmaps-a124e4d4-8748-4426-ac65-b0baceef46da to disappear
Apr 26 14:40:10.932: INFO: Pod pod-configmaps-a124e4d4-8748-4426-ac65-b0baceef46da no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:40:10.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9783" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":303,"completed":194,"skipped":3059,"failed":0}
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:40:10.955: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Apr 26 14:40:11.067: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Apr 26 14:40:11.092: INFO: Number of nodes with available pods: 0
Apr 26 14:40:11.092: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Apr 26 14:40:11.131: INFO: Number of nodes with available pods: 0
Apr 26 14:40:11.131: INFO: Node elastic-hugle-5f9d9b5555-mj2j9 is running more than one daemon pod
Apr 26 14:40:12.140: INFO: Number of nodes with available pods: 0
Apr 26 14:40:12.140: INFO: Node elastic-hugle-5f9d9b5555-mj2j9 is running more than one daemon pod
Apr 26 14:40:13.137: INFO: Number of nodes with available pods: 0
Apr 26 14:40:13.137: INFO: Node elastic-hugle-5f9d9b5555-mj2j9 is running more than one daemon pod
Apr 26 14:40:14.137: INFO: Number of nodes with available pods: 1
Apr 26 14:40:14.137: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Apr 26 14:40:14.190: INFO: Number of nodes with available pods: 0
Apr 26 14:40:14.190: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Apr 26 14:40:14.221: INFO: Number of nodes with available pods: 0
Apr 26 14:40:14.221: INFO: Node elastic-hugle-5f9d9b5555-mj2j9 is running more than one daemon pod
Apr 26 14:40:15.228: INFO: Number of nodes with available pods: 0
Apr 26 14:40:15.229: INFO: Node elastic-hugle-5f9d9b5555-mj2j9 is running more than one daemon pod
Apr 26 14:40:16.236: INFO: Number of nodes with available pods: 0
Apr 26 14:40:16.236: INFO: Node elastic-hugle-5f9d9b5555-mj2j9 is running more than one daemon pod
Apr 26 14:40:17.229: INFO: Number of nodes with available pods: 0
Apr 26 14:40:17.229: INFO: Node elastic-hugle-5f9d9b5555-mj2j9 is running more than one daemon pod
Apr 26 14:40:18.228: INFO: Number of nodes with available pods: 0
Apr 26 14:40:18.228: INFO: Node elastic-hugle-5f9d9b5555-mj2j9 is running more than one daemon pod
Apr 26 14:40:19.228: INFO: Number of nodes with available pods: 0
Apr 26 14:40:19.228: INFO: Node elastic-hugle-5f9d9b5555-mj2j9 is running more than one daemon pod
Apr 26 14:40:20.228: INFO: Number of nodes with available pods: 0
Apr 26 14:40:20.228: INFO: Node elastic-hugle-5f9d9b5555-mj2j9 is running more than one daemon pod
Apr 26 14:40:21.228: INFO: Number of nodes with available pods: 0
Apr 26 14:40:21.228: INFO: Node elastic-hugle-5f9d9b5555-mj2j9 is running more than one daemon pod
Apr 26 14:40:22.231: INFO: Number of nodes with available pods: 0
Apr 26 14:40:22.231: INFO: Node elastic-hugle-5f9d9b5555-mj2j9 is running more than one daemon pod
Apr 26 14:40:23.228: INFO: Number of nodes with available pods: 0
Apr 26 14:40:23.228: INFO: Node elastic-hugle-5f9d9b5555-mj2j9 is running more than one daemon pod
Apr 26 14:40:24.228: INFO: Number of nodes with available pods: 0
Apr 26 14:40:24.228: INFO: Node elastic-hugle-5f9d9b5555-mj2j9 is running more than one daemon pod
Apr 26 14:40:25.236: INFO: Number of nodes with available pods: 0
Apr 26 14:40:25.236: INFO: Node elastic-hugle-5f9d9b5555-mj2j9 is running more than one daemon pod
Apr 26 14:40:26.230: INFO: Number of nodes with available pods: 0
Apr 26 14:40:26.230: INFO: Node elastic-hugle-5f9d9b5555-mj2j9 is running more than one daemon pod
Apr 26 14:40:27.229: INFO: Number of nodes with available pods: 0
Apr 26 14:40:27.229: INFO: Node elastic-hugle-5f9d9b5555-mj2j9 is running more than one daemon pod
Apr 26 14:40:28.234: INFO: Number of nodes with available pods: 0
Apr 26 14:40:28.234: INFO: Node elastic-hugle-5f9d9b5555-mj2j9 is running more than one daemon pod
Apr 26 14:40:29.239: INFO: Number of nodes with available pods: 0
Apr 26 14:40:29.239: INFO: Node elastic-hugle-5f9d9b5555-mj2j9 is running more than one daemon pod
Apr 26 14:40:30.228: INFO: Number of nodes with available pods: 1
Apr 26 14:40:30.228: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1386, will wait for the garbage collector to delete the pods
Apr 26 14:40:30.313: INFO: Deleting DaemonSet.extensions daemon-set took: 13.083311ms
Apr 26 14:40:30.913: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.361037ms
Apr 26 14:40:33.619: INFO: Number of nodes with available pods: 0
Apr 26 14:40:33.619: INFO: Number of running nodes: 0, number of available pods: 0
Apr 26 14:40:33.625: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1386/daemonsets","resourceVersion":"38148"},"items":null}

Apr 26 14:40:33.630: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1386/pods","resourceVersion":"38148"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:40:33.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1386" for this suite.

• [SLOW TEST:22.763 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":303,"completed":195,"skipped":3062,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:40:33.719: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 26 14:40:34.361: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 26 14:40:36.380: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755044834, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755044834, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755044834, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755044834, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 26 14:40:39.482: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:40:39.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-491" for this suite.
STEP: Destroying namespace "webhook-491-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.269 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":303,"completed":196,"skipped":3071,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:40:39.989: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:40:46.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-970" for this suite.
STEP: Destroying namespace "nsdeletetest-7596" for this suite.
Apr 26 14:40:46.365: INFO: Namespace nsdeletetest-7596 was already deleted
STEP: Destroying namespace "nsdeletetest-8935" for this suite.

• [SLOW TEST:6.392 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":303,"completed":197,"skipped":3077,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:40:46.383: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:40:50.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9564" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":198,"skipped":3112,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:40:50.616: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-d3ea0b1f-68d5-4524-9f4a-efbb0a4678a1
STEP: Creating a pod to test consume configMaps
Apr 26 14:40:50.733: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e8259dbd-77ce-405d-8ce7-5192959a2758" in namespace "projected-4865" to be "Succeeded or Failed"
Apr 26 14:40:50.744: INFO: Pod "pod-projected-configmaps-e8259dbd-77ce-405d-8ce7-5192959a2758": Phase="Pending", Reason="", readiness=false. Elapsed: 11.11015ms
Apr 26 14:40:52.751: INFO: Pod "pod-projected-configmaps-e8259dbd-77ce-405d-8ce7-5192959a2758": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0181907s
Apr 26 14:40:54.760: INFO: Pod "pod-projected-configmaps-e8259dbd-77ce-405d-8ce7-5192959a2758": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026838751s
STEP: Saw pod success
Apr 26 14:40:54.760: INFO: Pod "pod-projected-configmaps-e8259dbd-77ce-405d-8ce7-5192959a2758" satisfied condition "Succeeded or Failed"
Apr 26 14:40:54.767: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod pod-projected-configmaps-e8259dbd-77ce-405d-8ce7-5192959a2758 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 26 14:40:54.872: INFO: Waiting for pod pod-projected-configmaps-e8259dbd-77ce-405d-8ce7-5192959a2758 to disappear
Apr 26 14:40:54.882: INFO: Pod pod-projected-configmaps-e8259dbd-77ce-405d-8ce7-5192959a2758 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:40:54.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4865" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":303,"completed":199,"skipped":3128,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:40:54.919: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Apr 26 14:40:59.539: INFO: Successfully updated pod "adopt-release-6sgws"
STEP: Checking that the Job readopts the Pod
Apr 26 14:40:59.539: INFO: Waiting up to 15m0s for pod "adopt-release-6sgws" in namespace "job-9340" to be "adopted"
Apr 26 14:40:59.547: INFO: Pod "adopt-release-6sgws": Phase="Running", Reason="", readiness=true. Elapsed: 7.651605ms
Apr 26 14:41:01.553: INFO: Pod "adopt-release-6sgws": Phase="Running", Reason="", readiness=true. Elapsed: 2.013576146s
Apr 26 14:41:01.553: INFO: Pod "adopt-release-6sgws" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Apr 26 14:41:02.074: INFO: Successfully updated pod "adopt-release-6sgws"
STEP: Checking that the Job releases the Pod
Apr 26 14:41:02.074: INFO: Waiting up to 15m0s for pod "adopt-release-6sgws" in namespace "job-9340" to be "released"
Apr 26 14:41:02.082: INFO: Pod "adopt-release-6sgws": Phase="Running", Reason="", readiness=true. Elapsed: 8.430475ms
Apr 26 14:41:02.083: INFO: Pod "adopt-release-6sgws" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:41:02.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9340" for this suite.

• [SLOW TEST:7.196 seconds]
[sig-apps] Job
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":303,"completed":200,"skipped":3147,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:41:02.119: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Apr 26 14:41:02.199: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:41:22.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7366" for this suite.

• [SLOW TEST:20.869 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":303,"completed":201,"skipped":3211,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:41:22.991: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Apr 26 14:41:23.074: INFO: Waiting up to 5m0s for pod "pod-ed8e9a5f-0680-4c13-86e2-af7419fe574f" in namespace "emptydir-9506" to be "Succeeded or Failed"
Apr 26 14:41:23.081: INFO: Pod "pod-ed8e9a5f-0680-4c13-86e2-af7419fe574f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.212185ms
Apr 26 14:41:25.089: INFO: Pod "pod-ed8e9a5f-0680-4c13-86e2-af7419fe574f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015107255s
Apr 26 14:41:27.097: INFO: Pod "pod-ed8e9a5f-0680-4c13-86e2-af7419fe574f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022235457s
STEP: Saw pod success
Apr 26 14:41:27.097: INFO: Pod "pod-ed8e9a5f-0680-4c13-86e2-af7419fe574f" satisfied condition "Succeeded or Failed"
Apr 26 14:41:27.102: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-w8lg4 pod pod-ed8e9a5f-0680-4c13-86e2-af7419fe574f container test-container: <nil>
STEP: delete the pod
Apr 26 14:41:27.185: INFO: Waiting for pod pod-ed8e9a5f-0680-4c13-86e2-af7419fe574f to disappear
Apr 26 14:41:27.192: INFO: Pod pod-ed8e9a5f-0680-4c13-86e2-af7419fe574f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:41:27.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9506" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":202,"skipped":3215,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:41:27.238: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:41:55.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2080" for this suite.

• [SLOW TEST:28.758 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":303,"completed":203,"skipped":3233,"failed":0}
SS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:41:55.997: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8121
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-8121
I0426 14:41:56.111755      21 runners.go:190] Created replication controller with name: externalname-service, namespace: services-8121, replica count: 2
I0426 14:41:59.162390      21 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 26 14:41:59.162: INFO: Creating new exec pod
Apr 26 14:42:04.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-8121 execpod5f9jr -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Apr 26 14:42:04.810: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Apr 26 14:42:04.810: INFO: stdout: ""
Apr 26 14:42:04.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-8121 execpod5f9jr -- /bin/sh -x -c nc -zv -t -w 2 10.240.17.172 80'
Apr 26 14:42:05.415: INFO: stderr: "+ nc -zv -t -w 2 10.240.17.172 80\nConnection to 10.240.17.172 80 port [tcp/http] succeeded!\n"
Apr 26 14:42:05.415: INFO: stdout: ""
Apr 26 14:42:05.415: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:42:05.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8121" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:9.485 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":303,"completed":204,"skipped":3235,"failed":0}
SSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:42:05.485: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod with failed condition
STEP: updating the pod
Apr 26 14:44:06.140: INFO: Successfully updated pod "var-expansion-940534b1-bf82-4d58-8a4a-e4289ee68839"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Apr 26 14:44:08.179: INFO: Deleting pod "var-expansion-940534b1-bf82-4d58-8a4a-e4289ee68839" in namespace "var-expansion-7399"
Apr 26 14:44:08.192: INFO: Wait up to 5m0s for pod "var-expansion-940534b1-bf82-4d58-8a4a-e4289ee68839" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:44:42.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7399" for this suite.

• [SLOW TEST:156.739 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":303,"completed":205,"skipped":3241,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:44:42.226: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Apr 26 14:44:42.292: INFO: PodSpec: initContainers in spec.initContainers
Apr 26 14:45:27.773: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-71d192dd-39e2-4eb3-9ed4-579b943c3387", GenerateName:"", Namespace:"init-container-9177", SelfLink:"/api/v1/namespaces/init-container-9177/pods/pod-init-71d192dd-39e2-4eb3-9ed4-579b943c3387", UID:"7032b4de-0479-4c8f-8d85-7e332b77bc3a", ResourceVersion:"40087", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63755045082, loc:(*time.Location)(0x77108c0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"292128781"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"172.25.1.221/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc004d7e040), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004d7e060)}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc004d7e080), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004d7e0a0)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc004d7e0c0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004d7e0e0)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-f4bjv", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc007dba040), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-f4bjv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-f4bjv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-f4bjv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0010b22a8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"elastic-hugle-5f9d9b5555-8444w", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc001f2e000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0010b2340)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0010b2360)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0010b2368), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0010b236c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc00053a070), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755045082, loc:(*time.Location)(0x77108c0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755045082, loc:(*time.Location)(0x77108c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755045082, loc:(*time.Location)(0x77108c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755045082, loc:(*time.Location)(0x77108c0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.1.11", PodIP:"172.25.1.221", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.25.1.221"}}, StartTime:(*v1.Time)(0xc004d7e100), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001f2e0e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001f2e150)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://e87746e0051738ad15c72b791be4f0ebb1678cf5d2b5879c300850eb099631b1", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004d7e140), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004d7e120), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc0010b23ff)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:45:27.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9177" for this suite.

• [SLOW TEST:45.570 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":303,"completed":206,"skipped":3294,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:45:27.798: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-219
STEP: creating service affinity-clusterip in namespace services-219
STEP: creating replication controller affinity-clusterip in namespace services-219
I0426 14:45:27.894568      21 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-219, replica count: 3
I0426 14:45:30.949795      21 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 26 14:45:30.960: INFO: Creating new exec pod
Apr 26 14:45:35.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-219 execpod-affinityck7p4 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Apr 26 14:45:36.643: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Apr 26 14:45:36.643: INFO: stdout: ""
Apr 26 14:45:36.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-219 execpod-affinityck7p4 -- /bin/sh -x -c nc -zv -t -w 2 10.240.27.92 80'
Apr 26 14:45:37.290: INFO: stderr: "+ nc -zv -t -w 2 10.240.27.92 80\nConnection to 10.240.27.92 80 port [tcp/http] succeeded!\n"
Apr 26 14:45:37.290: INFO: stdout: ""
Apr 26 14:45:37.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-219 execpod-affinityck7p4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.27.92:80/ ; done'
Apr 26 14:45:39.977: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.92:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.92:80/\n"
Apr 26 14:45:39.977: INFO: stdout: "\naffinity-clusterip-2zbqc\naffinity-clusterip-2zbqc\naffinity-clusterip-2zbqc\naffinity-clusterip-2zbqc\naffinity-clusterip-2zbqc\naffinity-clusterip-2zbqc\naffinity-clusterip-2zbqc\naffinity-clusterip-2zbqc\naffinity-clusterip-2zbqc\naffinity-clusterip-2zbqc\naffinity-clusterip-2zbqc\naffinity-clusterip-2zbqc\naffinity-clusterip-2zbqc\naffinity-clusterip-2zbqc\naffinity-clusterip-2zbqc\naffinity-clusterip-2zbqc"
Apr 26 14:45:39.977: INFO: Received response from host: affinity-clusterip-2zbqc
Apr 26 14:45:39.977: INFO: Received response from host: affinity-clusterip-2zbqc
Apr 26 14:45:39.977: INFO: Received response from host: affinity-clusterip-2zbqc
Apr 26 14:45:39.977: INFO: Received response from host: affinity-clusterip-2zbqc
Apr 26 14:45:39.977: INFO: Received response from host: affinity-clusterip-2zbqc
Apr 26 14:45:39.977: INFO: Received response from host: affinity-clusterip-2zbqc
Apr 26 14:45:39.977: INFO: Received response from host: affinity-clusterip-2zbqc
Apr 26 14:45:39.977: INFO: Received response from host: affinity-clusterip-2zbqc
Apr 26 14:45:39.977: INFO: Received response from host: affinity-clusterip-2zbqc
Apr 26 14:45:39.977: INFO: Received response from host: affinity-clusterip-2zbqc
Apr 26 14:45:39.977: INFO: Received response from host: affinity-clusterip-2zbqc
Apr 26 14:45:39.977: INFO: Received response from host: affinity-clusterip-2zbqc
Apr 26 14:45:39.977: INFO: Received response from host: affinity-clusterip-2zbqc
Apr 26 14:45:39.977: INFO: Received response from host: affinity-clusterip-2zbqc
Apr 26 14:45:39.977: INFO: Received response from host: affinity-clusterip-2zbqc
Apr 26 14:45:39.977: INFO: Received response from host: affinity-clusterip-2zbqc
Apr 26 14:45:39.977: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-219, will wait for the garbage collector to delete the pods
Apr 26 14:45:40.073: INFO: Deleting ReplicationController affinity-clusterip took: 12.462475ms
Apr 26 14:45:40.674: INFO: Terminating ReplicationController affinity-clusterip pods took: 600.180924ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:45:55.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-219" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:28.037 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":303,"completed":207,"skipped":3297,"failed":0}
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:45:55.843: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-6568
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating statefulset ss in namespace statefulset-6568
Apr 26 14:45:55.960: INFO: Found 0 stateful pods, waiting for 1
Apr 26 14:46:05.971: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Apr 26 14:46:06.011: INFO: Deleting all statefulset in ns statefulset-6568
Apr 26 14:46:06.018: INFO: Scaling statefulset ss to 0
Apr 26 14:46:26.056: INFO: Waiting for statefulset status.replicas updated to 0
Apr 26 14:46:26.063: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:46:26.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6568" for this suite.

• [SLOW TEST:30.275 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":303,"completed":208,"skipped":3300,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:46:26.114: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Apr 26 14:46:26.188: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2d74581f-0a06-4627-994f-6cc21ebe4e3d" in namespace "projected-4972" to be "Succeeded or Failed"
Apr 26 14:46:26.198: INFO: Pod "downwardapi-volume-2d74581f-0a06-4627-994f-6cc21ebe4e3d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.221138ms
Apr 26 14:46:28.206: INFO: Pod "downwardapi-volume-2d74581f-0a06-4627-994f-6cc21ebe4e3d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017784099s
Apr 26 14:46:30.213: INFO: Pod "downwardapi-volume-2d74581f-0a06-4627-994f-6cc21ebe4e3d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025336829s
STEP: Saw pod success
Apr 26 14:46:30.213: INFO: Pod "downwardapi-volume-2d74581f-0a06-4627-994f-6cc21ebe4e3d" satisfied condition "Succeeded or Failed"
Apr 26 14:46:30.222: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod downwardapi-volume-2d74581f-0a06-4627-994f-6cc21ebe4e3d container client-container: <nil>
STEP: delete the pod
Apr 26 14:46:30.312: INFO: Waiting for pod downwardapi-volume-2d74581f-0a06-4627-994f-6cc21ebe4e3d to disappear
Apr 26 14:46:30.318: INFO: Pod downwardapi-volume-2d74581f-0a06-4627-994f-6cc21ebe4e3d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:46:30.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4972" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":303,"completed":209,"skipped":3302,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:46:30.347: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-e7ef813b-a9f0-4693-b5e8-c4cf4776e85f
STEP: Creating configMap with name cm-test-opt-upd-8b431681-31c9-47c1-a9e6-e6db1f01cf72
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-e7ef813b-a9f0-4693-b5e8-c4cf4776e85f
STEP: Updating configmap cm-test-opt-upd-8b431681-31c9-47c1-a9e6-e6db1f01cf72
STEP: Creating configMap with name cm-test-opt-create-818d923f-bba5-43f6-9fa2-fcad19c9d172
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:47:53.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6568" for this suite.

• [SLOW TEST:83.434 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":303,"completed":210,"skipped":3326,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:47:53.782: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Apr 26 14:47:53.877: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-dd003a41-1b1d-41df-9a56-2423d55c3b36" in namespace "security-context-test-7976" to be "Succeeded or Failed"
Apr 26 14:47:53.888: INFO: Pod "alpine-nnp-false-dd003a41-1b1d-41df-9a56-2423d55c3b36": Phase="Pending", Reason="", readiness=false. Elapsed: 11.046122ms
Apr 26 14:47:55.895: INFO: Pod "alpine-nnp-false-dd003a41-1b1d-41df-9a56-2423d55c3b36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017866271s
Apr 26 14:47:57.906: INFO: Pod "alpine-nnp-false-dd003a41-1b1d-41df-9a56-2423d55c3b36": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028832547s
Apr 26 14:47:59.919: INFO: Pod "alpine-nnp-false-dd003a41-1b1d-41df-9a56-2423d55c3b36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041482138s
Apr 26 14:47:59.919: INFO: Pod "alpine-nnp-false-dd003a41-1b1d-41df-9a56-2423d55c3b36" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 14:47:59.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7976" for this suite.

• [SLOW TEST:6.229 seconds]
[k8s.io] Security Context
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:291
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":211,"skipped":3342,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 14:48:00.011: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-7089
Apr 26 14:48:02.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-7089 kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Apr 26 14:48:03.118: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Apr 26 14:48:03.118: INFO: stdout: "ipvs"
Apr 26 14:48:03.118: INFO: proxyMode: ipvs
Apr 26 14:48:03.132: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Apr 26 14:48:03.141: INFO: Pod kube-proxy-mode-detector still exists
Apr 26 14:48:05.141: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Apr 26 14:48:05.147: INFO: Pod kube-proxy-mode-detector still exists
Apr 26 14:48:07.142: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Apr 26 14:48:07.154: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-7089
STEP: creating replication controller affinity-clusterip-timeout in namespace services-7089
I0426 14:48:07.196548      21 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-7089, replica count: 3
I0426 14:48:10.254504      21 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 26 14:48:10.271: INFO: Creating new exec pod
Apr 26 14:48:15.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-7089 execpod-affinity4496j -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Apr 26 14:48:15.934: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Apr 26 14:48:15.934: INFO: stdout: ""
Apr 26 14:48:15.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-7089 execpod-affinity4496j -- /bin/sh -x -c nc -zv -t -w 2 10.240.26.55 80'
Apr 26 14:48:16.529: INFO: stderr: "+ nc -zv -t -w 2 10.240.26.55 80\nConnection to 10.240.26.55 80 port [tcp/http] succeeded!\n"
Apr 26 14:48:16.529: INFO: stdout: ""
Apr 26 14:48:16.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-7089 execpod-affinity4496j -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.26.55:80/ ; done'
Apr 26 14:48:17.237: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.26.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.26.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.26.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.26.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.26.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.26.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.26.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.26.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.26.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.26.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.26.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.26.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.26.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.26.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.26.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.26.55:80/\n"
Apr 26 14:48:17.237: INFO: stdout: "\naffinity-clusterip-timeout-qr94p\naffinity-clusterip-timeout-qr94p\naffinity-clusterip-timeout-qr94p\naffinity-clusterip-timeout-qr94p\naffinity-clusterip-timeout-qr94p\naffinity-clusterip-timeout-qr94p\naffinity-clusterip-timeout-qr94p\naffinity-clusterip-timeout-qr94p\naffinity-clusterip-timeout-qr94p\naffinity-clusterip-timeout-qr94p\naffinity-clusterip-timeout-qr94p\naffinity-clusterip-timeout-qr94p\naffinity-clusterip-timeout-qr94p\naffinity-clusterip-timeout-qr94p\naffinity-clusterip-timeout-qr94p\naffinity-clusterip-timeout-qr94p"
Apr 26 14:48:17.237: INFO: Received response from host: affinity-clusterip-timeout-qr94p
Apr 26 14:48:17.237: INFO: Received response from host: affinity-clusterip-timeout-qr94p
Apr 26 14:48:17.237: INFO: Received response from host: affinity-clusterip-timeout-qr94p
Apr 26 14:48:17.237: INFO: Received response from host: affinity-clusterip-timeout-qr94p
Apr 26 14:48:17.237: INFO: Received response from host: affinity-clusterip-timeout-qr94p
Apr 26 14:48:17.237: INFO: Received response from host: affinity-clusterip-timeout-qr94p
Apr 26 14:48:17.237: INFO: Received response from host: affinity-clusterip-timeout-qr94p
Apr 26 14:48:17.237: INFO: Received response from host: affinity-clusterip-timeout-qr94p
Apr 26 14:48:17.237: INFO: Received response from host: affinity-clusterip-timeout-qr94p
Apr 26 14:48:17.237: INFO: Received response from host: affinity-clusterip-timeout-qr94p
Apr 26 14:48:17.237: INFO: Received response from host: affinity-clusterip-timeout-qr94p
Apr 26 14:48:17.237: INFO: Received response from host: affinity-clusterip-timeout-qr94p
Apr 26 14:48:17.237: INFO: Received response from host: affinity-clusterip-timeout-qr94p
Apr 26 14:48:17.237: INFO: Received response from host: affinity-clusterip-timeout-qr94p
Apr 26 14:48:17.237: INFO: Received response from host: affinity-clusterip-timeout-qr94p
Apr 26 14:48:17.237: INFO: Received response from host: affinity-clusterip-timeout-qr94p
Apr 26 14:48:17.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-7089 execpod-affinity4496j -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.240.26.55:80/'
Apr 26 14:48:17.910: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.240.26.55:80/\n"
Apr 26 14:48:17.910: INFO: stdout: "affinity-clusterip-timeout-qr94p"
Apr 26 14:50:22.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-7089 execpod-affinity4496j -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.240.26.55:80/'
Apr 26 14:50:23.558: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.240.26.55:80/\n"
Apr 26 14:50:23.558: INFO: stdout: "affinity-clusterip-timeout-qr94p"
Apr 26 14:52:28.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-7089 execpod-affinity4496j -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.240.26.55:80/'
Apr 26 14:52:29.251: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.240.26.55:80/\n"
Apr 26 14:52:29.251: INFO: stdout: "affinity-clusterip-timeout-qr94p"
Apr 26 14:54:34.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-7089 execpod-affinity4496j -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.240.26.55:80/'
Apr 26 14:54:34.933: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.240.26.55:80/\n"
Apr 26 14:54:34.933: INFO: stdout: "affinity-clusterip-timeout-qr94p"
Apr 26 14:56:39.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-7089 execpod-affinity4496j -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.240.26.55:80/'
Apr 26 14:56:40.577: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.240.26.55:80/\n"
Apr 26 14:56:40.577: INFO: stdout: "affinity-clusterip-timeout-qr94p"
Apr 26 14:58:45.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-7089 execpod-affinity4496j -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.240.26.55:80/'
Apr 26 14:58:46.587: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.240.26.55:80/\n"
Apr 26 14:58:46.587: INFO: stdout: "affinity-clusterip-timeout-qr94p"
Apr 26 15:00:51.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-7089 execpod-affinity4496j -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.240.26.55:80/'
Apr 26 15:00:52.244: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.240.26.55:80/\n"
Apr 26 15:00:52.244: INFO: stdout: "affinity-clusterip-timeout-qr94p"
Apr 26 15:02:57.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-7089 execpod-affinity4496j -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.240.26.55:80/'
Apr 26 15:02:57.899: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.240.26.55:80/\n"
Apr 26 15:02:57.899: INFO: stdout: "affinity-clusterip-timeout-5tpzf"
Apr 26 15:02:57.899: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-7089, will wait for the garbage collector to delete the pods
Apr 26 15:02:58.002: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 13.440338ms
Apr 26 15:02:58.603: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 600.470497ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:03:11.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7089" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:911.864 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":303,"completed":212,"skipped":3356,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:03:11.877: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override all
Apr 26 15:03:11.954: INFO: Waiting up to 5m0s for pod "client-containers-7e81f703-c586-4bd1-a303-c0d6eb086f69" in namespace "containers-1695" to be "Succeeded or Failed"
Apr 26 15:03:11.968: INFO: Pod "client-containers-7e81f703-c586-4bd1-a303-c0d6eb086f69": Phase="Pending", Reason="", readiness=false. Elapsed: 13.277932ms
Apr 26 15:03:13.975: INFO: Pod "client-containers-7e81f703-c586-4bd1-a303-c0d6eb086f69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020626178s
Apr 26 15:03:15.982: INFO: Pod "client-containers-7e81f703-c586-4bd1-a303-c0d6eb086f69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027540008s
STEP: Saw pod success
Apr 26 15:03:15.982: INFO: Pod "client-containers-7e81f703-c586-4bd1-a303-c0d6eb086f69" satisfied condition "Succeeded or Failed"
Apr 26 15:03:15.991: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod client-containers-7e81f703-c586-4bd1-a303-c0d6eb086f69 container test-container: <nil>
STEP: delete the pod
Apr 26 15:03:16.037: INFO: Waiting for pod client-containers-7e81f703-c586-4bd1-a303-c0d6eb086f69 to disappear
Apr 26 15:03:16.042: INFO: Pod client-containers-7e81f703-c586-4bd1-a303-c0d6eb086f69 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:03:16.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1695" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":303,"completed":213,"skipped":3367,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:03:16.065: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:03:16.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4903" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":303,"completed":214,"skipped":3413,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:03:16.168: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Apr 26 15:03:20.814: INFO: Successfully updated pod "pod-update-208a7892-3b19-4dde-a18f-4e50722fa400"
STEP: verifying the updated pod is in kubernetes
Apr 26 15:03:20.830: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:03:20.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2324" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":303,"completed":215,"skipped":3458,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:03:20.857: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-ebee3ec4-4191-45a3-bfb5-c0ed7d9d13e6
STEP: Creating secret with name s-test-opt-upd-9a64fbd8-98ec-4fcd-b89a-17ec7a8799c2
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-ebee3ec4-4191-45a3-bfb5-c0ed7d9d13e6
STEP: Updating secret s-test-opt-upd-9a64fbd8-98ec-4fcd-b89a-17ec7a8799c2
STEP: Creating secret with name s-test-opt-create-d78f1ae4-0350-429f-99b6-86a970d5c1a8
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:03:29.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8284" for this suite.

• [SLOW TEST:8.635 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":303,"completed":216,"skipped":3487,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:03:29.492: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Apr 26 15:03:30.159: INFO: Checking APIGroup: apiregistration.k8s.io
Apr 26 15:03:30.162: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Apr 26 15:03:30.162: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Apr 26 15:03:30.162: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Apr 26 15:03:30.162: INFO: Checking APIGroup: extensions
Apr 26 15:03:30.165: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Apr 26 15:03:30.165: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Apr 26 15:03:30.165: INFO: extensions/v1beta1 matches extensions/v1beta1
Apr 26 15:03:30.165: INFO: Checking APIGroup: apps
Apr 26 15:03:30.168: INFO: PreferredVersion.GroupVersion: apps/v1
Apr 26 15:03:30.168: INFO: Versions found [{apps/v1 v1}]
Apr 26 15:03:30.168: INFO: apps/v1 matches apps/v1
Apr 26 15:03:30.168: INFO: Checking APIGroup: events.k8s.io
Apr 26 15:03:30.171: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Apr 26 15:03:30.171: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Apr 26 15:03:30.171: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Apr 26 15:03:30.172: INFO: Checking APIGroup: authentication.k8s.io
Apr 26 15:03:30.179: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Apr 26 15:03:30.179: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Apr 26 15:03:30.179: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Apr 26 15:03:30.179: INFO: Checking APIGroup: authorization.k8s.io
Apr 26 15:03:30.182: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Apr 26 15:03:30.182: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Apr 26 15:03:30.182: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Apr 26 15:03:30.182: INFO: Checking APIGroup: autoscaling
Apr 26 15:03:30.186: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Apr 26 15:03:30.186: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Apr 26 15:03:30.186: INFO: autoscaling/v1 matches autoscaling/v1
Apr 26 15:03:30.186: INFO: Checking APIGroup: batch
Apr 26 15:03:30.189: INFO: PreferredVersion.GroupVersion: batch/v1
Apr 26 15:03:30.189: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Apr 26 15:03:30.189: INFO: batch/v1 matches batch/v1
Apr 26 15:03:30.189: INFO: Checking APIGroup: certificates.k8s.io
Apr 26 15:03:30.192: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Apr 26 15:03:30.192: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Apr 26 15:03:30.192: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Apr 26 15:03:30.192: INFO: Checking APIGroup: networking.k8s.io
Apr 26 15:03:30.195: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Apr 26 15:03:30.195: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Apr 26 15:03:30.195: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Apr 26 15:03:30.195: INFO: Checking APIGroup: policy
Apr 26 15:03:30.197: INFO: PreferredVersion.GroupVersion: policy/v1beta1
Apr 26 15:03:30.197: INFO: Versions found [{policy/v1beta1 v1beta1}]
Apr 26 15:03:30.197: INFO: policy/v1beta1 matches policy/v1beta1
Apr 26 15:03:30.197: INFO: Checking APIGroup: rbac.authorization.k8s.io
Apr 26 15:03:30.200: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Apr 26 15:03:30.200: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Apr 26 15:03:30.200: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Apr 26 15:03:30.200: INFO: Checking APIGroup: storage.k8s.io
Apr 26 15:03:30.203: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Apr 26 15:03:30.203: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Apr 26 15:03:30.203: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Apr 26 15:03:30.203: INFO: Checking APIGroup: admissionregistration.k8s.io
Apr 26 15:03:30.206: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Apr 26 15:03:30.206: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Apr 26 15:03:30.206: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Apr 26 15:03:30.206: INFO: Checking APIGroup: apiextensions.k8s.io
Apr 26 15:03:30.209: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Apr 26 15:03:30.209: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Apr 26 15:03:30.209: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Apr 26 15:03:30.209: INFO: Checking APIGroup: scheduling.k8s.io
Apr 26 15:03:30.211: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Apr 26 15:03:30.211: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
Apr 26 15:03:30.211: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Apr 26 15:03:30.211: INFO: Checking APIGroup: coordination.k8s.io
Apr 26 15:03:30.214: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Apr 26 15:03:30.214: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Apr 26 15:03:30.214: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Apr 26 15:03:30.214: INFO: Checking APIGroup: node.k8s.io
Apr 26 15:03:30.217: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1beta1
Apr 26 15:03:30.217: INFO: Versions found [{node.k8s.io/v1beta1 v1beta1}]
Apr 26 15:03:30.217: INFO: node.k8s.io/v1beta1 matches node.k8s.io/v1beta1
Apr 26 15:03:30.217: INFO: Checking APIGroup: discovery.k8s.io
Apr 26 15:03:30.220: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
Apr 26 15:03:30.220: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
Apr 26 15:03:30.220: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
Apr 26 15:03:30.220: INFO: Checking APIGroup: crd.projectcalico.org
Apr 26 15:03:30.223: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Apr 26 15:03:30.223: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Apr 26 15:03:30.223: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Apr 26 15:03:30.223: INFO: Checking APIGroup: cluster.k8s.io
Apr 26 15:03:30.226: INFO: PreferredVersion.GroupVersion: cluster.k8s.io/v1alpha1
Apr 26 15:03:30.226: INFO: Versions found [{cluster.k8s.io/v1alpha1 v1alpha1}]
Apr 26 15:03:30.226: INFO: cluster.k8s.io/v1alpha1 matches cluster.k8s.io/v1alpha1
Apr 26 15:03:30.226: INFO: Checking APIGroup: snapshot.storage.k8s.io
Apr 26 15:03:30.229: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1alpha1
Apr 26 15:03:30.229: INFO: Versions found [{snapshot.storage.k8s.io/v1alpha1 v1alpha1}]
Apr 26 15:03:30.229: INFO: snapshot.storage.k8s.io/v1alpha1 matches snapshot.storage.k8s.io/v1alpha1
Apr 26 15:03:30.229: INFO: Checking APIGroup: metrics.k8s.io
Apr 26 15:03:30.232: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Apr 26 15:03:30.232: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Apr 26 15:03:30.232: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:03:30.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-7829" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":303,"completed":217,"skipped":3492,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:03:30.261: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap that has name configmap-test-emptyKey-208ec314-5c69-4a8c-b4ff-d13a9b5a48b1
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:03:30.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2891" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":303,"completed":218,"skipped":3512,"failed":0}

------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:03:30.381: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Apr 26 15:03:30.516: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:03:30.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5609" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":303,"completed":219,"skipped":3512,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:03:30.601: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Apr 26 15:03:30.691: INFO: Waiting up to 5m0s for pod "downward-api-1d392196-edf7-4647-8059-b0a7b5c7c939" in namespace "downward-api-9207" to be "Succeeded or Failed"
Apr 26 15:03:30.702: INFO: Pod "downward-api-1d392196-edf7-4647-8059-b0a7b5c7c939": Phase="Pending", Reason="", readiness=false. Elapsed: 10.506987ms
Apr 26 15:03:32.708: INFO: Pod "downward-api-1d392196-edf7-4647-8059-b0a7b5c7c939": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017479861s
Apr 26 15:03:34.716: INFO: Pod "downward-api-1d392196-edf7-4647-8059-b0a7b5c7c939": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02450287s
STEP: Saw pod success
Apr 26 15:03:34.716: INFO: Pod "downward-api-1d392196-edf7-4647-8059-b0a7b5c7c939" satisfied condition "Succeeded or Failed"
Apr 26 15:03:34.721: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-w8lg4 pod downward-api-1d392196-edf7-4647-8059-b0a7b5c7c939 container dapi-container: <nil>
STEP: delete the pod
Apr 26 15:03:34.771: INFO: Waiting for pod downward-api-1d392196-edf7-4647-8059-b0a7b5c7c939 to disappear
Apr 26 15:03:34.778: INFO: Pod downward-api-1d392196-edf7-4647-8059-b0a7b5c7c939 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:03:34.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9207" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":303,"completed":220,"skipped":3524,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:03:34.804: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-1511
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-1511
STEP: creating replication controller externalsvc in namespace services-1511
I0426 15:03:34.967979      21 runners.go:190] Created replication controller with name: externalsvc, namespace: services-1511, replica count: 2
I0426 15:03:38.018400      21 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Apr 26 15:03:38.089: INFO: Creating new exec pod
Apr 26 15:03:42.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-1511 execpodjnsmb -- /bin/sh -x -c nslookup clusterip-service.services-1511.svc.cluster.local'
Apr 26 15:03:42.774: INFO: stderr: "+ nslookup clusterip-service.services-1511.svc.cluster.local\n"
Apr 26 15:03:42.774: INFO: stdout: "Server:\t\t10.240.16.10\nAddress:\t10.240.16.10#53\n\nclusterip-service.services-1511.svc.cluster.local\tcanonical name = externalsvc.services-1511.svc.cluster.local.\nName:\texternalsvc.services-1511.svc.cluster.local\nAddress: 10.240.22.225\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1511, will wait for the garbage collector to delete the pods
Apr 26 15:03:42.847: INFO: Deleting ReplicationController externalsvc took: 16.462786ms
Apr 26 15:03:43.447: INFO: Terminating ReplicationController externalsvc pods took: 600.26177ms
Apr 26 15:03:51.884: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:03:51.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1511" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:17.129 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":303,"completed":221,"skipped":3541,"failed":0}
S
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:03:51.933: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:03:52.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-9771" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":303,"completed":222,"skipped":3542,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:03:52.020: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Apr 26 15:04:00.162: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 26 15:04:00.169: INFO: Pod pod-with-prestop-http-hook still exists
Apr 26 15:04:02.169: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 26 15:04:02.176: INFO: Pod pod-with-prestop-http-hook still exists
Apr 26 15:04:04.169: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 26 15:04:04.177: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:04:04.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1986" for this suite.

• [SLOW TEST:12.237 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":303,"completed":223,"skipped":3549,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:04:04.258: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Apr 26 15:04:04.333: INFO: Waiting up to 5m0s for pod "pod-a14d664a-9f67-405b-88a0-55348a77e691" in namespace "emptydir-387" to be "Succeeded or Failed"
Apr 26 15:04:04.342: INFO: Pod "pod-a14d664a-9f67-405b-88a0-55348a77e691": Phase="Pending", Reason="", readiness=false. Elapsed: 8.462512ms
Apr 26 15:04:06.350: INFO: Pod "pod-a14d664a-9f67-405b-88a0-55348a77e691": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016184854s
Apr 26 15:04:08.356: INFO: Pod "pod-a14d664a-9f67-405b-88a0-55348a77e691": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0228997s
STEP: Saw pod success
Apr 26 15:04:08.356: INFO: Pod "pod-a14d664a-9f67-405b-88a0-55348a77e691" satisfied condition "Succeeded or Failed"
Apr 26 15:04:08.362: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod pod-a14d664a-9f67-405b-88a0-55348a77e691 container test-container: <nil>
STEP: delete the pod
Apr 26 15:04:08.408: INFO: Waiting for pod pod-a14d664a-9f67-405b-88a0-55348a77e691 to disappear
Apr 26 15:04:08.414: INFO: Pod pod-a14d664a-9f67-405b-88a0-55348a77e691 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:04:08.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-387" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":224,"skipped":3567,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:04:08.436: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Apr 26 15:04:08.517: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-e7e4eb34-1722-41b2-99ce-eb35eaf50150" in namespace "security-context-test-6808" to be "Succeeded or Failed"
Apr 26 15:04:08.525: INFO: Pod "busybox-readonly-false-e7e4eb34-1722-41b2-99ce-eb35eaf50150": Phase="Pending", Reason="", readiness=false. Elapsed: 7.53044ms
Apr 26 15:04:10.531: INFO: Pod "busybox-readonly-false-e7e4eb34-1722-41b2-99ce-eb35eaf50150": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014168507s
Apr 26 15:04:12.538: INFO: Pod "busybox-readonly-false-e7e4eb34-1722-41b2-99ce-eb35eaf50150": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020918456s
Apr 26 15:04:12.538: INFO: Pod "busybox-readonly-false-e7e4eb34-1722-41b2-99ce-eb35eaf50150" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:04:12.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6808" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":303,"completed":225,"skipped":3578,"failed":0}
SSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:04:12.566: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Apr 26 15:04:12.691: INFO: starting watch
STEP: patching
STEP: updating
Apr 26 15:04:12.713: INFO: waiting for watch events with expected annotations
Apr 26 15:04:12.713: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:04:12.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-7668" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":303,"completed":226,"skipped":3583,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:04:12.802: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-cb05fa83-44e5-426f-baab-b2dc33fb5ade
STEP: Creating a pod to test consume configMaps
Apr 26 15:04:12.903: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-10b18ad7-6fd5-46a5-8ac3-9df91c838228" in namespace "projected-3336" to be "Succeeded or Failed"
Apr 26 15:04:12.912: INFO: Pod "pod-projected-configmaps-10b18ad7-6fd5-46a5-8ac3-9df91c838228": Phase="Pending", Reason="", readiness=false. Elapsed: 8.814965ms
Apr 26 15:04:14.917: INFO: Pod "pod-projected-configmaps-10b18ad7-6fd5-46a5-8ac3-9df91c838228": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014595491s
Apr 26 15:04:16.924: INFO: Pod "pod-projected-configmaps-10b18ad7-6fd5-46a5-8ac3-9df91c838228": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020693558s
STEP: Saw pod success
Apr 26 15:04:16.924: INFO: Pod "pod-projected-configmaps-10b18ad7-6fd5-46a5-8ac3-9df91c838228" satisfied condition "Succeeded or Failed"
Apr 26 15:04:16.932: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod pod-projected-configmaps-10b18ad7-6fd5-46a5-8ac3-9df91c838228 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 26 15:04:16.987: INFO: Waiting for pod pod-projected-configmaps-10b18ad7-6fd5-46a5-8ac3-9df91c838228 to disappear
Apr 26 15:04:16.994: INFO: Pod pod-projected-configmaps-10b18ad7-6fd5-46a5-8ac3-9df91c838228 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:04:16.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3336" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":303,"completed":227,"skipped":3588,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:04:17.026: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:04:30.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5674" for this suite.

• [SLOW TEST:13.247 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":303,"completed":228,"skipped":3601,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:04:30.278: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Apr 26 15:04:34.413: INFO: &Pod{ObjectMeta:{send-events-7599fa02-d4ea-48cd-9fd1-b42470128af2  events-8591 /api/v1/namespaces/events-8591/pods/send-events-7599fa02-d4ea-48cd-9fd1-b42470128af2 7c74b85c-b1fd-46a8-8396-a13ff808d8ca 46265 0 2021-04-26 15:04:30 +0000 UTC <nil> <nil> map[name:foo time:354546264] map[cni.projectcalico.org/podIP:172.25.1.237/32] [] []  [{e2e.test Update v1 2021-04-26 15:04:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-04-26 15:04:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{}}}}} {kubelet Update v1 2021-04-26 15:04:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.237\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7mlt4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7mlt4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7mlt4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:elastic-hugle-5f9d9b5555-8444w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 15:04:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 15:04:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 15:04:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-26 15:04:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.11,PodIP:172.25.1.237,StartTime:2021-04-26 15:04:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-26 15:04:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:docker://1caf80fbd856e3fa4f42c3458a9a2dc000eab9bebe8a62ffcd8396bb6a1e29af,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.237,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Apr 26 15:04:36.421: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Apr 26 15:04:38.428: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:04:38.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8591" for this suite.

• [SLOW TEST:8.183 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":303,"completed":229,"skipped":3624,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:04:38.463: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Apr 26 15:04:41.593: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:04:41.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2568" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":303,"completed":230,"skipped":3646,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:04:41.647: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 26 15:04:42.133: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 26 15:04:44.150: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755046282, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755046282, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755046282, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755046282, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 26 15:04:47.178: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:04:47.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9556" for this suite.
STEP: Destroying namespace "webhook-9556-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.265 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":303,"completed":231,"skipped":3655,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:04:47.924: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Apr 26 15:04:48.037: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6877 /api/v1/namespaces/watch-6877/configmaps/e2e-watch-test-label-changed 62130bb6-b714-4463-ac53-b66749b6be9c 46462 0 2021-04-26 15:04:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-04-26 15:04:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 26 15:04:48.038: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6877 /api/v1/namespaces/watch-6877/configmaps/e2e-watch-test-label-changed 62130bb6-b714-4463-ac53-b66749b6be9c 46463 0 2021-04-26 15:04:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-04-26 15:04:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 26 15:04:48.038: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6877 /api/v1/namespaces/watch-6877/configmaps/e2e-watch-test-label-changed 62130bb6-b714-4463-ac53-b66749b6be9c 46464 0 2021-04-26 15:04:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-04-26 15:04:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Apr 26 15:04:58.099: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6877 /api/v1/namespaces/watch-6877/configmaps/e2e-watch-test-label-changed 62130bb6-b714-4463-ac53-b66749b6be9c 46543 0 2021-04-26 15:04:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-04-26 15:04:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 26 15:04:58.099: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6877 /api/v1/namespaces/watch-6877/configmaps/e2e-watch-test-label-changed 62130bb6-b714-4463-ac53-b66749b6be9c 46544 0 2021-04-26 15:04:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-04-26 15:04:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 26 15:04:58.100: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6877 /api/v1/namespaces/watch-6877/configmaps/e2e-watch-test-label-changed 62130bb6-b714-4463-ac53-b66749b6be9c 46545 0 2021-04-26 15:04:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-04-26 15:04:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:04:58.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6877" for this suite.

• [SLOW TEST:10.202 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":303,"completed":232,"skipped":3726,"failed":0}
SSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:04:58.126: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pod templates
Apr 26 15:04:58.191: INFO: created test-podtemplate-1
Apr 26 15:04:58.201: INFO: created test-podtemplate-2
Apr 26 15:04:58.212: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Apr 26 15:04:58.218: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Apr 26 15:04:58.249: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:04:58.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-3490" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":303,"completed":233,"skipped":3731,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:04:58.277: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:05:03.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-311" for this suite.

• [SLOW TEST:5.153 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":303,"completed":234,"skipped":3747,"failed":0}
S
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:05:03.430: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Apr 26 15:05:03.556: INFO: Waiting up to 5m0s for pod "downward-api-11069f86-620c-4f22-99e0-df506733d8d9" in namespace "downward-api-8335" to be "Succeeded or Failed"
Apr 26 15:05:03.573: INFO: Pod "downward-api-11069f86-620c-4f22-99e0-df506733d8d9": Phase="Pending", Reason="", readiness=false. Elapsed: 17.118425ms
Apr 26 15:05:05.580: INFO: Pod "downward-api-11069f86-620c-4f22-99e0-df506733d8d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023716288s
Apr 26 15:05:07.586: INFO: Pod "downward-api-11069f86-620c-4f22-99e0-df506733d8d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030276999s
STEP: Saw pod success
Apr 26 15:05:07.586: INFO: Pod "downward-api-11069f86-620c-4f22-99e0-df506733d8d9" satisfied condition "Succeeded or Failed"
Apr 26 15:05:07.591: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-w8lg4 pod downward-api-11069f86-620c-4f22-99e0-df506733d8d9 container dapi-container: <nil>
STEP: delete the pod
Apr 26 15:05:07.679: INFO: Waiting for pod downward-api-11069f86-620c-4f22-99e0-df506733d8d9 to disappear
Apr 26 15:05:07.689: INFO: Pod downward-api-11069f86-620c-4f22-99e0-df506733d8d9 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:05:07.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8335" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":303,"completed":235,"skipped":3748,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:05:07.715: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Apr 26 15:05:07.806: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 26 15:06:07.856: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
Apr 26 15:06:07.889: INFO: Created pod: pod0-sched-preemption-low-priority
Apr 26 15:06:07.919: INFO: Created pod: pod1-sched-preemption-medium-priority
Apr 26 15:06:07.959: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:06:18.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-195" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:70.464 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":303,"completed":236,"skipped":3765,"failed":0}
SS
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:06:18.182: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:06:18.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3970" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":303,"completed":237,"skipped":3767,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:06:18.328: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6429.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6429.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6429.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6429.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 26 15:06:22.609: INFO: DNS probes using dns-test-e2f6d732-093b-4bf3-9d0f-1c783218e6a3 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6429.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6429.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6429.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6429.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 26 15:06:26.902: INFO: DNS probes using dns-test-155f47af-33b6-4ce6-9d71-b46ce93c7566 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6429.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6429.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6429.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6429.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 26 15:06:31.312: INFO: DNS probes using dns-test-08b6d83e-ccb8-4c00-8b89-4e1a0b33aa41 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:06:31.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6429" for this suite.

• [SLOW TEST:13.070 seconds]
[sig-network] DNS
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":303,"completed":238,"skipped":3780,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:06:31.402: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Apr 26 15:06:39.589: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 26 15:06:39.594: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 26 15:06:41.595: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 26 15:06:41.602: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 26 15:06:43.595: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 26 15:06:43.602: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 26 15:06:45.595: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 26 15:06:45.602: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 26 15:06:47.595: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 26 15:06:47.602: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 26 15:06:49.595: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 26 15:06:49.602: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 26 15:06:51.595: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 26 15:06:51.602: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 26 15:06:53.595: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 26 15:06:53.602: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:06:53.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1682" for this suite.

• [SLOW TEST:22.227 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":303,"completed":239,"skipped":3811,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:06:53.635: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-5066
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 26 15:06:53.723: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr 26 15:06:53.806: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr 26 15:06:55.821: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 26 15:06:57.812: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 26 15:06:59.813: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 26 15:07:01.812: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 26 15:07:03.814: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 26 15:07:05.817: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 26 15:07:07.813: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 26 15:07:09.813: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 26 15:07:11.813: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 26 15:07:13.812: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 26 15:07:15.812: INFO: The status of Pod netserver-0 is Running (Ready = true)
Apr 26 15:07:15.827: INFO: The status of Pod netserver-1 is Running (Ready = true)
Apr 26 15:07:15.838: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Apr 26 15:07:19.913: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.1.247:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5066 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 26 15:07:19.913: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
Apr 26 15:07:20.433: INFO: Found all expected endpoints: [netserver-0]
Apr 26 15:07:20.440: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.2.49:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5066 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 26 15:07:20.440: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
Apr 26 15:07:21.059: INFO: Found all expected endpoints: [netserver-1]
Apr 26 15:07:21.068: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.0.103:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5066 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 26 15:07:21.068: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
Apr 26 15:07:21.648: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:07:21.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5066" for this suite.

• [SLOW TEST:28.036 seconds]
[sig-network] Networking
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":240,"skipped":3825,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:07:21.672: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Apr 26 15:07:21.757: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1143 /api/v1/namespaces/watch-1143/configmaps/e2e-watch-test-watch-closed e69e1204-cf8f-4c40-ac12-a5125b44483a 47664 0 2021-04-26 15:07:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-04-26 15:07:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 26 15:07:21.757: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1143 /api/v1/namespaces/watch-1143/configmaps/e2e-watch-test-watch-closed e69e1204-cf8f-4c40-ac12-a5125b44483a 47665 0 2021-04-26 15:07:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-04-26 15:07:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Apr 26 15:07:21.788: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1143 /api/v1/namespaces/watch-1143/configmaps/e2e-watch-test-watch-closed e69e1204-cf8f-4c40-ac12-a5125b44483a 47666 0 2021-04-26 15:07:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-04-26 15:07:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 26 15:07:21.788: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1143 /api/v1/namespaces/watch-1143/configmaps/e2e-watch-test-watch-closed e69e1204-cf8f-4c40-ac12-a5125b44483a 47667 0 2021-04-26 15:07:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-04-26 15:07:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:07:21.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1143" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":303,"completed":241,"skipped":3831,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Lease
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:07:21.818: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Lease
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:07:22.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-3580" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":303,"completed":242,"skipped":3857,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:07:22.025: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:07:38.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8156" for this suite.

• [SLOW TEST:16.269 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":303,"completed":243,"skipped":3860,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:07:38.296: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Apr 26 15:07:38.360: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
Apr 26 15:07:42.082: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:07:56.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4241" for this suite.

• [SLOW TEST:18.076 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":303,"completed":244,"skipped":3899,"failed":0}
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:07:56.374: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-4730
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-4730
STEP: Creating statefulset with conflicting port in namespace statefulset-4730
STEP: Waiting until pod test-pod will start running in namespace statefulset-4730
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-4730
Apr 26 15:08:00.549: INFO: Observed stateful pod in namespace: statefulset-4730, name: ss-0, uid: 970a3227-33dc-4292-aded-6fc390633166, status phase: Pending. Waiting for statefulset controller to delete.
Apr 26 15:08:00.915: INFO: Observed stateful pod in namespace: statefulset-4730, name: ss-0, uid: 970a3227-33dc-4292-aded-6fc390633166, status phase: Failed. Waiting for statefulset controller to delete.
Apr 26 15:08:00.930: INFO: Observed stateful pod in namespace: statefulset-4730, name: ss-0, uid: 970a3227-33dc-4292-aded-6fc390633166, status phase: Failed. Waiting for statefulset controller to delete.
Apr 26 15:08:00.945: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-4730
STEP: Removing pod with conflicting port in namespace statefulset-4730
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-4730 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Apr 26 15:08:05.011: INFO: Deleting all statefulset in ns statefulset-4730
Apr 26 15:08:05.018: INFO: Scaling statefulset ss to 0
Apr 26 15:08:15.047: INFO: Waiting for statefulset status.replicas updated to 0
Apr 26 15:08:15.052: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:08:15.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4730" for this suite.

• [SLOW TEST:18.720 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":303,"completed":245,"skipped":3905,"failed":0}
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:08:15.094: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Apr 26 15:08:18.237: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:08:18.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6432" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":303,"completed":246,"skipped":3909,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:08:18.288: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Apr 26 15:08:18.379: INFO: Waiting up to 5m0s for pod "downward-api-cc778032-411c-46f1-907a-bf779bc545f5" in namespace "downward-api-7314" to be "Succeeded or Failed"
Apr 26 15:08:18.392: INFO: Pod "downward-api-cc778032-411c-46f1-907a-bf779bc545f5": Phase="Pending", Reason="", readiness=false. Elapsed: 13.48941ms
Apr 26 15:08:20.398: INFO: Pod "downward-api-cc778032-411c-46f1-907a-bf779bc545f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019353351s
STEP: Saw pod success
Apr 26 15:08:20.399: INFO: Pod "downward-api-cc778032-411c-46f1-907a-bf779bc545f5" satisfied condition "Succeeded or Failed"
Apr 26 15:08:20.405: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod downward-api-cc778032-411c-46f1-907a-bf779bc545f5 container dapi-container: <nil>
STEP: delete the pod
Apr 26 15:08:20.453: INFO: Waiting for pod downward-api-cc778032-411c-46f1-907a-bf779bc545f5 to disappear
Apr 26 15:08:20.459: INFO: Pod downward-api-cc778032-411c-46f1-907a-bf779bc545f5 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:08:20.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7314" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":303,"completed":247,"skipped":3931,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:08:20.488: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Apr 26 15:08:20.586: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Apr 26 15:08:24.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 --namespace=crd-publish-openapi-3774 create -f -'
Apr 26 15:08:25.051: INFO: stderr: ""
Apr 26 15:08:25.051: INFO: stdout: "e2e-test-crd-publish-openapi-2876-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Apr 26 15:08:25.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 --namespace=crd-publish-openapi-3774 delete e2e-test-crd-publish-openapi-2876-crds test-cr'
Apr 26 15:08:25.158: INFO: stderr: ""
Apr 26 15:08:25.158: INFO: stdout: "e2e-test-crd-publish-openapi-2876-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Apr 26 15:08:25.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 --namespace=crd-publish-openapi-3774 apply -f -'
Apr 26 15:08:25.420: INFO: stderr: ""
Apr 26 15:08:25.420: INFO: stdout: "e2e-test-crd-publish-openapi-2876-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Apr 26 15:08:25.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 --namespace=crd-publish-openapi-3774 delete e2e-test-crd-publish-openapi-2876-crds test-cr'
Apr 26 15:08:25.520: INFO: stderr: ""
Apr 26 15:08:25.520: INFO: stdout: "e2e-test-crd-publish-openapi-2876-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Apr 26 15:08:25.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 explain e2e-test-crd-publish-openapi-2876-crds'
Apr 26 15:08:25.762: INFO: stderr: ""
Apr 26 15:08:25.762: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2876-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:08:29.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3774" for this suite.

• [SLOW TEST:8.987 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":303,"completed":248,"skipped":3946,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:08:29.476: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Apr 26 15:08:29.541: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Apr 26 15:08:33.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 --namespace=crd-publish-openapi-6198 create -f -'
Apr 26 15:08:33.935: INFO: stderr: ""
Apr 26 15:08:33.935: INFO: stdout: "e2e-test-crd-publish-openapi-8170-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Apr 26 15:08:33.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 --namespace=crd-publish-openapi-6198 delete e2e-test-crd-publish-openapi-8170-crds test-cr'
Apr 26 15:08:34.074: INFO: stderr: ""
Apr 26 15:08:34.074: INFO: stdout: "e2e-test-crd-publish-openapi-8170-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Apr 26 15:08:34.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 --namespace=crd-publish-openapi-6198 apply -f -'
Apr 26 15:08:34.303: INFO: stderr: ""
Apr 26 15:08:34.303: INFO: stdout: "e2e-test-crd-publish-openapi-8170-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Apr 26 15:08:34.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 --namespace=crd-publish-openapi-6198 delete e2e-test-crd-publish-openapi-8170-crds test-cr'
Apr 26 15:08:34.442: INFO: stderr: ""
Apr 26 15:08:34.442: INFO: stdout: "e2e-test-crd-publish-openapi-8170-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Apr 26 15:08:34.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 explain e2e-test-crd-publish-openapi-8170-crds'
Apr 26 15:08:34.654: INFO: stderr: ""
Apr 26 15:08:34.654: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8170-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:08:38.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6198" for this suite.

• [SLOW TEST:8.888 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":303,"completed":249,"skipped":4011,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:08:38.365: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Apr 26 15:08:38.431: INFO: Waiting up to 5m0s for pod "pod-f11fe6e5-820f-40a7-b814-efc45fd9321f" in namespace "emptydir-444" to be "Succeeded or Failed"
Apr 26 15:08:38.443: INFO: Pod "pod-f11fe6e5-820f-40a7-b814-efc45fd9321f": Phase="Pending", Reason="", readiness=false. Elapsed: 11.768314ms
Apr 26 15:08:40.449: INFO: Pod "pod-f11fe6e5-820f-40a7-b814-efc45fd9321f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018314019s
Apr 26 15:08:42.456: INFO: Pod "pod-f11fe6e5-820f-40a7-b814-efc45fd9321f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025027156s
STEP: Saw pod success
Apr 26 15:08:42.456: INFO: Pod "pod-f11fe6e5-820f-40a7-b814-efc45fd9321f" satisfied condition "Succeeded or Failed"
Apr 26 15:08:42.462: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod pod-f11fe6e5-820f-40a7-b814-efc45fd9321f container test-container: <nil>
STEP: delete the pod
Apr 26 15:08:42.513: INFO: Waiting for pod pod-f11fe6e5-820f-40a7-b814-efc45fd9321f to disappear
Apr 26 15:08:42.519: INFO: Pod pod-f11fe6e5-820f-40a7-b814-efc45fd9321f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:08:42.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-444" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":250,"skipped":4021,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:08:42.541: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Apr 26 15:08:42.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-911'
Apr 26 15:08:42.708: INFO: stderr: ""
Apr 26 15:08:42.708: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Apr 26 15:08:42.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 get pod e2e-test-httpd-pod -o json --namespace=kubectl-911'
Apr 26 15:08:42.802: INFO: stderr: ""
Apr 26 15:08:42.802: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2021-04-26T15:08:42Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-04-26T15:08:42Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:message\": {},\n                                \"f:reason\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:message\": {},\n                                \"f:reason\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-04-26T15:08:42Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-911\",\n        \"resourceVersion\": \"48485\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-911/pods/e2e-test-httpd-pod\",\n        \"uid\": \"001b6d04-9b43-4b06-a7b0-6917aee54996\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-qv4kv\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"elastic-hugle-5f9d9b5555-8444w\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-qv4kv\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-qv4kv\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-04-26T15:08:42Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-04-26T15:08:42Z\",\n                \"message\": \"containers with unready status: [e2e-test-httpd-pod]\",\n                \"reason\": \"ContainersNotReady\",\n                \"status\": \"False\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-04-26T15:08:42Z\",\n                \"message\": \"containers with unready status: [e2e-test-httpd-pod]\",\n                \"reason\": \"ContainersNotReady\",\n                \"status\": \"False\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-04-26T15:08:42Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": false,\n                \"restartCount\": 0,\n                \"started\": false,\n                \"state\": {\n                    \"waiting\": {\n                        \"reason\": \"ContainerCreating\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.1.11\",\n        \"phase\": \"Pending\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-04-26T15:08:42Z\"\n    }\n}\n"
Apr 26 15:08:42.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 replace -f - --dry-run server --namespace=kubectl-911'
Apr 26 15:08:43.080: INFO: stderr: "W0426 15:08:42.866015    1149 helpers.go:553] --dry-run is deprecated and can be replaced with --dry-run=client.\n"
Apr 26 15:08:43.080: INFO: stdout: "pod/e2e-test-httpd-pod replaced (dry run)\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
Apr 26 15:08:43.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 delete pods e2e-test-httpd-pod --namespace=kubectl-911'
Apr 26 15:08:45.727: INFO: stderr: ""
Apr 26 15:08:45.727: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:08:45.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-911" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":303,"completed":251,"skipped":4048,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:08:45.747: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Apr 26 15:08:45.814: INFO: Waiting up to 5m0s for pod "downward-api-d5510306-2373-42ae-ab86-3c9fa85d4904" in namespace "downward-api-7980" to be "Succeeded or Failed"
Apr 26 15:08:45.824: INFO: Pod "downward-api-d5510306-2373-42ae-ab86-3c9fa85d4904": Phase="Pending", Reason="", readiness=false. Elapsed: 9.95261ms
Apr 26 15:08:47.834: INFO: Pod "downward-api-d5510306-2373-42ae-ab86-3c9fa85d4904": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020161078s
STEP: Saw pod success
Apr 26 15:08:47.834: INFO: Pod "downward-api-d5510306-2373-42ae-ab86-3c9fa85d4904" satisfied condition "Succeeded or Failed"
Apr 26 15:08:47.843: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod downward-api-d5510306-2373-42ae-ab86-3c9fa85d4904 container dapi-container: <nil>
STEP: delete the pod
Apr 26 15:08:47.911: INFO: Waiting for pod downward-api-d5510306-2373-42ae-ab86-3c9fa85d4904 to disappear
Apr 26 15:08:47.926: INFO: Pod downward-api-d5510306-2373-42ae-ab86-3c9fa85d4904 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:08:47.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7980" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":303,"completed":252,"skipped":4066,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:08:47.953: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-093eb8c0-e576-4ed2-ae58-28fd4ee0e7e8
STEP: Creating a pod to test consume configMaps
Apr 26 15:08:48.072: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-54786674-f616-4692-a7d5-3ccaf5f3d3c1" in namespace "projected-2801" to be "Succeeded or Failed"
Apr 26 15:08:48.081: INFO: Pod "pod-projected-configmaps-54786674-f616-4692-a7d5-3ccaf5f3d3c1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.087415ms
Apr 26 15:08:50.088: INFO: Pod "pod-projected-configmaps-54786674-f616-4692-a7d5-3ccaf5f3d3c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01586536s
STEP: Saw pod success
Apr 26 15:08:50.088: INFO: Pod "pod-projected-configmaps-54786674-f616-4692-a7d5-3ccaf5f3d3c1" satisfied condition "Succeeded or Failed"
Apr 26 15:08:50.093: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod pod-projected-configmaps-54786674-f616-4692-a7d5-3ccaf5f3d3c1 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 26 15:08:50.129: INFO: Waiting for pod pod-projected-configmaps-54786674-f616-4692-a7d5-3ccaf5f3d3c1 to disappear
Apr 26 15:08:50.136: INFO: Pod pod-projected-configmaps-54786674-f616-4692-a7d5-3ccaf5f3d3c1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:08:50.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2801" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":303,"completed":253,"skipped":4074,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:08:50.168: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Apr 26 15:08:50.229: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:09:09.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1018" for this suite.

• [SLOW TEST:18.987 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":303,"completed":254,"skipped":4117,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:09:09.158: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 26 15:09:09.785: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 26 15:09:11.803: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755046549, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755046549, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755046549, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755046549, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 26 15:09:14.828: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Apr 26 15:09:14.835: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:09:16.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2778" for this suite.
STEP: Destroying namespace "webhook-2778-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.547 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":303,"completed":255,"skipped":4118,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:09:16.707: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Apr 26 15:09:22.872: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:09:22.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0426 15:09:22.872357      21 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0426 15:09:22.872390      21 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0426 15:09:22.872395      21 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-3990" for this suite.

• [SLOW TEST:6.190 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":303,"completed":256,"skipped":4131,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:09:22.898: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Apr 26 15:09:23.003: INFO: Waiting up to 5m0s for pod "downwardapi-volume-06a0fda2-aa3e-4eb3-8856-a213460cc02d" in namespace "downward-api-769" to be "Succeeded or Failed"
Apr 26 15:09:23.012: INFO: Pod "downwardapi-volume-06a0fda2-aa3e-4eb3-8856-a213460cc02d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.93493ms
Apr 26 15:09:25.018: INFO: Pod "downwardapi-volume-06a0fda2-aa3e-4eb3-8856-a213460cc02d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015249531s
Apr 26 15:09:27.025: INFO: Pod "downwardapi-volume-06a0fda2-aa3e-4eb3-8856-a213460cc02d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021849224s
STEP: Saw pod success
Apr 26 15:09:27.025: INFO: Pod "downwardapi-volume-06a0fda2-aa3e-4eb3-8856-a213460cc02d" satisfied condition "Succeeded or Failed"
Apr 26 15:09:27.030: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod downwardapi-volume-06a0fda2-aa3e-4eb3-8856-a213460cc02d container client-container: <nil>
STEP: delete the pod
Apr 26 15:09:27.109: INFO: Waiting for pod downwardapi-volume-06a0fda2-aa3e-4eb3-8856-a213460cc02d to disappear
Apr 26 15:09:27.114: INFO: Pod downwardapi-volume-06a0fda2-aa3e-4eb3-8856-a213460cc02d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:09:27.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-769" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":303,"completed":257,"skipped":4136,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:09:27.136: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4540
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-4540
I0426 15:09:27.281197      21 runners.go:190] Created replication controller with name: externalname-service, namespace: services-4540, replica count: 2
I0426 15:09:30.331590      21 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 26 15:09:30.331: INFO: Creating new exec pod
Apr 26 15:09:35.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-4540 execpodvpcq7 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Apr 26 15:09:35.996: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Apr 26 15:09:35.996: INFO: stdout: ""
Apr 26 15:09:35.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-4540 execpodvpcq7 -- /bin/sh -x -c nc -zv -t -w 2 10.240.21.50 80'
Apr 26 15:09:36.660: INFO: stderr: "+ nc -zv -t -w 2 10.240.21.50 80\nConnection to 10.240.21.50 80 port [tcp/http] succeeded!\n"
Apr 26 15:09:36.660: INFO: stdout: ""
Apr 26 15:09:36.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-4540 execpodvpcq7 -- /bin/sh -x -c nc -zv -t -w 2 192.168.1.11 32696'
Apr 26 15:09:37.277: INFO: stderr: "+ nc -zv -t -w 2 192.168.1.11 32696\nConnection to 192.168.1.11 32696 port [tcp/32696] succeeded!\n"
Apr 26 15:09:37.277: INFO: stdout: ""
Apr 26 15:09:37.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-4540 execpodvpcq7 -- /bin/sh -x -c nc -zv -t -w 2 192.168.1.7 32696'
Apr 26 15:09:37.968: INFO: stderr: "+ nc -zv -t -w 2 192.168.1.7 32696\nConnection to 192.168.1.7 32696 port [tcp/32696] succeeded!\n"
Apr 26 15:09:37.968: INFO: stdout: ""
Apr 26 15:09:37.969: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:09:38.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4540" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:10.909 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":303,"completed":258,"skipped":4191,"failed":0}
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:09:38.047: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:163
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:09:38.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8971" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":303,"completed":259,"skipped":4191,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:09:38.156: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Apr 26 15:09:42.857: INFO: Successfully updated pod "labelsupdate41580806-f817-466c-94e5-b8c289ec7047"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:09:44.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2929" for this suite.

• [SLOW TEST:6.770 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":303,"completed":260,"skipped":4244,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:09:44.931: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Apr 26 15:09:45.006: INFO: Waiting up to 5m0s for pod "downwardapi-volume-77ad453e-5d66-4b54-9938-2f3c8496b958" in namespace "projected-7920" to be "Succeeded or Failed"
Apr 26 15:09:45.014: INFO: Pod "downwardapi-volume-77ad453e-5d66-4b54-9938-2f3c8496b958": Phase="Pending", Reason="", readiness=false. Elapsed: 7.994233ms
Apr 26 15:09:47.023: INFO: Pod "downwardapi-volume-77ad453e-5d66-4b54-9938-2f3c8496b958": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016706502s
Apr 26 15:09:49.043: INFO: Pod "downwardapi-volume-77ad453e-5d66-4b54-9938-2f3c8496b958": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037375852s
STEP: Saw pod success
Apr 26 15:09:49.043: INFO: Pod "downwardapi-volume-77ad453e-5d66-4b54-9938-2f3c8496b958" satisfied condition "Succeeded or Failed"
Apr 26 15:09:49.051: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod downwardapi-volume-77ad453e-5d66-4b54-9938-2f3c8496b958 container client-container: <nil>
STEP: delete the pod
Apr 26 15:09:49.105: INFO: Waiting for pod downwardapi-volume-77ad453e-5d66-4b54-9938-2f3c8496b958 to disappear
Apr 26 15:09:49.112: INFO: Pod downwardapi-volume-77ad453e-5d66-4b54-9938-2f3c8496b958 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:09:49.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7920" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":303,"completed":261,"skipped":4255,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:09:49.156: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 26 15:09:50.160: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 26 15:09:52.180: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755046590, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755046590, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755046590, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755046590, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 26 15:09:55.206: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:09:55.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-180" for this suite.
STEP: Destroying namespace "webhook-180-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.426 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":303,"completed":262,"skipped":4263,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:09:55.584: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Apr 26 15:09:55.640: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 26 15:09:55.657: INFO: Waiting for terminating namespaces to be deleted...
Apr 26 15:09:55.677: INFO: 
Logging pods the apiserver thinks is on node elastic-hugle-5f9d9b5555-8444w before test
Apr 26 15:09:55.693: INFO: canal-9hndk from kube-system started at 2021-04-26 13:19:15 +0000 UTC (2 container statuses recorded)
Apr 26 15:09:55.693: INFO: 	Container calico-node ready: true, restart count 0
Apr 26 15:09:55.693: INFO: 	Container kube-flannel ready: true, restart count 0
Apr 26 15:09:55.693: INFO: coredns-85c978995d-bmmb9 from kube-system started at 2021-04-26 13:19:44 +0000 UTC (1 container statuses recorded)
Apr 26 15:09:55.693: INFO: 	Container coredns ready: true, restart count 0
Apr 26 15:09:55.694: INFO: csi-cinder-nodeplugin-flatcar-nbzzj from kube-system started at 2021-04-26 13:19:37 +0000 UTC (2 container statuses recorded)
Apr 26 15:09:55.694: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Apr 26 15:09:55.694: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 26 15:09:55.694: INFO: kube-proxy-qvhqj from kube-system started at 2021-04-26 13:19:15 +0000 UTC (1 container statuses recorded)
Apr 26 15:09:55.694: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 26 15:09:55.694: INFO: logrotate-p98vn from kube-system started at 2021-04-26 13:19:37 +0000 UTC (1 container statuses recorded)
Apr 26 15:09:55.694: INFO: 	Container logrotate ready: true, restart count 0
Apr 26 15:09:55.694: INFO: node-local-dns-pfs8r from kube-system started at 2021-04-26 13:19:37 +0000 UTC (1 container statuses recorded)
Apr 26 15:09:55.695: INFO: 	Container node-cache ready: true, restart count 0
Apr 26 15:09:55.695: INFO: user-ssh-keys-agent-vck9l from kube-system started at 2021-04-26 13:19:15 +0000 UTC (1 container statuses recorded)
Apr 26 15:09:55.695: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Apr 26 15:09:55.695: INFO: dashboard-metrics-scraper-975c84c89-fc9kf from kubernetes-dashboard started at 2021-04-26 13:19:45 +0000 UTC (1 container statuses recorded)
Apr 26 15:09:55.695: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Apr 26 15:09:55.695: INFO: sonobuoy from sonobuoy started at 2021-04-26 13:33:20 +0000 UTC (1 container statuses recorded)
Apr 26 15:09:55.695: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 26 15:09:55.695: INFO: sonobuoy-systemd-logs-daemon-set-83692f989abe48af-6hhl5 from sonobuoy started at 2021-04-26 13:33:27 +0000 UTC (2 container statuses recorded)
Apr 26 15:09:55.696: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Apr 26 15:09:55.696: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 26 15:09:55.696: INFO: sample-webhook-deployment-cbccbf6bb-kmlmt from webhook-180 started at 2021-04-26 15:09:50 +0000 UTC (1 container statuses recorded)
Apr 26 15:09:55.696: INFO: 	Container sample-webhook ready: true, restart count 0
Apr 26 15:09:55.696: INFO: 
Logging pods the apiserver thinks is on node elastic-hugle-5f9d9b5555-mj2j9 before test
Apr 26 15:09:55.712: INFO: canal-vrzgr from kube-system started at 2021-04-26 13:19:17 +0000 UTC (2 container statuses recorded)
Apr 26 15:09:55.712: INFO: 	Container calico-node ready: true, restart count 0
Apr 26 15:09:55.712: INFO: 	Container kube-flannel ready: true, restart count 0
Apr 26 15:09:55.712: INFO: csi-cinder-controllerplugin-0 from kube-system started at 2021-04-26 13:19:45 +0000 UTC (5 container statuses recorded)
Apr 26 15:09:55.712: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Apr 26 15:09:55.712: INFO: 	Container csi-attacher ready: true, restart count 0
Apr 26 15:09:55.712: INFO: 	Container csi-provisioner ready: true, restart count 0
Apr 26 15:09:55.712: INFO: 	Container csi-resizer ready: true, restart count 0
Apr 26 15:09:55.712: INFO: 	Container csi-snapshotter ready: true, restart count 0
Apr 26 15:09:55.712: INFO: csi-cinder-nodeplugin-flatcar-dt2hz from kube-system started at 2021-04-26 13:19:37 +0000 UTC (2 container statuses recorded)
Apr 26 15:09:55.712: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Apr 26 15:09:55.712: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 26 15:09:55.712: INFO: kube-proxy-6gbkb from kube-system started at 2021-04-26 13:19:17 +0000 UTC (1 container statuses recorded)
Apr 26 15:09:55.712: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 26 15:09:55.712: INFO: logrotate-27s5t from kube-system started at 2021-04-26 13:19:37 +0000 UTC (1 container statuses recorded)
Apr 26 15:09:55.712: INFO: 	Container logrotate ready: true, restart count 0
Apr 26 15:09:55.712: INFO: node-local-dns-qtvvc from kube-system started at 2021-04-26 13:19:37 +0000 UTC (1 container statuses recorded)
Apr 26 15:09:55.712: INFO: 	Container node-cache ready: true, restart count 0
Apr 26 15:09:55.712: INFO: user-ssh-keys-agent-46lff from kube-system started at 2021-04-26 13:19:17 +0000 UTC (1 container statuses recorded)
Apr 26 15:09:55.712: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Apr 26 15:09:55.712: INFO: sonobuoy-systemd-logs-daemon-set-83692f989abe48af-ppqzq from sonobuoy started at 2021-04-26 13:33:27 +0000 UTC (2 container statuses recorded)
Apr 26 15:09:55.712: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Apr 26 15:09:55.712: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 26 15:09:55.712: INFO: 
Logging pods the apiserver thinks is on node elastic-hugle-5f9d9b5555-w8lg4 before test
Apr 26 15:09:55.729: INFO: canal-xcnxd from kube-system started at 2021-04-26 13:19:12 +0000 UTC (2 container statuses recorded)
Apr 26 15:09:55.729: INFO: 	Container calico-node ready: true, restart count 0
Apr 26 15:09:55.729: INFO: 	Container kube-flannel ready: true, restart count 1
Apr 26 15:09:55.729: INFO: coredns-85c978995d-gzh5v from kube-system started at 2021-04-26 13:19:44 +0000 UTC (1 container statuses recorded)
Apr 26 15:09:55.729: INFO: 	Container coredns ready: true, restart count 0
Apr 26 15:09:55.729: INFO: csi-cinder-nodeplugin-flatcar-gqw8g from kube-system started at 2021-04-26 13:19:42 +0000 UTC (2 container statuses recorded)
Apr 26 15:09:55.729: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Apr 26 15:09:55.729: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 26 15:09:55.729: INFO: kube-proxy-xkvxw from kube-system started at 2021-04-26 13:19:12 +0000 UTC (1 container statuses recorded)
Apr 26 15:09:55.729: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 26 15:09:55.729: INFO: logrotate-xhwmz from kube-system started at 2021-04-26 13:19:42 +0000 UTC (1 container statuses recorded)
Apr 26 15:09:55.729: INFO: 	Container logrotate ready: true, restart count 0
Apr 26 15:09:55.729: INFO: node-local-dns-4jkgn from kube-system started at 2021-04-26 13:19:42 +0000 UTC (1 container statuses recorded)
Apr 26 15:09:55.729: INFO: 	Container node-cache ready: true, restart count 0
Apr 26 15:09:55.729: INFO: openvpn-client-6c9dd998bc-hzr48 from kube-system started at 2021-04-26 13:19:45 +0000 UTC (2 container statuses recorded)
Apr 26 15:09:55.729: INFO: 	Container dnat-controller ready: true, restart count 0
Apr 26 15:09:55.729: INFO: 	Container openvpn-client ready: true, restart count 0
Apr 26 15:09:55.729: INFO: user-ssh-keys-agent-hr99m from kube-system started at 2021-04-26 13:19:12 +0000 UTC (1 container statuses recorded)
Apr 26 15:09:55.729: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Apr 26 15:09:55.729: INFO: dashboard-metrics-scraper-975c84c89-drmjh from kubernetes-dashboard started at 2021-04-26 13:19:45 +0000 UTC (1 container statuses recorded)
Apr 26 15:09:55.729: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Apr 26 15:09:55.729: INFO: labelsupdate41580806-f817-466c-94e5-b8c289ec7047 from projected-2929 started at 2021-04-26 15:09:38 +0000 UTC (1 container statuses recorded)
Apr 26 15:09:55.729: INFO: 	Container client-container ready: false, restart count 0
Apr 26 15:09:55.729: INFO: sonobuoy-e2e-job-c567c19005014a0f from sonobuoy started at 2021-04-26 13:33:27 +0000 UTC (2 container statuses recorded)
Apr 26 15:09:55.730: INFO: 	Container e2e ready: true, restart count 0
Apr 26 15:09:55.730: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 15:09:55.730: INFO: sonobuoy-systemd-logs-daemon-set-83692f989abe48af-rg644 from sonobuoy started at 2021-04-26 13:33:27 +0000 UTC (2 container statuses recorded)
Apr 26 15:09:55.730: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Apr 26 15:09:55.730: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-4d9660e8-6268-4401-99fd-1a60c6e1642d 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-4d9660e8-6268-4401-99fd-1a60c6e1642d off the node elastic-hugle-5f9d9b5555-8444w
STEP: verifying the node doesn't have the label kubernetes.io/e2e-4d9660e8-6268-4401-99fd-1a60c6e1642d
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:15:03.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5925" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:308.388 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":303,"completed":263,"skipped":4264,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:15:03.973: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Apr 26 15:15:04.083: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ffdd065a-6c23-41a1-b00e-a77859cf5469" in namespace "projected-4145" to be "Succeeded or Failed"
Apr 26 15:15:04.091: INFO: Pod "downwardapi-volume-ffdd065a-6c23-41a1-b00e-a77859cf5469": Phase="Pending", Reason="", readiness=false. Elapsed: 8.621246ms
Apr 26 15:15:06.103: INFO: Pod "downwardapi-volume-ffdd065a-6c23-41a1-b00e-a77859cf5469": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019908651s
Apr 26 15:15:08.110: INFO: Pod "downwardapi-volume-ffdd065a-6c23-41a1-b00e-a77859cf5469": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02748574s
STEP: Saw pod success
Apr 26 15:15:08.110: INFO: Pod "downwardapi-volume-ffdd065a-6c23-41a1-b00e-a77859cf5469" satisfied condition "Succeeded or Failed"
Apr 26 15:15:08.118: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod downwardapi-volume-ffdd065a-6c23-41a1-b00e-a77859cf5469 container client-container: <nil>
STEP: delete the pod
Apr 26 15:15:08.172: INFO: Waiting for pod downwardapi-volume-ffdd065a-6c23-41a1-b00e-a77859cf5469 to disappear
Apr 26 15:15:08.177: INFO: Pod downwardapi-volume-ffdd065a-6c23-41a1-b00e-a77859cf5469 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:15:08.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4145" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":303,"completed":264,"skipped":4271,"failed":0}
SSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:15:08.197: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Apr 26 15:15:08.284: INFO: The status of Pod test-webserver-8b471c9f-f913-4fb9-b19b-975e05d1a33c is Pending, waiting for it to be Running (with Ready = true)
Apr 26 15:15:10.290: INFO: The status of Pod test-webserver-8b471c9f-f913-4fb9-b19b-975e05d1a33c is Pending, waiting for it to be Running (with Ready = true)
Apr 26 15:15:12.295: INFO: The status of Pod test-webserver-8b471c9f-f913-4fb9-b19b-975e05d1a33c is Running (Ready = false)
Apr 26 15:15:14.290: INFO: The status of Pod test-webserver-8b471c9f-f913-4fb9-b19b-975e05d1a33c is Running (Ready = false)
Apr 26 15:15:16.290: INFO: The status of Pod test-webserver-8b471c9f-f913-4fb9-b19b-975e05d1a33c is Running (Ready = false)
Apr 26 15:15:18.292: INFO: The status of Pod test-webserver-8b471c9f-f913-4fb9-b19b-975e05d1a33c is Running (Ready = false)
Apr 26 15:15:20.291: INFO: The status of Pod test-webserver-8b471c9f-f913-4fb9-b19b-975e05d1a33c is Running (Ready = false)
Apr 26 15:15:22.291: INFO: The status of Pod test-webserver-8b471c9f-f913-4fb9-b19b-975e05d1a33c is Running (Ready = false)
Apr 26 15:15:24.293: INFO: The status of Pod test-webserver-8b471c9f-f913-4fb9-b19b-975e05d1a33c is Running (Ready = false)
Apr 26 15:15:26.290: INFO: The status of Pod test-webserver-8b471c9f-f913-4fb9-b19b-975e05d1a33c is Running (Ready = false)
Apr 26 15:15:28.293: INFO: The status of Pod test-webserver-8b471c9f-f913-4fb9-b19b-975e05d1a33c is Running (Ready = false)
Apr 26 15:15:30.291: INFO: The status of Pod test-webserver-8b471c9f-f913-4fb9-b19b-975e05d1a33c is Running (Ready = true)
Apr 26 15:15:30.296: INFO: Container started at 2021-04-26 15:15:10 +0000 UTC, pod became ready at 2021-04-26 15:15:29 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:15:30.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7941" for this suite.

• [SLOW TEST:22.119 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":303,"completed":265,"skipped":4277,"failed":0}
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:15:30.317: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Apr 26 15:15:30.384: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 26 15:15:30.401: INFO: Waiting for terminating namespaces to be deleted...
Apr 26 15:15:30.406: INFO: 
Logging pods the apiserver thinks is on node elastic-hugle-5f9d9b5555-8444w before test
Apr 26 15:15:30.423: INFO: test-webserver-8b471c9f-f913-4fb9-b19b-975e05d1a33c from container-probe-7941 started at 2021-04-26 15:15:08 +0000 UTC (1 container statuses recorded)
Apr 26 15:15:30.423: INFO: 	Container test-webserver ready: true, restart count 0
Apr 26 15:15:30.423: INFO: canal-9hndk from kube-system started at 2021-04-26 13:19:15 +0000 UTC (2 container statuses recorded)
Apr 26 15:15:30.423: INFO: 	Container calico-node ready: true, restart count 0
Apr 26 15:15:30.423: INFO: 	Container kube-flannel ready: true, restart count 0
Apr 26 15:15:30.423: INFO: coredns-85c978995d-bmmb9 from kube-system started at 2021-04-26 13:19:44 +0000 UTC (1 container statuses recorded)
Apr 26 15:15:30.423: INFO: 	Container coredns ready: true, restart count 0
Apr 26 15:15:30.423: INFO: csi-cinder-nodeplugin-flatcar-nbzzj from kube-system started at 2021-04-26 13:19:37 +0000 UTC (2 container statuses recorded)
Apr 26 15:15:30.423: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Apr 26 15:15:30.423: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 26 15:15:30.423: INFO: kube-proxy-qvhqj from kube-system started at 2021-04-26 13:19:15 +0000 UTC (1 container statuses recorded)
Apr 26 15:15:30.423: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 26 15:15:30.423: INFO: logrotate-p98vn from kube-system started at 2021-04-26 13:19:37 +0000 UTC (1 container statuses recorded)
Apr 26 15:15:30.423: INFO: 	Container logrotate ready: true, restart count 0
Apr 26 15:15:30.423: INFO: node-local-dns-pfs8r from kube-system started at 2021-04-26 13:19:37 +0000 UTC (1 container statuses recorded)
Apr 26 15:15:30.423: INFO: 	Container node-cache ready: true, restart count 0
Apr 26 15:15:30.423: INFO: user-ssh-keys-agent-vck9l from kube-system started at 2021-04-26 13:19:15 +0000 UTC (1 container statuses recorded)
Apr 26 15:15:30.423: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Apr 26 15:15:30.423: INFO: dashboard-metrics-scraper-975c84c89-fc9kf from kubernetes-dashboard started at 2021-04-26 13:19:45 +0000 UTC (1 container statuses recorded)
Apr 26 15:15:30.423: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Apr 26 15:15:30.423: INFO: sonobuoy from sonobuoy started at 2021-04-26 13:33:20 +0000 UTC (1 container statuses recorded)
Apr 26 15:15:30.423: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 26 15:15:30.423: INFO: sonobuoy-systemd-logs-daemon-set-83692f989abe48af-6hhl5 from sonobuoy started at 2021-04-26 13:33:27 +0000 UTC (2 container statuses recorded)
Apr 26 15:15:30.423: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Apr 26 15:15:30.423: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 26 15:15:30.423: INFO: 
Logging pods the apiserver thinks is on node elastic-hugle-5f9d9b5555-mj2j9 before test
Apr 26 15:15:30.434: INFO: canal-vrzgr from kube-system started at 2021-04-26 13:19:17 +0000 UTC (2 container statuses recorded)
Apr 26 15:15:30.434: INFO: 	Container calico-node ready: true, restart count 0
Apr 26 15:15:30.434: INFO: 	Container kube-flannel ready: true, restart count 0
Apr 26 15:15:30.434: INFO: csi-cinder-controllerplugin-0 from kube-system started at 2021-04-26 13:19:45 +0000 UTC (5 container statuses recorded)
Apr 26 15:15:30.434: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Apr 26 15:15:30.435: INFO: 	Container csi-attacher ready: true, restart count 0
Apr 26 15:15:30.435: INFO: 	Container csi-provisioner ready: true, restart count 0
Apr 26 15:15:30.435: INFO: 	Container csi-resizer ready: true, restart count 0
Apr 26 15:15:30.435: INFO: 	Container csi-snapshotter ready: true, restart count 0
Apr 26 15:15:30.435: INFO: csi-cinder-nodeplugin-flatcar-dt2hz from kube-system started at 2021-04-26 13:19:37 +0000 UTC (2 container statuses recorded)
Apr 26 15:15:30.435: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Apr 26 15:15:30.435: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 26 15:15:30.435: INFO: kube-proxy-6gbkb from kube-system started at 2021-04-26 13:19:17 +0000 UTC (1 container statuses recorded)
Apr 26 15:15:30.435: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 26 15:15:30.435: INFO: logrotate-27s5t from kube-system started at 2021-04-26 13:19:37 +0000 UTC (1 container statuses recorded)
Apr 26 15:15:30.435: INFO: 	Container logrotate ready: true, restart count 0
Apr 26 15:15:30.435: INFO: node-local-dns-qtvvc from kube-system started at 2021-04-26 13:19:37 +0000 UTC (1 container statuses recorded)
Apr 26 15:15:30.435: INFO: 	Container node-cache ready: true, restart count 0
Apr 26 15:15:30.435: INFO: user-ssh-keys-agent-46lff from kube-system started at 2021-04-26 13:19:17 +0000 UTC (1 container statuses recorded)
Apr 26 15:15:30.435: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Apr 26 15:15:30.435: INFO: sonobuoy-systemd-logs-daemon-set-83692f989abe48af-ppqzq from sonobuoy started at 2021-04-26 13:33:27 +0000 UTC (2 container statuses recorded)
Apr 26 15:15:30.435: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Apr 26 15:15:30.435: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 26 15:15:30.435: INFO: 
Logging pods the apiserver thinks is on node elastic-hugle-5f9d9b5555-w8lg4 before test
Apr 26 15:15:30.449: INFO: canal-xcnxd from kube-system started at 2021-04-26 13:19:12 +0000 UTC (2 container statuses recorded)
Apr 26 15:15:30.449: INFO: 	Container calico-node ready: true, restart count 0
Apr 26 15:15:30.449: INFO: 	Container kube-flannel ready: true, restart count 1
Apr 26 15:15:30.449: INFO: coredns-85c978995d-gzh5v from kube-system started at 2021-04-26 13:19:44 +0000 UTC (1 container statuses recorded)
Apr 26 15:15:30.449: INFO: 	Container coredns ready: true, restart count 0
Apr 26 15:15:30.449: INFO: csi-cinder-nodeplugin-flatcar-gqw8g from kube-system started at 2021-04-26 13:19:42 +0000 UTC (2 container statuses recorded)
Apr 26 15:15:30.449: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Apr 26 15:15:30.449: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 26 15:15:30.449: INFO: kube-proxy-xkvxw from kube-system started at 2021-04-26 13:19:12 +0000 UTC (1 container statuses recorded)
Apr 26 15:15:30.449: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 26 15:15:30.449: INFO: logrotate-xhwmz from kube-system started at 2021-04-26 13:19:42 +0000 UTC (1 container statuses recorded)
Apr 26 15:15:30.449: INFO: 	Container logrotate ready: true, restart count 0
Apr 26 15:15:30.449: INFO: node-local-dns-4jkgn from kube-system started at 2021-04-26 13:19:42 +0000 UTC (1 container statuses recorded)
Apr 26 15:15:30.449: INFO: 	Container node-cache ready: true, restart count 0
Apr 26 15:15:30.449: INFO: openvpn-client-6c9dd998bc-hzr48 from kube-system started at 2021-04-26 13:19:45 +0000 UTC (2 container statuses recorded)
Apr 26 15:15:30.449: INFO: 	Container dnat-controller ready: true, restart count 0
Apr 26 15:15:30.449: INFO: 	Container openvpn-client ready: true, restart count 0
Apr 26 15:15:30.449: INFO: user-ssh-keys-agent-hr99m from kube-system started at 2021-04-26 13:19:12 +0000 UTC (1 container statuses recorded)
Apr 26 15:15:30.449: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Apr 26 15:15:30.449: INFO: dashboard-metrics-scraper-975c84c89-drmjh from kubernetes-dashboard started at 2021-04-26 13:19:45 +0000 UTC (1 container statuses recorded)
Apr 26 15:15:30.449: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Apr 26 15:15:30.449: INFO: sonobuoy-e2e-job-c567c19005014a0f from sonobuoy started at 2021-04-26 13:33:27 +0000 UTC (2 container statuses recorded)
Apr 26 15:15:30.450: INFO: 	Container e2e ready: true, restart count 0
Apr 26 15:15:30.450: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 26 15:15:30.450: INFO: sonobuoy-systemd-logs-daemon-set-83692f989abe48af-rg644 from sonobuoy started at 2021-04-26 13:33:27 +0000 UTC (2 container statuses recorded)
Apr 26 15:15:30.450: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Apr 26 15:15:30.450: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-6e5e642f-3345-4e1f-bfcd-b18e115d04f1 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-6e5e642f-3345-4e1f-bfcd-b18e115d04f1 off the node elastic-hugle-5f9d9b5555-8444w
STEP: verifying the node doesn't have the label kubernetes.io/e2e-6e5e642f-3345-4e1f-bfcd-b18e115d04f1
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:15:38.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4752" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:8.371 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":303,"completed":266,"skipped":4284,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:15:38.694: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Apr 26 15:15:38.820: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aa89d1d4-12a2-4bc6-a2df-25e7ddfe68fc" in namespace "downward-api-3554" to be "Succeeded or Failed"
Apr 26 15:15:38.826: INFO: Pod "downwardapi-volume-aa89d1d4-12a2-4bc6-a2df-25e7ddfe68fc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.445549ms
Apr 26 15:15:40.834: INFO: Pod "downwardapi-volume-aa89d1d4-12a2-4bc6-a2df-25e7ddfe68fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01418528s
Apr 26 15:15:42.841: INFO: Pod "downwardapi-volume-aa89d1d4-12a2-4bc6-a2df-25e7ddfe68fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021104486s
STEP: Saw pod success
Apr 26 15:15:42.841: INFO: Pod "downwardapi-volume-aa89d1d4-12a2-4bc6-a2df-25e7ddfe68fc" satisfied condition "Succeeded or Failed"
Apr 26 15:15:42.847: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-w8lg4 pod downwardapi-volume-aa89d1d4-12a2-4bc6-a2df-25e7ddfe68fc container client-container: <nil>
STEP: delete the pod
Apr 26 15:15:42.925: INFO: Waiting for pod downwardapi-volume-aa89d1d4-12a2-4bc6-a2df-25e7ddfe68fc to disappear
Apr 26 15:15:42.930: INFO: Pod downwardapi-volume-aa89d1d4-12a2-4bc6-a2df-25e7ddfe68fc no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:15:42.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3554" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":303,"completed":267,"skipped":4302,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:15:42.963: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Apr 26 15:15:43.020: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
Apr 26 15:15:46.718: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:16:01.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5976" for this suite.

• [SLOW TEST:18.420 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":303,"completed":268,"skipped":4362,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:16:01.384: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Apr 26 15:16:09.549: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2751 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 26 15:16:09.549: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
Apr 26 15:16:10.065: INFO: Exec stderr: ""
Apr 26 15:16:10.066: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2751 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 26 15:16:10.066: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
Apr 26 15:16:10.608: INFO: Exec stderr: ""
Apr 26 15:16:10.609: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2751 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 26 15:16:10.609: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
Apr 26 15:16:11.157: INFO: Exec stderr: ""
Apr 26 15:16:11.157: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2751 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 26 15:16:11.157: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
Apr 26 15:16:11.760: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Apr 26 15:16:11.760: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2751 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 26 15:16:11.760: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
Apr 26 15:16:12.260: INFO: Exec stderr: ""
Apr 26 15:16:12.260: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2751 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 26 15:16:12.260: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
Apr 26 15:16:12.862: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Apr 26 15:16:12.862: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2751 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 26 15:16:12.862: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
Apr 26 15:16:13.433: INFO: Exec stderr: ""
Apr 26 15:16:13.433: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2751 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 26 15:16:13.433: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
Apr 26 15:16:13.945: INFO: Exec stderr: ""
Apr 26 15:16:13.945: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2751 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 26 15:16:13.945: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
Apr 26 15:16:14.508: INFO: Exec stderr: ""
Apr 26 15:16:14.508: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2751 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 26 15:16:14.508: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
Apr 26 15:16:15.056: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:16:15.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-2751" for this suite.

• [SLOW TEST:13.697 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":269,"skipped":4395,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:16:15.082: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Apr 26 15:16:15.179: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bfe6095e-9625-418d-a0be-fcc5bdfea83e" in namespace "downward-api-692" to be "Succeeded or Failed"
Apr 26 15:16:15.188: INFO: Pod "downwardapi-volume-bfe6095e-9625-418d-a0be-fcc5bdfea83e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.927996ms
Apr 26 15:16:17.195: INFO: Pod "downwardapi-volume-bfe6095e-9625-418d-a0be-fcc5bdfea83e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015782182s
STEP: Saw pod success
Apr 26 15:16:17.195: INFO: Pod "downwardapi-volume-bfe6095e-9625-418d-a0be-fcc5bdfea83e" satisfied condition "Succeeded or Failed"
Apr 26 15:16:17.200: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-mj2j9 pod downwardapi-volume-bfe6095e-9625-418d-a0be-fcc5bdfea83e container client-container: <nil>
STEP: delete the pod
Apr 26 15:16:17.281: INFO: Waiting for pod downwardapi-volume-bfe6095e-9625-418d-a0be-fcc5bdfea83e to disappear
Apr 26 15:16:17.286: INFO: Pod downwardapi-volume-bfe6095e-9625-418d-a0be-fcc5bdfea83e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:16:17.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-692" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":270,"skipped":4397,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:16:17.306: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Apr 26 15:16:17.405: INFO: Waiting up to 5m0s for pod "pod-850d0863-5166-4d58-a055-ae503fc2a38f" in namespace "emptydir-1512" to be "Succeeded or Failed"
Apr 26 15:16:17.424: INFO: Pod "pod-850d0863-5166-4d58-a055-ae503fc2a38f": Phase="Pending", Reason="", readiness=false. Elapsed: 19.464968ms
Apr 26 15:16:19.432: INFO: Pod "pod-850d0863-5166-4d58-a055-ae503fc2a38f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026995933s
STEP: Saw pod success
Apr 26 15:16:19.432: INFO: Pod "pod-850d0863-5166-4d58-a055-ae503fc2a38f" satisfied condition "Succeeded or Failed"
Apr 26 15:16:19.438: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-mj2j9 pod pod-850d0863-5166-4d58-a055-ae503fc2a38f container test-container: <nil>
STEP: delete the pod
Apr 26 15:16:19.508: INFO: Waiting for pod pod-850d0863-5166-4d58-a055-ae503fc2a38f to disappear
Apr 26 15:16:19.515: INFO: Pod pod-850d0863-5166-4d58-a055-ae503fc2a38f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:16:19.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1512" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":271,"skipped":4410,"failed":0}
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:16:19.546: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0426 15:16:20.790376      21 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0426 15:16:20.790789      21 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0426 15:16:20.790988      21 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Apr 26 15:16:20.791: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:16:20.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5032" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":303,"completed":272,"skipped":4412,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:16:20.811: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-6156
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 26 15:16:20.870: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr 26 15:16:20.950: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr 26 15:16:22.957: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr 26 15:16:24.957: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 26 15:16:26.956: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 26 15:16:28.958: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 26 15:16:30.959: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 26 15:16:32.957: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 26 15:16:34.957: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 26 15:16:36.958: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 26 15:16:38.957: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 26 15:16:40.957: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 26 15:16:42.958: INFO: The status of Pod netserver-0 is Running (Ready = true)
Apr 26 15:16:42.969: INFO: The status of Pod netserver-1 is Running (Ready = true)
Apr 26 15:16:42.979: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Apr 26 15:16:47.030: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.26:8080/dial?request=hostname&protocol=udp&host=172.25.1.25&port=8081&tries=1'] Namespace:pod-network-test-6156 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 26 15:16:47.030: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
Apr 26 15:16:47.558: INFO: Waiting for responses: map[]
Apr 26 15:16:47.567: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.26:8080/dial?request=hostname&protocol=udp&host=172.25.2.55&port=8081&tries=1'] Namespace:pod-network-test-6156 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 26 15:16:47.567: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
Apr 26 15:16:48.098: INFO: Waiting for responses: map[]
Apr 26 15:16:48.104: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.26:8080/dial?request=hostname&protocol=udp&host=172.25.0.112&port=8081&tries=1'] Namespace:pod-network-test-6156 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 26 15:16:48.104: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
Apr 26 15:16:48.614: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:16:48.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6156" for this suite.

• [SLOW TEST:27.826 seconds]
[sig-network] Networking
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":303,"completed":273,"skipped":4421,"failed":0}
SSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:16:48.637: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8543.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-8543.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8543.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8543.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-8543.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8543.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 26 15:17:03.465: INFO: DNS probes using dns-8543/dns-test-e5534d24-be7d-4035-b333-744aa7a2cf4f succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:17:03.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8543" for this suite.

• [SLOW TEST:14.938 seconds]
[sig-network] DNS
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":303,"completed":274,"skipped":4428,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:17:03.579: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-2038
STEP: creating service affinity-nodeport in namespace services-2038
STEP: creating replication controller affinity-nodeport in namespace services-2038
I0426 15:17:03.716933      21 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-2038, replica count: 3
I0426 15:17:06.767306      21 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 26 15:17:06.789: INFO: Creating new exec pod
Apr 26 15:17:11.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-2038 execpod-affinity5cbc6 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
Apr 26 15:17:12.461: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Apr 26 15:17:12.461: INFO: stdout: ""
Apr 26 15:17:12.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-2038 execpod-affinity5cbc6 -- /bin/sh -x -c nc -zv -t -w 2 10.240.26.114 80'
Apr 26 15:17:13.131: INFO: stderr: "+ nc -zv -t -w 2 10.240.26.114 80\nConnection to 10.240.26.114 80 port [tcp/http] succeeded!\n"
Apr 26 15:17:13.131: INFO: stdout: ""
Apr 26 15:17:13.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-2038 execpod-affinity5cbc6 -- /bin/sh -x -c nc -zv -t -w 2 192.168.1.7 32602'
Apr 26 15:17:13.760: INFO: stderr: "+ nc -zv -t -w 2 192.168.1.7 32602\nConnection to 192.168.1.7 32602 port [tcp/32602] succeeded!\n"
Apr 26 15:17:13.760: INFO: stdout: ""
Apr 26 15:17:13.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-2038 execpod-affinity5cbc6 -- /bin/sh -x -c nc -zv -t -w 2 192.168.1.11 32602'
Apr 26 15:17:14.360: INFO: stderr: "+ nc -zv -t -w 2 192.168.1.11 32602\nConnection to 192.168.1.11 32602 port [tcp/32602] succeeded!\n"
Apr 26 15:17:14.360: INFO: stdout: ""
Apr 26 15:17:14.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-2038 execpod-affinity5cbc6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.1.11:32602/ ; done'
Apr 26 15:17:15.035: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32602/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32602/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32602/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32602/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32602/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32602/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32602/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32602/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32602/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32602/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32602/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32602/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32602/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32602/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32602/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:32602/\n"
Apr 26 15:17:15.035: INFO: stdout: "\naffinity-nodeport-j9hl9\naffinity-nodeport-j9hl9\naffinity-nodeport-j9hl9\naffinity-nodeport-j9hl9\naffinity-nodeport-j9hl9\naffinity-nodeport-j9hl9\naffinity-nodeport-j9hl9\naffinity-nodeport-j9hl9\naffinity-nodeport-j9hl9\naffinity-nodeport-j9hl9\naffinity-nodeport-j9hl9\naffinity-nodeport-j9hl9\naffinity-nodeport-j9hl9\naffinity-nodeport-j9hl9\naffinity-nodeport-j9hl9\naffinity-nodeport-j9hl9"
Apr 26 15:17:15.035: INFO: Received response from host: affinity-nodeport-j9hl9
Apr 26 15:17:15.035: INFO: Received response from host: affinity-nodeport-j9hl9
Apr 26 15:17:15.035: INFO: Received response from host: affinity-nodeport-j9hl9
Apr 26 15:17:15.035: INFO: Received response from host: affinity-nodeport-j9hl9
Apr 26 15:17:15.035: INFO: Received response from host: affinity-nodeport-j9hl9
Apr 26 15:17:15.035: INFO: Received response from host: affinity-nodeport-j9hl9
Apr 26 15:17:15.035: INFO: Received response from host: affinity-nodeport-j9hl9
Apr 26 15:17:15.035: INFO: Received response from host: affinity-nodeport-j9hl9
Apr 26 15:17:15.035: INFO: Received response from host: affinity-nodeport-j9hl9
Apr 26 15:17:15.035: INFO: Received response from host: affinity-nodeport-j9hl9
Apr 26 15:17:15.035: INFO: Received response from host: affinity-nodeport-j9hl9
Apr 26 15:17:15.035: INFO: Received response from host: affinity-nodeport-j9hl9
Apr 26 15:17:15.035: INFO: Received response from host: affinity-nodeport-j9hl9
Apr 26 15:17:15.035: INFO: Received response from host: affinity-nodeport-j9hl9
Apr 26 15:17:15.035: INFO: Received response from host: affinity-nodeport-j9hl9
Apr 26 15:17:15.035: INFO: Received response from host: affinity-nodeport-j9hl9
Apr 26 15:17:15.035: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-2038, will wait for the garbage collector to delete the pods
Apr 26 15:17:15.139: INFO: Deleting ReplicationController affinity-nodeport took: 20.976227ms
Apr 26 15:17:15.740: INFO: Terminating ReplicationController affinity-nodeport pods took: 600.844451ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:17:21.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2038" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:18.327 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":303,"completed":275,"skipped":4439,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:17:21.906: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Apr 26 15:17:22.025: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"213a6981-1a9d-482d-870a-8ab478de3748", Controller:(*bool)(0xc0048408ba), BlockOwnerDeletion:(*bool)(0xc0048408bb)}}
Apr 26 15:17:22.037: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"6b718fce-87cc-499f-a019-7907f3ebda07", Controller:(*bool)(0xc0047f29b6), BlockOwnerDeletion:(*bool)(0xc0047f29b7)}}
Apr 26 15:17:22.052: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"36b8295e-f0f1-4141-8e4c-bccf2fb0614c", Controller:(*bool)(0xc004840bc6), BlockOwnerDeletion:(*bool)(0xc004840bc7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:17:27.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5626" for this suite.

• [SLOW TEST:5.222 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":303,"completed":276,"skipped":4445,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:17:27.130: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 26 15:17:27.891: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 26 15:17:29.910: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755047047, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755047047, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755047047, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755047047, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 26 15:17:32.933: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:17:33.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7293" for this suite.
STEP: Destroying namespace "webhook-7293-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.319 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":303,"completed":277,"skipped":4461,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:17:33.453: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7688.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7688.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 26 15:17:38.327: INFO: DNS probes using dns-7688/dns-test-bd90bf27-703c-4735-80b9-780dd88d94c7 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:17:38.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7688" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":303,"completed":278,"skipped":4470,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:17:38.382: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Apr 26 15:17:38.510: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3228 /api/v1/namespaces/watch-3228/configmaps/e2e-watch-test-resource-version c462f4c1-0a2c-4ad3-bcbc-59d42843a9b9 52509 0 2021-04-26 15:17:38 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-04-26 15:17:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 26 15:17:38.510: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3228 /api/v1/namespaces/watch-3228/configmaps/e2e-watch-test-resource-version c462f4c1-0a2c-4ad3-bcbc-59d42843a9b9 52512 0 2021-04-26 15:17:38 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-04-26 15:17:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:17:38.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3228" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":303,"completed":279,"skipped":4477,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:17:38.537: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-d1e697b9-af1c-4cf6-bc13-b08290cfc0f2
STEP: Creating a pod to test consume secrets
Apr 26 15:17:38.678: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c14db8ac-961c-4e41-bb6c-3c68ecb41cdc" in namespace "projected-7421" to be "Succeeded or Failed"
Apr 26 15:17:38.689: INFO: Pod "pod-projected-secrets-c14db8ac-961c-4e41-bb6c-3c68ecb41cdc": Phase="Pending", Reason="", readiness=false. Elapsed: 11.135585ms
Apr 26 15:17:40.696: INFO: Pod "pod-projected-secrets-c14db8ac-961c-4e41-bb6c-3c68ecb41cdc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018287197s
Apr 26 15:17:42.704: INFO: Pod "pod-projected-secrets-c14db8ac-961c-4e41-bb6c-3c68ecb41cdc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02582995s
STEP: Saw pod success
Apr 26 15:17:42.704: INFO: Pod "pod-projected-secrets-c14db8ac-961c-4e41-bb6c-3c68ecb41cdc" satisfied condition "Succeeded or Failed"
Apr 26 15:17:42.709: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod pod-projected-secrets-c14db8ac-961c-4e41-bb6c-3c68ecb41cdc container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 26 15:17:42.752: INFO: Waiting for pod pod-projected-secrets-c14db8ac-961c-4e41-bb6c-3c68ecb41cdc to disappear
Apr 26 15:17:42.757: INFO: Pod pod-projected-secrets-c14db8ac-961c-4e41-bb6c-3c68ecb41cdc no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:17:42.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7421" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":280,"skipped":4492,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:17:42.779: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Apr 26 15:17:42.849: INFO: Waiting up to 5m0s for pod "pod-07a907a4-c2db-43a9-a0eb-620e65dff25a" in namespace "emptydir-1937" to be "Succeeded or Failed"
Apr 26 15:17:42.862: INFO: Pod "pod-07a907a4-c2db-43a9-a0eb-620e65dff25a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.220826ms
Apr 26 15:17:44.869: INFO: Pod "pod-07a907a4-c2db-43a9-a0eb-620e65dff25a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01995463s
Apr 26 15:17:46.876: INFO: Pod "pod-07a907a4-c2db-43a9-a0eb-620e65dff25a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026637752s
STEP: Saw pod success
Apr 26 15:17:46.876: INFO: Pod "pod-07a907a4-c2db-43a9-a0eb-620e65dff25a" satisfied condition "Succeeded or Failed"
Apr 26 15:17:46.881: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod pod-07a907a4-c2db-43a9-a0eb-620e65dff25a container test-container: <nil>
STEP: delete the pod
Apr 26 15:17:46.926: INFO: Waiting for pod pod-07a907a4-c2db-43a9-a0eb-620e65dff25a to disappear
Apr 26 15:17:46.932: INFO: Pod pod-07a907a4-c2db-43a9-a0eb-620e65dff25a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:17:46.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1937" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":281,"skipped":4492,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:17:46.954: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Apr 26 15:17:47.077: INFO: Number of nodes with available pods: 0
Apr 26 15:17:47.077: INFO: Node elastic-hugle-5f9d9b5555-8444w is running more than one daemon pod
Apr 26 15:17:48.132: INFO: Number of nodes with available pods: 0
Apr 26 15:17:48.132: INFO: Node elastic-hugle-5f9d9b5555-8444w is running more than one daemon pod
Apr 26 15:17:49.113: INFO: Number of nodes with available pods: 0
Apr 26 15:17:49.113: INFO: Node elastic-hugle-5f9d9b5555-8444w is running more than one daemon pod
Apr 26 15:17:50.097: INFO: Number of nodes with available pods: 3
Apr 26 15:17:50.097: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Apr 26 15:17:50.161: INFO: Number of nodes with available pods: 2
Apr 26 15:17:50.161: INFO: Node elastic-hugle-5f9d9b5555-w8lg4 is running more than one daemon pod
Apr 26 15:17:51.177: INFO: Number of nodes with available pods: 2
Apr 26 15:17:51.177: INFO: Node elastic-hugle-5f9d9b5555-w8lg4 is running more than one daemon pod
Apr 26 15:17:52.198: INFO: Number of nodes with available pods: 2
Apr 26 15:17:52.198: INFO: Node elastic-hugle-5f9d9b5555-w8lg4 is running more than one daemon pod
Apr 26 15:17:53.176: INFO: Number of nodes with available pods: 2
Apr 26 15:17:53.177: INFO: Node elastic-hugle-5f9d9b5555-w8lg4 is running more than one daemon pod
Apr 26 15:17:54.177: INFO: Number of nodes with available pods: 2
Apr 26 15:17:54.177: INFO: Node elastic-hugle-5f9d9b5555-w8lg4 is running more than one daemon pod
Apr 26 15:17:55.176: INFO: Number of nodes with available pods: 2
Apr 26 15:17:55.176: INFO: Node elastic-hugle-5f9d9b5555-w8lg4 is running more than one daemon pod
Apr 26 15:17:56.179: INFO: Number of nodes with available pods: 2
Apr 26 15:17:56.179: INFO: Node elastic-hugle-5f9d9b5555-w8lg4 is running more than one daemon pod
Apr 26 15:17:57.178: INFO: Number of nodes with available pods: 3
Apr 26 15:17:57.178: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2418, will wait for the garbage collector to delete the pods
Apr 26 15:17:57.254: INFO: Deleting DaemonSet.extensions daemon-set took: 13.795006ms
Apr 26 15:17:57.854: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.256126ms
Apr 26 15:18:07.460: INFO: Number of nodes with available pods: 0
Apr 26 15:18:07.460: INFO: Number of running nodes: 0, number of available pods: 0
Apr 26 15:18:07.468: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2418/daemonsets","resourceVersion":"52828"},"items":null}

Apr 26 15:18:07.473: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2418/pods","resourceVersion":"52829"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:18:07.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2418" for this suite.

• [SLOW TEST:20.567 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":303,"completed":282,"skipped":4506,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:18:07.527: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Apr 26 15:18:07.616: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7ee1d29f-89e8-4912-87a9-3db2e7793d60" in namespace "downward-api-9489" to be "Succeeded or Failed"
Apr 26 15:18:07.626: INFO: Pod "downwardapi-volume-7ee1d29f-89e8-4912-87a9-3db2e7793d60": Phase="Pending", Reason="", readiness=false. Elapsed: 9.917989ms
Apr 26 15:18:09.634: INFO: Pod "downwardapi-volume-7ee1d29f-89e8-4912-87a9-3db2e7793d60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01846611s
Apr 26 15:18:11.643: INFO: Pod "downwardapi-volume-7ee1d29f-89e8-4912-87a9-3db2e7793d60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026999344s
STEP: Saw pod success
Apr 26 15:18:11.643: INFO: Pod "downwardapi-volume-7ee1d29f-89e8-4912-87a9-3db2e7793d60" satisfied condition "Succeeded or Failed"
Apr 26 15:18:11.651: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod downwardapi-volume-7ee1d29f-89e8-4912-87a9-3db2e7793d60 container client-container: <nil>
STEP: delete the pod
Apr 26 15:18:11.691: INFO: Waiting for pod downwardapi-volume-7ee1d29f-89e8-4912-87a9-3db2e7793d60 to disappear
Apr 26 15:18:11.697: INFO: Pod downwardapi-volume-7ee1d29f-89e8-4912-87a9-3db2e7793d60 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:18:11.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9489" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":303,"completed":283,"skipped":4566,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:18:11.717: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-97a7f110-ae49-4282-b677-0a4aa6c66708
STEP: Creating configMap with name cm-test-opt-upd-ab189e42-b86d-490b-82fe-4e3128ac6a2a
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-97a7f110-ae49-4282-b677-0a4aa6c66708
STEP: Updating configmap cm-test-opt-upd-ab189e42-b86d-490b-82fe-4e3128ac6a2a
STEP: Creating configMap with name cm-test-opt-create-d3d9b0f3-7bb2-40ca-b8a3-584b9ddd76aa
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:19:43.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2812" for this suite.

• [SLOW TEST:91.742 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":303,"completed":284,"skipped":4567,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:19:43.459: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Apr 26 15:19:43.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 create -f - --namespace=kubectl-7204'
Apr 26 15:19:44.290: INFO: stderr: ""
Apr 26 15:19:44.290: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Apr 26 15:19:45.298: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 26 15:19:45.298: INFO: Found 0 / 1
Apr 26 15:19:46.298: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 26 15:19:46.298: INFO: Found 0 / 1
Apr 26 15:19:47.298: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 26 15:19:47.298: INFO: Found 1 / 1
Apr 26 15:19:47.298: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Apr 26 15:19:47.305: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 26 15:19:47.305: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 26 15:19:47.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 patch pod agnhost-primary-ghd8c --namespace=kubectl-7204 -p {"metadata":{"annotations":{"x":"y"}}}'
Apr 26 15:19:47.437: INFO: stderr: ""
Apr 26 15:19:47.437: INFO: stdout: "pod/agnhost-primary-ghd8c patched\n"
STEP: checking annotations
Apr 26 15:19:47.445: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 26 15:19:47.445: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:19:47.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7204" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":303,"completed":285,"skipped":4594,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:19:47.477: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Apr 26 15:19:47.553: INFO: Waiting up to 5m0s for pod "downwardapi-volume-06e2571d-d6c9-4a1c-b806-a943192893a8" in namespace "projected-1134" to be "Succeeded or Failed"
Apr 26 15:19:47.562: INFO: Pod "downwardapi-volume-06e2571d-d6c9-4a1c-b806-a943192893a8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.957498ms
Apr 26 15:19:49.568: INFO: Pod "downwardapi-volume-06e2571d-d6c9-4a1c-b806-a943192893a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015274542s
Apr 26 15:19:51.575: INFO: Pod "downwardapi-volume-06e2571d-d6c9-4a1c-b806-a943192893a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022524274s
STEP: Saw pod success
Apr 26 15:19:51.575: INFO: Pod "downwardapi-volume-06e2571d-d6c9-4a1c-b806-a943192893a8" satisfied condition "Succeeded or Failed"
Apr 26 15:19:51.580: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-w8lg4 pod downwardapi-volume-06e2571d-d6c9-4a1c-b806-a943192893a8 container client-container: <nil>
STEP: delete the pod
Apr 26 15:19:51.641: INFO: Waiting for pod downwardapi-volume-06e2571d-d6c9-4a1c-b806-a943192893a8 to disappear
Apr 26 15:19:51.646: INFO: Pod downwardapi-volume-06e2571d-d6c9-4a1c-b806-a943192893a8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:19:51.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1134" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":286,"skipped":4620,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:19:51.663: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Apr 26 15:19:51.722: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:19:52.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6087" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":303,"completed":287,"skipped":4623,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:19:52.345: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-2434
Apr 26 15:19:56.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-2434 kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Apr 26 15:19:57.121: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Apr 26 15:19:57.121: INFO: stdout: "ipvs"
Apr 26 15:19:57.121: INFO: proxyMode: ipvs
Apr 26 15:19:57.134: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Apr 26 15:19:57.141: INFO: Pod kube-proxy-mode-detector still exists
Apr 26 15:19:59.141: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Apr 26 15:19:59.148: INFO: Pod kube-proxy-mode-detector still exists
Apr 26 15:20:01.143: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Apr 26 15:20:01.149: INFO: Pod kube-proxy-mode-detector still exists
Apr 26 15:20:03.141: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Apr 26 15:20:03.147: INFO: Pod kube-proxy-mode-detector still exists
Apr 26 15:20:05.141: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Apr 26 15:20:05.147: INFO: Pod kube-proxy-mode-detector still exists
Apr 26 15:20:07.143: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Apr 26 15:20:07.158: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-2434
STEP: creating replication controller affinity-nodeport-timeout in namespace services-2434
I0426 15:20:07.223564      21 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-2434, replica count: 3
I0426 15:20:10.276520      21 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 26 15:20:10.295: INFO: Creating new exec pod
Apr 26 15:20:15.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-2434 execpod-affinityrbk88 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
Apr 26 15:20:15.966: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Apr 26 15:20:15.966: INFO: stdout: ""
Apr 26 15:20:15.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-2434 execpod-affinityrbk88 -- /bin/sh -x -c nc -zv -t -w 2 10.240.22.124 80'
Apr 26 15:20:16.573: INFO: stderr: "+ nc -zv -t -w 2 10.240.22.124 80\nConnection to 10.240.22.124 80 port [tcp/http] succeeded!\n"
Apr 26 15:20:16.573: INFO: stdout: ""
Apr 26 15:20:16.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-2434 execpod-affinityrbk88 -- /bin/sh -x -c nc -zv -t -w 2 192.168.1.7 30622'
Apr 26 15:20:17.224: INFO: stderr: "+ nc -zv -t -w 2 192.168.1.7 30622\nConnection to 192.168.1.7 30622 port [tcp/30622] succeeded!\n"
Apr 26 15:20:17.224: INFO: stdout: ""
Apr 26 15:20:17.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-2434 execpod-affinityrbk88 -- /bin/sh -x -c nc -zv -t -w 2 192.168.1.11 30622'
Apr 26 15:20:17.902: INFO: stderr: "+ nc -zv -t -w 2 192.168.1.11 30622\nConnection to 192.168.1.11 30622 port [tcp/30622] succeeded!\n"
Apr 26 15:20:17.902: INFO: stdout: ""
Apr 26 15:20:17.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-2434 execpod-affinityrbk88 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.1.11:30622/ ; done'
Apr 26 15:20:18.585: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:30622/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:30622/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:30622/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:30622/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:30622/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:30622/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:30622/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:30622/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:30622/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:30622/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:30622/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:30622/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:30622/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:30622/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:30622/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.11:30622/\n"
Apr 26 15:20:18.585: INFO: stdout: "\naffinity-nodeport-timeout-g8jsp\naffinity-nodeport-timeout-g8jsp\naffinity-nodeport-timeout-g8jsp\naffinity-nodeport-timeout-g8jsp\naffinity-nodeport-timeout-g8jsp\naffinity-nodeport-timeout-g8jsp\naffinity-nodeport-timeout-g8jsp\naffinity-nodeport-timeout-g8jsp\naffinity-nodeport-timeout-g8jsp\naffinity-nodeport-timeout-g8jsp\naffinity-nodeport-timeout-g8jsp\naffinity-nodeport-timeout-g8jsp\naffinity-nodeport-timeout-g8jsp\naffinity-nodeport-timeout-g8jsp\naffinity-nodeport-timeout-g8jsp\naffinity-nodeport-timeout-g8jsp"
Apr 26 15:20:18.585: INFO: Received response from host: affinity-nodeport-timeout-g8jsp
Apr 26 15:20:18.585: INFO: Received response from host: affinity-nodeport-timeout-g8jsp
Apr 26 15:20:18.585: INFO: Received response from host: affinity-nodeport-timeout-g8jsp
Apr 26 15:20:18.585: INFO: Received response from host: affinity-nodeport-timeout-g8jsp
Apr 26 15:20:18.585: INFO: Received response from host: affinity-nodeport-timeout-g8jsp
Apr 26 15:20:18.585: INFO: Received response from host: affinity-nodeport-timeout-g8jsp
Apr 26 15:20:18.585: INFO: Received response from host: affinity-nodeport-timeout-g8jsp
Apr 26 15:20:18.585: INFO: Received response from host: affinity-nodeport-timeout-g8jsp
Apr 26 15:20:18.585: INFO: Received response from host: affinity-nodeport-timeout-g8jsp
Apr 26 15:20:18.585: INFO: Received response from host: affinity-nodeport-timeout-g8jsp
Apr 26 15:20:18.585: INFO: Received response from host: affinity-nodeport-timeout-g8jsp
Apr 26 15:20:18.585: INFO: Received response from host: affinity-nodeport-timeout-g8jsp
Apr 26 15:20:18.585: INFO: Received response from host: affinity-nodeport-timeout-g8jsp
Apr 26 15:20:18.585: INFO: Received response from host: affinity-nodeport-timeout-g8jsp
Apr 26 15:20:18.585: INFO: Received response from host: affinity-nodeport-timeout-g8jsp
Apr 26 15:20:18.585: INFO: Received response from host: affinity-nodeport-timeout-g8jsp
Apr 26 15:20:18.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-2434 execpod-affinityrbk88 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.1.11:30622/'
Apr 26 15:20:19.202: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.1.11:30622/\n"
Apr 26 15:20:19.202: INFO: stdout: "affinity-nodeport-timeout-g8jsp"
Apr 26 15:22:24.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-2434 execpod-affinityrbk88 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.1.11:30622/'
Apr 26 15:22:24.833: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.1.11:30622/\n"
Apr 26 15:22:24.833: INFO: stdout: "affinity-nodeport-timeout-g8jsp"
Apr 26 15:24:29.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-2434 execpod-affinityrbk88 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.1.11:30622/'
Apr 26 15:24:30.507: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.1.11:30622/\n"
Apr 26 15:24:30.507: INFO: stdout: "affinity-nodeport-timeout-g8jsp"
Apr 26 15:26:35.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-2434 execpod-affinityrbk88 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.1.11:30622/'
Apr 26 15:26:36.180: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.1.11:30622/\n"
Apr 26 15:26:36.180: INFO: stdout: "affinity-nodeport-timeout-g8jsp"
Apr 26 15:28:41.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-2434 execpod-affinityrbk88 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.1.11:30622/'
Apr 26 15:28:41.825: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.1.11:30622/\n"
Apr 26 15:28:41.825: INFO: stdout: "affinity-nodeport-timeout-g8jsp"
Apr 26 15:30:46.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-2434 execpod-affinityrbk88 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.1.11:30622/'
Apr 26 15:30:47.872: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.1.11:30622/\n"
Apr 26 15:30:47.872: INFO: stdout: "affinity-nodeport-timeout-g8jsp"
Apr 26 15:32:52.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-2434 execpod-affinityrbk88 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.1.11:30622/'
Apr 26 15:32:53.518: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.1.11:30622/\n"
Apr 26 15:32:53.518: INFO: stdout: "affinity-nodeport-timeout-g8jsp"
Apr 26 15:34:58.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-610780823 exec --namespace=services-2434 execpod-affinityrbk88 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.1.11:30622/'
Apr 26 15:34:59.200: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.1.11:30622/\n"
Apr 26 15:34:59.200: INFO: stdout: "affinity-nodeport-timeout-2drpr"
Apr 26 15:34:59.201: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-2434, will wait for the garbage collector to delete the pods
Apr 26 15:34:59.426: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 26.940235ms
Apr 26 15:35:00.027: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 600.321499ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:35:04.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2434" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:912.374 seconds]
[sig-network] Services
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":303,"completed":288,"skipped":4630,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:35:04.720: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Apr 26 15:35:09.461: INFO: Successfully updated pod "annotationupdatee7eaf592-cc25-47b6-adbc-d039c1d264ca"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:35:11.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9614" for this suite.

• [SLOW TEST:6.820 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":303,"completed":289,"skipped":4635,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:35:11.542: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Apr 26 15:35:11.646: INFO: Waiting up to 5m0s for pod "pod-7ff07d63-1fcc-4a46-ab47-a9bcc9226fa1" in namespace "emptydir-8372" to be "Succeeded or Failed"
Apr 26 15:35:11.653: INFO: Pod "pod-7ff07d63-1fcc-4a46-ab47-a9bcc9226fa1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.931424ms
Apr 26 15:35:13.659: INFO: Pod "pod-7ff07d63-1fcc-4a46-ab47-a9bcc9226fa1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012787558s
Apr 26 15:35:15.667: INFO: Pod "pod-7ff07d63-1fcc-4a46-ab47-a9bcc9226fa1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020624809s
STEP: Saw pod success
Apr 26 15:35:15.667: INFO: Pod "pod-7ff07d63-1fcc-4a46-ab47-a9bcc9226fa1" satisfied condition "Succeeded or Failed"
Apr 26 15:35:15.673: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod pod-7ff07d63-1fcc-4a46-ab47-a9bcc9226fa1 container test-container: <nil>
STEP: delete the pod
Apr 26 15:35:15.723: INFO: Waiting for pod pod-7ff07d63-1fcc-4a46-ab47-a9bcc9226fa1 to disappear
Apr 26 15:35:15.730: INFO: Pod pod-7ff07d63-1fcc-4a46-ab47-a9bcc9226fa1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:35:15.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8372" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":290,"skipped":4645,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:35:15.757: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:35:19.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-845" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":303,"completed":291,"skipped":4657,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:35:19.987: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:35:37.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6128" for this suite.

• [SLOW TEST:17.236 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":303,"completed":292,"skipped":4662,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:35:37.225: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-b090cdea-b9d8-43d7-ae51-079f7a697e3b in namespace container-probe-8075
Apr 26 15:35:41.428: INFO: Started pod busybox-b090cdea-b9d8-43d7-ae51-079f7a697e3b in namespace container-probe-8075
STEP: checking the pod's current state and verifying that restartCount is present
Apr 26 15:35:41.434: INFO: Initial restart count of pod busybox-b090cdea-b9d8-43d7-ae51-079f7a697e3b is 0
Apr 26 15:36:27.677: INFO: Restart count of pod container-probe-8075/busybox-b090cdea-b9d8-43d7-ae51-079f7a697e3b is now 1 (46.242878911s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:36:27.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8075" for this suite.

• [SLOW TEST:50.544 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":303,"completed":293,"skipped":4670,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:36:27.772: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:36:27.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-1816" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":303,"completed":294,"skipped":4693,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:36:27.946: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-5bea08e1-13d7-4dd1-859b-b0c2aa2a56cb
STEP: Creating a pod to test consume configMaps
Apr 26 15:36:28.034: INFO: Waiting up to 5m0s for pod "pod-configmaps-57973b9b-a0cf-481d-a1aa-3957c1c8d980" in namespace "configmap-819" to be "Succeeded or Failed"
Apr 26 15:36:28.047: INFO: Pod "pod-configmaps-57973b9b-a0cf-481d-a1aa-3957c1c8d980": Phase="Pending", Reason="", readiness=false. Elapsed: 12.784961ms
Apr 26 15:36:30.056: INFO: Pod "pod-configmaps-57973b9b-a0cf-481d-a1aa-3957c1c8d980": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022157108s
Apr 26 15:36:32.066: INFO: Pod "pod-configmaps-57973b9b-a0cf-481d-a1aa-3957c1c8d980": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031524233s
STEP: Saw pod success
Apr 26 15:36:32.066: INFO: Pod "pod-configmaps-57973b9b-a0cf-481d-a1aa-3957c1c8d980" satisfied condition "Succeeded or Failed"
Apr 26 15:36:32.071: INFO: Trying to get logs from node elastic-hugle-5f9d9b5555-8444w pod pod-configmaps-57973b9b-a0cf-481d-a1aa-3957c1c8d980 container configmap-volume-test: <nil>
STEP: delete the pod
Apr 26 15:36:32.124: INFO: Waiting for pod pod-configmaps-57973b9b-a0cf-481d-a1aa-3957c1c8d980 to disappear
Apr 26 15:36:32.131: INFO: Pod pod-configmaps-57973b9b-a0cf-481d-a1aa-3957c1c8d980 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:36:32.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-819" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":303,"completed":295,"skipped":4700,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:36:32.158: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:36:48.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4878" for this suite.

• [SLOW TEST:16.196 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":303,"completed":296,"skipped":4727,"failed":0}
S
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:36:48.357: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should find a service from listing all namespaces [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:36:48.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1893" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":303,"completed":297,"skipped":4728,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:36:48.464: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Apr 26 15:36:53.131: INFO: Successfully updated pod "labelsupdate86b336a7-154b-42f1-9440-d68a7b2554b9"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:36:55.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4153" for this suite.

• [SLOW TEST:6.737 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":303,"completed":298,"skipped":4739,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:36:55.206: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Apr 26 15:36:55.272: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:36:56.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3749" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":303,"completed":299,"skipped":4819,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:36:56.352: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Apr 26 15:37:04.552: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 26 15:37:04.561: INFO: Pod pod-with-poststart-http-hook still exists
Apr 26 15:37:06.561: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 26 15:37:06.571: INFO: Pod pod-with-poststart-http-hook still exists
Apr 26 15:37:08.561: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 26 15:37:08.570: INFO: Pod pod-with-poststart-http-hook still exists
Apr 26 15:37:10.561: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 26 15:37:10.570: INFO: Pod pod-with-poststart-http-hook still exists
Apr 26 15:37:12.561: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 26 15:37:12.573: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:37:12.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4965" for this suite.

• [SLOW TEST:16.253 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":303,"completed":300,"skipped":4827,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:37:12.605: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 26 15:37:13.765: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 26 15:37:15.789: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755048233, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755048233, loc:(*time.Location)(0x77108c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755048233, loc:(*time.Location)(0x77108c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755048233, loc:(*time.Location)(0x77108c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 26 15:37:18.824: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:37:29.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5513" for this suite.
STEP: Destroying namespace "webhook-5513-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:17.445 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":303,"completed":301,"skipped":4838,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:37:30.054: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6962.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6962.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6962.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6962.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6962.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6962.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 26 15:37:34.827: INFO: DNS probes using dns-6962/dns-test-46a55f02-62d5-4f3d-9706-35a0e6222537 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:37:34.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6962" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":303,"completed":302,"skipped":4891,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 26 15:37:34.903: INFO: >>> kubeConfig: /tmp/kubeconfig-610780823
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.4-rc.0.51+5f1e5cafd33a88/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 26 15:37:35.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8783" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":303,"completed":303,"skipped":4930,"failed":0}
SApr 26 15:37:35.099: INFO: Running AfterSuite actions on all nodes
Apr 26 15:37:35.099: INFO: Running AfterSuite actions on node 1
Apr 26 15:37:35.099: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":303,"completed":303,"skipped":4931,"failed":0}

Ran 303 of 5234 Specs in 7424.881 seconds
SUCCESS! -- 303 Passed | 0 Failed | 0 Pending | 4931 Skipped
PASS

Ginkgo ran 1 suite in 2h3m46.828621876s
Test Suite Passed
