I0610 15:40:00.314641      26 test_context.go:416] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-615187751
I0610 15:40:00.314678      26 test_context.go:429] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0610 15:40:00.314879      26 e2e.go:129] Starting e2e run "4f102d75-248c-4634-8f62-c18949c3d47f" on Ginkgo node 1
{"msg":"Test Suite starting","total":305,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1623339596 - Will randomize all specs
Will run 305 of 5484 specs

Jun 10 15:40:00.346: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
E0610 15:40:00.348827      26 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Jun 10 15:40:00.364: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jun 10 15:40:00.472: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jun 10 15:40:00.576: INFO: 9 / 9 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jun 10 15:40:00.576: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Jun 10 15:40:00.576: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jun 10 15:40:00.595: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jun 10 15:40:00.595: INFO: e2e test version: v1.19.11
Jun 10 15:40:00.597: INFO: kube-apiserver version: v1.19.11
Jun 10 15:40:00.597: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
Jun 10 15:40:00.606: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:40:00.607: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename webhook
Jun 10 15:40:00.672: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Jun 10 15:40:00.692: INFO: No PSP annotation exists on dry run pod; assuming PodSecurityPolicy is disabled
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 10 15:40:02.306: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jun 10 15:40:04.325: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758936402, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758936402, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758936402, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758936402, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 10 15:40:06.337: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758936402, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758936402, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758936402, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758936402, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 10 15:40:08.332: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758936402, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758936402, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758936402, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758936402, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 10 15:40:10.359: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758936402, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758936402, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758936402, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758936402, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 10 15:40:12.329: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758936402, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758936402, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758936402, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758936402, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 10 15:40:15.378: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 10 15:40:15.383: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1034-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:40:16.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1934" for this suite.
STEP: Destroying namespace "webhook-1934-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:16.500 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":305,"completed":1,"skipped":20,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:40:17.110: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1546
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun 10 15:40:17.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-6695 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Jun 10 15:40:17.789: INFO: stderr: ""
Jun 10 15:40:17.789: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Jun 10 15:40:32.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-6695 get pod e2e-test-httpd-pod -o json'
Jun 10 15:40:33.046: INFO: stderr: ""
Jun 10 15:40:33.046: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"192.168.24.130/32\",\n            \"cni.projectcalico.org/podIPs\": \"192.168.24.130/32\"\n        },\n        \"creationTimestamp\": \"2021-06-10T15:40:17Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-06-10T15:40:17Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:annotations\": {\n                            \".\": {},\n                            \"f:cni.projectcalico.org/podIP\": {},\n                            \"f:cni.projectcalico.org/podIPs\": {}\n                        }\n                    }\n                },\n                \"manager\": \"calico\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-06-10T15:40:18Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"192.168.24.130\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-06-10T15:40:30Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6695\",\n        \"resourceVersion\": \"6043\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-6695/pods/e2e-test-httpd-pod\",\n        \"uid\": \"035377cc-a52f-4a95-9c86-ee35b1901dcd\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-5xplk\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"target-cluster-md-0-6b59c4f65-5pcx5\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-5xplk\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-5xplk\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-06-10T15:40:17Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-06-10T15:40:30Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-06-10T15:40:30Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-06-10T15:40:17Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://425e07c443987c22d5d4c8b69508d7cf5778bd3f7c4d9a64a325858a7c068207\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-06-10T15:40:30Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.17.0.6\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.24.130\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.24.130\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-06-10T15:40:17Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jun 10 15:40:33.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-6695 replace -f -'
Jun 10 15:40:34.096: INFO: stderr: ""
Jun 10 15:40:34.096: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1550
Jun 10 15:40:34.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-6695 delete pods e2e-test-httpd-pod'
Jun 10 15:40:45.516: INFO: stderr: ""
Jun 10 15:40:45.516: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:40:45.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6695" for this suite.

• [SLOW TEST:28.434 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1543
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":305,"completed":2,"skipped":42,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:40:45.545: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jun 10 15:40:46.351: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jun 10 15:40:48.361: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758936446, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758936446, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758936446, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758936446, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 10 15:40:51.377: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 10 15:40:51.383: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:40:52.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-6883" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:7.290 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":305,"completed":3,"skipped":49,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:40:52.835: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jun 10 15:40:54.505: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 10 15:40:57.569: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 10 15:40:57.573: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:40:58.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-3415" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:6.490 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":305,"completed":4,"skipped":61,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:40:59.326: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:40:59.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9001" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":305,"completed":5,"skipped":85,"failed":0}
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:40:59.556: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-1831
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-1831
STEP: Creating statefulset with conflicting port in namespace statefulset-1831
STEP: Waiting until pod test-pod will start running in namespace statefulset-1831
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-1831
Jun 10 15:41:03.842: INFO: Observed stateful pod in namespace: statefulset-1831, name: ss-0, uid: e09a5734-dd19-481a-ae35-e2c698f6526a, status phase: Pending. Waiting for statefulset controller to delete.
Jun 10 15:41:04.360: INFO: Observed stateful pod in namespace: statefulset-1831, name: ss-0, uid: e09a5734-dd19-481a-ae35-e2c698f6526a, status phase: Failed. Waiting for statefulset controller to delete.
Jun 10 15:41:04.370: INFO: Observed stateful pod in namespace: statefulset-1831, name: ss-0, uid: e09a5734-dd19-481a-ae35-e2c698f6526a, status phase: Failed. Waiting for statefulset controller to delete.
Jun 10 15:41:04.379: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-1831
STEP: Removing pod with conflicting port in namespace statefulset-1831
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-1831 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jun 10 15:41:08.463: INFO: Deleting all statefulset in ns statefulset-1831
Jun 10 15:41:08.478: INFO: Scaling statefulset ss to 0
Jun 10 15:41:18.523: INFO: Waiting for statefulset status.replicas updated to 0
Jun 10 15:41:18.539: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:41:18.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1831" for this suite.

• [SLOW TEST:19.021 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":305,"completed":6,"skipped":88,"failed":0}
SSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:41:18.577: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 10 15:41:18.635: INFO: Creating ReplicaSet my-hostname-basic-9e4ab91a-80b8-418c-9dc9-f2e8b5f0555a
Jun 10 15:41:18.646: INFO: Pod name my-hostname-basic-9e4ab91a-80b8-418c-9dc9-f2e8b5f0555a: Found 0 pods out of 1
Jun 10 15:41:23.662: INFO: Pod name my-hostname-basic-9e4ab91a-80b8-418c-9dc9-f2e8b5f0555a: Found 1 pods out of 1
Jun 10 15:41:23.662: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-9e4ab91a-80b8-418c-9dc9-f2e8b5f0555a" is running
Jun 10 15:41:23.669: INFO: Pod "my-hostname-basic-9e4ab91a-80b8-418c-9dc9-f2e8b5f0555a-674wh" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-06-10 15:41:18 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-06-10 15:41:20 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-06-10 15:41:20 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-06-10 15:41:18 +0000 UTC Reason: Message:}])
Jun 10 15:41:23.669: INFO: Trying to dial the pod
Jun 10 15:41:28.693: INFO: Controller my-hostname-basic-9e4ab91a-80b8-418c-9dc9-f2e8b5f0555a: Got expected result from replica 1 [my-hostname-basic-9e4ab91a-80b8-418c-9dc9-f2e8b5f0555a-674wh]: "my-hostname-basic-9e4ab91a-80b8-418c-9dc9-f2e8b5f0555a-674wh", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:41:28.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8472" for this suite.

• [SLOW TEST:10.138 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":305,"completed":7,"skipped":91,"failed":0}
SSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:41:28.716: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 10 15:41:28.837: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: creating replication controller svc-latency-rc in namespace svc-latency-306
I0610 15:41:28.883451      26 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-306, replica count: 1
I0610 15:41:29.937417      26 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0610 15:41:30.938517      26 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0610 15:41:31.939124      26 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0610 15:41:32.939527      26 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0610 15:41:33.940380      26 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0610 15:41:34.941454      26 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 10 15:41:35.069: INFO: Created: latency-svc-lk9d5
Jun 10 15:41:35.122: INFO: Got endpoints: latency-svc-lk9d5 [80.42858ms]
Jun 10 15:41:35.244: INFO: Created: latency-svc-sgrxt
Jun 10 15:41:35.266: INFO: Got endpoints: latency-svc-sgrxt [141.44337ms]
Jun 10 15:41:35.305: INFO: Created: latency-svc-lbcrq
Jun 10 15:41:35.313: INFO: Got endpoints: latency-svc-lbcrq [187.222663ms]
Jun 10 15:41:35.370: INFO: Created: latency-svc-nqsgf
Jun 10 15:41:35.391: INFO: Got endpoints: latency-svc-nqsgf [264.134824ms]
Jun 10 15:41:35.441: INFO: Created: latency-svc-mn9dk
Jun 10 15:41:35.446: INFO: Got endpoints: latency-svc-mn9dk [320.428182ms]
Jun 10 15:41:35.466: INFO: Created: latency-svc-tbrxt
Jun 10 15:41:35.498: INFO: Got endpoints: latency-svc-tbrxt [371.222997ms]
Jun 10 15:41:35.515: INFO: Created: latency-svc-c6l6b
Jun 10 15:41:35.519: INFO: Got endpoints: latency-svc-c6l6b [390.127635ms]
Jun 10 15:41:35.541: INFO: Created: latency-svc-wg2md
Jun 10 15:41:35.543: INFO: Got endpoints: latency-svc-wg2md [415.231514ms]
Jun 10 15:41:35.571: INFO: Created: latency-svc-n9dzp
Jun 10 15:41:35.610: INFO: Got endpoints: latency-svc-n9dzp [481.157746ms]
Jun 10 15:41:35.704: INFO: Created: latency-svc-vj4rz
Jun 10 15:41:35.709: INFO: Got endpoints: latency-svc-vj4rz [579.668973ms]
Jun 10 15:41:35.737: INFO: Created: latency-svc-74f96
Jun 10 15:41:35.759: INFO: Got endpoints: latency-svc-74f96 [632.376481ms]
Jun 10 15:41:35.806: INFO: Created: latency-svc-5nj5g
Jun 10 15:41:35.823: INFO: Got endpoints: latency-svc-5nj5g [696.403131ms]
Jun 10 15:41:35.833: INFO: Created: latency-svc-7rf5s
Jun 10 15:41:35.861: INFO: Got endpoints: latency-svc-7rf5s [729.494ms]
Jun 10 15:41:35.891: INFO: Created: latency-svc-vphsx
Jun 10 15:41:35.913: INFO: Got endpoints: latency-svc-vphsx [782.792589ms]
Jun 10 15:41:35.934: INFO: Created: latency-svc-xscqp
Jun 10 15:41:35.952: INFO: Created: latency-svc-2crt2
Jun 10 15:41:35.952: INFO: Got endpoints: latency-svc-xscqp [826.451781ms]
Jun 10 15:41:35.977: INFO: Got endpoints: latency-svc-2crt2 [851.818252ms]
Jun 10 15:41:36.012: INFO: Created: latency-svc-mxmqv
Jun 10 15:41:36.021: INFO: Got endpoints: latency-svc-mxmqv [754.482567ms]
Jun 10 15:41:36.036: INFO: Created: latency-svc-7vb8d
Jun 10 15:41:36.053: INFO: Got endpoints: latency-svc-7vb8d [739.089477ms]
Jun 10 15:41:36.086: INFO: Created: latency-svc-nsvlb
Jun 10 15:41:36.087: INFO: Got endpoints: latency-svc-nsvlb [695.946587ms]
Jun 10 15:41:36.120: INFO: Created: latency-svc-jtw2v
Jun 10 15:41:36.133: INFO: Got endpoints: latency-svc-jtw2v [686.127211ms]
Jun 10 15:41:36.190: INFO: Created: latency-svc-2ghgt
Jun 10 15:41:36.202: INFO: Got endpoints: latency-svc-2ghgt [703.070291ms]
Jun 10 15:41:36.251: INFO: Created: latency-svc-nsln6
Jun 10 15:41:36.317: INFO: Got endpoints: latency-svc-nsln6 [798.563336ms]
Jun 10 15:41:36.404: INFO: Created: latency-svc-2wnmh
Jun 10 15:41:36.421: INFO: Got endpoints: latency-svc-2wnmh [877.758898ms]
Jun 10 15:41:36.477: INFO: Created: latency-svc-8m54p
Jun 10 15:41:36.504: INFO: Got endpoints: latency-svc-8m54p [893.066908ms]
Jun 10 15:41:36.523: INFO: Created: latency-svc-lzzjh
Jun 10 15:41:36.530: INFO: Got endpoints: latency-svc-lzzjh [820.806302ms]
Jun 10 15:41:36.544: INFO: Created: latency-svc-6s887
Jun 10 15:41:36.554: INFO: Got endpoints: latency-svc-6s887 [794.516314ms]
Jun 10 15:41:36.572: INFO: Created: latency-svc-h9tjt
Jun 10 15:41:36.589: INFO: Got endpoints: latency-svc-h9tjt [766.043993ms]
Jun 10 15:41:36.659: INFO: Created: latency-svc-598fh
Jun 10 15:41:36.683: INFO: Created: latency-svc-h456v
Jun 10 15:41:36.684: INFO: Got endpoints: latency-svc-598fh [823.022701ms]
Jun 10 15:41:36.710: INFO: Created: latency-svc-hml9z
Jun 10 15:41:36.713: INFO: Got endpoints: latency-svc-h456v [799.743714ms]
Jun 10 15:41:36.730: INFO: Created: latency-svc-7brp5
Jun 10 15:41:36.732: INFO: Got endpoints: latency-svc-hml9z [778.657584ms]
Jun 10 15:41:36.756: INFO: Got endpoints: latency-svc-7brp5 [778.17486ms]
Jun 10 15:41:36.777: INFO: Created: latency-svc-kc2pt
Jun 10 15:41:36.798: INFO: Created: latency-svc-kft76
Jun 10 15:41:36.811: INFO: Got endpoints: latency-svc-kft76 [757.829657ms]
Jun 10 15:41:36.826: INFO: Got endpoints: latency-svc-kc2pt [804.730495ms]
Jun 10 15:41:36.866: INFO: Created: latency-svc-b2wrv
Jun 10 15:41:36.873: INFO: Got endpoints: latency-svc-b2wrv [786.114596ms]
Jun 10 15:41:36.912: INFO: Created: latency-svc-t558v
Jun 10 15:41:36.918: INFO: Got endpoints: latency-svc-t558v [785.385589ms]
Jun 10 15:41:36.971: INFO: Created: latency-svc-r6pd7
Jun 10 15:41:36.990: INFO: Got endpoints: latency-svc-r6pd7 [787.629056ms]
Jun 10 15:41:37.014: INFO: Created: latency-svc-9f6rc
Jun 10 15:41:37.022: INFO: Got endpoints: latency-svc-9f6rc [704.93883ms]
Jun 10 15:41:37.038: INFO: Created: latency-svc-4r82d
Jun 10 15:41:37.064: INFO: Got endpoints: latency-svc-4r82d [642.987745ms]
Jun 10 15:41:37.081: INFO: Created: latency-svc-6qshr
Jun 10 15:41:37.172: INFO: Got endpoints: latency-svc-6qshr [667.74202ms]
Jun 10 15:41:37.197: INFO: Created: latency-svc-nbcxw
Jun 10 15:41:37.222: INFO: Got endpoints: latency-svc-nbcxw [691.448275ms]
Jun 10 15:41:37.277: INFO: Created: latency-svc-2bjhx
Jun 10 15:41:37.312: INFO: Got endpoints: latency-svc-2bjhx [758.206459ms]
Jun 10 15:41:37.444: INFO: Created: latency-svc-z6q7z
Jun 10 15:41:37.444: INFO: Got endpoints: latency-svc-z6q7z [854.328491ms]
Jun 10 15:41:37.484: INFO: Created: latency-svc-nxf94
Jun 10 15:41:37.505: INFO: Got endpoints: latency-svc-nxf94 [820.778169ms]
Jun 10 15:41:37.533: INFO: Created: latency-svc-rk6wx
Jun 10 15:41:37.548: INFO: Got endpoints: latency-svc-rk6wx [834.774675ms]
Jun 10 15:41:37.562: INFO: Created: latency-svc-5kq84
Jun 10 15:41:37.575: INFO: Got endpoints: latency-svc-5kq84 [843.683452ms]
Jun 10 15:41:37.604: INFO: Created: latency-svc-jmkfx
Jun 10 15:41:37.614: INFO: Got endpoints: latency-svc-jmkfx [857.768065ms]
Jun 10 15:41:37.620: INFO: Created: latency-svc-87trx
Jun 10 15:41:37.630: INFO: Got endpoints: latency-svc-87trx [818.730914ms]
Jun 10 15:41:37.648: INFO: Created: latency-svc-j98v8
Jun 10 15:41:37.658: INFO: Got endpoints: latency-svc-j98v8 [832.212659ms]
Jun 10 15:41:37.703: INFO: Created: latency-svc-w5gvc
Jun 10 15:41:37.705: INFO: Created: latency-svc-x2xtp
Jun 10 15:41:37.715: INFO: Got endpoints: latency-svc-w5gvc [795.836415ms]
Jun 10 15:41:37.715: INFO: Got endpoints: latency-svc-x2xtp [842.059759ms]
Jun 10 15:41:37.720: INFO: Created: latency-svc-r52kt
Jun 10 15:41:37.738: INFO: Got endpoints: latency-svc-r52kt [747.93759ms]
Jun 10 15:41:37.766: INFO: Created: latency-svc-ss5mg
Jun 10 15:41:37.771: INFO: Got endpoints: latency-svc-ss5mg [747.997324ms]
Jun 10 15:41:37.785: INFO: Created: latency-svc-dq7tt
Jun 10 15:41:37.789: INFO: Got endpoints: latency-svc-dq7tt [725.295916ms]
Jun 10 15:41:37.807: INFO: Created: latency-svc-8whsb
Jun 10 15:41:37.839: INFO: Got endpoints: latency-svc-8whsb [666.924576ms]
Jun 10 15:41:37.864: INFO: Created: latency-svc-fxth2
Jun 10 15:41:37.882: INFO: Got endpoints: latency-svc-fxth2 [659.665514ms]
Jun 10 15:41:37.888: INFO: Created: latency-svc-mttnd
Jun 10 15:41:37.898: INFO: Got endpoints: latency-svc-mttnd [585.793123ms]
Jun 10 15:41:37.908: INFO: Created: latency-svc-nfnlk
Jun 10 15:41:37.917: INFO: Got endpoints: latency-svc-nfnlk [473.300582ms]
Jun 10 15:41:37.934: INFO: Created: latency-svc-glpgq
Jun 10 15:41:37.945: INFO: Got endpoints: latency-svc-glpgq [439.527107ms]
Jun 10 15:41:37.954: INFO: Created: latency-svc-9p2wg
Jun 10 15:41:37.966: INFO: Got endpoints: latency-svc-9p2wg [417.920241ms]
Jun 10 15:41:37.973: INFO: Created: latency-svc-gs9mv
Jun 10 15:41:37.993: INFO: Got endpoints: latency-svc-gs9mv [48.698085ms]
Jun 10 15:41:37.994: INFO: Created: latency-svc-ckl7l
Jun 10 15:41:38.007: INFO: Got endpoints: latency-svc-ckl7l [431.142796ms]
Jun 10 15:41:38.025: INFO: Created: latency-svc-g8wr6
Jun 10 15:41:38.036: INFO: Got endpoints: latency-svc-g8wr6 [421.79658ms]
Jun 10 15:41:38.039: INFO: Created: latency-svc-qgwws
Jun 10 15:41:38.049: INFO: Got endpoints: latency-svc-qgwws [419.184473ms]
Jun 10 15:41:38.057: INFO: Created: latency-svc-qbr8c
Jun 10 15:41:38.064: INFO: Got endpoints: latency-svc-qbr8c [405.659938ms]
Jun 10 15:41:38.069: INFO: Created: latency-svc-76t5z
Jun 10 15:41:38.090: INFO: Got endpoints: latency-svc-76t5z [375.353286ms]
Jun 10 15:41:38.108: INFO: Created: latency-svc-kbmbz
Jun 10 15:41:38.151: INFO: Created: latency-svc-rrxm4
Jun 10 15:41:38.167: INFO: Got endpoints: latency-svc-kbmbz [452.348856ms]
Jun 10 15:41:38.183: INFO: Got endpoints: latency-svc-rrxm4 [445.541294ms]
Jun 10 15:41:38.190: INFO: Created: latency-svc-qtn2g
Jun 10 15:41:38.229: INFO: Got endpoints: latency-svc-qtn2g [458.73417ms]
Jun 10 15:41:38.254: INFO: Created: latency-svc-mh547
Jun 10 15:41:38.259: INFO: Got endpoints: latency-svc-mh547 [470.000049ms]
Jun 10 15:41:38.268: INFO: Created: latency-svc-jmmz2
Jun 10 15:41:38.277: INFO: Got endpoints: latency-svc-jmmz2 [438.090546ms]
Jun 10 15:41:38.346: INFO: Created: latency-svc-zfsvf
Jun 10 15:41:38.363: INFO: Created: latency-svc-ss459
Jun 10 15:41:38.387: INFO: Got endpoints: latency-svc-zfsvf [505.472041ms]
Jun 10 15:41:38.456: INFO: Created: latency-svc-qgdbd
Jun 10 15:41:38.467: INFO: Got endpoints: latency-svc-ss459 [568.011418ms]
Jun 10 15:41:38.471: INFO: Got endpoints: latency-svc-qgdbd [552.991993ms]
Jun 10 15:41:38.481: INFO: Created: latency-svc-j257l
Jun 10 15:41:38.493: INFO: Got endpoints: latency-svc-j257l [527.030251ms]
Jun 10 15:41:38.510: INFO: Created: latency-svc-vc2fz
Jun 10 15:41:38.521: INFO: Got endpoints: latency-svc-vc2fz [528.018181ms]
Jun 10 15:41:38.539: INFO: Created: latency-svc-7mff6
Jun 10 15:41:38.541: INFO: Got endpoints: latency-svc-7mff6 [534.250807ms]
Jun 10 15:41:38.555: INFO: Created: latency-svc-5cr4c
Jun 10 15:41:38.561: INFO: Got endpoints: latency-svc-5cr4c [525.057379ms]
Jun 10 15:41:38.578: INFO: Created: latency-svc-cz9hk
Jun 10 15:41:38.596: INFO: Got endpoints: latency-svc-cz9hk [546.266889ms]
Jun 10 15:41:38.621: INFO: Created: latency-svc-qgxr4
Jun 10 15:41:38.628: INFO: Got endpoints: latency-svc-qgxr4 [563.68247ms]
Jun 10 15:41:38.635: INFO: Created: latency-svc-n4z5h
Jun 10 15:41:38.654: INFO: Got endpoints: latency-svc-n4z5h [562.821383ms]
Jun 10 15:41:38.660: INFO: Created: latency-svc-jbsbc
Jun 10 15:41:38.696: INFO: Got endpoints: latency-svc-jbsbc [528.386759ms]
Jun 10 15:41:38.705: INFO: Created: latency-svc-m2dmn
Jun 10 15:41:38.706: INFO: Got endpoints: latency-svc-m2dmn [522.052786ms]
Jun 10 15:41:38.721: INFO: Created: latency-svc-c8569
Jun 10 15:41:38.732: INFO: Got endpoints: latency-svc-c8569 [501.920501ms]
Jun 10 15:41:38.744: INFO: Created: latency-svc-vs67n
Jun 10 15:41:38.760: INFO: Created: latency-svc-kmfrb
Jun 10 15:41:38.763: INFO: Got endpoints: latency-svc-vs67n [501.850754ms]
Jun 10 15:41:38.768: INFO: Got endpoints: latency-svc-kmfrb [489.777289ms]
Jun 10 15:41:38.783: INFO: Created: latency-svc-kbf7k
Jun 10 15:41:38.788: INFO: Got endpoints: latency-svc-kbf7k [400.545772ms]
Jun 10 15:41:38.802: INFO: Created: latency-svc-74vpr
Jun 10 15:41:38.820: INFO: Got endpoints: latency-svc-74vpr [353.641373ms]
Jun 10 15:41:38.830: INFO: Created: latency-svc-fpg6c
Jun 10 15:41:38.842: INFO: Got endpoints: latency-svc-fpg6c [371.726393ms]
Jun 10 15:41:38.852: INFO: Created: latency-svc-2cthw
Jun 10 15:41:38.873: INFO: Got endpoints: latency-svc-2cthw [380.140317ms]
Jun 10 15:41:38.903: INFO: Created: latency-svc-hxznl
Jun 10 15:41:38.984: INFO: Got endpoints: latency-svc-hxznl [462.941521ms]
Jun 10 15:41:39.003: INFO: Created: latency-svc-m4gdm
Jun 10 15:41:39.041: INFO: Created: latency-svc-9nhx2
Jun 10 15:41:39.056: INFO: Got endpoints: latency-svc-m4gdm [514.658418ms]
Jun 10 15:41:39.061: INFO: Got endpoints: latency-svc-9nhx2 [500.229769ms]
Jun 10 15:41:39.079: INFO: Created: latency-svc-7l6z4
Jun 10 15:41:39.132: INFO: Got endpoints: latency-svc-7l6z4 [536.367291ms]
Jun 10 15:41:39.165: INFO: Created: latency-svc-fgcmh
Jun 10 15:41:39.194: INFO: Got endpoints: latency-svc-fgcmh [565.716919ms]
Jun 10 15:41:39.267: INFO: Created: latency-svc-n42hl
Jun 10 15:41:39.304: INFO: Got endpoints: latency-svc-n42hl [649.458749ms]
Jun 10 15:41:39.382: INFO: Created: latency-svc-d8m4t
Jun 10 15:41:39.382: INFO: Got endpoints: latency-svc-d8m4t [685.988001ms]
Jun 10 15:41:39.398: INFO: Created: latency-svc-27vtn
Jun 10 15:41:39.404: INFO: Got endpoints: latency-svc-27vtn [697.561125ms]
Jun 10 15:41:39.441: INFO: Created: latency-svc-hhwhk
Jun 10 15:41:39.466: INFO: Got endpoints: latency-svc-hhwhk [733.442361ms]
Jun 10 15:41:39.474: INFO: Created: latency-svc-s5hkn
Jun 10 15:41:39.499: INFO: Got endpoints: latency-svc-s5hkn [735.950349ms]
Jun 10 15:41:39.661: INFO: Created: latency-svc-kmt8q
Jun 10 15:41:39.666: INFO: Created: latency-svc-cqb2q
Jun 10 15:41:39.678: INFO: Got endpoints: latency-svc-kmt8q [909.806779ms]
Jun 10 15:41:39.686: INFO: Got endpoints: latency-svc-cqb2q [898.047697ms]
Jun 10 15:41:39.693: INFO: Created: latency-svc-vc98b
Jun 10 15:41:39.713: INFO: Got endpoints: latency-svc-vc98b [892.25726ms]
Jun 10 15:41:39.720: INFO: Created: latency-svc-fxj5d
Jun 10 15:41:39.726: INFO: Got endpoints: latency-svc-fxj5d [883.593092ms]
Jun 10 15:41:39.744: INFO: Created: latency-svc-dn5x4
Jun 10 15:41:39.768: INFO: Got endpoints: latency-svc-dn5x4 [894.551017ms]
Jun 10 15:41:39.775: INFO: Created: latency-svc-qbzkd
Jun 10 15:41:39.793: INFO: Created: latency-svc-rh7dx
Jun 10 15:41:39.796: INFO: Got endpoints: latency-svc-qbzkd [811.1158ms]
Jun 10 15:41:39.821: INFO: Got endpoints: latency-svc-rh7dx [764.785445ms]
Jun 10 15:41:39.879: INFO: Created: latency-svc-2q2r2
Jun 10 15:41:39.890: INFO: Got endpoints: latency-svc-2q2r2 [828.974676ms]
Jun 10 15:41:39.907: INFO: Created: latency-svc-tp47z
Jun 10 15:41:39.931: INFO: Got endpoints: latency-svc-tp47z [798.493512ms]
Jun 10 15:41:39.954: INFO: Created: latency-svc-jrsqc
Jun 10 15:41:39.965: INFO: Got endpoints: latency-svc-jrsqc [769.962158ms]
Jun 10 15:41:39.990: INFO: Created: latency-svc-lvcl2
Jun 10 15:41:39.995: INFO: Got endpoints: latency-svc-lvcl2 [690.995262ms]
Jun 10 15:41:40.005: INFO: Created: latency-svc-fnz75
Jun 10 15:41:40.016: INFO: Got endpoints: latency-svc-fnz75 [633.93942ms]
Jun 10 15:41:40.055: INFO: Created: latency-svc-fqb8p
Jun 10 15:41:40.110: INFO: Got endpoints: latency-svc-fqb8p [706.228941ms]
Jun 10 15:41:40.172: INFO: Created: latency-svc-ccqhz
Jun 10 15:41:40.176: INFO: Got endpoints: latency-svc-ccqhz [709.770231ms]
Jun 10 15:41:40.287: INFO: Created: latency-svc-wnlm5
Jun 10 15:41:40.306: INFO: Got endpoints: latency-svc-wnlm5 [806.674128ms]
Jun 10 15:41:40.353: INFO: Created: latency-svc-2qbtd
Jun 10 15:41:40.389: INFO: Got endpoints: latency-svc-2qbtd [710.231859ms]
Jun 10 15:41:40.429: INFO: Created: latency-svc-4bqww
Jun 10 15:41:40.443: INFO: Got endpoints: latency-svc-4bqww [757.278792ms]
Jun 10 15:41:40.482: INFO: Created: latency-svc-wj2vr
Jun 10 15:41:40.483: INFO: Got endpoints: latency-svc-wj2vr [770.322495ms]
Jun 10 15:41:40.512: INFO: Created: latency-svc-6q656
Jun 10 15:41:40.534: INFO: Got endpoints: latency-svc-6q656 [807.459656ms]
Jun 10 15:41:40.552: INFO: Created: latency-svc-s2k48
Jun 10 15:41:40.569: INFO: Got endpoints: latency-svc-s2k48 [800.973105ms]
Jun 10 15:41:40.588: INFO: Created: latency-svc-hm42w
Jun 10 15:41:40.597: INFO: Got endpoints: latency-svc-hm42w [801.791703ms]
Jun 10 15:41:40.628: INFO: Created: latency-svc-vwvdb
Jun 10 15:41:40.645: INFO: Got endpoints: latency-svc-vwvdb [823.806408ms]
Jun 10 15:41:40.652: INFO: Created: latency-svc-srgmv
Jun 10 15:41:40.656: INFO: Got endpoints: latency-svc-srgmv [765.138541ms]
Jun 10 15:41:40.672: INFO: Created: latency-svc-tpcg4
Jun 10 15:41:40.675: INFO: Got endpoints: latency-svc-tpcg4 [743.894157ms]
Jun 10 15:41:40.700: INFO: Created: latency-svc-7hr6p
Jun 10 15:41:40.712: INFO: Got endpoints: latency-svc-7hr6p [747.011474ms]
Jun 10 15:41:40.724: INFO: Created: latency-svc-fk7tw
Jun 10 15:41:40.755: INFO: Got endpoints: latency-svc-fk7tw [759.202715ms]
Jun 10 15:41:40.762: INFO: Created: latency-svc-gdrnf
Jun 10 15:41:40.768: INFO: Got endpoints: latency-svc-gdrnf [751.270908ms]
Jun 10 15:41:40.786: INFO: Created: latency-svc-8qtgq
Jun 10 15:41:40.827: INFO: Got endpoints: latency-svc-8qtgq [716.747476ms]
Jun 10 15:41:40.841: INFO: Created: latency-svc-9b4g2
Jun 10 15:41:40.885: INFO: Got endpoints: latency-svc-9b4g2 [708.64682ms]
Jun 10 15:41:40.893: INFO: Created: latency-svc-tjjm9
Jun 10 15:41:40.904: INFO: Got endpoints: latency-svc-tjjm9 [597.794157ms]
Jun 10 15:41:40.955: INFO: Created: latency-svc-cdk4b
Jun 10 15:41:40.955: INFO: Got endpoints: latency-svc-cdk4b [566.437982ms]
Jun 10 15:41:40.959: INFO: Created: latency-svc-njgs4
Jun 10 15:41:40.970: INFO: Created: latency-svc-l95d5
Jun 10 15:41:40.974: INFO: Got endpoints: latency-svc-njgs4 [490.404117ms]
Jun 10 15:41:40.985: INFO: Created: latency-svc-v5vfb
Jun 10 15:41:41.002: INFO: Got endpoints: latency-svc-l95d5 [558.328481ms]
Jun 10 15:41:41.002: INFO: Got endpoints: latency-svc-v5vfb [468.036489ms]
Jun 10 15:41:41.038: INFO: Created: latency-svc-hvkvx
Jun 10 15:41:41.038: INFO: Got endpoints: latency-svc-hvkvx [469.014405ms]
Jun 10 15:41:41.045: INFO: Created: latency-svc-85bjq
Jun 10 15:41:41.189: INFO: Got endpoints: latency-svc-85bjq [591.335746ms]
Jun 10 15:41:41.211: INFO: Created: latency-svc-zrn6h
Jun 10 15:41:41.211: INFO: Created: latency-svc-8cs9l
Jun 10 15:41:41.227: INFO: Got endpoints: latency-svc-8cs9l [570.771742ms]
Jun 10 15:41:41.258: INFO: Got endpoints: latency-svc-zrn6h [612.500264ms]
Jun 10 15:41:41.259: INFO: Created: latency-svc-t4pzf
Jun 10 15:41:41.265: INFO: Got endpoints: latency-svc-t4pzf [590.056143ms]
Jun 10 15:41:41.279: INFO: Created: latency-svc-dgm9t
Jun 10 15:41:41.310: INFO: Created: latency-svc-8855c
Jun 10 15:41:41.332: INFO: Got endpoints: latency-svc-dgm9t [620.669787ms]
Jun 10 15:41:41.338: INFO: Got endpoints: latency-svc-8855c [583.713206ms]
Jun 10 15:41:41.376: INFO: Created: latency-svc-fsp44
Jun 10 15:41:41.381: INFO: Created: latency-svc-hc9z5
Jun 10 15:41:41.401: INFO: Got endpoints: latency-svc-fsp44 [632.875362ms]
Jun 10 15:41:41.404: INFO: Got endpoints: latency-svc-hc9z5 [576.304848ms]
Jun 10 15:41:41.455: INFO: Created: latency-svc-dl58t
Jun 10 15:41:41.455: INFO: Got endpoints: latency-svc-dl58t [570.25128ms]
Jun 10 15:41:41.507: INFO: Created: latency-svc-74pxv
Jun 10 15:41:41.516: INFO: Got endpoints: latency-svc-74pxv [611.392982ms]
Jun 10 15:41:41.558: INFO: Created: latency-svc-vnhl4
Jun 10 15:41:41.575: INFO: Got endpoints: latency-svc-vnhl4 [620.467512ms]
Jun 10 15:41:41.607: INFO: Created: latency-svc-v4hq2
Jun 10 15:41:41.618: INFO: Got endpoints: latency-svc-v4hq2 [643.739659ms]
Jun 10 15:41:41.625: INFO: Created: latency-svc-tkvtb
Jun 10 15:41:41.634: INFO: Got endpoints: latency-svc-tkvtb [631.957186ms]
Jun 10 15:41:41.653: INFO: Created: latency-svc-ndjq7
Jun 10 15:41:41.660: INFO: Got endpoints: latency-svc-ndjq7 [658.220907ms]
Jun 10 15:41:41.679: INFO: Created: latency-svc-4tnr2
Jun 10 15:41:41.695: INFO: Got endpoints: latency-svc-4tnr2 [657.450057ms]
Jun 10 15:41:41.744: INFO: Created: latency-svc-sbhnk
Jun 10 15:41:41.753: INFO: Got endpoints: latency-svc-sbhnk [564.572286ms]
Jun 10 15:41:41.790: INFO: Created: latency-svc-srkrj
Jun 10 15:41:41.797: INFO: Got endpoints: latency-svc-srkrj [569.961006ms]
Jun 10 15:41:41.805: INFO: Created: latency-svc-m9z22
Jun 10 15:41:41.822: INFO: Got endpoints: latency-svc-m9z22 [563.81536ms]
Jun 10 15:41:41.842: INFO: Created: latency-svc-j6jrr
Jun 10 15:41:41.864: INFO: Got endpoints: latency-svc-j6jrr [598.387407ms]
Jun 10 15:41:41.880: INFO: Created: latency-svc-jj95q
Jun 10 15:41:41.897: INFO: Got endpoints: latency-svc-jj95q [564.804173ms]
Jun 10 15:41:41.901: INFO: Created: latency-svc-p4x22
Jun 10 15:41:41.918: INFO: Got endpoints: latency-svc-p4x22 [579.920694ms]
Jun 10 15:41:41.928: INFO: Created: latency-svc-rlbj9
Jun 10 15:41:41.942: INFO: Got endpoints: latency-svc-rlbj9 [540.267855ms]
Jun 10 15:41:41.950: INFO: Created: latency-svc-p2jc5
Jun 10 15:41:41.965: INFO: Got endpoints: latency-svc-p2jc5 [561.577686ms]
Jun 10 15:41:41.978: INFO: Created: latency-svc-tk8gf
Jun 10 15:41:41.986: INFO: Got endpoints: latency-svc-tk8gf [530.466996ms]
Jun 10 15:41:42.009: INFO: Created: latency-svc-7qfzw
Jun 10 15:41:42.009: INFO: Got endpoints: latency-svc-7qfzw [493.157373ms]
Jun 10 15:41:42.026: INFO: Created: latency-svc-p49j5
Jun 10 15:41:42.028: INFO: Got endpoints: latency-svc-p49j5 [452.299269ms]
Jun 10 15:41:42.043: INFO: Created: latency-svc-8k9rm
Jun 10 15:41:42.047: INFO: Got endpoints: latency-svc-8k9rm [429.36048ms]
Jun 10 15:41:42.063: INFO: Created: latency-svc-mp8fz
Jun 10 15:41:42.066: INFO: Got endpoints: latency-svc-mp8fz [431.983776ms]
Jun 10 15:41:42.068: INFO: Created: latency-svc-jrx9b
Jun 10 15:41:42.075: INFO: Got endpoints: latency-svc-jrx9b [415.020131ms]
Jun 10 15:41:42.110: INFO: Created: latency-svc-q6vbf
Jun 10 15:41:42.139: INFO: Got endpoints: latency-svc-q6vbf [443.8988ms]
Jun 10 15:41:42.216: INFO: Created: latency-svc-df788
Jun 10 15:41:42.251: INFO: Created: latency-svc-wrmq6
Jun 10 15:41:42.291: INFO: Got endpoints: latency-svc-df788 [537.210217ms]
Jun 10 15:41:42.295: INFO: Got endpoints: latency-svc-wrmq6 [498.522611ms]
Jun 10 15:41:42.315: INFO: Created: latency-svc-9qdht
Jun 10 15:41:42.328: INFO: Got endpoints: latency-svc-9qdht [506.440881ms]
Jun 10 15:41:42.348: INFO: Created: latency-svc-thfcj
Jun 10 15:41:42.380: INFO: Got endpoints: latency-svc-thfcj [516.681311ms]
Jun 10 15:41:42.434: INFO: Created: latency-svc-wtgl7
Jun 10 15:41:42.434: INFO: Got endpoints: latency-svc-wtgl7 [536.648576ms]
Jun 10 15:41:42.471: INFO: Created: latency-svc-rtt9d
Jun 10 15:41:42.471: INFO: Got endpoints: latency-svc-rtt9d [552.847752ms]
Jun 10 15:41:42.481: INFO: Created: latency-svc-7zmkb
Jun 10 15:41:42.515: INFO: Got endpoints: latency-svc-7zmkb [573.138972ms]
Jun 10 15:41:42.528: INFO: Created: latency-svc-j82bx
Jun 10 15:41:42.538: INFO: Got endpoints: latency-svc-j82bx [572.333656ms]
Jun 10 15:41:42.549: INFO: Created: latency-svc-xx2r7
Jun 10 15:41:42.558: INFO: Got endpoints: latency-svc-xx2r7 [572.417179ms]
Jun 10 15:41:42.561: INFO: Created: latency-svc-f72g9
Jun 10 15:41:42.572: INFO: Created: latency-svc-ghcnr
Jun 10 15:41:42.585: INFO: Created: latency-svc-9n5pj
Jun 10 15:41:42.589: INFO: Got endpoints: latency-svc-f72g9 [580.025283ms]
Jun 10 15:41:42.604: INFO: Created: latency-svc-xvnxx
Jun 10 15:41:42.636: INFO: Got endpoints: latency-svc-ghcnr [608.128197ms]
Jun 10 15:41:42.643: INFO: Created: latency-svc-lglhb
Jun 10 15:41:42.643: INFO: Created: latency-svc-mmpbk
Jun 10 15:41:42.660: INFO: Created: latency-svc-f4hxl
Jun 10 15:41:42.667: INFO: Created: latency-svc-h5pnr
Jun 10 15:41:42.686: INFO: Got endpoints: latency-svc-9n5pj [639.018361ms]
Jun 10 15:41:42.693: INFO: Created: latency-svc-j7jk6
Jun 10 15:41:42.704: INFO: Created: latency-svc-tn9p7
Jun 10 15:41:42.721: INFO: Created: latency-svc-88k89
Jun 10 15:41:42.750: INFO: Got endpoints: latency-svc-xvnxx [684.631782ms]
Jun 10 15:41:42.771: INFO: Created: latency-svc-9vggb
Jun 10 15:41:42.785: INFO: Created: latency-svc-ksj9p
Jun 10 15:41:42.795: INFO: Got endpoints: latency-svc-mmpbk [719.599594ms]
Jun 10 15:41:42.808: INFO: Created: latency-svc-v5d6l
Jun 10 15:41:42.825: INFO: Created: latency-svc-t5qsd
Jun 10 15:41:42.833: INFO: Got endpoints: latency-svc-lglhb [693.67192ms]
Jun 10 15:41:42.850: INFO: Created: latency-svc-dwz78
Jun 10 15:41:42.878: INFO: Created: latency-svc-25ttq
Jun 10 15:41:42.888: INFO: Got endpoints: latency-svc-f4hxl [596.83527ms]
Jun 10 15:41:42.903: INFO: Created: latency-svc-6mwks
Jun 10 15:41:42.914: INFO: Created: latency-svc-lqntw
Jun 10 15:41:42.927: INFO: Created: latency-svc-zbhdm
Jun 10 15:41:42.935: INFO: Got endpoints: latency-svc-h5pnr [640.092811ms]
Jun 10 15:41:42.955: INFO: Created: latency-svc-z28ld
Jun 10 15:41:42.963: INFO: Created: latency-svc-2sdd5
Jun 10 15:41:42.981: INFO: Created: latency-svc-g4sxx
Jun 10 15:41:42.986: INFO: Got endpoints: latency-svc-j7jk6 [657.922958ms]
Jun 10 15:41:42.996: INFO: Created: latency-svc-fkqpl
Jun 10 15:41:43.036: INFO: Got endpoints: latency-svc-tn9p7 [655.286071ms]
Jun 10 15:41:43.068: INFO: Created: latency-svc-tf9dk
Jun 10 15:41:43.082: INFO: Got endpoints: latency-svc-88k89 [647.932084ms]
Jun 10 15:41:43.112: INFO: Created: latency-svc-5mszm
Jun 10 15:41:43.139: INFO: Got endpoints: latency-svc-9vggb [667.836662ms]
Jun 10 15:41:43.172: INFO: Created: latency-svc-xvjb2
Jun 10 15:41:43.206: INFO: Got endpoints: latency-svc-ksj9p [691.402622ms]
Jun 10 15:41:43.247: INFO: Got endpoints: latency-svc-v5d6l [709.127836ms]
Jun 10 15:41:43.250: INFO: Created: latency-svc-d2z6k
Jun 10 15:41:43.302: INFO: Got endpoints: latency-svc-t5qsd [743.085467ms]
Jun 10 15:41:43.341: INFO: Got endpoints: latency-svc-dwz78 [752.338779ms]
Jun 10 15:41:43.385: INFO: Got endpoints: latency-svc-25ttq [749.131111ms]
Jun 10 15:41:43.440: INFO: Got endpoints: latency-svc-6mwks [753.972887ms]
Jun 10 15:41:43.487: INFO: Got endpoints: latency-svc-lqntw [736.981329ms]
Jun 10 15:41:43.536: INFO: Got endpoints: latency-svc-zbhdm [741.137306ms]
Jun 10 15:41:43.582: INFO: Got endpoints: latency-svc-z28ld [748.942027ms]
Jun 10 15:41:43.632: INFO: Got endpoints: latency-svc-2sdd5 [744.064849ms]
Jun 10 15:41:43.686: INFO: Got endpoints: latency-svc-g4sxx [750.563854ms]
Jun 10 15:41:43.734: INFO: Got endpoints: latency-svc-fkqpl [747.44651ms]
Jun 10 15:41:43.790: INFO: Got endpoints: latency-svc-tf9dk [754.183645ms]
Jun 10 15:41:43.845: INFO: Got endpoints: latency-svc-5mszm [763.186831ms]
Jun 10 15:41:43.889: INFO: Got endpoints: latency-svc-xvjb2 [750.252466ms]
Jun 10 15:41:43.941: INFO: Got endpoints: latency-svc-d2z6k [734.198493ms]
Jun 10 15:41:43.941: INFO: Latencies: [48.698085ms 141.44337ms 187.222663ms 264.134824ms 320.428182ms 353.641373ms 371.222997ms 371.726393ms 375.353286ms 380.140317ms 390.127635ms 400.545772ms 405.659938ms 415.020131ms 415.231514ms 417.920241ms 419.184473ms 421.79658ms 429.36048ms 431.142796ms 431.983776ms 438.090546ms 439.527107ms 443.8988ms 445.541294ms 452.299269ms 452.348856ms 458.73417ms 462.941521ms 468.036489ms 469.014405ms 470.000049ms 473.300582ms 481.157746ms 489.777289ms 490.404117ms 493.157373ms 498.522611ms 500.229769ms 501.850754ms 501.920501ms 505.472041ms 506.440881ms 514.658418ms 516.681311ms 522.052786ms 525.057379ms 527.030251ms 528.018181ms 528.386759ms 530.466996ms 534.250807ms 536.367291ms 536.648576ms 537.210217ms 540.267855ms 546.266889ms 552.847752ms 552.991993ms 558.328481ms 561.577686ms 562.821383ms 563.68247ms 563.81536ms 564.572286ms 564.804173ms 565.716919ms 566.437982ms 568.011418ms 569.961006ms 570.25128ms 570.771742ms 572.333656ms 572.417179ms 573.138972ms 576.304848ms 579.668973ms 579.920694ms 580.025283ms 583.713206ms 585.793123ms 590.056143ms 591.335746ms 596.83527ms 597.794157ms 598.387407ms 608.128197ms 611.392982ms 612.500264ms 620.467512ms 620.669787ms 631.957186ms 632.376481ms 632.875362ms 633.93942ms 639.018361ms 640.092811ms 642.987745ms 643.739659ms 647.932084ms 649.458749ms 655.286071ms 657.450057ms 657.922958ms 658.220907ms 659.665514ms 666.924576ms 667.74202ms 667.836662ms 684.631782ms 685.988001ms 686.127211ms 690.995262ms 691.402622ms 691.448275ms 693.67192ms 695.946587ms 696.403131ms 697.561125ms 703.070291ms 704.93883ms 706.228941ms 708.64682ms 709.127836ms 709.770231ms 710.231859ms 716.747476ms 719.599594ms 725.295916ms 729.494ms 733.442361ms 734.198493ms 735.950349ms 736.981329ms 739.089477ms 741.137306ms 743.085467ms 743.894157ms 744.064849ms 747.011474ms 747.44651ms 747.93759ms 747.997324ms 748.942027ms 749.131111ms 750.252466ms 750.563854ms 751.270908ms 752.338779ms 753.972887ms 754.183645ms 754.482567ms 757.278792ms 757.829657ms 758.206459ms 759.202715ms 763.186831ms 764.785445ms 765.138541ms 766.043993ms 769.962158ms 770.322495ms 778.17486ms 778.657584ms 782.792589ms 785.385589ms 786.114596ms 787.629056ms 794.516314ms 795.836415ms 798.493512ms 798.563336ms 799.743714ms 800.973105ms 801.791703ms 804.730495ms 806.674128ms 807.459656ms 811.1158ms 818.730914ms 820.778169ms 820.806302ms 823.022701ms 823.806408ms 826.451781ms 828.974676ms 832.212659ms 834.774675ms 842.059759ms 843.683452ms 851.818252ms 854.328491ms 857.768065ms 877.758898ms 883.593092ms 892.25726ms 893.066908ms 894.551017ms 898.047697ms 909.806779ms]
Jun 10 15:41:43.941: INFO: 50 %ile: 649.458749ms
Jun 10 15:41:43.941: INFO: 90 %ile: 820.778169ms
Jun 10 15:41:43.941: INFO: 99 %ile: 898.047697ms
Jun 10 15:41:43.941: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:41:43.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-306" for this suite.

• [SLOW TEST:15.246 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":305,"completed":8,"skipped":96,"failed":0}
SS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:41:43.962: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating pod
Jun 10 15:41:48.073: INFO: Pod pod-hostip-ea84d6b4-8155-46c3-bcb5-38751b990fa7 has hostIP: 172.17.0.5
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:41:48.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4217" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":305,"completed":9,"skipped":98,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:41:48.086: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:41:57.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3639" for this suite.
STEP: Destroying namespace "nsdeletetest-391" for this suite.
Jun 10 15:41:57.442: INFO: Namespace nsdeletetest-391 was already deleted
STEP: Destroying namespace "nsdeletetest-4537" for this suite.

• [SLOW TEST:9.400 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":305,"completed":10,"skipped":116,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:41:57.488: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-4363
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-4363
STEP: creating replication controller externalsvc in namespace services-4363
I0610 15:41:57.871970      26 runners.go:190] Created replication controller with name: externalsvc, namespace: services-4363, replica count: 2
I0610 15:42:00.922429      26 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0610 15:42:03.923240      26 runners.go:190] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0610 15:42:06.923424      26 runners.go:190] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0610 15:42:09.924698      26 runners.go:190] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0610 15:42:12.925177      26 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Jun 10 15:42:12.947: INFO: Creating new exec pod
Jun 10 15:42:14.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=services-4363 exec execpodtd6ss -- /bin/sh -x -c nslookup clusterip-service.services-4363.svc.cluster.local'
Jun 10 15:42:15.239: INFO: stderr: "+ nslookup clusterip-service.services-4363.svc.cluster.local\n"
Jun 10 15:42:15.239: INFO: stdout: "Server:\t\t10.128.0.10\nAddress:\t10.128.0.10#53\n\nclusterip-service.services-4363.svc.cluster.local\tcanonical name = externalsvc.services-4363.svc.cluster.local.\nName:\texternalsvc.services-4363.svc.cluster.local\nAddress: 10.140.235.100\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4363, will wait for the garbage collector to delete the pods
Jun 10 15:42:15.298: INFO: Deleting ReplicationController externalsvc took: 5.821996ms
Jun 10 15:42:15.798: INFO: Terminating ReplicationController externalsvc pods took: 500.265618ms
Jun 10 15:42:25.554: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:42:25.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4363" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:28.113 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":305,"completed":11,"skipped":145,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:42:25.602: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-1815
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Jun 10 15:42:25.722: INFO: Found 0 stateful pods, waiting for 3
Jun 10 15:42:35.726: INFO: Found 2 stateful pods, waiting for 3
Jun 10 15:42:45.991: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 10 15:42:45.991: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 10 15:42:45.991: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jun 10 15:42:55.727: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 10 15:42:55.727: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 10 15:42:55.727: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jun 10 15:42:55.762: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jun 10 15:43:05.795: INFO: Updating stateful set ss2
Jun 10 15:43:05.803: INFO: Waiting for Pod statefulset-1815/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 10 15:43:15.811: INFO: Waiting for Pod statefulset-1815/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Jun 10 15:43:26.336: INFO: Found 2 stateful pods, waiting for 3
Jun 10 15:43:36.345: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 10 15:43:36.345: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 10 15:43:36.345: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jun 10 15:43:36.375: INFO: Updating stateful set ss2
Jun 10 15:43:36.406: INFO: Waiting for Pod statefulset-1815/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 10 15:43:46.435: INFO: Updating stateful set ss2
Jun 10 15:43:46.448: INFO: Waiting for StatefulSet statefulset-1815/ss2 to complete update
Jun 10 15:43:46.448: INFO: Waiting for Pod statefulset-1815/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 10 15:43:56.453: INFO: Waiting for StatefulSet statefulset-1815/ss2 to complete update
Jun 10 15:43:56.453: INFO: Waiting for Pod statefulset-1815/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 10 15:44:06.483: INFO: Waiting for StatefulSet statefulset-1815/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jun 10 15:44:16.455: INFO: Deleting all statefulset in ns statefulset-1815
Jun 10 15:44:16.460: INFO: Scaling statefulset ss2 to 0
Jun 10 15:44:46.472: INFO: Waiting for statefulset status.replicas updated to 0
Jun 10 15:44:46.474: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:44:46.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1815" for this suite.

• [SLOW TEST:140.893 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":305,"completed":12,"skipped":165,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:44:46.497: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Jun 10 15:44:46.621: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
Jun 10 15:45:02.272: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:46:01.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2868" for this suite.

• [SLOW TEST:74.859 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":305,"completed":13,"skipped":242,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:46:01.358: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 10 15:46:01.438: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a98f1af8-a58e-432c-ac58-dfa157c6fd0d" in namespace "projected-9692" to be "Succeeded or Failed"
Jun 10 15:46:01.443: INFO: Pod "downwardapi-volume-a98f1af8-a58e-432c-ac58-dfa157c6fd0d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.775622ms
Jun 10 15:46:03.448: INFO: Pod "downwardapi-volume-a98f1af8-a58e-432c-ac58-dfa157c6fd0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009844376s
STEP: Saw pod success
Jun 10 15:46:03.448: INFO: Pod "downwardapi-volume-a98f1af8-a58e-432c-ac58-dfa157c6fd0d" satisfied condition "Succeeded or Failed"
Jun 10 15:46:03.451: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod downwardapi-volume-a98f1af8-a58e-432c-ac58-dfa157c6fd0d container client-container: <nil>
STEP: delete the pod
Jun 10 15:46:03.508: INFO: Waiting for pod downwardapi-volume-a98f1af8-a58e-432c-ac58-dfa157c6fd0d to disappear
Jun 10 15:46:03.513: INFO: Pod downwardapi-volume-a98f1af8-a58e-432c-ac58-dfa157c6fd0d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:46:03.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9692" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":305,"completed":14,"skipped":260,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:46:03.536: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating all guestbook components
Jun 10 15:46:03.594: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jun 10 15:46:03.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-6553 create -f -'
Jun 10 15:46:04.333: INFO: stderr: ""
Jun 10 15:46:04.333: INFO: stdout: "service/agnhost-replica created\n"
Jun 10 15:46:04.334: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jun 10 15:46:04.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-6553 create -f -'
Jun 10 15:46:05.046: INFO: stderr: ""
Jun 10 15:46:05.046: INFO: stdout: "service/agnhost-primary created\n"
Jun 10 15:46:05.046: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jun 10 15:46:05.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-6553 create -f -'
Jun 10 15:46:05.656: INFO: stderr: ""
Jun 10 15:46:05.656: INFO: stdout: "service/frontend created\n"
Jun 10 15:46:05.657: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jun 10 15:46:05.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-6553 create -f -'
Jun 10 15:46:06.170: INFO: stderr: ""
Jun 10 15:46:06.170: INFO: stdout: "deployment.apps/frontend created\n"
Jun 10 15:46:06.170: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun 10 15:46:06.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-6553 create -f -'
Jun 10 15:46:06.830: INFO: stderr: ""
Jun 10 15:46:06.830: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jun 10 15:46:06.830: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun 10 15:46:06.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-6553 create -f -'
Jun 10 15:46:07.711: INFO: stderr: ""
Jun 10 15:46:07.711: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Jun 10 15:46:07.711: INFO: Waiting for all frontend pods to be Running.
Jun 10 15:46:22.765: INFO: Waiting for frontend to serve content.
Jun 10 15:46:22.772: INFO: Trying to add a new entry to the guestbook.
Jun 10 15:46:22.779: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jun 10 15:46:22.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-6553 delete --grace-period=0 --force -f -'
Jun 10 15:46:22.971: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 10 15:46:22.971: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Jun 10 15:46:22.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-6553 delete --grace-period=0 --force -f -'
Jun 10 15:46:23.267: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 10 15:46:23.267: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jun 10 15:46:23.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-6553 delete --grace-period=0 --force -f -'
Jun 10 15:46:23.593: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 10 15:46:23.593: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun 10 15:46:23.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-6553 delete --grace-period=0 --force -f -'
Jun 10 15:46:23.830: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 10 15:46:23.830: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun 10 15:46:23.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-6553 delete --grace-period=0 --force -f -'
Jun 10 15:46:24.083: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 10 15:46:24.083: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jun 10 15:46:24.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-6553 delete --grace-period=0 --force -f -'
Jun 10 15:46:24.366: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 10 15:46:24.366: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:46:24.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6553" for this suite.

• [SLOW TEST:21.035 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:342
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":305,"completed":15,"skipped":265,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:46:24.570: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:46:24.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9597" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":305,"completed":16,"skipped":266,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:46:25.003: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun 10 15:46:25.395: INFO: Waiting up to 5m0s for pod "pod-51b898f8-c869-45c7-a0f6-276e4eb04922" in namespace "emptydir-5839" to be "Succeeded or Failed"
Jun 10 15:46:25.420: INFO: Pod "pod-51b898f8-c869-45c7-a0f6-276e4eb04922": Phase="Pending", Reason="", readiness=false. Elapsed: 24.964168ms
Jun 10 15:46:27.430: INFO: Pod "pod-51b898f8-c869-45c7-a0f6-276e4eb04922": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034916993s
Jun 10 15:46:29.434: INFO: Pod "pod-51b898f8-c869-45c7-a0f6-276e4eb04922": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039227506s
STEP: Saw pod success
Jun 10 15:46:29.434: INFO: Pod "pod-51b898f8-c869-45c7-a0f6-276e4eb04922" satisfied condition "Succeeded or Failed"
Jun 10 15:46:29.437: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-5pcx5 pod pod-51b898f8-c869-45c7-a0f6-276e4eb04922 container test-container: <nil>
STEP: delete the pod
Jun 10 15:46:29.487: INFO: Waiting for pod pod-51b898f8-c869-45c7-a0f6-276e4eb04922 to disappear
Jun 10 15:46:29.492: INFO: Pod pod-51b898f8-c869-45c7-a0f6-276e4eb04922 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:46:29.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5839" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":17,"skipped":290,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:46:29.506: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:46:42.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-972" for this suite.

• [SLOW TEST:13.164 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":305,"completed":18,"skipped":291,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:46:42.670: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service nodeport-test with type=NodePort in namespace services-8578
STEP: creating replication controller nodeport-test in namespace services-8578
I0610 15:46:42.779444      26 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-8578, replica count: 2
I0610 15:46:45.830455      26 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 10 15:46:45.830: INFO: Creating new exec pod
Jun 10 15:46:48.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=services-8578 exec execpoddltc5 -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Jun 10 15:46:49.219: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jun 10 15:46:49.219: INFO: stdout: ""
Jun 10 15:46:49.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=services-8578 exec execpoddltc5 -- /bin/sh -x -c nc -zv -t -w 2 10.131.165.196 80'
Jun 10 15:46:49.546: INFO: stderr: "+ nc -zv -t -w 2 10.131.165.196 80\nConnection to 10.131.165.196 80 port [tcp/http] succeeded!\n"
Jun 10 15:46:49.546: INFO: stdout: ""
Jun 10 15:46:49.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=services-8578 exec execpoddltc5 -- /bin/sh -x -c nc -zv -t -w 2 172.17.0.6 30559'
Jun 10 15:46:49.849: INFO: stderr: "+ nc -zv -t -w 2 172.17.0.6 30559\nConnection to 172.17.0.6 30559 port [tcp/30559] succeeded!\n"
Jun 10 15:46:49.849: INFO: stdout: ""
Jun 10 15:46:49.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=services-8578 exec execpoddltc5 -- /bin/sh -x -c nc -zv -t -w 2 172.17.0.4 30559'
Jun 10 15:46:50.108: INFO: stderr: "+ nc -zv -t -w 2 172.17.0.4 30559\nConnection to 172.17.0.4 30559 port [tcp/30559] succeeded!\n"
Jun 10 15:46:50.108: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:46:50.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8578" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:7.452 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":305,"completed":19,"skipped":360,"failed":0}
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:46:50.122: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 10 15:46:50.248: INFO: Create a RollingUpdate DaemonSet
Jun 10 15:46:50.258: INFO: Check that daemon pods launch on every node of the cluster
Jun 10 15:46:50.286: INFO: Number of nodes with available pods: 0
Jun 10 15:46:50.286: INFO: Node target-cluster-control-plane-8m52s is running more than one daemon pod
Jun 10 15:46:51.303: INFO: Number of nodes with available pods: 0
Jun 10 15:46:51.304: INFO: Node target-cluster-control-plane-8m52s is running more than one daemon pod
Jun 10 15:46:52.301: INFO: Number of nodes with available pods: 1
Jun 10 15:46:52.301: INFO: Node target-cluster-md-0-6b59c4f65-5pcx5 is running more than one daemon pod
Jun 10 15:46:53.307: INFO: Number of nodes with available pods: 1
Jun 10 15:46:53.307: INFO: Node target-cluster-md-0-6b59c4f65-5pcx5 is running more than one daemon pod
Jun 10 15:46:54.294: INFO: Number of nodes with available pods: 3
Jun 10 15:46:54.294: INFO: Number of running nodes: 3, number of available pods: 3
Jun 10 15:46:54.294: INFO: Update the DaemonSet to trigger a rollout
Jun 10 15:46:54.305: INFO: Updating DaemonSet daemon-set
Jun 10 15:47:06.324: INFO: Roll back the DaemonSet before rollout is complete
Jun 10 15:47:06.335: INFO: Updating DaemonSet daemon-set
Jun 10 15:47:06.336: INFO: Make sure DaemonSet rollback is complete
Jun 10 15:47:06.344: INFO: Wrong image for pod: daemon-set-lftpv. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 10 15:47:06.344: INFO: Pod daemon-set-lftpv is not available
Jun 10 15:47:07.356: INFO: Wrong image for pod: daemon-set-lftpv. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 10 15:47:07.356: INFO: Pod daemon-set-lftpv is not available
Jun 10 15:47:08.357: INFO: Pod daemon-set-8l87b is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2444, will wait for the garbage collector to delete the pods
Jun 10 15:47:08.431: INFO: Deleting DaemonSet.extensions daemon-set took: 7.768912ms
Jun 10 15:47:08.532: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.787307ms
Jun 10 15:47:19.136: INFO: Number of nodes with available pods: 0
Jun 10 15:47:19.136: INFO: Number of running nodes: 0, number of available pods: 0
Jun 10 15:47:19.139: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2444/daemonsets","resourceVersion":"11871"},"items":null}

Jun 10 15:47:19.141: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2444/pods","resourceVersion":"11871"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:47:19.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2444" for this suite.

• [SLOW TEST:29.053 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":305,"completed":20,"skipped":363,"failed":0}
S
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:47:19.175: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:47:19.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8128" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":305,"completed":21,"skipped":364,"failed":0}

------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:47:19.304: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Jun 10 15:47:19.348: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:47:26.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7391" for this suite.

• [SLOW TEST:7.505 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":305,"completed":22,"skipped":364,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:47:26.808: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 10 15:47:26.912: INFO: Waiting up to 5m0s for pod "downwardapi-volume-508a322e-2221-4332-b9b7-d92efcc4833f" in namespace "projected-4799" to be "Succeeded or Failed"
Jun 10 15:47:26.915: INFO: Pod "downwardapi-volume-508a322e-2221-4332-b9b7-d92efcc4833f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.211114ms
Jun 10 15:47:28.922: INFO: Pod "downwardapi-volume-508a322e-2221-4332-b9b7-d92efcc4833f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010220615s
STEP: Saw pod success
Jun 10 15:47:28.922: INFO: Pod "downwardapi-volume-508a322e-2221-4332-b9b7-d92efcc4833f" satisfied condition "Succeeded or Failed"
Jun 10 15:47:28.925: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-5pcx5 pod downwardapi-volume-508a322e-2221-4332-b9b7-d92efcc4833f container client-container: <nil>
STEP: delete the pod
Jun 10 15:47:28.946: INFO: Waiting for pod downwardapi-volume-508a322e-2221-4332-b9b7-d92efcc4833f to disappear
Jun 10 15:47:28.952: INFO: Pod downwardapi-volume-508a322e-2221-4332-b9b7-d92efcc4833f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:47:28.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4799" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":23,"skipped":370,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:47:28.963: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Jun 10 15:47:31.563: INFO: Successfully updated pod "labelsupdate496749a2-f8f1-4c08-841b-04c049d4398c"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:47:33.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-651" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":305,"completed":24,"skipped":378,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:47:33.604: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1385
STEP: creating an pod
Jun 10 15:47:33.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-2100 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.20 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Jun 10 15:47:33.794: INFO: stderr: ""
Jun 10 15:47:33.794: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Waiting for log generator to start.
Jun 10 15:47:33.794: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jun 10 15:47:33.794: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-2100" to be "running and ready, or succeeded"
Jun 10 15:47:33.814: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 19.359399ms
Jun 10 15:47:35.822: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.027547223s
Jun 10 15:47:35.822: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jun 10 15:47:35.822: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Jun 10 15:47:35.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-2100 logs logs-generator logs-generator'
Jun 10 15:47:35.958: INFO: stderr: ""
Jun 10 15:47:35.958: INFO: stdout: "I0610 15:47:34.700897       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/s2r 471\nI0610 15:47:34.901676       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/xhx 270\nI0610 15:47:35.101293       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/5wh 414\nI0610 15:47:35.301945       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/cxv 483\nI0610 15:47:35.501069       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/sn4k 209\nI0610 15:47:35.701024       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/cdd 446\nI0610 15:47:35.901489       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/hw5 271\n"
STEP: limiting log lines
Jun 10 15:47:35.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-2100 logs logs-generator logs-generator --tail=1'
Jun 10 15:47:36.120: INFO: stderr: ""
Jun 10 15:47:36.120: INFO: stdout: "I0610 15:47:36.101947       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/zmt 369\n"
Jun 10 15:47:36.120: INFO: got output "I0610 15:47:36.101947       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/zmt 369\n"
STEP: limiting log bytes
Jun 10 15:47:36.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-2100 logs logs-generator logs-generator --limit-bytes=1'
Jun 10 15:47:36.280: INFO: stderr: ""
Jun 10 15:47:36.280: INFO: stdout: "I"
Jun 10 15:47:36.280: INFO: got output "I"
STEP: exposing timestamps
Jun 10 15:47:36.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-2100 logs logs-generator logs-generator --tail=1 --timestamps'
Jun 10 15:47:36.418: INFO: stderr: ""
Jun 10 15:47:36.418: INFO: stdout: "2021-06-10T15:47:36.301639942Z I0610 15:47:36.301397       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/9kkb 274\n"
Jun 10 15:47:36.418: INFO: got output "2021-06-10T15:47:36.301639942Z I0610 15:47:36.301397       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/9kkb 274\n"
STEP: restricting to a time range
Jun 10 15:47:38.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-2100 logs logs-generator logs-generator --since=1s'
Jun 10 15:47:39.200: INFO: stderr: ""
Jun 10 15:47:39.200: INFO: stdout: "I0610 15:47:38.301121       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/29m 578\nI0610 15:47:38.500958       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/99b 405\nI0610 15:47:38.701054       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/nh2c 362\nI0610 15:47:38.901288       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/67v 264\nI0610 15:47:39.101570       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/default/pods/h6m4 276\n"
Jun 10 15:47:39.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-2100 logs logs-generator logs-generator --since=24h'
Jun 10 15:47:39.441: INFO: stderr: ""
Jun 10 15:47:39.441: INFO: stdout: "I0610 15:47:34.700897       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/s2r 471\nI0610 15:47:34.901676       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/xhx 270\nI0610 15:47:35.101293       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/5wh 414\nI0610 15:47:35.301945       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/cxv 483\nI0610 15:47:35.501069       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/sn4k 209\nI0610 15:47:35.701024       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/cdd 446\nI0610 15:47:35.901489       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/hw5 271\nI0610 15:47:36.101947       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/zmt 369\nI0610 15:47:36.301397       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/9kkb 274\nI0610 15:47:36.501120       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/mlv 263\nI0610 15:47:36.701596       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/fzk9 508\nI0610 15:47:36.902437       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/shg8 348\nI0610 15:47:37.101878       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/kc8 469\nI0610 15:47:37.301387       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/2sr 559\nI0610 15:47:37.501174       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/hl6 397\nI0610 15:47:37.701237       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/b274 356\nI0610 15:47:37.901168       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/7kk 271\nI0610 15:47:38.101032       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/kb2l 384\nI0610 15:47:38.301121       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/29m 578\nI0610 15:47:38.500958       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/99b 405\nI0610 15:47:38.701054       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/nh2c 362\nI0610 15:47:38.901288       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/67v 264\nI0610 15:47:39.101570       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/default/pods/h6m4 276\nI0610 15:47:39.301552       1 logs_generator.go:76] 23 POST /api/v1/namespaces/kube-system/pods/rjx 390\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1390
Jun 10 15:47:39.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-2100 delete pod logs-generator'
Jun 10 15:47:41.554: INFO: stderr: ""
Jun 10 15:47:41.554: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:47:41.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2100" for this suite.

• [SLOW TEST:7.964 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1382
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":305,"completed":25,"skipped":386,"failed":0}
SSS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:47:41.568: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pods
Jun 10 15:47:41.627: INFO: created test-pod-1
Jun 10 15:47:41.640: INFO: created test-pod-2
Jun 10 15:47:41.674: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:47:41.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2544" for this suite.
•{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":305,"completed":26,"skipped":389,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:47:41.835: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Jun 10 15:47:41.958: INFO: Waiting up to 5m0s for pod "downward-api-26138b23-f1d6-45f4-b25d-37ee483b9fdf" in namespace "downward-api-7495" to be "Succeeded or Failed"
Jun 10 15:47:41.962: INFO: Pod "downward-api-26138b23-f1d6-45f4-b25d-37ee483b9fdf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.698646ms
Jun 10 15:47:43.965: INFO: Pod "downward-api-26138b23-f1d6-45f4-b25d-37ee483b9fdf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006693838s
STEP: Saw pod success
Jun 10 15:47:43.965: INFO: Pod "downward-api-26138b23-f1d6-45f4-b25d-37ee483b9fdf" satisfied condition "Succeeded or Failed"
Jun 10 15:47:43.968: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod downward-api-26138b23-f1d6-45f4-b25d-37ee483b9fdf container dapi-container: <nil>
STEP: delete the pod
Jun 10 15:47:43.997: INFO: Waiting for pod downward-api-26138b23-f1d6-45f4-b25d-37ee483b9fdf to disappear
Jun 10 15:47:44.000: INFO: Pod downward-api-26138b23-f1d6-45f4-b25d-37ee483b9fdf no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:47:44.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7495" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":305,"completed":27,"skipped":401,"failed":0}
SS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:47:44.011: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jun 10 15:47:44.074: INFO: Pod name pod-release: Found 0 pods out of 1
Jun 10 15:47:49.078: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:47:50.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1672" for this suite.

• [SLOW TEST:6.103 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":305,"completed":28,"skipped":403,"failed":0}
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:47:50.114: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-2f660d55-f82d-43c0-8cba-fa348d8b4acf
STEP: Creating a pod to test consume secrets
Jun 10 15:47:50.168: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1b780170-c6fd-4a68-92d9-ed654425adb7" in namespace "projected-1264" to be "Succeeded or Failed"
Jun 10 15:47:50.171: INFO: Pod "pod-projected-secrets-1b780170-c6fd-4a68-92d9-ed654425adb7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.310232ms
Jun 10 15:47:52.177: INFO: Pod "pod-projected-secrets-1b780170-c6fd-4a68-92d9-ed654425adb7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009204213s
STEP: Saw pod success
Jun 10 15:47:52.177: INFO: Pod "pod-projected-secrets-1b780170-c6fd-4a68-92d9-ed654425adb7" satisfied condition "Succeeded or Failed"
Jun 10 15:47:52.183: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod pod-projected-secrets-1b780170-c6fd-4a68-92d9-ed654425adb7 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 10 15:47:52.216: INFO: Waiting for pod pod-projected-secrets-1b780170-c6fd-4a68-92d9-ed654425adb7 to disappear
Jun 10 15:47:52.220: INFO: Pod pod-projected-secrets-1b780170-c6fd-4a68-92d9-ed654425adb7 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:47:52.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1264" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":29,"skipped":406,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:47:52.243: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0610 15:47:53.021253      26 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jun 10 15:48:55.039: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:48:55.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9336" for this suite.

• [SLOW TEST:62.814 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":305,"completed":30,"skipped":410,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:48:55.057: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-c745ca4a-ee89-4793-aa37-1ff7f3c08659
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-c745ca4a-ee89-4793-aa37-1ff7f3c08659
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:48:59.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-51" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":31,"skipped":416,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:48:59.199: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jun 10 15:48:59.273: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jun 10 15:48:59.276: INFO: starting watch
STEP: patching
STEP: updating
Jun 10 15:48:59.293: INFO: waiting for watch events with expected annotations
Jun 10 15:48:59.293: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:48:59.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-5137" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":305,"completed":32,"skipped":436,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:48:59.379: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2420
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-2420
I0610 15:48:59.487132      26 runners.go:190] Created replication controller with name: externalname-service, namespace: services-2420, replica count: 2
Jun 10 15:49:02.537: INFO: Creating new exec pod
I0610 15:49:02.537750      26 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 10 15:49:05.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=services-2420 exec execpodzlq5l -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jun 10 15:49:05.876: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun 10 15:49:05.876: INFO: stdout: ""
Jun 10 15:49:05.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=services-2420 exec execpodzlq5l -- /bin/sh -x -c nc -zv -t -w 2 10.140.132.165 80'
Jun 10 15:49:06.143: INFO: stderr: "+ nc -zv -t -w 2 10.140.132.165 80\nConnection to 10.140.132.165 80 port [tcp/http] succeeded!\n"
Jun 10 15:49:06.143: INFO: stdout: ""
Jun 10 15:49:06.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=services-2420 exec execpodzlq5l -- /bin/sh -x -c nc -zv -t -w 2 172.17.0.5 32610'
Jun 10 15:49:06.468: INFO: stderr: "+ nc -zv -t -w 2 172.17.0.5 32610\nConnection to 172.17.0.5 32610 port [tcp/32610] succeeded!\n"
Jun 10 15:49:06.468: INFO: stdout: ""
Jun 10 15:49:06.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=services-2420 exec execpodzlq5l -- /bin/sh -x -c nc -zv -t -w 2 172.17.0.4 32610'
Jun 10 15:49:06.695: INFO: stderr: "+ nc -zv -t -w 2 172.17.0.4 32610\nConnection to 172.17.0.4 32610 port [tcp/32610] succeeded!\n"
Jun 10 15:49:06.695: INFO: stdout: ""
Jun 10 15:49:06.695: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:49:06.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2420" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:7.380 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":305,"completed":33,"skipped":443,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:49:06.759: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:49:06.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9714" for this suite.
STEP: Destroying namespace "nspatchtest-f227867c-3b5b-43e8-972d-4d286109d745-7251" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":305,"completed":34,"skipped":451,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:49:06.932: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Jun 10 15:49:11.579: INFO: Successfully updated pod "annotationupdated936a736-08ba-4021-bd1e-8efd40b03991"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:49:13.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4211" for this suite.

• [SLOW TEST:6.734 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":305,"completed":35,"skipped":465,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:49:13.667: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jun 10 15:49:13.762: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:49:25.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1613" for this suite.

• [SLOW TEST:11.800 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":305,"completed":36,"skipped":474,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:49:25.472: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-5d56829d-ca61-4980-8ae4-90444db34556
STEP: Creating a pod to test consume secrets
Jun 10 15:49:25.537: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d4bbb50d-2473-4d87-8f04-3d8e6f6e7d90" in namespace "projected-7741" to be "Succeeded or Failed"
Jun 10 15:49:25.541: INFO: Pod "pod-projected-secrets-d4bbb50d-2473-4d87-8f04-3d8e6f6e7d90": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054158ms
Jun 10 15:49:27.550: INFO: Pod "pod-projected-secrets-d4bbb50d-2473-4d87-8f04-3d8e6f6e7d90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013174212s
STEP: Saw pod success
Jun 10 15:49:27.550: INFO: Pod "pod-projected-secrets-d4bbb50d-2473-4d87-8f04-3d8e6f6e7d90" satisfied condition "Succeeded or Failed"
Jun 10 15:49:27.555: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod pod-projected-secrets-d4bbb50d-2473-4d87-8f04-3d8e6f6e7d90 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 10 15:49:27.586: INFO: Waiting for pod pod-projected-secrets-d4bbb50d-2473-4d87-8f04-3d8e6f6e7d90 to disappear
Jun 10 15:49:27.591: INFO: Pod pod-projected-secrets-d4bbb50d-2473-4d87-8f04-3d8e6f6e7d90 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:49:27.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7741" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":37,"skipped":488,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:49:27.619: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 10 15:49:27.703: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jun 10 15:49:28.798: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:49:28.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2496" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":305,"completed":38,"skipped":512,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:49:28.829: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun 10 15:49:28.905: INFO: Waiting up to 5m0s for pod "pod-b6dd4ddc-ce9e-45e9-af07-e7c592346a00" in namespace "emptydir-237" to be "Succeeded or Failed"
Jun 10 15:49:28.921: INFO: Pod "pod-b6dd4ddc-ce9e-45e9-af07-e7c592346a00": Phase="Pending", Reason="", readiness=false. Elapsed: 16.492974ms
Jun 10 15:49:30.930: INFO: Pod "pod-b6dd4ddc-ce9e-45e9-af07-e7c592346a00": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025312716s
STEP: Saw pod success
Jun 10 15:49:30.930: INFO: Pod "pod-b6dd4ddc-ce9e-45e9-af07-e7c592346a00" satisfied condition "Succeeded or Failed"
Jun 10 15:49:30.935: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod pod-b6dd4ddc-ce9e-45e9-af07-e7c592346a00 container test-container: <nil>
STEP: delete the pod
Jun 10 15:49:30.957: INFO: Waiting for pod pod-b6dd4ddc-ce9e-45e9-af07-e7c592346a00 to disappear
Jun 10 15:49:30.962: INFO: Pod pod-b6dd4ddc-ce9e-45e9-af07-e7c592346a00 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:49:30.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-237" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":39,"skipped":514,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:49:30.970: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 10 15:49:31.064: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8257bb45-7539-4af0-949d-d05dbc6ad931" in namespace "projected-8422" to be "Succeeded or Failed"
Jun 10 15:49:31.067: INFO: Pod "downwardapi-volume-8257bb45-7539-4af0-949d-d05dbc6ad931": Phase="Pending", Reason="", readiness=false. Elapsed: 3.129238ms
Jun 10 15:49:33.071: INFO: Pod "downwardapi-volume-8257bb45-7539-4af0-949d-d05dbc6ad931": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006672154s
Jun 10 15:49:35.076: INFO: Pod "downwardapi-volume-8257bb45-7539-4af0-949d-d05dbc6ad931": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011743981s
STEP: Saw pod success
Jun 10 15:49:35.076: INFO: Pod "downwardapi-volume-8257bb45-7539-4af0-949d-d05dbc6ad931" satisfied condition "Succeeded or Failed"
Jun 10 15:49:35.108: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod downwardapi-volume-8257bb45-7539-4af0-949d-d05dbc6ad931 container client-container: <nil>
STEP: delete the pod
Jun 10 15:49:35.160: INFO: Waiting for pod downwardapi-volume-8257bb45-7539-4af0-949d-d05dbc6ad931 to disappear
Jun 10 15:49:35.166: INFO: Pod downwardapi-volume-8257bb45-7539-4af0-949d-d05dbc6ad931 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:49:35.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8422" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":305,"completed":40,"skipped":520,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:49:35.204: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-3a36185d-3974-4c2f-b563-24f170b43e0a
STEP: Creating a pod to test consume configMaps
Jun 10 15:49:35.294: INFO: Waiting up to 5m0s for pod "pod-configmaps-792436a0-e06b-47d3-bc20-8c41b604df8b" in namespace "configmap-5545" to be "Succeeded or Failed"
Jun 10 15:49:35.303: INFO: Pod "pod-configmaps-792436a0-e06b-47d3-bc20-8c41b604df8b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.234893ms
Jun 10 15:49:37.308: INFO: Pod "pod-configmaps-792436a0-e06b-47d3-bc20-8c41b604df8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0142318s
STEP: Saw pod success
Jun 10 15:49:37.308: INFO: Pod "pod-configmaps-792436a0-e06b-47d3-bc20-8c41b604df8b" satisfied condition "Succeeded or Failed"
Jun 10 15:49:37.311: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod pod-configmaps-792436a0-e06b-47d3-bc20-8c41b604df8b container configmap-volume-test: <nil>
STEP: delete the pod
Jun 10 15:49:37.341: INFO: Waiting for pod pod-configmaps-792436a0-e06b-47d3-bc20-8c41b604df8b to disappear
Jun 10 15:49:37.350: INFO: Pod pod-configmaps-792436a0-e06b-47d3-bc20-8c41b604df8b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:49:37.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5545" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":305,"completed":41,"skipped":537,"failed":0}
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:49:37.361: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 10 15:49:37.503: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jun 10 15:49:37.521: INFO: Number of nodes with available pods: 0
Jun 10 15:49:37.521: INFO: Node target-cluster-control-plane-8m52s is running more than one daemon pod
Jun 10 15:49:38.530: INFO: Number of nodes with available pods: 0
Jun 10 15:49:38.530: INFO: Node target-cluster-control-plane-8m52s is running more than one daemon pod
Jun 10 15:49:39.527: INFO: Number of nodes with available pods: 1
Jun 10 15:49:39.527: INFO: Node target-cluster-control-plane-8m52s is running more than one daemon pod
Jun 10 15:49:40.594: INFO: Number of nodes with available pods: 1
Jun 10 15:49:40.594: INFO: Node target-cluster-control-plane-8m52s is running more than one daemon pod
Jun 10 15:49:41.540: INFO: Number of nodes with available pods: 3
Jun 10 15:49:41.540: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jun 10 15:49:41.576: INFO: Wrong image for pod: daemon-set-jz7dv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 15:49:41.576: INFO: Wrong image for pod: daemon-set-qx7tz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 15:49:41.576: INFO: Wrong image for pod: daemon-set-vjzpc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 15:49:42.590: INFO: Wrong image for pod: daemon-set-jz7dv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 15:49:42.590: INFO: Wrong image for pod: daemon-set-qx7tz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 15:49:42.590: INFO: Wrong image for pod: daemon-set-vjzpc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 15:49:42.590: INFO: Pod daemon-set-vjzpc is not available
Jun 10 15:49:43.598: INFO: Wrong image for pod: daemon-set-jz7dv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 15:49:43.598: INFO: Wrong image for pod: daemon-set-qx7tz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 15:49:43.598: INFO: Wrong image for pod: daemon-set-vjzpc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 15:49:43.598: INFO: Pod daemon-set-vjzpc is not available
Jun 10 15:49:44.593: INFO: Wrong image for pod: daemon-set-jz7dv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 15:49:44.593: INFO: Wrong image for pod: daemon-set-qx7tz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 15:49:44.593: INFO: Wrong image for pod: daemon-set-vjzpc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 15:49:44.593: INFO: Pod daemon-set-vjzpc is not available
Jun 10 15:49:45.599: INFO: Wrong image for pod: daemon-set-jz7dv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 15:49:45.599: INFO: Wrong image for pod: daemon-set-qx7tz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 15:49:45.599: INFO: Pod daemon-set-z72xx is not available
Jun 10 15:49:46.591: INFO: Wrong image for pod: daemon-set-jz7dv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 15:49:46.591: INFO: Wrong image for pod: daemon-set-qx7tz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 15:49:46.591: INFO: Pod daemon-set-z72xx is not available
Jun 10 15:49:47.591: INFO: Wrong image for pod: daemon-set-jz7dv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 15:49:47.591: INFO: Wrong image for pod: daemon-set-qx7tz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 15:49:48.590: INFO: Wrong image for pod: daemon-set-jz7dv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 15:49:48.590: INFO: Wrong image for pod: daemon-set-qx7tz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 15:49:48.590: INFO: Pod daemon-set-qx7tz is not available
Jun 10 15:49:49.590: INFO: Wrong image for pod: daemon-set-jz7dv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 15:49:49.590: INFO: Wrong image for pod: daemon-set-qx7tz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 15:49:49.590: INFO: Pod daemon-set-qx7tz is not available
Jun 10 15:49:50.591: INFO: Wrong image for pod: daemon-set-jz7dv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 15:49:50.591: INFO: Wrong image for pod: daemon-set-qx7tz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 15:49:50.591: INFO: Pod daemon-set-qx7tz is not available
Jun 10 15:49:51.590: INFO: Wrong image for pod: daemon-set-jz7dv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 15:49:51.590: INFO: Wrong image for pod: daemon-set-qx7tz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 15:49:51.590: INFO: Pod daemon-set-qx7tz is not available
Jun 10 15:49:52.591: INFO: Wrong image for pod: daemon-set-jz7dv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 15:49:52.591: INFO: Wrong image for pod: daemon-set-qx7tz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 15:49:52.591: INFO: Pod daemon-set-qx7tz is not available
Jun 10 15:49:53.591: INFO: Wrong image for pod: daemon-set-jz7dv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 15:49:53.591: INFO: Wrong image for pod: daemon-set-qx7tz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 15:49:53.591: INFO: Pod daemon-set-qx7tz is not available
Jun 10 15:49:54.591: INFO: Wrong image for pod: daemon-set-jz7dv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 15:49:54.591: INFO: Wrong image for pod: daemon-set-qx7tz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 15:49:54.591: INFO: Pod daemon-set-qx7tz is not available
Jun 10 15:49:55.594: INFO: Wrong image for pod: daemon-set-jz7dv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 15:49:55.594: INFO: Wrong image for pod: daemon-set-qx7tz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 15:49:55.594: INFO: Pod daemon-set-qx7tz is not available
Jun 10 15:49:56.591: INFO: Wrong image for pod: daemon-set-jz7dv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 15:49:56.591: INFO: Wrong image for pod: daemon-set-qx7tz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 15:49:56.591: INFO: Pod daemon-set-qx7tz is not available
Jun 10 15:49:57.591: INFO: Wrong image for pod: daemon-set-jz7dv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 15:49:57.591: INFO: Wrong image for pod: daemon-set-qx7tz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 15:49:57.591: INFO: Pod daemon-set-qx7tz is not available
Jun 10 15:49:58.592: INFO: Wrong image for pod: daemon-set-jz7dv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 15:49:58.592: INFO: Wrong image for pod: daemon-set-qx7tz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 15:49:58.592: INFO: Pod daemon-set-qx7tz is not available
Jun 10 15:49:59.593: INFO: Pod daemon-set-jfrtf is not available
Jun 10 15:49:59.593: INFO: Wrong image for pod: daemon-set-jz7dv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 15:50:00.603: INFO: Pod daemon-set-jfrtf is not available
Jun 10 15:50:00.603: INFO: Wrong image for pod: daemon-set-jz7dv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 15:50:01.592: INFO: Wrong image for pod: daemon-set-jz7dv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 15:50:02.593: INFO: Wrong image for pod: daemon-set-jz7dv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 15:50:02.593: INFO: Pod daemon-set-jz7dv is not available
Jun 10 15:50:03.591: INFO: Wrong image for pod: daemon-set-jz7dv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 15:50:03.591: INFO: Pod daemon-set-jz7dv is not available
Jun 10 15:50:04.591: INFO: Wrong image for pod: daemon-set-jz7dv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 15:50:04.591: INFO: Pod daemon-set-jz7dv is not available
Jun 10 15:50:05.591: INFO: Pod daemon-set-qntnt is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Jun 10 15:50:05.650: INFO: Number of nodes with available pods: 2
Jun 10 15:50:05.650: INFO: Node target-cluster-md-0-6b59c4f65-5pcx5 is running more than one daemon pod
Jun 10 15:50:06.658: INFO: Number of nodes with available pods: 2
Jun 10 15:50:06.658: INFO: Node target-cluster-md-0-6b59c4f65-5pcx5 is running more than one daemon pod
Jun 10 15:50:07.659: INFO: Number of nodes with available pods: 2
Jun 10 15:50:07.659: INFO: Node target-cluster-md-0-6b59c4f65-5pcx5 is running more than one daemon pod
Jun 10 15:50:08.656: INFO: Number of nodes with available pods: 3
Jun 10 15:50:08.656: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4065, will wait for the garbage collector to delete the pods
Jun 10 15:50:08.725: INFO: Deleting DaemonSet.extensions daemon-set took: 6.063798ms
Jun 10 15:50:10.225: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.50011135s
Jun 10 15:50:19.136: INFO: Number of nodes with available pods: 0
Jun 10 15:50:19.137: INFO: Number of running nodes: 0, number of available pods: 0
Jun 10 15:50:19.142: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4065/daemonsets","resourceVersion":"13904"},"items":null}

Jun 10 15:50:19.149: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4065/pods","resourceVersion":"13904"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:50:19.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4065" for this suite.

• [SLOW TEST:41.828 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":305,"completed":42,"skipped":542,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:50:19.189: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 10 15:50:19.227: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:50:19.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6536" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":305,"completed":43,"skipped":544,"failed":0}
S
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:50:19.814: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-8007, will wait for the garbage collector to delete the pods
Jun 10 15:50:23.945: INFO: Deleting Job.batch foo took: 5.501234ms
Jun 10 15:50:25.349: INFO: Terminating Job.batch foo pods took: 1.403567105s
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:51:05.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8007" for this suite.

• [SLOW TEST:45.648 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":305,"completed":44,"skipped":545,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:51:05.462: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 10 15:51:06.945: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:0, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758937066, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758937066, loc:(*time.Location)(0x770e980)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-cbccbf6bb\""}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 10 15:51:09.964: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:51:22.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7527" for this suite.
STEP: Destroying namespace "webhook-7527-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:16.798 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":305,"completed":45,"skipped":554,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:51:22.261: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-projected-6mw7
STEP: Creating a pod to test atomic-volume-subpath
Jun 10 15:51:22.350: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-6mw7" in namespace "subpath-8814" to be "Succeeded or Failed"
Jun 10 15:51:22.353: INFO: Pod "pod-subpath-test-projected-6mw7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.015856ms
Jun 10 15:51:24.364: INFO: Pod "pod-subpath-test-projected-6mw7": Phase="Running", Reason="", readiness=true. Elapsed: 2.014164344s
Jun 10 15:51:26.370: INFO: Pod "pod-subpath-test-projected-6mw7": Phase="Running", Reason="", readiness=true. Elapsed: 4.019700256s
Jun 10 15:51:28.375: INFO: Pod "pod-subpath-test-projected-6mw7": Phase="Running", Reason="", readiness=true. Elapsed: 6.024852631s
Jun 10 15:51:30.379: INFO: Pod "pod-subpath-test-projected-6mw7": Phase="Running", Reason="", readiness=true. Elapsed: 8.028650098s
Jun 10 15:51:32.383: INFO: Pod "pod-subpath-test-projected-6mw7": Phase="Running", Reason="", readiness=true. Elapsed: 10.032657139s
Jun 10 15:51:34.386: INFO: Pod "pod-subpath-test-projected-6mw7": Phase="Running", Reason="", readiness=true. Elapsed: 12.035506834s
Jun 10 15:51:36.389: INFO: Pod "pod-subpath-test-projected-6mw7": Phase="Running", Reason="", readiness=true. Elapsed: 14.039329979s
Jun 10 15:51:38.392: INFO: Pod "pod-subpath-test-projected-6mw7": Phase="Running", Reason="", readiness=true. Elapsed: 16.042292944s
Jun 10 15:51:40.396: INFO: Pod "pod-subpath-test-projected-6mw7": Phase="Running", Reason="", readiness=true. Elapsed: 18.046444015s
Jun 10 15:51:42.400: INFO: Pod "pod-subpath-test-projected-6mw7": Phase="Running", Reason="", readiness=true. Elapsed: 20.049829747s
Jun 10 15:51:44.416: INFO: Pod "pod-subpath-test-projected-6mw7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.06608598s
STEP: Saw pod success
Jun 10 15:51:44.416: INFO: Pod "pod-subpath-test-projected-6mw7" satisfied condition "Succeeded or Failed"
Jun 10 15:51:44.422: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod pod-subpath-test-projected-6mw7 container test-container-subpath-projected-6mw7: <nil>
STEP: delete the pod
Jun 10 15:51:44.453: INFO: Waiting for pod pod-subpath-test-projected-6mw7 to disappear
Jun 10 15:51:44.457: INFO: Pod pod-subpath-test-projected-6mw7 no longer exists
STEP: Deleting pod pod-subpath-test-projected-6mw7
Jun 10 15:51:44.458: INFO: Deleting pod "pod-subpath-test-projected-6mw7" in namespace "subpath-8814"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:51:44.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8814" for this suite.

• [SLOW TEST:22.203 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":305,"completed":46,"skipped":620,"failed":0}
SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:51:44.464: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-3311
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating statefulset ss in namespace statefulset-3311
Jun 10 15:51:44.527: INFO: Found 0 stateful pods, waiting for 1
Jun 10 15:51:54.531: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jun 10 15:51:54.546: INFO: Deleting all statefulset in ns statefulset-3311
Jun 10 15:51:54.553: INFO: Scaling statefulset ss to 0
Jun 10 15:52:14.602: INFO: Waiting for statefulset status.replicas updated to 0
Jun 10 15:52:14.604: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:52:14.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3311" for this suite.

• [SLOW TEST:30.173 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":305,"completed":47,"skipped":624,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:52:14.638: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun 10 15:52:14.740: INFO: Waiting up to 5m0s for pod "pod-f2102805-e5d2-4767-9b3c-7232f549ccf9" in namespace "emptydir-8681" to be "Succeeded or Failed"
Jun 10 15:52:14.745: INFO: Pod "pod-f2102805-e5d2-4767-9b3c-7232f549ccf9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.433792ms
Jun 10 15:52:16.748: INFO: Pod "pod-f2102805-e5d2-4767-9b3c-7232f549ccf9": Phase="Running", Reason="", readiness=true. Elapsed: 2.007485237s
Jun 10 15:52:18.762: INFO: Pod "pod-f2102805-e5d2-4767-9b3c-7232f549ccf9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022313764s
STEP: Saw pod success
Jun 10 15:52:18.762: INFO: Pod "pod-f2102805-e5d2-4767-9b3c-7232f549ccf9" satisfied condition "Succeeded or Failed"
Jun 10 15:52:18.771: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod pod-f2102805-e5d2-4767-9b3c-7232f549ccf9 container test-container: <nil>
STEP: delete the pod
Jun 10 15:52:18.821: INFO: Waiting for pod pod-f2102805-e5d2-4767-9b3c-7232f549ccf9 to disappear
Jun 10 15:52:18.826: INFO: Pod pod-f2102805-e5d2-4767-9b3c-7232f549ccf9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:52:18.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8681" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":48,"skipped":633,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:52:18.854: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun 10 15:52:18.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-73 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Jun 10 15:52:19.410: INFO: stderr: ""
Jun 10 15:52:19.410: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Jun 10 15:52:19.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-73 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "docker.io/library/busybox:1.29"}]}} --dry-run=server'
Jun 10 15:52:20.423: INFO: stderr: ""
Jun 10 15:52:20.424: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
Jun 10 15:52:20.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-73 delete pods e2e-test-httpd-pod'
Jun 10 15:52:35.433: INFO: stderr: ""
Jun 10 15:52:35.433: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:52:35.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-73" for this suite.

• [SLOW TEST:16.592 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:902
    should check if kubectl can dry-run update Pods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":305,"completed":49,"skipped":636,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:52:35.447: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Jun 10 15:52:37.516: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-7079 PodName:var-expansion-0ab3d7b0-fae9-4285-a9d6-c51f6ad6a667 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 10 15:52:37.516: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: test for file in mounted path
Jun 10 15:52:37.625: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-7079 PodName:var-expansion-0ab3d7b0-fae9-4285-a9d6-c51f6ad6a667 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 10 15:52:37.625: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: updating the annotation value
Jun 10 15:52:38.268: INFO: Successfully updated pod "var-expansion-0ab3d7b0-fae9-4285-a9d6-c51f6ad6a667"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Jun 10 15:52:38.287: INFO: Deleting pod "var-expansion-0ab3d7b0-fae9-4285-a9d6-c51f6ad6a667" in namespace "var-expansion-7079"
Jun 10 15:52:38.304: INFO: Wait up to 5m0s for pod "var-expansion-0ab3d7b0-fae9-4285-a9d6-c51f6ad6a667" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:53:12.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7079" for this suite.

• [SLOW TEST:36.892 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":305,"completed":50,"skipped":661,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:53:12.339: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Jun 10 15:53:14.940: INFO: Successfully updated pod "annotationupdatebc167190-9fd5-4d9a-8bbb-f42f8791dc9f"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:53:16.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-48" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":305,"completed":51,"skipped":682,"failed":0}
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:53:16.961: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jun 10 15:53:16.991: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 10 15:53:16.998: INFO: Waiting for terminating namespaces to be deleted...
Jun 10 15:53:17.001: INFO: 
Logging pods the apiserver thinks is on node target-cluster-control-plane-8m52s before test
Jun 10 15:53:17.013: INFO: calico-kube-controllers-7c5d656c49-x6mc4 from calico-system started at 2021-06-10 15:29:10 +0000 UTC (1 container statuses recorded)
Jun 10 15:53:17.013: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 10 15:53:17.013: INFO: calico-node-phlfp from calico-system started at 2021-06-10 15:28:49 +0000 UTC (1 container statuses recorded)
Jun 10 15:53:17.013: INFO: 	Container calico-node ready: true, restart count 0
Jun 10 15:53:17.013: INFO: calico-typha-5d4587dc9c-7g8gs from calico-system started at 2021-06-10 15:28:49 +0000 UTC (1 container statuses recorded)
Jun 10 15:53:17.013: INFO: 	Container calico-typha ready: true, restart count 0
Jun 10 15:53:17.013: INFO: capd-controller-manager-7c89f6ddd7-dc686 from capd-system started at 2021-06-10 15:31:47 +0000 UTC (2 container statuses recorded)
Jun 10 15:53:17.013: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 10 15:53:17.013: INFO: 	Container manager ready: true, restart count 0
Jun 10 15:53:17.013: INFO: capi-kubeadm-bootstrap-controller-manager-6f669ccd7c-gtbcn from capi-kubeadm-bootstrap-system started at 2021-06-10 15:31:34 +0000 UTC (2 container statuses recorded)
Jun 10 15:53:17.013: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 10 15:53:17.013: INFO: 	Container manager ready: true, restart count 0
Jun 10 15:53:17.013: INFO: capi-kubeadm-control-plane-controller-manager-5c95f59c5c-l4jvx from capi-kubeadm-control-plane-system started at 2021-06-10 15:31:42 +0000 UTC (2 container statuses recorded)
Jun 10 15:53:17.013: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 10 15:53:17.013: INFO: 	Container manager ready: true, restart count 0
Jun 10 15:53:17.013: INFO: capi-controller-manager-5f677d7d65-tzt7h from capi-system started at 2021-06-10 15:31:31 +0000 UTC (2 container statuses recorded)
Jun 10 15:53:17.013: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 10 15:53:17.013: INFO: 	Container manager ready: true, restart count 0
Jun 10 15:53:17.013: INFO: capi-controller-manager-745689557d-77m69 from capi-webhook-system started at 2021-06-10 15:31:29 +0000 UTC (2 container statuses recorded)
Jun 10 15:53:17.013: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 10 15:53:17.013: INFO: 	Container manager ready: true, restart count 0
Jun 10 15:53:17.013: INFO: capi-kubeadm-bootstrap-controller-manager-6949f44db8-hn6kq from capi-webhook-system started at 2021-06-10 15:31:31 +0000 UTC (2 container statuses recorded)
Jun 10 15:53:17.013: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 10 15:53:17.013: INFO: 	Container manager ready: true, restart count 0
Jun 10 15:53:17.013: INFO: capi-kubeadm-control-plane-controller-manager-7b6c4bf48d-x8pzk from capi-webhook-system started at 2021-06-10 15:31:35 +0000 UTC (2 container statuses recorded)
Jun 10 15:53:17.013: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 10 15:53:17.013: INFO: 	Container manager ready: true, restart count 0
Jun 10 15:53:17.013: INFO: cert-manager-768bf64dd4-4lhmj from cert-manager started at 2021-06-10 15:29:10 +0000 UTC (1 container statuses recorded)
Jun 10 15:53:17.013: INFO: 	Container cert-manager ready: true, restart count 0
Jun 10 15:53:17.013: INFO: cert-manager-cainjector-646879549c-pbn4t from cert-manager started at 2021-06-10 15:29:10 +0000 UTC (1 container statuses recorded)
Jun 10 15:53:17.013: INFO: 	Container cert-manager ready: true, restart count 0
Jun 10 15:53:17.014: INFO: cert-manager-webhook-6dc9ccc9fb-6gg4f from cert-manager started at 2021-06-10 15:29:10 +0000 UTC (1 container statuses recorded)
Jun 10 15:53:17.014: INFO: 	Container cert-manager ready: true, restart count 0
Jun 10 15:53:17.014: INFO: coredns-f9fd979d6-2dcsm from kube-system started at 2021-06-10 15:29:10 +0000 UTC (1 container statuses recorded)
Jun 10 15:53:17.014: INFO: 	Container coredns ready: true, restart count 0
Jun 10 15:53:17.014: INFO: coredns-f9fd979d6-zsjpx from kube-system started at 2021-06-10 15:29:21 +0000 UTC (1 container statuses recorded)
Jun 10 15:53:17.014: INFO: 	Container coredns ready: true, restart count 0
Jun 10 15:53:17.014: INFO: etcd-target-cluster-control-plane-8m52s from kube-system started at 2021-06-10 15:22:59 +0000 UTC (1 container statuses recorded)
Jun 10 15:53:17.014: INFO: 	Container etcd ready: true, restart count 0
Jun 10 15:53:17.014: INFO: kube-apiserver-target-cluster-control-plane-8m52s from kube-system started at 2021-06-10 15:22:59 +0000 UTC (1 container statuses recorded)
Jun 10 15:53:17.014: INFO: 	Container kube-apiserver ready: true, restart count 0
Jun 10 15:53:17.014: INFO: kube-controller-manager-target-cluster-control-plane-8m52s from kube-system started at 2021-06-10 15:22:59 +0000 UTC (1 container statuses recorded)
Jun 10 15:53:17.014: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jun 10 15:53:17.014: INFO: kube-proxy-hz9zf from kube-system started at 2021-06-10 15:23:04 +0000 UTC (1 container statuses recorded)
Jun 10 15:53:17.014: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 10 15:53:17.014: INFO: kube-scheduler-target-cluster-control-plane-8m52s from kube-system started at 2021-06-10 15:22:59 +0000 UTC (1 container statuses recorded)
Jun 10 15:53:17.014: INFO: 	Container kube-scheduler ready: true, restart count 0
Jun 10 15:53:17.014: INFO: sonobuoy-systemd-logs-daemon-set-0467f197b1c74b53-bs8vn from sonobuoy started at 2021-06-10 15:38:55 +0000 UTC (2 container statuses recorded)
Jun 10 15:53:17.014: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 10 15:53:17.014: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 10 15:53:17.014: INFO: tigera-operator-5b76777d49-9zvzd from tigera-operator started at 2021-06-10 15:28:37 +0000 UTC (1 container statuses recorded)
Jun 10 15:53:17.014: INFO: 	Container tigera-operator ready: true, restart count 0
Jun 10 15:53:17.014: INFO: 
Logging pods the apiserver thinks is on node target-cluster-md-0-6b59c4f65-5pcx5 before test
Jun 10 15:53:17.020: INFO: calico-node-dvbdj from calico-system started at 2021-06-10 15:34:05 +0000 UTC (1 container statuses recorded)
Jun 10 15:53:17.020: INFO: 	Container calico-node ready: true, restart count 0
Jun 10 15:53:17.020: INFO: calico-typha-5d4587dc9c-lq9tq from calico-system started at 2021-06-10 15:34:47 +0000 UTC (1 container statuses recorded)
Jun 10 15:53:17.020: INFO: 	Container calico-typha ready: true, restart count 0
Jun 10 15:53:17.020: INFO: kube-proxy-dklkq from kube-system started at 2021-06-10 15:34:05 +0000 UTC (1 container statuses recorded)
Jun 10 15:53:17.020: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 10 15:53:17.020: INFO: sonobuoy-e2e-job-04c20f7a477e40c3 from sonobuoy started at 2021-06-10 15:38:55 +0000 UTC (2 container statuses recorded)
Jun 10 15:53:17.020: INFO: 	Container e2e ready: true, restart count 0
Jun 10 15:53:17.020: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 10 15:53:17.020: INFO: sonobuoy-systemd-logs-daemon-set-0467f197b1c74b53-gj6jj from sonobuoy started at 2021-06-10 15:38:55 +0000 UTC (2 container statuses recorded)
Jun 10 15:53:17.020: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 10 15:53:17.020: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 10 15:53:17.020: INFO: 
Logging pods the apiserver thinks is on node target-cluster-md-0-6b59c4f65-cqpjz before test
Jun 10 15:53:17.030: INFO: calico-node-j7mxn from calico-system started at 2021-06-10 15:34:05 +0000 UTC (1 container statuses recorded)
Jun 10 15:53:17.030: INFO: 	Container calico-node ready: true, restart count 0
Jun 10 15:53:17.030: INFO: calico-typha-5d4587dc9c-ckl68 from calico-system started at 2021-06-10 15:34:47 +0000 UTC (1 container statuses recorded)
Jun 10 15:53:17.030: INFO: 	Container calico-typha ready: true, restart count 0
Jun 10 15:53:17.030: INFO: kube-proxy-mc9rx from kube-system started at 2021-06-10 15:34:05 +0000 UTC (1 container statuses recorded)
Jun 10 15:53:17.030: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 10 15:53:17.030: INFO: annotationupdatebc167190-9fd5-4d9a-8bbb-f42f8791dc9f from projected-48 started at 2021-06-10 15:53:12 +0000 UTC (1 container statuses recorded)
Jun 10 15:53:17.030: INFO: 	Container client-container ready: true, restart count 0
Jun 10 15:53:17.030: INFO: sonobuoy from sonobuoy started at 2021-06-10 15:38:49 +0000 UTC (1 container statuses recorded)
Jun 10 15:53:17.030: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 10 15:53:17.030: INFO: sonobuoy-systemd-logs-daemon-set-0467f197b1c74b53-cf9lb from sonobuoy started at 2021-06-10 15:38:55 +0000 UTC (2 container statuses recorded)
Jun 10 15:53:17.030: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 10 15:53:17.030: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1687437d846fcb8d], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:53:18.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2133" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":305,"completed":52,"skipped":691,"failed":0}
SSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:53:18.091: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 10 15:55:18.214: INFO: Deleting pod "var-expansion-b8c37c28-b81b-4509-9d4c-7a574601471b" in namespace "var-expansion-7666"
Jun 10 15:55:18.218: INFO: Wait up to 5m0s for pod "var-expansion-b8c37c28-b81b-4509-9d4c-7a574601471b" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:55:20.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7666" for this suite.

• [SLOW TEST:122.170 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":305,"completed":53,"skipped":694,"failed":0}
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:55:20.262: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1307
STEP: creating the pod
Jun 10 15:55:20.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-3388 create -f -'
Jun 10 15:55:20.843: INFO: stderr: ""
Jun 10 15:55:20.843: INFO: stdout: "pod/pause created\n"
Jun 10 15:55:20.843: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jun 10 15:55:20.843: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-3388" to be "running and ready"
Jun 10 15:55:20.864: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 20.701972ms
Jun 10 15:55:22.868: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.024475008s
Jun 10 15:55:22.868: INFO: Pod "pause" satisfied condition "running and ready"
Jun 10 15:55:22.868: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: adding the label testing-label with value testing-label-value to a pod
Jun 10 15:55:22.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-3388 label pods pause testing-label=testing-label-value'
Jun 10 15:55:23.043: INFO: stderr: ""
Jun 10 15:55:23.043: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jun 10 15:55:23.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-3388 get pod pause -L testing-label'
Jun 10 15:55:23.177: INFO: stderr: ""
Jun 10 15:55:23.177: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jun 10 15:55:23.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-3388 label pods pause testing-label-'
Jun 10 15:55:23.354: INFO: stderr: ""
Jun 10 15:55:23.354: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jun 10 15:55:23.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-3388 get pod pause -L testing-label'
Jun 10 15:55:23.507: INFO: stderr: ""
Jun 10 15:55:23.507: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1313
STEP: using delete to clean up resources
Jun 10 15:55:23.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-3388 delete --grace-period=0 --force -f -'
Jun 10 15:55:23.686: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 10 15:55:23.686: INFO: stdout: "pod \"pause\" force deleted\n"
Jun 10 15:55:23.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-3388 get rc,svc -l name=pause --no-headers'
Jun 10 15:55:23.924: INFO: stderr: "No resources found in kubectl-3388 namespace.\n"
Jun 10 15:55:23.924: INFO: stdout: ""
Jun 10 15:55:23.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-3388 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 10 15:55:24.153: INFO: stderr: ""
Jun 10 15:55:24.153: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:55:24.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3388" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":305,"completed":54,"skipped":694,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:55:24.167: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 10 15:55:26.409: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 10 15:55:28.427: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758937326, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758937326, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758937326, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758937326, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 10 15:55:31.459: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Jun 10 15:55:31.480: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:55:31.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8794" for this suite.
STEP: Destroying namespace "webhook-8794-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.473 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":305,"completed":55,"skipped":709,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:55:31.640: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0610 15:56:11.774268      26 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jun 10 15:57:13.785: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
Jun 10 15:57:13.785: INFO: Deleting pod "simpletest.rc-8lbfm" in namespace "gc-5814"
Jun 10 15:57:13.800: INFO: Deleting pod "simpletest.rc-962ws" in namespace "gc-5814"
Jun 10 15:57:13.813: INFO: Deleting pod "simpletest.rc-9tvjj" in namespace "gc-5814"
Jun 10 15:57:13.850: INFO: Deleting pod "simpletest.rc-bbzb8" in namespace "gc-5814"
Jun 10 15:57:13.903: INFO: Deleting pod "simpletest.rc-f88gd" in namespace "gc-5814"
Jun 10 15:57:13.948: INFO: Deleting pod "simpletest.rc-ffggb" in namespace "gc-5814"
Jun 10 15:57:14.119: INFO: Deleting pod "simpletest.rc-nkr6k" in namespace "gc-5814"
Jun 10 15:57:14.213: INFO: Deleting pod "simpletest.rc-r8tnw" in namespace "gc-5814"
Jun 10 15:57:14.314: INFO: Deleting pod "simpletest.rc-sqmvl" in namespace "gc-5814"
Jun 10 15:57:14.420: INFO: Deleting pod "simpletest.rc-tftk4" in namespace "gc-5814"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:57:14.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5814" for this suite.

• [SLOW TEST:102.902 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":305,"completed":56,"skipped":718,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:57:14.543: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jun 10 15:57:20.959: INFO: &Pod{ObjectMeta:{send-events-7b7dd408-aaa6-4faa-b88a-c3c58b6e182e  events-8343 /api/v1/namespaces/events-8343/pods/send-events-7b7dd408-aaa6-4faa-b88a-c3c58b6e182e 57b28726-887d-4e79-8ede-28f341b4f013 17075 0 2021-06-10 15:57:14 +0000 UTC <nil> <nil> map[name:foo time:792569288] map[cni.projectcalico.org/podIP:192.168.78.118/32 cni.projectcalico.org/podIPs:192.168.78.118/32] [] []  [{e2e.test Update v1 2021-06-10 15:57:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-10 15:57:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-06-10 15:57:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.78.118\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-xh757,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-xh757,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-xh757,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:target-cluster-md-0-6b59c4f65-cqpjz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 15:57:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 15:57:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 15:57:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 15:57:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.17.0.5,PodIP:192.168.78.118,StartTime:2021-06-10 15:57:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-10 15:57:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:containerd://29de39152780138460c114b516f5feaa61dd5e45440e80db480b5d7d3f3d893c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.78.118,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Jun 10 15:57:22.972: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jun 10 15:57:24.975: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:57:24.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8343" for this suite.

• [SLOW TEST:10.453 seconds]
[k8s.io] [sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":305,"completed":57,"skipped":743,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:57:24.997: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 10 15:57:25.026: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:57:32.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5416" for this suite.

• [SLOW TEST:7.212 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":305,"completed":58,"skipped":751,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:57:32.209: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 10 15:57:32.265: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:57:33.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9173" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":305,"completed":59,"skipped":754,"failed":0}
S
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:57:33.314: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-667.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-667.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-667.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-667.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-667.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-667.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-667.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-667.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-667.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-667.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-667.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-667.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-667.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-667.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-667.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-667.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-667.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-667.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 10 15:57:53.528: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-667.svc.cluster.local from pod dns-667/dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d: the server could not find the requested resource (get pods dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d)
Jun 10 15:57:53.532: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-667.svc.cluster.local from pod dns-667/dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d: the server could not find the requested resource (get pods dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d)
Jun 10 15:57:53.537: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-667.svc.cluster.local from pod dns-667/dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d: the server could not find the requested resource (get pods dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d)
Jun 10 15:57:53.540: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-667.svc.cluster.local from pod dns-667/dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d: the server could not find the requested resource (get pods dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d)
Jun 10 15:57:53.551: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-667.svc.cluster.local from pod dns-667/dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d: the server could not find the requested resource (get pods dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d)
Jun 10 15:57:53.555: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-667.svc.cluster.local from pod dns-667/dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d: the server could not find the requested resource (get pods dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d)
Jun 10 15:57:53.561: INFO: Unable to read jessie_udp@dns-test-service-2.dns-667.svc.cluster.local from pod dns-667/dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d: the server could not find the requested resource (get pods dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d)
Jun 10 15:57:53.564: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-667.svc.cluster.local from pod dns-667/dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d: the server could not find the requested resource (get pods dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d)
Jun 10 15:57:53.571: INFO: Lookups using dns-667/dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-667.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-667.svc.cluster.local wheezy_udp@dns-test-service-2.dns-667.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-667.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-667.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-667.svc.cluster.local jessie_udp@dns-test-service-2.dns-667.svc.cluster.local jessie_tcp@dns-test-service-2.dns-667.svc.cluster.local]

Jun 10 15:57:58.577: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-667.svc.cluster.local from pod dns-667/dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d: the server could not find the requested resource (get pods dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d)
Jun 10 15:57:58.581: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-667.svc.cluster.local from pod dns-667/dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d: the server could not find the requested resource (get pods dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d)
Jun 10 15:57:58.585: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-667.svc.cluster.local from pod dns-667/dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d: the server could not find the requested resource (get pods dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d)
Jun 10 15:57:58.587: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-667.svc.cluster.local from pod dns-667/dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d: the server could not find the requested resource (get pods dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d)
Jun 10 15:57:58.598: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-667.svc.cluster.local from pod dns-667/dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d: the server could not find the requested resource (get pods dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d)
Jun 10 15:57:58.602: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-667.svc.cluster.local from pod dns-667/dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d: the server could not find the requested resource (get pods dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d)
Jun 10 15:57:58.604: INFO: Unable to read jessie_udp@dns-test-service-2.dns-667.svc.cluster.local from pod dns-667/dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d: the server could not find the requested resource (get pods dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d)
Jun 10 15:57:58.607: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-667.svc.cluster.local from pod dns-667/dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d: the server could not find the requested resource (get pods dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d)
Jun 10 15:57:58.611: INFO: Lookups using dns-667/dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-667.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-667.svc.cluster.local wheezy_udp@dns-test-service-2.dns-667.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-667.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-667.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-667.svc.cluster.local jessie_udp@dns-test-service-2.dns-667.svc.cluster.local jessie_tcp@dns-test-service-2.dns-667.svc.cluster.local]

Jun 10 15:58:03.576: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-667.svc.cluster.local from pod dns-667/dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d: the server could not find the requested resource (get pods dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d)
Jun 10 15:58:03.582: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-667.svc.cluster.local from pod dns-667/dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d: the server could not find the requested resource (get pods dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d)
Jun 10 15:58:03.587: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-667.svc.cluster.local from pod dns-667/dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d: the server could not find the requested resource (get pods dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d)
Jun 10 15:58:03.592: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-667.svc.cluster.local from pod dns-667/dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d: the server could not find the requested resource (get pods dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d)
Jun 10 15:58:03.605: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-667.svc.cluster.local from pod dns-667/dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d: the server could not find the requested resource (get pods dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d)
Jun 10 15:58:03.609: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-667.svc.cluster.local from pod dns-667/dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d: the server could not find the requested resource (get pods dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d)
Jun 10 15:58:03.613: INFO: Unable to read jessie_udp@dns-test-service-2.dns-667.svc.cluster.local from pod dns-667/dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d: the server could not find the requested resource (get pods dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d)
Jun 10 15:58:03.617: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-667.svc.cluster.local from pod dns-667/dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d: the server could not find the requested resource (get pods dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d)
Jun 10 15:58:03.624: INFO: Lookups using dns-667/dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-667.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-667.svc.cluster.local wheezy_udp@dns-test-service-2.dns-667.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-667.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-667.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-667.svc.cluster.local jessie_udp@dns-test-service-2.dns-667.svc.cluster.local jessie_tcp@dns-test-service-2.dns-667.svc.cluster.local]

Jun 10 15:58:08.614: INFO: DNS probes using dns-667/dns-test-735c0f5c-d185-415a-988f-9e68f44fe11d succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:58:08.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-667" for this suite.

• [SLOW TEST:35.461 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":305,"completed":60,"skipped":755,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:58:08.776: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0610 15:58:19.343390      26 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jun 10 15:59:21.390: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
Jun 10 15:59:21.390: INFO: Deleting pod "simpletest-rc-to-be-deleted-48g8n" in namespace "gc-3863"
Jun 10 15:59:21.418: INFO: Deleting pod "simpletest-rc-to-be-deleted-78htk" in namespace "gc-3863"
Jun 10 15:59:21.464: INFO: Deleting pod "simpletest-rc-to-be-deleted-b9hmm" in namespace "gc-3863"
Jun 10 15:59:21.498: INFO: Deleting pod "simpletest-rc-to-be-deleted-fksvf" in namespace "gc-3863"
Jun 10 15:59:21.579: INFO: Deleting pod "simpletest-rc-to-be-deleted-k8hbl" in namespace "gc-3863"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:59:21.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3863" for this suite.

• [SLOW TEST:73.013 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":305,"completed":61,"skipped":760,"failed":0}
SSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:59:21.790: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:59:22.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-6867" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":305,"completed":62,"skipped":766,"failed":0}

------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:59:22.115: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-6848
STEP: creating service affinity-clusterip-transition in namespace services-6848
STEP: creating replication controller affinity-clusterip-transition in namespace services-6848
I0610 15:59:22.424985      26 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-6848, replica count: 3
I0610 15:59:25.476746      26 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0610 15:59:28.476927      26 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 10 15:59:28.485: INFO: Creating new exec pod
Jun 10 15:59:31.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=services-6848 exec execpod-affinitywst8v -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
Jun 10 15:59:31.756: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jun 10 15:59:31.756: INFO: stdout: ""
Jun 10 15:59:31.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=services-6848 exec execpod-affinitywst8v -- /bin/sh -x -c nc -zv -t -w 2 10.131.224.223 80'
Jun 10 15:59:32.040: INFO: stderr: "+ nc -zv -t -w 2 10.131.224.223 80\nConnection to 10.131.224.223 80 port [tcp/http] succeeded!\n"
Jun 10 15:59:32.040: INFO: stdout: ""
Jun 10 15:59:32.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=services-6848 exec execpod-affinitywst8v -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.131.224.223:80/ ; done'
Jun 10 15:59:32.635: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.131.224.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.131.224.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.131.224.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.131.224.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.131.224.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.131.224.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.131.224.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.131.224.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.131.224.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.131.224.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.131.224.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.131.224.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.131.224.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.131.224.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.131.224.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.131.224.223:80/\n"
Jun 10 15:59:32.635: INFO: stdout: "\naffinity-clusterip-transition-gqjf7\naffinity-clusterip-transition-t54wk\naffinity-clusterip-transition-t54wk\naffinity-clusterip-transition-t54wk\naffinity-clusterip-transition-gqjf7\naffinity-clusterip-transition-gqjf7\naffinity-clusterip-transition-gqjf7\naffinity-clusterip-transition-t54wk\naffinity-clusterip-transition-gqjf7\naffinity-clusterip-transition-gqjf7\naffinity-clusterip-transition-gqjf7\naffinity-clusterip-transition-t54wk\naffinity-clusterip-transition-ppjjs\naffinity-clusterip-transition-t54wk\naffinity-clusterip-transition-gqjf7\naffinity-clusterip-transition-t54wk"
Jun 10 15:59:32.635: INFO: Received response from host: affinity-clusterip-transition-gqjf7
Jun 10 15:59:32.635: INFO: Received response from host: affinity-clusterip-transition-t54wk
Jun 10 15:59:32.635: INFO: Received response from host: affinity-clusterip-transition-t54wk
Jun 10 15:59:32.635: INFO: Received response from host: affinity-clusterip-transition-t54wk
Jun 10 15:59:32.635: INFO: Received response from host: affinity-clusterip-transition-gqjf7
Jun 10 15:59:32.635: INFO: Received response from host: affinity-clusterip-transition-gqjf7
Jun 10 15:59:32.635: INFO: Received response from host: affinity-clusterip-transition-gqjf7
Jun 10 15:59:32.635: INFO: Received response from host: affinity-clusterip-transition-t54wk
Jun 10 15:59:32.635: INFO: Received response from host: affinity-clusterip-transition-gqjf7
Jun 10 15:59:32.635: INFO: Received response from host: affinity-clusterip-transition-gqjf7
Jun 10 15:59:32.635: INFO: Received response from host: affinity-clusterip-transition-gqjf7
Jun 10 15:59:32.635: INFO: Received response from host: affinity-clusterip-transition-t54wk
Jun 10 15:59:32.635: INFO: Received response from host: affinity-clusterip-transition-ppjjs
Jun 10 15:59:32.635: INFO: Received response from host: affinity-clusterip-transition-t54wk
Jun 10 15:59:32.635: INFO: Received response from host: affinity-clusterip-transition-gqjf7
Jun 10 15:59:32.635: INFO: Received response from host: affinity-clusterip-transition-t54wk
Jun 10 15:59:32.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=services-6848 exec execpod-affinitywst8v -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.131.224.223:80/ ; done'
Jun 10 15:59:33.261: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.131.224.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.131.224.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.131.224.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.131.224.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.131.224.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.131.224.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.131.224.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.131.224.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.131.224.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.131.224.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.131.224.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.131.224.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.131.224.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.131.224.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.131.224.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.131.224.223:80/\n"
Jun 10 15:59:33.262: INFO: stdout: "\naffinity-clusterip-transition-t54wk\naffinity-clusterip-transition-t54wk\naffinity-clusterip-transition-t54wk\naffinity-clusterip-transition-t54wk\naffinity-clusterip-transition-t54wk\naffinity-clusterip-transition-t54wk\naffinity-clusterip-transition-t54wk\naffinity-clusterip-transition-t54wk\naffinity-clusterip-transition-t54wk\naffinity-clusterip-transition-t54wk\naffinity-clusterip-transition-t54wk\naffinity-clusterip-transition-t54wk\naffinity-clusterip-transition-t54wk\naffinity-clusterip-transition-t54wk\naffinity-clusterip-transition-t54wk\naffinity-clusterip-transition-t54wk"
Jun 10 15:59:33.262: INFO: Received response from host: affinity-clusterip-transition-t54wk
Jun 10 15:59:33.262: INFO: Received response from host: affinity-clusterip-transition-t54wk
Jun 10 15:59:33.262: INFO: Received response from host: affinity-clusterip-transition-t54wk
Jun 10 15:59:33.262: INFO: Received response from host: affinity-clusterip-transition-t54wk
Jun 10 15:59:33.262: INFO: Received response from host: affinity-clusterip-transition-t54wk
Jun 10 15:59:33.262: INFO: Received response from host: affinity-clusterip-transition-t54wk
Jun 10 15:59:33.262: INFO: Received response from host: affinity-clusterip-transition-t54wk
Jun 10 15:59:33.262: INFO: Received response from host: affinity-clusterip-transition-t54wk
Jun 10 15:59:33.262: INFO: Received response from host: affinity-clusterip-transition-t54wk
Jun 10 15:59:33.262: INFO: Received response from host: affinity-clusterip-transition-t54wk
Jun 10 15:59:33.262: INFO: Received response from host: affinity-clusterip-transition-t54wk
Jun 10 15:59:33.262: INFO: Received response from host: affinity-clusterip-transition-t54wk
Jun 10 15:59:33.262: INFO: Received response from host: affinity-clusterip-transition-t54wk
Jun 10 15:59:33.262: INFO: Received response from host: affinity-clusterip-transition-t54wk
Jun 10 15:59:33.262: INFO: Received response from host: affinity-clusterip-transition-t54wk
Jun 10 15:59:33.262: INFO: Received response from host: affinity-clusterip-transition-t54wk
Jun 10 15:59:33.262: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-6848, will wait for the garbage collector to delete the pods
Jun 10 15:59:33.427: INFO: Deleting ReplicationController affinity-clusterip-transition took: 22.402195ms
Jun 10 15:59:34.827: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 1.400293555s
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 15:59:49.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6848" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:27.128 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":63,"skipped":766,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 15:59:49.243: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 10 15:59:49.334: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Creating first CR 
Jun 10 15:59:49.965: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-06-10T15:59:49Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-06-10T15:59:49Z]] name:name1 resourceVersion:18540 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:fdfb734d-14ec-414b-afaf-c06f6a1ea309] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Jun 10 15:59:59.971: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-06-10T15:59:59Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-06-10T15:59:59Z]] name:name2 resourceVersion:18625 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:7c7c2037-78bd-4dda-a93b-14ea3d911ce2] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Jun 10 16:00:09.978: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-06-10T15:59:49Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-06-10T16:00:09Z]] name:name1 resourceVersion:18681 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:fdfb734d-14ec-414b-afaf-c06f6a1ea309] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Jun 10 16:00:19.985: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-06-10T15:59:59Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-06-10T16:00:19Z]] name:name2 resourceVersion:18737 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:7c7c2037-78bd-4dda-a93b-14ea3d911ce2] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Jun 10 16:00:30.002: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-06-10T15:59:49Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-06-10T16:00:09Z]] name:name1 resourceVersion:18792 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:fdfb734d-14ec-414b-afaf-c06f6a1ea309] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Jun 10 16:00:40.011: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-06-10T15:59:59Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-06-10T16:00:19Z]] name:name2 resourceVersion:18848 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:7c7c2037-78bd-4dda-a93b-14ea3d911ce2] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:00:50.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-1993" for this suite.

• [SLOW TEST:61.295 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":305,"completed":64,"skipped":774,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:00:50.538: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-f18b01ec-6606-49c7-bfae-b40b907e64e6
STEP: Creating a pod to test consume configMaps
Jun 10 16:00:50.612: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-afd3e95d-4be9-4ea8-877d-f78f9519b44b" in namespace "projected-6188" to be "Succeeded or Failed"
Jun 10 16:00:50.619: INFO: Pod "pod-projected-configmaps-afd3e95d-4be9-4ea8-877d-f78f9519b44b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.15987ms
Jun 10 16:00:52.623: INFO: Pod "pod-projected-configmaps-afd3e95d-4be9-4ea8-877d-f78f9519b44b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010716782s
STEP: Saw pod success
Jun 10 16:00:52.623: INFO: Pod "pod-projected-configmaps-afd3e95d-4be9-4ea8-877d-f78f9519b44b" satisfied condition "Succeeded or Failed"
Jun 10 16:00:52.626: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod pod-projected-configmaps-afd3e95d-4be9-4ea8-877d-f78f9519b44b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 10 16:00:52.658: INFO: Waiting for pod pod-projected-configmaps-afd3e95d-4be9-4ea8-877d-f78f9519b44b to disappear
Jun 10 16:00:52.662: INFO: Pod pod-projected-configmaps-afd3e95d-4be9-4ea8-877d-f78f9519b44b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:00:52.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6188" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":65,"skipped":797,"failed":0}
SS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:00:52.671: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-4jj6m in namespace proxy-5512
I0610 16:00:52.740954      26 runners.go:190] Created replication controller with name: proxy-service-4jj6m, namespace: proxy-5512, replica count: 1
I0610 16:00:53.791537      26 runners.go:190] proxy-service-4jj6m Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0610 16:00:54.793014      26 runners.go:190] proxy-service-4jj6m Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0610 16:00:55.793367      26 runners.go:190] proxy-service-4jj6m Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0610 16:00:56.793675      26 runners.go:190] proxy-service-4jj6m Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0610 16:00:57.793996      26 runners.go:190] proxy-service-4jj6m Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0610 16:00:58.795548      26 runners.go:190] proxy-service-4jj6m Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0610 16:00:59.798471      26 runners.go:190] proxy-service-4jj6m Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0610 16:01:00.799031      26 runners.go:190] proxy-service-4jj6m Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0610 16:01:01.799144      26 runners.go:190] proxy-service-4jj6m Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0610 16:01:02.799304      26 runners.go:190] proxy-service-4jj6m Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0610 16:01:03.799528      26 runners.go:190] proxy-service-4jj6m Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 10 16:01:03.802: INFO: setup took 11.093408634s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jun 10 16:01:03.811: INFO: (0) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6/proxy/rewriteme">test</a> (200; 8.743623ms)
Jun 10 16:01:03.815: INFO: (0) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:162/proxy/: bar (200; 12.474345ms)
Jun 10 16:01:03.835: INFO: (0) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:162/proxy/: bar (200; 33.138982ms)
Jun 10 16:01:03.835: INFO: (0) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:1080/proxy/rewriteme">... (200; 33.086451ms)
Jun 10 16:01:03.835: INFO: (0) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:160/proxy/: foo (200; 33.266292ms)
Jun 10 16:01:03.836: INFO: (0) /api/v1/namespaces/proxy-5512/services/http:proxy-service-4jj6m:portname2/proxy/: bar (200; 33.556575ms)
Jun 10 16:01:03.836: INFO: (0) /api/v1/namespaces/proxy-5512/services/proxy-service-4jj6m:portname1/proxy/: foo (200; 34.059145ms)
Jun 10 16:01:03.837: INFO: (0) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:460/proxy/: tls baz (200; 35.158489ms)
Jun 10 16:01:03.837: INFO: (0) /api/v1/namespaces/proxy-5512/services/https:proxy-service-4jj6m:tlsportname1/proxy/: tls baz (200; 35.164598ms)
Jun 10 16:01:03.840: INFO: (0) /api/v1/namespaces/proxy-5512/services/http:proxy-service-4jj6m:portname1/proxy/: foo (200; 38.3643ms)
Jun 10 16:01:03.841: INFO: (0) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:160/proxy/: foo (200; 38.586986ms)
Jun 10 16:01:03.842: INFO: (0) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:1080/proxy/rewriteme">test<... (200; 39.627614ms)
Jun 10 16:01:03.842: INFO: (0) /api/v1/namespaces/proxy-5512/services/proxy-service-4jj6m:portname2/proxy/: bar (200; 40.111216ms)
Jun 10 16:01:03.843: INFO: (0) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:462/proxy/: tls qux (200; 40.811029ms)
Jun 10 16:01:03.845: INFO: (0) /api/v1/namespaces/proxy-5512/services/https:proxy-service-4jj6m:tlsportname2/proxy/: tls qux (200; 43.111132ms)
Jun 10 16:01:03.851: INFO: (0) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:443/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:443/proxy/tlsrewritem... (200; 49.370133ms)
Jun 10 16:01:03.861: INFO: (1) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:162/proxy/: bar (200; 9.012936ms)
Jun 10 16:01:03.862: INFO: (1) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:1080/proxy/rewriteme">... (200; 10.391517ms)
Jun 10 16:01:03.864: INFO: (1) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:160/proxy/: foo (200; 11.981792ms)
Jun 10 16:01:03.864: INFO: (1) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:462/proxy/: tls qux (200; 12.564871ms)
Jun 10 16:01:03.865: INFO: (1) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:460/proxy/: tls baz (200; 12.759469ms)
Jun 10 16:01:03.869: INFO: (1) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:1080/proxy/rewriteme">test<... (200; 16.841594ms)
Jun 10 16:01:03.869: INFO: (1) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:443/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:443/proxy/tlsrewritem... (200; 17.116937ms)
Jun 10 16:01:03.871: INFO: (1) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6/proxy/rewriteme">test</a> (200; 19.433211ms)
Jun 10 16:01:03.871: INFO: (1) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:160/proxy/: foo (200; 19.636983ms)
Jun 10 16:01:03.872: INFO: (1) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:162/proxy/: bar (200; 20.224905ms)
Jun 10 16:01:03.874: INFO: (1) /api/v1/namespaces/proxy-5512/services/https:proxy-service-4jj6m:tlsportname1/proxy/: tls baz (200; 21.748891ms)
Jun 10 16:01:03.874: INFO: (1) /api/v1/namespaces/proxy-5512/services/proxy-service-4jj6m:portname1/proxy/: foo (200; 21.944832ms)
Jun 10 16:01:03.880: INFO: (1) /api/v1/namespaces/proxy-5512/services/http:proxy-service-4jj6m:portname2/proxy/: bar (200; 28.886221ms)
Jun 10 16:01:03.880: INFO: (1) /api/v1/namespaces/proxy-5512/services/http:proxy-service-4jj6m:portname1/proxy/: foo (200; 28.799801ms)
Jun 10 16:01:03.881: INFO: (1) /api/v1/namespaces/proxy-5512/services/https:proxy-service-4jj6m:tlsportname2/proxy/: tls qux (200; 28.903605ms)
Jun 10 16:01:03.882: INFO: (1) /api/v1/namespaces/proxy-5512/services/proxy-service-4jj6m:portname2/proxy/: bar (200; 29.804606ms)
Jun 10 16:01:03.893: INFO: (2) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:443/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:443/proxy/tlsrewritem... (200; 10.992609ms)
Jun 10 16:01:03.893: INFO: (2) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:460/proxy/: tls baz (200; 10.423536ms)
Jun 10 16:01:03.894: INFO: (2) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:162/proxy/: bar (200; 11.770166ms)
Jun 10 16:01:03.895: INFO: (2) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:160/proxy/: foo (200; 12.482052ms)
Jun 10 16:01:03.895: INFO: (2) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6/proxy/rewriteme">test</a> (200; 13.07428ms)
Jun 10 16:01:03.896: INFO: (2) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:162/proxy/: bar (200; 13.359163ms)
Jun 10 16:01:03.896: INFO: (2) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:1080/proxy/rewriteme">test<... (200; 13.85772ms)
Jun 10 16:01:03.896: INFO: (2) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:462/proxy/: tls qux (200; 14.315069ms)
Jun 10 16:01:03.896: INFO: (2) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:1080/proxy/rewriteme">... (200; 14.182372ms)
Jun 10 16:01:03.897: INFO: (2) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:160/proxy/: foo (200; 14.964348ms)
Jun 10 16:01:03.900: INFO: (2) /api/v1/namespaces/proxy-5512/services/proxy-service-4jj6m:portname2/proxy/: bar (200; 17.94029ms)
Jun 10 16:01:03.900: INFO: (2) /api/v1/namespaces/proxy-5512/services/proxy-service-4jj6m:portname1/proxy/: foo (200; 18.203194ms)
Jun 10 16:01:03.901: INFO: (2) /api/v1/namespaces/proxy-5512/services/https:proxy-service-4jj6m:tlsportname2/proxy/: tls qux (200; 18.949789ms)
Jun 10 16:01:03.901: INFO: (2) /api/v1/namespaces/proxy-5512/services/https:proxy-service-4jj6m:tlsportname1/proxy/: tls baz (200; 19.09453ms)
Jun 10 16:01:03.901: INFO: (2) /api/v1/namespaces/proxy-5512/services/http:proxy-service-4jj6m:portname2/proxy/: bar (200; 18.93464ms)
Jun 10 16:01:03.902: INFO: (2) /api/v1/namespaces/proxy-5512/services/http:proxy-service-4jj6m:portname1/proxy/: foo (200; 19.385359ms)
Jun 10 16:01:03.911: INFO: (3) /api/v1/namespaces/proxy-5512/services/proxy-service-4jj6m:portname2/proxy/: bar (200; 8.714823ms)
Jun 10 16:01:03.911: INFO: (3) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:443/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:443/proxy/tlsrewritem... (200; 8.446106ms)
Jun 10 16:01:03.912: INFO: (3) /api/v1/namespaces/proxy-5512/services/http:proxy-service-4jj6m:portname2/proxy/: bar (200; 9.454176ms)
Jun 10 16:01:03.913: INFO: (3) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:162/proxy/: bar (200; 10.68275ms)
Jun 10 16:01:03.913: INFO: (3) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:160/proxy/: foo (200; 10.195514ms)
Jun 10 16:01:03.913: INFO: (3) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:460/proxy/: tls baz (200; 10.433785ms)
Jun 10 16:01:03.913: INFO: (3) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:162/proxy/: bar (200; 10.49471ms)
Jun 10 16:01:03.913: INFO: (3) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:1080/proxy/rewriteme">test<... (200; 10.645859ms)
Jun 10 16:01:03.915: INFO: (3) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:160/proxy/: foo (200; 12.388145ms)
Jun 10 16:01:03.916: INFO: (3) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:462/proxy/: tls qux (200; 13.485405ms)
Jun 10 16:01:03.916: INFO: (3) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6/proxy/rewriteme">test</a> (200; 13.028181ms)
Jun 10 16:01:03.916: INFO: (3) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:1080/proxy/rewriteme">... (200; 14.113659ms)
Jun 10 16:01:03.919: INFO: (3) /api/v1/namespaces/proxy-5512/services/https:proxy-service-4jj6m:tlsportname1/proxy/: tls baz (200; 16.003353ms)
Jun 10 16:01:03.921: INFO: (3) /api/v1/namespaces/proxy-5512/services/http:proxy-service-4jj6m:portname1/proxy/: foo (200; 18.602711ms)
Jun 10 16:01:03.921: INFO: (3) /api/v1/namespaces/proxy-5512/services/proxy-service-4jj6m:portname1/proxy/: foo (200; 18.999115ms)
Jun 10 16:01:03.922: INFO: (3) /api/v1/namespaces/proxy-5512/services/https:proxy-service-4jj6m:tlsportname2/proxy/: tls qux (200; 18.889566ms)
Jun 10 16:01:03.928: INFO: (4) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:1080/proxy/rewriteme">test<... (200; 5.821249ms)
Jun 10 16:01:03.931: INFO: (4) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:160/proxy/: foo (200; 9.02086ms)
Jun 10 16:01:03.931: INFO: (4) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6/proxy/rewriteme">test</a> (200; 8.848761ms)
Jun 10 16:01:03.931: INFO: (4) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:1080/proxy/rewriteme">... (200; 9.172034ms)
Jun 10 16:01:03.931: INFO: (4) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:162/proxy/: bar (200; 8.833627ms)
Jun 10 16:01:03.931: INFO: (4) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:160/proxy/: foo (200; 8.538803ms)
Jun 10 16:01:03.931: INFO: (4) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:162/proxy/: bar (200; 9.20107ms)
Jun 10 16:01:03.931: INFO: (4) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:460/proxy/: tls baz (200; 8.826607ms)
Jun 10 16:01:03.933: INFO: (4) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:462/proxy/: tls qux (200; 11.078133ms)
Jun 10 16:01:03.933: INFO: (4) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:443/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:443/proxy/tlsrewritem... (200; 10.026257ms)
Jun 10 16:01:03.937: INFO: (4) /api/v1/namespaces/proxy-5512/services/http:proxy-service-4jj6m:portname2/proxy/: bar (200; 14.013149ms)
Jun 10 16:01:03.938: INFO: (4) /api/v1/namespaces/proxy-5512/services/https:proxy-service-4jj6m:tlsportname1/proxy/: tls baz (200; 15.634344ms)
Jun 10 16:01:03.938: INFO: (4) /api/v1/namespaces/proxy-5512/services/https:proxy-service-4jj6m:tlsportname2/proxy/: tls qux (200; 15.589971ms)
Jun 10 16:01:03.938: INFO: (4) /api/v1/namespaces/proxy-5512/services/proxy-service-4jj6m:portname2/proxy/: bar (200; 16.334293ms)
Jun 10 16:01:03.938: INFO: (4) /api/v1/namespaces/proxy-5512/services/proxy-service-4jj6m:portname1/proxy/: foo (200; 16.469409ms)
Jun 10 16:01:03.941: INFO: (4) /api/v1/namespaces/proxy-5512/services/http:proxy-service-4jj6m:portname1/proxy/: foo (200; 19.100571ms)
Jun 10 16:01:03.951: INFO: (5) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:162/proxy/: bar (200; 8.457426ms)
Jun 10 16:01:03.952: INFO: (5) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:162/proxy/: bar (200; 10.053826ms)
Jun 10 16:01:03.952: INFO: (5) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:160/proxy/: foo (200; 10.254855ms)
Jun 10 16:01:03.953: INFO: (5) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:1080/proxy/rewriteme">... (200; 10.038654ms)
Jun 10 16:01:03.954: INFO: (5) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:160/proxy/: foo (200; 12.295333ms)
Jun 10 16:01:03.955: INFO: (5) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:462/proxy/: tls qux (200; 13.581864ms)
Jun 10 16:01:03.955: INFO: (5) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:1080/proxy/rewriteme">test<... (200; 12.066361ms)
Jun 10 16:01:03.955: INFO: (5) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:443/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:443/proxy/tlsrewritem... (200; 13.319755ms)
Jun 10 16:01:03.955: INFO: (5) /api/v1/namespaces/proxy-5512/services/https:proxy-service-4jj6m:tlsportname2/proxy/: tls qux (200; 13.830591ms)
Jun 10 16:01:03.955: INFO: (5) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6/proxy/rewriteme">test</a> (200; 12.288264ms)
Jun 10 16:01:03.956: INFO: (5) /api/v1/namespaces/proxy-5512/services/proxy-service-4jj6m:portname1/proxy/: foo (200; 13.962566ms)
Jun 10 16:01:03.957: INFO: (5) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:460/proxy/: tls baz (200; 13.981688ms)
Jun 10 16:01:03.960: INFO: (5) /api/v1/namespaces/proxy-5512/services/https:proxy-service-4jj6m:tlsportname1/proxy/: tls baz (200; 16.288222ms)
Jun 10 16:01:03.960: INFO: (5) /api/v1/namespaces/proxy-5512/services/http:proxy-service-4jj6m:portname1/proxy/: foo (200; 16.665997ms)
Jun 10 16:01:03.960: INFO: (5) /api/v1/namespaces/proxy-5512/services/proxy-service-4jj6m:portname2/proxy/: bar (200; 17.197588ms)
Jun 10 16:01:03.961: INFO: (5) /api/v1/namespaces/proxy-5512/services/http:proxy-service-4jj6m:portname2/proxy/: bar (200; 18.822696ms)
Jun 10 16:01:03.967: INFO: (6) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6/proxy/rewriteme">test</a> (200; 4.721164ms)
Jun 10 16:01:03.968: INFO: (6) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:460/proxy/: tls baz (200; 5.672583ms)
Jun 10 16:01:03.968: INFO: (6) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:160/proxy/: foo (200; 6.314176ms)
Jun 10 16:01:03.968: INFO: (6) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:162/proxy/: bar (200; 6.379428ms)
Jun 10 16:01:03.969: INFO: (6) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:1080/proxy/rewriteme">... (200; 7.292322ms)
Jun 10 16:01:03.970: INFO: (6) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:443/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:443/proxy/tlsrewritem... (200; 9.493955ms)
Jun 10 16:01:03.971: INFO: (6) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:1080/proxy/rewriteme">test<... (200; 9.156709ms)
Jun 10 16:01:03.972: INFO: (6) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:160/proxy/: foo (200; 11.10317ms)
Jun 10 16:01:03.974: INFO: (6) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:162/proxy/: bar (200; 11.625418ms)
Jun 10 16:01:03.974: INFO: (6) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:462/proxy/: tls qux (200; 12.623387ms)
Jun 10 16:01:03.978: INFO: (6) /api/v1/namespaces/proxy-5512/services/proxy-service-4jj6m:portname2/proxy/: bar (200; 16.308217ms)
Jun 10 16:01:03.978: INFO: (6) /api/v1/namespaces/proxy-5512/services/https:proxy-service-4jj6m:tlsportname2/proxy/: tls qux (200; 17.03628ms)
Jun 10 16:01:03.978: INFO: (6) /api/v1/namespaces/proxy-5512/services/https:proxy-service-4jj6m:tlsportname1/proxy/: tls baz (200; 15.581104ms)
Jun 10 16:01:03.981: INFO: (6) /api/v1/namespaces/proxy-5512/services/http:proxy-service-4jj6m:portname1/proxy/: foo (200; 18.927485ms)
Jun 10 16:01:03.981: INFO: (6) /api/v1/namespaces/proxy-5512/services/http:proxy-service-4jj6m:portname2/proxy/: bar (200; 19.941352ms)
Jun 10 16:01:03.982: INFO: (6) /api/v1/namespaces/proxy-5512/services/proxy-service-4jj6m:portname1/proxy/: foo (200; 20.629121ms)
Jun 10 16:01:03.991: INFO: (7) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:160/proxy/: foo (200; 7.765818ms)
Jun 10 16:01:03.991: INFO: (7) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:460/proxy/: tls baz (200; 9.583999ms)
Jun 10 16:01:03.991: INFO: (7) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6/proxy/rewriteme">test</a> (200; 8.156023ms)
Jun 10 16:01:03.992: INFO: (7) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:162/proxy/: bar (200; 8.373191ms)
Jun 10 16:01:03.993: INFO: (7) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:462/proxy/: tls qux (200; 10.129569ms)
Jun 10 16:01:03.994: INFO: (7) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:443/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:443/proxy/tlsrewritem... (200; 11.154454ms)
Jun 10 16:01:03.994: INFO: (7) /api/v1/namespaces/proxy-5512/services/proxy-service-4jj6m:portname1/proxy/: foo (200; 10.989413ms)
Jun 10 16:01:03.994: INFO: (7) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:160/proxy/: foo (200; 11.457839ms)
Jun 10 16:01:03.994: INFO: (7) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:1080/proxy/rewriteme">... (200; 11.229029ms)
Jun 10 16:01:03.995: INFO: (7) /api/v1/namespaces/proxy-5512/services/https:proxy-service-4jj6m:tlsportname1/proxy/: tls baz (200; 12.819962ms)
Jun 10 16:01:03.996: INFO: (7) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:162/proxy/: bar (200; 12.774674ms)
Jun 10 16:01:03.998: INFO: (7) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:1080/proxy/rewriteme">test<... (200; 14.130982ms)
Jun 10 16:01:03.999: INFO: (7) /api/v1/namespaces/proxy-5512/services/https:proxy-service-4jj6m:tlsportname2/proxy/: tls qux (200; 16.344493ms)
Jun 10 16:01:03.999: INFO: (7) /api/v1/namespaces/proxy-5512/services/http:proxy-service-4jj6m:portname2/proxy/: bar (200; 16.492055ms)
Jun 10 16:01:03.999: INFO: (7) /api/v1/namespaces/proxy-5512/services/http:proxy-service-4jj6m:portname1/proxy/: foo (200; 16.225034ms)
Jun 10 16:01:04.000: INFO: (7) /api/v1/namespaces/proxy-5512/services/proxy-service-4jj6m:portname2/proxy/: bar (200; 16.718104ms)
Jun 10 16:01:04.003: INFO: (8) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:462/proxy/: tls qux (200; 3.570477ms)
Jun 10 16:01:04.008: INFO: (8) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:162/proxy/: bar (200; 8.069461ms)
Jun 10 16:01:04.010: INFO: (8) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:460/proxy/: tls baz (200; 9.2782ms)
Jun 10 16:01:04.010: INFO: (8) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:1080/proxy/rewriteme">test<... (200; 9.437329ms)
Jun 10 16:01:04.011: INFO: (8) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6/proxy/rewriteme">test</a> (200; 9.986537ms)
Jun 10 16:01:04.013: INFO: (8) /api/v1/namespaces/proxy-5512/services/http:proxy-service-4jj6m:portname2/proxy/: bar (200; 13.484208ms)
Jun 10 16:01:04.013: INFO: (8) /api/v1/namespaces/proxy-5512/services/proxy-service-4jj6m:portname1/proxy/: foo (200; 13.159142ms)
Jun 10 16:01:04.013: INFO: (8) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:1080/proxy/rewriteme">... (200; 13.068718ms)
Jun 10 16:01:04.013: INFO: (8) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:160/proxy/: foo (200; 13.614108ms)
Jun 10 16:01:04.014: INFO: (8) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:162/proxy/: bar (200; 13.134092ms)
Jun 10 16:01:04.014: INFO: (8) /api/v1/namespaces/proxy-5512/services/proxy-service-4jj6m:portname2/proxy/: bar (200; 13.598582ms)
Jun 10 16:01:04.014: INFO: (8) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:160/proxy/: foo (200; 13.933268ms)
Jun 10 16:01:04.014: INFO: (8) /api/v1/namespaces/proxy-5512/services/https:proxy-service-4jj6m:tlsportname1/proxy/: tls baz (200; 13.400025ms)
Jun 10 16:01:04.016: INFO: (8) /api/v1/namespaces/proxy-5512/services/https:proxy-service-4jj6m:tlsportname2/proxy/: tls qux (200; 16.5008ms)
Jun 10 16:01:04.016: INFO: (8) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:443/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:443/proxy/tlsrewritem... (200; 16.172388ms)
Jun 10 16:01:04.016: INFO: (8) /api/v1/namespaces/proxy-5512/services/http:proxy-service-4jj6m:portname1/proxy/: foo (200; 15.697625ms)
Jun 10 16:01:04.024: INFO: (9) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:1080/proxy/rewriteme">... (200; 7.022002ms)
Jun 10 16:01:04.025: INFO: (9) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:160/proxy/: foo (200; 8.515742ms)
Jun 10 16:01:04.026: INFO: (9) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6/proxy/rewriteme">test</a> (200; 9.182478ms)
Jun 10 16:01:04.032: INFO: (9) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:162/proxy/: bar (200; 14.688596ms)
Jun 10 16:01:04.033: INFO: (9) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:460/proxy/: tls baz (200; 15.373833ms)
Jun 10 16:01:04.033: INFO: (9) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:160/proxy/: foo (200; 15.839591ms)
Jun 10 16:01:04.033: INFO: (9) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:1080/proxy/rewriteme">test<... (200; 16.181737ms)
Jun 10 16:01:04.033: INFO: (9) /api/v1/namespaces/proxy-5512/services/https:proxy-service-4jj6m:tlsportname1/proxy/: tls baz (200; 15.987967ms)
Jun 10 16:01:04.033: INFO: (9) /api/v1/namespaces/proxy-5512/services/https:proxy-service-4jj6m:tlsportname2/proxy/: tls qux (200; 17.214938ms)
Jun 10 16:01:04.033: INFO: (9) /api/v1/namespaces/proxy-5512/services/proxy-service-4jj6m:portname2/proxy/: bar (200; 16.704595ms)
Jun 10 16:01:04.033: INFO: (9) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:462/proxy/: tls qux (200; 16.911ms)
Jun 10 16:01:04.034: INFO: (9) /api/v1/namespaces/proxy-5512/services/proxy-service-4jj6m:portname1/proxy/: foo (200; 16.870366ms)
Jun 10 16:01:04.034: INFO: (9) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:162/proxy/: bar (200; 16.234332ms)
Jun 10 16:01:04.034: INFO: (9) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:443/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:443/proxy/tlsrewritem... (200; 17.972441ms)
Jun 10 16:01:04.035: INFO: (9) /api/v1/namespaces/proxy-5512/services/http:proxy-service-4jj6m:portname2/proxy/: bar (200; 18.490347ms)
Jun 10 16:01:04.035: INFO: (9) /api/v1/namespaces/proxy-5512/services/http:proxy-service-4jj6m:portname1/proxy/: foo (200; 17.867926ms)
Jun 10 16:01:04.044: INFO: (10) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:1080/proxy/rewriteme">test<... (200; 7.577713ms)
Jun 10 16:01:04.044: INFO: (10) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:162/proxy/: bar (200; 8.514386ms)
Jun 10 16:01:04.045: INFO: (10) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:162/proxy/: bar (200; 9.129857ms)
Jun 10 16:01:04.047: INFO: (10) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6/proxy/rewriteme">test</a> (200; 10.631821ms)
Jun 10 16:01:04.047: INFO: (10) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:462/proxy/: tls qux (200; 11.347954ms)
Jun 10 16:01:04.047: INFO: (10) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:160/proxy/: foo (200; 11.027625ms)
Jun 10 16:01:04.047: INFO: (10) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:460/proxy/: tls baz (200; 10.381874ms)
Jun 10 16:01:04.047: INFO: (10) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:1080/proxy/rewriteme">... (200; 11.158749ms)
Jun 10 16:01:04.047: INFO: (10) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:443/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:443/proxy/tlsrewritem... (200; 11.547283ms)
Jun 10 16:01:04.048: INFO: (10) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:160/proxy/: foo (200; 12.655778ms)
Jun 10 16:01:04.054: INFO: (10) /api/v1/namespaces/proxy-5512/services/https:proxy-service-4jj6m:tlsportname2/proxy/: tls qux (200; 19.199919ms)
Jun 10 16:01:04.057: INFO: (10) /api/v1/namespaces/proxy-5512/services/proxy-service-4jj6m:portname2/proxy/: bar (200; 21.011934ms)
Jun 10 16:01:04.058: INFO: (10) /api/v1/namespaces/proxy-5512/services/http:proxy-service-4jj6m:portname2/proxy/: bar (200; 22.465873ms)
Jun 10 16:01:04.058: INFO: (10) /api/v1/namespaces/proxy-5512/services/https:proxy-service-4jj6m:tlsportname1/proxy/: tls baz (200; 21.470926ms)
Jun 10 16:01:04.059: INFO: (10) /api/v1/namespaces/proxy-5512/services/proxy-service-4jj6m:portname1/proxy/: foo (200; 23.886932ms)
Jun 10 16:01:04.059: INFO: (10) /api/v1/namespaces/proxy-5512/services/http:proxy-service-4jj6m:portname1/proxy/: foo (200; 23.438819ms)
Jun 10 16:01:04.082: INFO: (11) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:162/proxy/: bar (200; 21.31874ms)
Jun 10 16:01:04.082: INFO: (11) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:462/proxy/: tls qux (200; 21.505891ms)
Jun 10 16:01:04.082: INFO: (11) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:160/proxy/: foo (200; 22.155826ms)
Jun 10 16:01:04.083: INFO: (11) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:1080/proxy/rewriteme">... (200; 22.446693ms)
Jun 10 16:01:04.091: INFO: (11) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:460/proxy/: tls baz (200; 30.261756ms)
Jun 10 16:01:04.099: INFO: (11) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:160/proxy/: foo (200; 38.936295ms)
Jun 10 16:01:04.099: INFO: (11) /api/v1/namespaces/proxy-5512/services/proxy-service-4jj6m:portname1/proxy/: foo (200; 39.00256ms)
Jun 10 16:01:04.100: INFO: (11) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:1080/proxy/rewriteme">test<... (200; 40.389059ms)
Jun 10 16:01:04.100: INFO: (11) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:162/proxy/: bar (200; 40.87013ms)
Jun 10 16:01:04.100: INFO: (11) /api/v1/namespaces/proxy-5512/services/https:proxy-service-4jj6m:tlsportname1/proxy/: tls baz (200; 40.151351ms)
Jun 10 16:01:04.101: INFO: (11) /api/v1/namespaces/proxy-5512/services/http:proxy-service-4jj6m:portname2/proxy/: bar (200; 40.71423ms)
Jun 10 16:01:04.103: INFO: (11) /api/v1/namespaces/proxy-5512/services/http:proxy-service-4jj6m:portname1/proxy/: foo (200; 43.217711ms)
Jun 10 16:01:04.103: INFO: (11) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:443/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:443/proxy/tlsrewritem... (200; 42.599312ms)
Jun 10 16:01:04.103: INFO: (11) /api/v1/namespaces/proxy-5512/services/https:proxy-service-4jj6m:tlsportname2/proxy/: tls qux (200; 42.552462ms)
Jun 10 16:01:04.103: INFO: (11) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6/proxy/rewriteme">test</a> (200; 43.220443ms)
Jun 10 16:01:04.103: INFO: (11) /api/v1/namespaces/proxy-5512/services/proxy-service-4jj6m:portname2/proxy/: bar (200; 42.584169ms)
Jun 10 16:01:04.117: INFO: (12) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:162/proxy/: bar (200; 13.173922ms)
Jun 10 16:01:04.117: INFO: (12) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:162/proxy/: bar (200; 13.152588ms)
Jun 10 16:01:04.117: INFO: (12) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:1080/proxy/rewriteme">test<... (200; 13.801499ms)
Jun 10 16:01:04.117: INFO: (12) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:460/proxy/: tls baz (200; 13.614329ms)
Jun 10 16:01:04.117: INFO: (12) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:443/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:443/proxy/tlsrewritem... (200; 13.588589ms)
Jun 10 16:01:04.117: INFO: (12) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:1080/proxy/rewriteme">... (200; 13.721128ms)
Jun 10 16:01:04.117: INFO: (12) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:160/proxy/: foo (200; 13.893757ms)
Jun 10 16:01:04.117: INFO: (12) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:160/proxy/: foo (200; 13.71199ms)
Jun 10 16:01:04.118: INFO: (12) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6/proxy/rewriteme">test</a> (200; 15.022232ms)
Jun 10 16:01:04.118: INFO: (12) /api/v1/namespaces/proxy-5512/services/https:proxy-service-4jj6m:tlsportname1/proxy/: tls baz (200; 14.876547ms)
Jun 10 16:01:04.119: INFO: (12) /api/v1/namespaces/proxy-5512/services/http:proxy-service-4jj6m:portname1/proxy/: foo (200; 15.468159ms)
Jun 10 16:01:04.119: INFO: (12) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:462/proxy/: tls qux (200; 15.649037ms)
Jun 10 16:01:04.119: INFO: (12) /api/v1/namespaces/proxy-5512/services/https:proxy-service-4jj6m:tlsportname2/proxy/: tls qux (200; 15.681872ms)
Jun 10 16:01:04.120: INFO: (12) /api/v1/namespaces/proxy-5512/services/http:proxy-service-4jj6m:portname2/proxy/: bar (200; 16.322029ms)
Jun 10 16:01:04.122: INFO: (12) /api/v1/namespaces/proxy-5512/services/proxy-service-4jj6m:portname1/proxy/: foo (200; 19.014328ms)
Jun 10 16:01:04.123: INFO: (12) /api/v1/namespaces/proxy-5512/services/proxy-service-4jj6m:portname2/proxy/: bar (200; 19.063509ms)
Jun 10 16:01:04.135: INFO: (13) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:1080/proxy/rewriteme">test<... (200; 11.971472ms)
Jun 10 16:01:04.135: INFO: (13) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:162/proxy/: bar (200; 12.540578ms)
Jun 10 16:01:04.136: INFO: (13) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:460/proxy/: tls baz (200; 13.129057ms)
Jun 10 16:01:04.136: INFO: (13) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:160/proxy/: foo (200; 11.93371ms)
Jun 10 16:01:04.136: INFO: (13) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:162/proxy/: bar (200; 13.366151ms)
Jun 10 16:01:04.136: INFO: (13) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:160/proxy/: foo (200; 12.666313ms)
Jun 10 16:01:04.137: INFO: (13) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6/proxy/rewriteme">test</a> (200; 14.230456ms)
Jun 10 16:01:04.137: INFO: (13) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:462/proxy/: tls qux (200; 13.002319ms)
Jun 10 16:01:04.137: INFO: (13) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:443/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:443/proxy/tlsrewritem... (200; 13.362332ms)
Jun 10 16:01:04.138: INFO: (13) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:1080/proxy/rewriteme">... (200; 13.591201ms)
Jun 10 16:01:04.146: INFO: (13) /api/v1/namespaces/proxy-5512/services/proxy-service-4jj6m:portname1/proxy/: foo (200; 22.102976ms)
Jun 10 16:01:04.147: INFO: (13) /api/v1/namespaces/proxy-5512/services/http:proxy-service-4jj6m:portname1/proxy/: foo (200; 23.787653ms)
Jun 10 16:01:04.147: INFO: (13) /api/v1/namespaces/proxy-5512/services/proxy-service-4jj6m:portname2/proxy/: bar (200; 22.384929ms)
Jun 10 16:01:04.147: INFO: (13) /api/v1/namespaces/proxy-5512/services/https:proxy-service-4jj6m:tlsportname1/proxy/: tls baz (200; 23.548322ms)
Jun 10 16:01:04.147: INFO: (13) /api/v1/namespaces/proxy-5512/services/https:proxy-service-4jj6m:tlsportname2/proxy/: tls qux (200; 23.472376ms)
Jun 10 16:01:04.147: INFO: (13) /api/v1/namespaces/proxy-5512/services/http:proxy-service-4jj6m:portname2/proxy/: bar (200; 23.506362ms)
Jun 10 16:01:04.157: INFO: (14) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:1080/proxy/rewriteme">... (200; 10.034779ms)
Jun 10 16:01:04.157: INFO: (14) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:162/proxy/: bar (200; 9.976837ms)
Jun 10 16:01:04.161: INFO: (14) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:160/proxy/: foo (200; 13.493806ms)
Jun 10 16:01:04.161: INFO: (14) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:460/proxy/: tls baz (200; 13.989654ms)
Jun 10 16:01:04.161: INFO: (14) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:162/proxy/: bar (200; 13.994327ms)
Jun 10 16:01:04.161: INFO: (14) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:443/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:443/proxy/tlsrewritem... (200; 13.904301ms)
Jun 10 16:01:04.163: INFO: (14) /api/v1/namespaces/proxy-5512/services/https:proxy-service-4jj6m:tlsportname2/proxy/: tls qux (200; 15.84224ms)
Jun 10 16:01:04.164: INFO: (14) /api/v1/namespaces/proxy-5512/services/http:proxy-service-4jj6m:portname1/proxy/: foo (200; 16.283548ms)
Jun 10 16:01:04.164: INFO: (14) /api/v1/namespaces/proxy-5512/services/proxy-service-4jj6m:portname2/proxy/: bar (200; 16.305907ms)
Jun 10 16:01:04.164: INFO: (14) /api/v1/namespaces/proxy-5512/services/proxy-service-4jj6m:portname1/proxy/: foo (200; 16.255394ms)
Jun 10 16:01:04.164: INFO: (14) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:160/proxy/: foo (200; 16.368218ms)
Jun 10 16:01:04.164: INFO: (14) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:1080/proxy/rewriteme">test<... (200; 16.792992ms)
Jun 10 16:01:04.164: INFO: (14) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6/proxy/rewriteme">test</a> (200; 16.531002ms)
Jun 10 16:01:04.165: INFO: (14) /api/v1/namespaces/proxy-5512/services/http:proxy-service-4jj6m:portname2/proxy/: bar (200; 17.58498ms)
Jun 10 16:01:04.166: INFO: (14) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:462/proxy/: tls qux (200; 18.545558ms)
Jun 10 16:01:04.166: INFO: (14) /api/v1/namespaces/proxy-5512/services/https:proxy-service-4jj6m:tlsportname1/proxy/: tls baz (200; 18.333323ms)
Jun 10 16:01:04.174: INFO: (15) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:162/proxy/: bar (200; 8.168687ms)
Jun 10 16:01:04.175: INFO: (15) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:443/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:443/proxy/tlsrewritem... (200; 8.267055ms)
Jun 10 16:01:04.175: INFO: (15) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:462/proxy/: tls qux (200; 9.318813ms)
Jun 10 16:01:04.176: INFO: (15) /api/v1/namespaces/proxy-5512/services/https:proxy-service-4jj6m:tlsportname1/proxy/: tls baz (200; 9.415452ms)
Jun 10 16:01:04.177: INFO: (15) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:160/proxy/: foo (200; 10.968958ms)
Jun 10 16:01:04.177: INFO: (15) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6/proxy/rewriteme">test</a> (200; 11.194357ms)
Jun 10 16:01:04.178: INFO: (15) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:460/proxy/: tls baz (200; 12.022329ms)
Jun 10 16:01:04.182: INFO: (15) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:1080/proxy/rewriteme">test<... (200; 15.694906ms)
Jun 10 16:01:04.182: INFO: (15) /api/v1/namespaces/proxy-5512/services/https:proxy-service-4jj6m:tlsportname2/proxy/: tls qux (200; 16.049846ms)
Jun 10 16:01:04.184: INFO: (15) /api/v1/namespaces/proxy-5512/services/proxy-service-4jj6m:portname1/proxy/: foo (200; 17.433167ms)
Jun 10 16:01:04.184: INFO: (15) /api/v1/namespaces/proxy-5512/services/http:proxy-service-4jj6m:portname2/proxy/: bar (200; 17.372814ms)
Jun 10 16:01:04.184: INFO: (15) /api/v1/namespaces/proxy-5512/services/proxy-service-4jj6m:portname2/proxy/: bar (200; 17.47458ms)
Jun 10 16:01:04.184: INFO: (15) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:162/proxy/: bar (200; 17.728694ms)
Jun 10 16:01:04.188: INFO: (15) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:160/proxy/: foo (200; 21.98839ms)
Jun 10 16:01:04.188: INFO: (15) /api/v1/namespaces/proxy-5512/services/http:proxy-service-4jj6m:portname1/proxy/: foo (200; 22.039431ms)
Jun 10 16:01:04.188: INFO: (15) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:1080/proxy/rewriteme">... (200; 22.044108ms)
Jun 10 16:01:04.200: INFO: (16) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:160/proxy/: foo (200; 11.169252ms)
Jun 10 16:01:04.201: INFO: (16) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6/proxy/rewriteme">test</a> (200; 11.901869ms)
Jun 10 16:01:04.202: INFO: (16) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:1080/proxy/rewriteme">test<... (200; 13.859167ms)
Jun 10 16:01:04.203: INFO: (16) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:462/proxy/: tls qux (200; 14.371642ms)
Jun 10 16:01:04.203: INFO: (16) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:162/proxy/: bar (200; 14.345911ms)
Jun 10 16:01:04.204: INFO: (16) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:460/proxy/: tls baz (200; 15.167675ms)
Jun 10 16:01:04.204: INFO: (16) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:162/proxy/: bar (200; 15.306704ms)
Jun 10 16:01:04.204: INFO: (16) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:443/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:443/proxy/tlsrewritem... (200; 15.182728ms)
Jun 10 16:01:04.204: INFO: (16) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:1080/proxy/rewriteme">... (200; 15.780856ms)
Jun 10 16:01:04.207: INFO: (16) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:160/proxy/: foo (200; 18.750094ms)
Jun 10 16:01:04.209: INFO: (16) /api/v1/namespaces/proxy-5512/services/proxy-service-4jj6m:portname1/proxy/: foo (200; 20.564015ms)
Jun 10 16:01:04.209: INFO: (16) /api/v1/namespaces/proxy-5512/services/proxy-service-4jj6m:portname2/proxy/: bar (200; 20.654481ms)
Jun 10 16:01:04.210: INFO: (16) /api/v1/namespaces/proxy-5512/services/http:proxy-service-4jj6m:portname2/proxy/: bar (200; 21.45646ms)
Jun 10 16:01:04.210: INFO: (16) /api/v1/namespaces/proxy-5512/services/http:proxy-service-4jj6m:portname1/proxy/: foo (200; 21.370855ms)
Jun 10 16:01:04.210: INFO: (16) /api/v1/namespaces/proxy-5512/services/https:proxy-service-4jj6m:tlsportname1/proxy/: tls baz (200; 21.527518ms)
Jun 10 16:01:04.210: INFO: (16) /api/v1/namespaces/proxy-5512/services/https:proxy-service-4jj6m:tlsportname2/proxy/: tls qux (200; 22.061627ms)
Jun 10 16:01:04.224: INFO: (17) /api/v1/namespaces/proxy-5512/services/proxy-service-4jj6m:portname1/proxy/: foo (200; 12.599612ms)
Jun 10 16:01:04.225: INFO: (17) /api/v1/namespaces/proxy-5512/services/http:proxy-service-4jj6m:portname2/proxy/: bar (200; 13.552532ms)
Jun 10 16:01:04.227: INFO: (17) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:460/proxy/: tls baz (200; 15.98385ms)
Jun 10 16:01:04.227: INFO: (17) /api/v1/namespaces/proxy-5512/services/https:proxy-service-4jj6m:tlsportname1/proxy/: tls baz (200; 16.027101ms)
Jun 10 16:01:04.229: INFO: (17) /api/v1/namespaces/proxy-5512/services/proxy-service-4jj6m:portname2/proxy/: bar (200; 17.954747ms)
Jun 10 16:01:04.229: INFO: (17) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:1080/proxy/rewriteme">... (200; 17.949591ms)
Jun 10 16:01:04.231: INFO: (17) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:1080/proxy/rewriteme">test<... (200; 18.682729ms)
Jun 10 16:01:04.231: INFO: (17) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:160/proxy/: foo (200; 18.960128ms)
Jun 10 16:01:04.231: INFO: (17) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:443/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:443/proxy/tlsrewritem... (200; 19.364944ms)
Jun 10 16:01:04.231: INFO: (17) /api/v1/namespaces/proxy-5512/services/https:proxy-service-4jj6m:tlsportname2/proxy/: tls qux (200; 19.59816ms)
Jun 10 16:01:04.231: INFO: (17) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:462/proxy/: tls qux (200; 19.35009ms)
Jun 10 16:01:04.231: INFO: (17) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:160/proxy/: foo (200; 19.397933ms)
Jun 10 16:01:04.231: INFO: (17) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6/proxy/rewriteme">test</a> (200; 19.281442ms)
Jun 10 16:01:04.231: INFO: (17) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:162/proxy/: bar (200; 19.196808ms)
Jun 10 16:01:04.231: INFO: (17) /api/v1/namespaces/proxy-5512/services/http:proxy-service-4jj6m:portname1/proxy/: foo (200; 19.573994ms)
Jun 10 16:01:04.231: INFO: (17) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:162/proxy/: bar (200; 19.674554ms)
Jun 10 16:01:04.253: INFO: (18) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:1080/proxy/rewriteme">test<... (200; 21.060712ms)
Jun 10 16:01:04.253: INFO: (18) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:160/proxy/: foo (200; 21.608386ms)
Jun 10 16:01:04.255: INFO: (18) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:1080/proxy/rewriteme">... (200; 23.186167ms)
Jun 10 16:01:04.255: INFO: (18) /api/v1/namespaces/proxy-5512/services/https:proxy-service-4jj6m:tlsportname2/proxy/: tls qux (200; 23.342645ms)
Jun 10 16:01:04.255: INFO: (18) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:462/proxy/: tls qux (200; 23.059963ms)
Jun 10 16:01:04.256: INFO: (18) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6/proxy/rewriteme">test</a> (200; 24.382382ms)
Jun 10 16:01:04.258: INFO: (18) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:162/proxy/: bar (200; 26.227192ms)
Jun 10 16:01:04.258: INFO: (18) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:162/proxy/: bar (200; 26.302455ms)
Jun 10 16:01:04.260: INFO: (18) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:443/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:443/proxy/tlsrewritem... (200; 27.783653ms)
Jun 10 16:01:04.261: INFO: (18) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:460/proxy/: tls baz (200; 29.176643ms)
Jun 10 16:01:04.262: INFO: (18) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:160/proxy/: foo (200; 29.740397ms)
Jun 10 16:01:04.266: INFO: (18) /api/v1/namespaces/proxy-5512/services/http:proxy-service-4jj6m:portname1/proxy/: foo (200; 33.7667ms)
Jun 10 16:01:04.266: INFO: (18) /api/v1/namespaces/proxy-5512/services/proxy-service-4jj6m:portname1/proxy/: foo (200; 34.333857ms)
Jun 10 16:01:04.267: INFO: (18) /api/v1/namespaces/proxy-5512/services/https:proxy-service-4jj6m:tlsportname1/proxy/: tls baz (200; 35.460396ms)
Jun 10 16:01:04.273: INFO: (18) /api/v1/namespaces/proxy-5512/services/proxy-service-4jj6m:portname2/proxy/: bar (200; 41.554578ms)
Jun 10 16:01:04.275: INFO: (18) /api/v1/namespaces/proxy-5512/services/http:proxy-service-4jj6m:portname2/proxy/: bar (200; 43.6902ms)
Jun 10 16:01:04.283: INFO: (19) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:160/proxy/: foo (200; 7.644836ms)
Jun 10 16:01:04.284: INFO: (19) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:160/proxy/: foo (200; 7.949693ms)
Jun 10 16:01:04.289: INFO: (19) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:460/proxy/: tls baz (200; 13.502648ms)
Jun 10 16:01:04.289: INFO: (19) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:1080/proxy/rewriteme">test<... (200; 13.331391ms)
Jun 10 16:01:04.289: INFO: (19) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:1080/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:1080/proxy/rewriteme">... (200; 13.45612ms)
Jun 10 16:01:04.289: INFO: (19) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6:162/proxy/: bar (200; 13.353822ms)
Jun 10 16:01:04.289: INFO: (19) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:443/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:443/proxy/tlsrewritem... (200; 13.670546ms)
Jun 10 16:01:04.289: INFO: (19) /api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6/proxy/: <a href="/api/v1/namespaces/proxy-5512/pods/proxy-service-4jj6m-wgmg6/proxy/rewriteme">test</a> (200; 13.662536ms)
Jun 10 16:01:04.289: INFO: (19) /api/v1/namespaces/proxy-5512/pods/https:proxy-service-4jj6m-wgmg6:462/proxy/: tls qux (200; 13.812269ms)
Jun 10 16:01:04.290: INFO: (19) /api/v1/namespaces/proxy-5512/pods/http:proxy-service-4jj6m-wgmg6:162/proxy/: bar (200; 14.234427ms)
Jun 10 16:01:04.293: INFO: (19) /api/v1/namespaces/proxy-5512/services/proxy-service-4jj6m:portname2/proxy/: bar (200; 17.549644ms)
Jun 10 16:01:04.293: INFO: (19) /api/v1/namespaces/proxy-5512/services/proxy-service-4jj6m:portname1/proxy/: foo (200; 17.471079ms)
Jun 10 16:01:04.293: INFO: (19) /api/v1/namespaces/proxy-5512/services/http:proxy-service-4jj6m:portname2/proxy/: bar (200; 17.462576ms)
Jun 10 16:01:04.293: INFO: (19) /api/v1/namespaces/proxy-5512/services/https:proxy-service-4jj6m:tlsportname1/proxy/: tls baz (200; 17.535133ms)
Jun 10 16:01:04.293: INFO: (19) /api/v1/namespaces/proxy-5512/services/https:proxy-service-4jj6m:tlsportname2/proxy/: tls qux (200; 17.551738ms)
Jun 10 16:01:04.294: INFO: (19) /api/v1/namespaces/proxy-5512/services/http:proxy-service-4jj6m:portname1/proxy/: foo (200; 18.091827ms)
STEP: deleting ReplicationController proxy-service-4jj6m in namespace proxy-5512, will wait for the garbage collector to delete the pods
Jun 10 16:01:04.364: INFO: Deleting ReplicationController proxy-service-4jj6m took: 16.88933ms
Jun 10 16:01:05.764: INFO: Terminating ReplicationController proxy-service-4jj6m pods took: 1.400440109s
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:01:08.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5512" for this suite.

• [SLOW TEST:15.403 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":305,"completed":66,"skipped":799,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:01:08.078: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Jun 10 16:01:08.147: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
Jun 10 16:01:22.668: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:02:28.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6122" for this suite.

• [SLOW TEST:80.871 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":305,"completed":67,"skipped":814,"failed":0}
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:02:28.950: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-93733239-85e0-4942-b094-bcdcfd978315
STEP: Creating a pod to test consume secrets
Jun 10 16:02:29.136: INFO: Waiting up to 5m0s for pod "pod-secrets-833acf34-e329-4cf5-98d9-78a2095d3163" in namespace "secrets-4093" to be "Succeeded or Failed"
Jun 10 16:02:29.178: INFO: Pod "pod-secrets-833acf34-e329-4cf5-98d9-78a2095d3163": Phase="Pending", Reason="", readiness=false. Elapsed: 42.17704ms
Jun 10 16:02:31.183: INFO: Pod "pod-secrets-833acf34-e329-4cf5-98d9-78a2095d3163": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046683655s
STEP: Saw pod success
Jun 10 16:02:31.183: INFO: Pod "pod-secrets-833acf34-e329-4cf5-98d9-78a2095d3163" satisfied condition "Succeeded or Failed"
Jun 10 16:02:31.186: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod pod-secrets-833acf34-e329-4cf5-98d9-78a2095d3163 container secret-volume-test: <nil>
STEP: delete the pod
Jun 10 16:02:31.248: INFO: Waiting for pod pod-secrets-833acf34-e329-4cf5-98d9-78a2095d3163 to disappear
Jun 10 16:02:31.267: INFO: Pod pod-secrets-833acf34-e329-4cf5-98d9-78a2095d3163 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:02:31.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4093" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":68,"skipped":815,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:02:31.282: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 10 16:02:32.312: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 10 16:02:34.347: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758937752, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758937752, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758937752, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758937752, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 10 16:02:37.366: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:02:37.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3146" for this suite.
STEP: Destroying namespace "webhook-3146-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.451 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":305,"completed":69,"skipped":819,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:02:37.734: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test env composition
Jun 10 16:02:37.830: INFO: Waiting up to 5m0s for pod "var-expansion-0c5c63ea-b422-4ccc-b368-a3d288d06aa5" in namespace "var-expansion-1" to be "Succeeded or Failed"
Jun 10 16:02:37.840: INFO: Pod "var-expansion-0c5c63ea-b422-4ccc-b368-a3d288d06aa5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.168223ms
Jun 10 16:02:39.844: INFO: Pod "var-expansion-0c5c63ea-b422-4ccc-b368-a3d288d06aa5": Phase="Running", Reason="", readiness=true. Elapsed: 2.013744149s
Jun 10 16:02:41.847: INFO: Pod "var-expansion-0c5c63ea-b422-4ccc-b368-a3d288d06aa5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01624522s
STEP: Saw pod success
Jun 10 16:02:41.847: INFO: Pod "var-expansion-0c5c63ea-b422-4ccc-b368-a3d288d06aa5" satisfied condition "Succeeded or Failed"
Jun 10 16:02:41.848: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod var-expansion-0c5c63ea-b422-4ccc-b368-a3d288d06aa5 container dapi-container: <nil>
STEP: delete the pod
Jun 10 16:02:41.873: INFO: Waiting for pod var-expansion-0c5c63ea-b422-4ccc-b368-a3d288d06aa5 to disappear
Jun 10 16:02:41.878: INFO: Pod var-expansion-0c5c63ea-b422-4ccc-b368-a3d288d06aa5 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:02:41.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":305,"completed":70,"skipped":834,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:02:41.886: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:02:53.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9028" for this suite.

• [SLOW TEST:11.200 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":305,"completed":71,"skipped":866,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:02:53.085: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 10 16:02:54.264: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 10 16:02:56.304: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758937774, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758937774, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758937774, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758937774, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 10 16:02:59.328: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:02:59.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5748" for this suite.
STEP: Destroying namespace "webhook-5748-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.499 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":305,"completed":72,"skipped":872,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:02:59.585: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override all
Jun 10 16:02:59.655: INFO: Waiting up to 5m0s for pod "client-containers-64c9d112-9453-447b-bac1-1c678180f513" in namespace "containers-4656" to be "Succeeded or Failed"
Jun 10 16:02:59.667: INFO: Pod "client-containers-64c9d112-9453-447b-bac1-1c678180f513": Phase="Pending", Reason="", readiness=false. Elapsed: 12.482452ms
Jun 10 16:03:01.671: INFO: Pod "client-containers-64c9d112-9453-447b-bac1-1c678180f513": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016138248s
STEP: Saw pod success
Jun 10 16:03:01.671: INFO: Pod "client-containers-64c9d112-9453-447b-bac1-1c678180f513" satisfied condition "Succeeded or Failed"
Jun 10 16:03:01.674: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod client-containers-64c9d112-9453-447b-bac1-1c678180f513 container test-container: <nil>
STEP: delete the pod
Jun 10 16:03:01.711: INFO: Waiting for pod client-containers-64c9d112-9453-447b-bac1-1c678180f513 to disappear
Jun 10 16:03:01.716: INFO: Pod client-containers-64c9d112-9453-447b-bac1-1c678180f513 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:03:01.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4656" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":305,"completed":73,"skipped":921,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:03:01.750: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Jun 10 16:03:01.849: INFO: namespace kubectl-3354
Jun 10 16:03:01.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-3354 create -f -'
Jun 10 16:03:02.870: INFO: stderr: ""
Jun 10 16:03:02.870: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jun 10 16:03:03.873: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 10 16:03:03.873: INFO: Found 0 / 1
Jun 10 16:03:04.879: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 10 16:03:04.879: INFO: Found 1 / 1
Jun 10 16:03:04.879: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 10 16:03:04.883: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 10 16:03:04.883: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 10 16:03:04.883: INFO: wait on agnhost-primary startup in kubectl-3354 
Jun 10 16:03:04.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-3354 logs agnhost-primary-xlxzx agnhost-primary'
Jun 10 16:03:05.058: INFO: stderr: ""
Jun 10 16:03:05.058: INFO: stdout: "Paused\n"
STEP: exposing RC
Jun 10 16:03:05.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-3354 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jun 10 16:03:05.260: INFO: stderr: ""
Jun 10 16:03:05.260: INFO: stdout: "service/rm2 exposed\n"
Jun 10 16:03:05.272: INFO: Service rm2 in namespace kubectl-3354 found.
STEP: exposing service
Jun 10 16:03:07.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-3354 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jun 10 16:03:07.494: INFO: stderr: ""
Jun 10 16:03:07.494: INFO: stdout: "service/rm3 exposed\n"
Jun 10 16:03:07.507: INFO: Service rm3 in namespace kubectl-3354 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:03:09.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3354" for this suite.

• [SLOW TEST:7.773 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1222
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":305,"completed":74,"skipped":930,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:03:09.523: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service multi-endpoint-test in namespace services-1611
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1611 to expose endpoints map[]
Jun 10 16:03:09.600: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Jun 10 16:03:10.609: INFO: successfully validated that service multi-endpoint-test in namespace services-1611 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-1611
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1611 to expose endpoints map[pod1:[100]]
Jun 10 16:03:12.655: INFO: successfully validated that service multi-endpoint-test in namespace services-1611 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-1611
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1611 to expose endpoints map[pod1:[100] pod2:[101]]
Jun 10 16:03:14.775: INFO: successfully validated that service multi-endpoint-test in namespace services-1611 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-1611
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1611 to expose endpoints map[pod2:[101]]
Jun 10 16:03:14.871: INFO: successfully validated that service multi-endpoint-test in namespace services-1611 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-1611
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1611 to expose endpoints map[]
Jun 10 16:03:15.058: INFO: successfully validated that service multi-endpoint-test in namespace services-1611 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:03:15.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1611" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:5.806 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":305,"completed":75,"skipped":943,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:03:15.329: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-05177da6-6288-4750-afd1-8d1eb9824258
STEP: Creating a pod to test consume configMaps
Jun 10 16:03:15.617: INFO: Waiting up to 5m0s for pod "pod-configmaps-d2dbbb18-8e86-4de2-9229-64a44bdaf8ec" in namespace "configmap-6671" to be "Succeeded or Failed"
Jun 10 16:03:15.637: INFO: Pod "pod-configmaps-d2dbbb18-8e86-4de2-9229-64a44bdaf8ec": Phase="Pending", Reason="", readiness=false. Elapsed: 19.547462ms
Jun 10 16:03:17.644: INFO: Pod "pod-configmaps-d2dbbb18-8e86-4de2-9229-64a44bdaf8ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026768012s
Jun 10 16:03:19.655: INFO: Pod "pod-configmaps-d2dbbb18-8e86-4de2-9229-64a44bdaf8ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037761453s
STEP: Saw pod success
Jun 10 16:03:19.655: INFO: Pod "pod-configmaps-d2dbbb18-8e86-4de2-9229-64a44bdaf8ec" satisfied condition "Succeeded or Failed"
Jun 10 16:03:19.665: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-5pcx5 pod pod-configmaps-d2dbbb18-8e86-4de2-9229-64a44bdaf8ec container configmap-volume-test: <nil>
STEP: delete the pod
Jun 10 16:03:19.724: INFO: Waiting for pod pod-configmaps-d2dbbb18-8e86-4de2-9229-64a44bdaf8ec to disappear
Jun 10 16:03:19.730: INFO: Pod pod-configmaps-d2dbbb18-8e86-4de2-9229-64a44bdaf8ec no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:03:19.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6671" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":76,"skipped":950,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:03:19.745: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 10 16:03:19.793: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jun 10 16:03:36.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=crd-publish-openapi-4292 --namespace=crd-publish-openapi-4292 create -f -'
Jun 10 16:03:37.766: INFO: stderr: ""
Jun 10 16:03:37.766: INFO: stdout: "e2e-test-crd-publish-openapi-1383-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jun 10 16:03:37.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=crd-publish-openapi-4292 --namespace=crd-publish-openapi-4292 delete e2e-test-crd-publish-openapi-1383-crds test-cr'
Jun 10 16:03:38.025: INFO: stderr: ""
Jun 10 16:03:38.025: INFO: stdout: "e2e-test-crd-publish-openapi-1383-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jun 10 16:03:38.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=crd-publish-openapi-4292 --namespace=crd-publish-openapi-4292 apply -f -'
Jun 10 16:03:38.689: INFO: stderr: ""
Jun 10 16:03:38.689: INFO: stdout: "e2e-test-crd-publish-openapi-1383-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jun 10 16:03:38.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=crd-publish-openapi-4292 --namespace=crd-publish-openapi-4292 delete e2e-test-crd-publish-openapi-1383-crds test-cr'
Jun 10 16:03:38.888: INFO: stderr: ""
Jun 10 16:03:38.888: INFO: stdout: "e2e-test-crd-publish-openapi-1383-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jun 10 16:03:38.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=crd-publish-openapi-4292 explain e2e-test-crd-publish-openapi-1383-crds'
Jun 10 16:03:39.434: INFO: stderr: ""
Jun 10 16:03:39.434: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1383-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:03:56.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4292" for this suite.

• [SLOW TEST:36.904 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":305,"completed":77,"skipped":953,"failed":0}
SSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:03:56.649: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-6234/configmap-test-cad0335b-b889-4fcf-b55b-a7ba113b00da
STEP: Creating a pod to test consume configMaps
Jun 10 16:03:56.710: INFO: Waiting up to 5m0s for pod "pod-configmaps-3282c942-4fc4-465a-9d53-c017bbe5a014" in namespace "configmap-6234" to be "Succeeded or Failed"
Jun 10 16:03:56.722: INFO: Pod "pod-configmaps-3282c942-4fc4-465a-9d53-c017bbe5a014": Phase="Pending", Reason="", readiness=false. Elapsed: 12.573595ms
Jun 10 16:03:58.729: INFO: Pod "pod-configmaps-3282c942-4fc4-465a-9d53-c017bbe5a014": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019523108s
Jun 10 16:04:00.734: INFO: Pod "pod-configmaps-3282c942-4fc4-465a-9d53-c017bbe5a014": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024247343s
STEP: Saw pod success
Jun 10 16:04:00.734: INFO: Pod "pod-configmaps-3282c942-4fc4-465a-9d53-c017bbe5a014" satisfied condition "Succeeded or Failed"
Jun 10 16:04:00.738: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod pod-configmaps-3282c942-4fc4-465a-9d53-c017bbe5a014 container env-test: <nil>
STEP: delete the pod
Jun 10 16:04:00.771: INFO: Waiting for pod pod-configmaps-3282c942-4fc4-465a-9d53-c017bbe5a014 to disappear
Jun 10 16:04:00.780: INFO: Pod pod-configmaps-3282c942-4fc4-465a-9d53-c017bbe5a014 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:04:00.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6234" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":305,"completed":78,"skipped":956,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:04:00.794: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-6310b8d7-4ddf-4b5b-aff3-57521adcb4c2 in namespace container-probe-1154
Jun 10 16:04:02.890: INFO: Started pod busybox-6310b8d7-4ddf-4b5b-aff3-57521adcb4c2 in namespace container-probe-1154
STEP: checking the pod's current state and verifying that restartCount is present
Jun 10 16:04:02.892: INFO: Initial restart count of pod busybox-6310b8d7-4ddf-4b5b-aff3-57521adcb4c2 is 0
Jun 10 16:04:49.055: INFO: Restart count of pod container-probe-1154/busybox-6310b8d7-4ddf-4b5b-aff3-57521adcb4c2 is now 1 (46.162481539s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:04:49.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1154" for this suite.

• [SLOW TEST:48.400 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":305,"completed":79,"skipped":987,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:04:49.208: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 10 16:04:49.356: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f0641b3c-5186-44e9-a3fe-ac89464e9f8f" in namespace "projected-4600" to be "Succeeded or Failed"
Jun 10 16:04:49.365: INFO: Pod "downwardapi-volume-f0641b3c-5186-44e9-a3fe-ac89464e9f8f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.255421ms
Jun 10 16:04:51.370: INFO: Pod "downwardapi-volume-f0641b3c-5186-44e9-a3fe-ac89464e9f8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013729277s
Jun 10 16:04:53.387: INFO: Pod "downwardapi-volume-f0641b3c-5186-44e9-a3fe-ac89464e9f8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031108882s
STEP: Saw pod success
Jun 10 16:04:53.388: INFO: Pod "downwardapi-volume-f0641b3c-5186-44e9-a3fe-ac89464e9f8f" satisfied condition "Succeeded or Failed"
Jun 10 16:04:53.398: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod downwardapi-volume-f0641b3c-5186-44e9-a3fe-ac89464e9f8f container client-container: <nil>
STEP: delete the pod
Jun 10 16:04:53.432: INFO: Waiting for pod downwardapi-volume-f0641b3c-5186-44e9-a3fe-ac89464e9f8f to disappear
Jun 10 16:04:53.436: INFO: Pod downwardapi-volume-f0641b3c-5186-44e9-a3fe-ac89464e9f8f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:04:53.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4600" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":305,"completed":80,"skipped":1038,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:04:53.447: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:05:24.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7417" for this suite.
STEP: Destroying namespace "nsdeletetest-6151" for this suite.
Jun 10 16:05:24.761: INFO: Namespace nsdeletetest-6151 was already deleted
STEP: Destroying namespace "nsdeletetest-2513" for this suite.

• [SLOW TEST:31.328 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":305,"completed":81,"skipped":1050,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:05:24.775: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun 10 16:05:24.887: INFO: Waiting up to 5m0s for pod "pod-5f9acaa9-57ea-43ce-9ae4-90fc3290c17f" in namespace "emptydir-8894" to be "Succeeded or Failed"
Jun 10 16:05:24.894: INFO: Pod "pod-5f9acaa9-57ea-43ce-9ae4-90fc3290c17f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.599989ms
Jun 10 16:05:26.897: INFO: Pod "pod-5f9acaa9-57ea-43ce-9ae4-90fc3290c17f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010917687s
Jun 10 16:05:28.901: INFO: Pod "pod-5f9acaa9-57ea-43ce-9ae4-90fc3290c17f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014783007s
STEP: Saw pod success
Jun 10 16:05:28.901: INFO: Pod "pod-5f9acaa9-57ea-43ce-9ae4-90fc3290c17f" satisfied condition "Succeeded or Failed"
Jun 10 16:05:28.904: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod pod-5f9acaa9-57ea-43ce-9ae4-90fc3290c17f container test-container: <nil>
STEP: delete the pod
Jun 10 16:05:28.935: INFO: Waiting for pod pod-5f9acaa9-57ea-43ce-9ae4-90fc3290c17f to disappear
Jun 10 16:05:28.947: INFO: Pod pod-5f9acaa9-57ea-43ce-9ae4-90fc3290c17f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:05:28.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8894" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":82,"skipped":1057,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:05:28.969: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Request ServerVersion
STEP: Confirm major version
Jun 10 16:05:29.056: INFO: Major version: 1
STEP: Confirm minor version
Jun 10 16:05:29.057: INFO: cleanMinorVersion: 19
Jun 10 16:05:29.057: INFO: Minor version: 19
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:05:29.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-7488" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":305,"completed":83,"skipped":1073,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:05:29.071: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Jun 10 16:05:29.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-990 create -f -'
Jun 10 16:05:30.190: INFO: stderr: ""
Jun 10 16:05:30.190: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jun 10 16:05:31.195: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 10 16:05:31.195: INFO: Found 0 / 1
Jun 10 16:05:32.196: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 10 16:05:32.196: INFO: Found 1 / 1
Jun 10 16:05:32.196: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jun 10 16:05:32.200: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 10 16:05:32.200: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 10 16:05:32.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-990 patch pod agnhost-primary-9k7pz -p {"metadata":{"annotations":{"x":"y"}}}'
Jun 10 16:05:32.445: INFO: stderr: ""
Jun 10 16:05:32.445: INFO: stdout: "pod/agnhost-primary-9k7pz patched\n"
STEP: checking annotations
Jun 10 16:05:32.456: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 10 16:05:32.456: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:05:32.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-990" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":305,"completed":84,"skipped":1075,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:05:32.476: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:05:35.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2278" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":305,"completed":85,"skipped":1213,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:05:35.650: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-0419eeba-20e8-4470-a547-ad6a8c9b8131
STEP: Creating configMap with name cm-test-opt-upd-06c58abf-583f-4df7-b82a-4447e5712855
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-0419eeba-20e8-4470-a547-ad6a8c9b8131
STEP: Updating configmap cm-test-opt-upd-06c58abf-583f-4df7-b82a-4447e5712855
STEP: Creating configMap with name cm-test-opt-create-e9d011b6-a03e-41b1-b89d-fd9ec7b40a3a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:06:54.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2931" for this suite.

• [SLOW TEST:78.592 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":86,"skipped":1227,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:06:54.242: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 10 16:06:55.263: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 10 16:06:57.272: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758938015, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758938015, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758938015, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758938015, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 10 16:07:00.289: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:07:00.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7533" for this suite.
STEP: Destroying namespace "webhook-7533-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.817 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":305,"completed":87,"skipped":1252,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:07:01.061: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Starting the proxy
Jun 10 16:07:01.160: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-8945 proxy --unix-socket=/tmp/kubectl-proxy-unix348312570/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:07:01.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8945" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":305,"completed":88,"skipped":1256,"failed":0}
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:07:01.412: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 10 16:07:03.549: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:07:03.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8948" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":89,"skipped":1260,"failed":0}
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:07:03.594: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-89b6d4cc-c3f0-4081-90ba-4b13a37a5827
STEP: Creating a pod to test consume secrets
Jun 10 16:07:03.659: INFO: Waiting up to 5m0s for pod "pod-secrets-33f0c19b-2417-4910-9f6d-e06afbd64dc4" in namespace "secrets-4744" to be "Succeeded or Failed"
Jun 10 16:07:03.666: INFO: Pod "pod-secrets-33f0c19b-2417-4910-9f6d-e06afbd64dc4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.484392ms
Jun 10 16:07:05.677: INFO: Pod "pod-secrets-33f0c19b-2417-4910-9f6d-e06afbd64dc4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017990003s
STEP: Saw pod success
Jun 10 16:07:05.677: INFO: Pod "pod-secrets-33f0c19b-2417-4910-9f6d-e06afbd64dc4" satisfied condition "Succeeded or Failed"
Jun 10 16:07:05.690: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-5pcx5 pod pod-secrets-33f0c19b-2417-4910-9f6d-e06afbd64dc4 container secret-volume-test: <nil>
STEP: delete the pod
Jun 10 16:07:05.812: INFO: Waiting for pod pod-secrets-33f0c19b-2417-4910-9f6d-e06afbd64dc4 to disappear
Jun 10 16:07:05.878: INFO: Pod pod-secrets-33f0c19b-2417-4910-9f6d-e06afbd64dc4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:07:05.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4744" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":90,"skipped":1265,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:07:05.927: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod with failed condition
STEP: updating the pod
Jun 10 16:09:06.572: INFO: Successfully updated pod "var-expansion-deb16e98-ab70-4567-9a36-df23e855a411"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Jun 10 16:09:08.583: INFO: Deleting pod "var-expansion-deb16e98-ab70-4567-9a36-df23e855a411" in namespace "var-expansion-6015"
Jun 10 16:09:08.589: INFO: Wait up to 5m0s for pod "var-expansion-deb16e98-ab70-4567-9a36-df23e855a411" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:09:46.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6015" for this suite.

• [SLOW TEST:160.683 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":305,"completed":91,"skipped":1273,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:09:46.611: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun 10 16:09:46.670: INFO: Number of nodes with available pods: 0
Jun 10 16:09:46.670: INFO: Node target-cluster-control-plane-8m52s is running more than one daemon pod
Jun 10 16:09:47.681: INFO: Number of nodes with available pods: 0
Jun 10 16:09:47.681: INFO: Node target-cluster-control-plane-8m52s is running more than one daemon pod
Jun 10 16:09:48.678: INFO: Number of nodes with available pods: 0
Jun 10 16:09:48.678: INFO: Node target-cluster-control-plane-8m52s is running more than one daemon pod
Jun 10 16:09:49.707: INFO: Number of nodes with available pods: 2
Jun 10 16:09:49.707: INFO: Node target-cluster-md-0-6b59c4f65-5pcx5 is running more than one daemon pod
Jun 10 16:09:50.675: INFO: Number of nodes with available pods: 3
Jun 10 16:09:50.675: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jun 10 16:09:50.697: INFO: Number of nodes with available pods: 2
Jun 10 16:09:50.697: INFO: Node target-cluster-md-0-6b59c4f65-5pcx5 is running more than one daemon pod
Jun 10 16:09:51.704: INFO: Number of nodes with available pods: 2
Jun 10 16:09:51.704: INFO: Node target-cluster-md-0-6b59c4f65-5pcx5 is running more than one daemon pod
Jun 10 16:09:52.705: INFO: Number of nodes with available pods: 2
Jun 10 16:09:52.705: INFO: Node target-cluster-md-0-6b59c4f65-5pcx5 is running more than one daemon pod
Jun 10 16:09:53.704: INFO: Number of nodes with available pods: 2
Jun 10 16:09:53.704: INFO: Node target-cluster-md-0-6b59c4f65-5pcx5 is running more than one daemon pod
Jun 10 16:09:54.703: INFO: Number of nodes with available pods: 2
Jun 10 16:09:54.703: INFO: Node target-cluster-md-0-6b59c4f65-5pcx5 is running more than one daemon pod
Jun 10 16:09:55.707: INFO: Number of nodes with available pods: 2
Jun 10 16:09:55.707: INFO: Node target-cluster-md-0-6b59c4f65-5pcx5 is running more than one daemon pod
Jun 10 16:09:56.706: INFO: Number of nodes with available pods: 3
Jun 10 16:09:56.706: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5251, will wait for the garbage collector to delete the pods
Jun 10 16:09:56.789: INFO: Deleting DaemonSet.extensions daemon-set took: 9.237789ms
Jun 10 16:09:58.193: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.403843948s
Jun 10 16:10:09.195: INFO: Number of nodes with available pods: 0
Jun 10 16:10:09.195: INFO: Number of running nodes: 0, number of available pods: 0
Jun 10 16:10:09.201: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5251/daemonsets","resourceVersion":"23162"},"items":null}

Jun 10 16:10:09.204: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5251/pods","resourceVersion":"23162"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:10:09.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5251" for this suite.

• [SLOW TEST:22.612 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":305,"completed":92,"skipped":1292,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:10:09.223: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-2361
Jun 10 16:10:11.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=services-2361 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jun 10 16:10:11.528: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jun 10 16:10:11.528: INFO: stdout: "iptables"
Jun 10 16:10:11.528: INFO: proxyMode: iptables
Jun 10 16:10:11.535: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jun 10 16:10:11.540: INFO: Pod kube-proxy-mode-detector still exists
Jun 10 16:10:13.540: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jun 10 16:10:13.543: INFO: Pod kube-proxy-mode-detector still exists
Jun 10 16:10:15.540: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jun 10 16:10:15.543: INFO: Pod kube-proxy-mode-detector still exists
Jun 10 16:10:17.540: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jun 10 16:10:17.548: INFO: Pod kube-proxy-mode-detector still exists
Jun 10 16:10:19.541: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jun 10 16:10:19.544: INFO: Pod kube-proxy-mode-detector still exists
Jun 10 16:10:21.540: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jun 10 16:10:21.542: INFO: Pod kube-proxy-mode-detector still exists
Jun 10 16:10:23.540: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jun 10 16:10:23.543: INFO: Pod kube-proxy-mode-detector still exists
Jun 10 16:10:25.540: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jun 10 16:10:25.551: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-2361
STEP: creating replication controller affinity-nodeport-timeout in namespace services-2361
I0610 16:10:25.604634      26 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-2361, replica count: 3
I0610 16:10:28.656164      26 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0610 16:10:31.656448      26 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 10 16:10:31.670: INFO: Creating new exec pod
Jun 10 16:10:34.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=services-2361 exec execpod-affinityzfpg7 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
Jun 10 16:10:34.961: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Jun 10 16:10:34.961: INFO: stdout: ""
Jun 10 16:10:34.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=services-2361 exec execpod-affinityzfpg7 -- /bin/sh -x -c nc -zv -t -w 2 10.131.2.83 80'
Jun 10 16:10:35.267: INFO: stderr: "+ nc -zv -t -w 2 10.131.2.83 80\nConnection to 10.131.2.83 80 port [tcp/http] succeeded!\n"
Jun 10 16:10:35.267: INFO: stdout: ""
Jun 10 16:10:35.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=services-2361 exec execpod-affinityzfpg7 -- /bin/sh -x -c nc -zv -t -w 2 172.17.0.4 32096'
Jun 10 16:10:35.510: INFO: stderr: "+ nc -zv -t -w 2 172.17.0.4 32096\nConnection to 172.17.0.4 32096 port [tcp/32096] succeeded!\n"
Jun 10 16:10:35.510: INFO: stdout: ""
Jun 10 16:10:35.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=services-2361 exec execpod-affinityzfpg7 -- /bin/sh -x -c nc -zv -t -w 2 172.17.0.6 32096'
Jun 10 16:10:35.851: INFO: stderr: "+ nc -zv -t -w 2 172.17.0.6 32096\nConnection to 172.17.0.6 32096 port [tcp/32096] succeeded!\n"
Jun 10 16:10:35.851: INFO: stdout: ""
Jun 10 16:10:35.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=services-2361 exec execpod-affinityzfpg7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.17.0.4:32096/ ; done'
Jun 10 16:10:36.237: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32096/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32096/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32096/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32096/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32096/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32096/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32096/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32096/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32096/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32096/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32096/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32096/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32096/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32096/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32096/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32096/\n"
Jun 10 16:10:36.237: INFO: stdout: "\naffinity-nodeport-timeout-fw247\naffinity-nodeport-timeout-fw247\naffinity-nodeport-timeout-fw247\naffinity-nodeport-timeout-fw247\naffinity-nodeport-timeout-fw247\naffinity-nodeport-timeout-fw247\naffinity-nodeport-timeout-fw247\naffinity-nodeport-timeout-fw247\naffinity-nodeport-timeout-fw247\naffinity-nodeport-timeout-fw247\naffinity-nodeport-timeout-fw247\naffinity-nodeport-timeout-fw247\naffinity-nodeport-timeout-fw247\naffinity-nodeport-timeout-fw247\naffinity-nodeport-timeout-fw247\naffinity-nodeport-timeout-fw247"
Jun 10 16:10:36.237: INFO: Received response from host: affinity-nodeport-timeout-fw247
Jun 10 16:10:36.237: INFO: Received response from host: affinity-nodeport-timeout-fw247
Jun 10 16:10:36.237: INFO: Received response from host: affinity-nodeport-timeout-fw247
Jun 10 16:10:36.237: INFO: Received response from host: affinity-nodeport-timeout-fw247
Jun 10 16:10:36.237: INFO: Received response from host: affinity-nodeport-timeout-fw247
Jun 10 16:10:36.237: INFO: Received response from host: affinity-nodeport-timeout-fw247
Jun 10 16:10:36.237: INFO: Received response from host: affinity-nodeport-timeout-fw247
Jun 10 16:10:36.237: INFO: Received response from host: affinity-nodeport-timeout-fw247
Jun 10 16:10:36.237: INFO: Received response from host: affinity-nodeport-timeout-fw247
Jun 10 16:10:36.237: INFO: Received response from host: affinity-nodeport-timeout-fw247
Jun 10 16:10:36.237: INFO: Received response from host: affinity-nodeport-timeout-fw247
Jun 10 16:10:36.237: INFO: Received response from host: affinity-nodeport-timeout-fw247
Jun 10 16:10:36.237: INFO: Received response from host: affinity-nodeport-timeout-fw247
Jun 10 16:10:36.237: INFO: Received response from host: affinity-nodeport-timeout-fw247
Jun 10 16:10:36.237: INFO: Received response from host: affinity-nodeport-timeout-fw247
Jun 10 16:10:36.237: INFO: Received response from host: affinity-nodeport-timeout-fw247
Jun 10 16:10:36.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=services-2361 exec execpod-affinityzfpg7 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.17.0.4:32096/'
Jun 10 16:10:36.516: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.17.0.4:32096/\n"
Jun 10 16:10:36.516: INFO: stdout: "affinity-nodeport-timeout-fw247"
Jun 10 16:10:51.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=services-2361 exec execpod-affinityzfpg7 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.17.0.4:32096/'
Jun 10 16:10:51.750: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.17.0.4:32096/\n"
Jun 10 16:10:51.750: INFO: stdout: "affinity-nodeport-timeout-7rj4l"
Jun 10 16:10:51.750: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-2361, will wait for the garbage collector to delete the pods
Jun 10 16:10:51.878: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 22.769963ms
Jun 10 16:10:53.378: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 1.500293823s
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:11:09.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2361" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:60.074 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":93,"skipped":1323,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:11:09.297: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-downwardapi-gzv5
STEP: Creating a pod to test atomic-volume-subpath
Jun 10 16:11:09.418: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-gzv5" in namespace "subpath-4391" to be "Succeeded or Failed"
Jun 10 16:11:09.439: INFO: Pod "pod-subpath-test-downwardapi-gzv5": Phase="Pending", Reason="", readiness=false. Elapsed: 21.111202ms
Jun 10 16:11:11.449: INFO: Pod "pod-subpath-test-downwardapi-gzv5": Phase="Running", Reason="", readiness=true. Elapsed: 2.030712778s
Jun 10 16:11:13.453: INFO: Pod "pod-subpath-test-downwardapi-gzv5": Phase="Running", Reason="", readiness=true. Elapsed: 4.035439571s
Jun 10 16:11:15.465: INFO: Pod "pod-subpath-test-downwardapi-gzv5": Phase="Running", Reason="", readiness=true. Elapsed: 6.047378298s
Jun 10 16:11:17.469: INFO: Pod "pod-subpath-test-downwardapi-gzv5": Phase="Running", Reason="", readiness=true. Elapsed: 8.051144991s
Jun 10 16:11:19.498: INFO: Pod "pod-subpath-test-downwardapi-gzv5": Phase="Running", Reason="", readiness=true. Elapsed: 10.080204699s
Jun 10 16:11:21.502: INFO: Pod "pod-subpath-test-downwardapi-gzv5": Phase="Running", Reason="", readiness=true. Elapsed: 12.08404013s
Jun 10 16:11:23.505: INFO: Pod "pod-subpath-test-downwardapi-gzv5": Phase="Running", Reason="", readiness=true. Elapsed: 14.087509316s
Jun 10 16:11:25.509: INFO: Pod "pod-subpath-test-downwardapi-gzv5": Phase="Running", Reason="", readiness=true. Elapsed: 16.091372503s
Jun 10 16:11:27.514: INFO: Pod "pod-subpath-test-downwardapi-gzv5": Phase="Running", Reason="", readiness=true. Elapsed: 18.095711772s
Jun 10 16:11:29.531: INFO: Pod "pod-subpath-test-downwardapi-gzv5": Phase="Running", Reason="", readiness=true. Elapsed: 20.113189748s
Jun 10 16:11:31.553: INFO: Pod "pod-subpath-test-downwardapi-gzv5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.135229961s
STEP: Saw pod success
Jun 10 16:11:31.553: INFO: Pod "pod-subpath-test-downwardapi-gzv5" satisfied condition "Succeeded or Failed"
Jun 10 16:11:31.555: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod pod-subpath-test-downwardapi-gzv5 container test-container-subpath-downwardapi-gzv5: <nil>
STEP: delete the pod
Jun 10 16:11:31.601: INFO: Waiting for pod pod-subpath-test-downwardapi-gzv5 to disappear
Jun 10 16:11:31.605: INFO: Pod pod-subpath-test-downwardapi-gzv5 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-gzv5
Jun 10 16:11:31.605: INFO: Deleting pod "pod-subpath-test-downwardapi-gzv5" in namespace "subpath-4391"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:11:31.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4391" for this suite.

• [SLOW TEST:22.323 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":305,"completed":94,"skipped":1339,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:11:31.621: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 10 16:11:31.664: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:11:33.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5465" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":305,"completed":95,"skipped":1357,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:11:33.768: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun 10 16:11:33.832: INFO: Waiting up to 5m0s for pod "pod-d50fb709-889e-4bac-9773-162f85426f6c" in namespace "emptydir-8112" to be "Succeeded or Failed"
Jun 10 16:11:33.840: INFO: Pod "pod-d50fb709-889e-4bac-9773-162f85426f6c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.682276ms
Jun 10 16:11:35.847: INFO: Pod "pod-d50fb709-889e-4bac-9773-162f85426f6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015086567s
STEP: Saw pod success
Jun 10 16:11:35.847: INFO: Pod "pod-d50fb709-889e-4bac-9773-162f85426f6c" satisfied condition "Succeeded or Failed"
Jun 10 16:11:35.852: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod pod-d50fb709-889e-4bac-9773-162f85426f6c container test-container: <nil>
STEP: delete the pod
Jun 10 16:11:35.878: INFO: Waiting for pod pod-d50fb709-889e-4bac-9773-162f85426f6c to disappear
Jun 10 16:11:35.880: INFO: Pod pod-d50fb709-889e-4bac-9773-162f85426f6c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:11:35.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8112" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":96,"skipped":1369,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:11:35.885: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:11:42.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1897" for this suite.

• [SLOW TEST:7.072 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":305,"completed":97,"skipped":1383,"failed":0}
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:11:42.958: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-6509
STEP: creating service affinity-nodeport in namespace services-6509
STEP: creating replication controller affinity-nodeport in namespace services-6509
I0610 16:11:43.067169      26 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-6509, replica count: 3
I0610 16:11:46.117750      26 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 10 16:11:46.139: INFO: Creating new exec pod
Jun 10 16:11:49.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=services-6509 exec execpod-affinity8vb9p -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
Jun 10 16:11:49.407: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jun 10 16:11:49.407: INFO: stdout: ""
Jun 10 16:11:49.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=services-6509 exec execpod-affinity8vb9p -- /bin/sh -x -c nc -zv -t -w 2 10.133.144.246 80'
Jun 10 16:11:49.671: INFO: stderr: "+ nc -zv -t -w 2 10.133.144.246 80\nConnection to 10.133.144.246 80 port [tcp/http] succeeded!\n"
Jun 10 16:11:49.671: INFO: stdout: ""
Jun 10 16:11:49.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=services-6509 exec execpod-affinity8vb9p -- /bin/sh -x -c nc -zv -t -w 2 172.17.0.5 31014'
Jun 10 16:11:49.900: INFO: stderr: "+ nc -zv -t -w 2 172.17.0.5 31014\nConnection to 172.17.0.5 31014 port [tcp/31014] succeeded!\n"
Jun 10 16:11:49.900: INFO: stdout: ""
Jun 10 16:11:49.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=services-6509 exec execpod-affinity8vb9p -- /bin/sh -x -c nc -zv -t -w 2 172.17.0.6 31014'
Jun 10 16:11:50.192: INFO: stderr: "+ nc -zv -t -w 2 172.17.0.6 31014\nConnection to 172.17.0.6 31014 port [tcp/31014] succeeded!\n"
Jun 10 16:11:50.192: INFO: stdout: ""
Jun 10 16:11:50.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=services-6509 exec execpod-affinity8vb9p -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.17.0.4:31014/ ; done'
Jun 10 16:11:50.629: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:31014/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:31014/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:31014/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:31014/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:31014/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:31014/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:31014/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:31014/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:31014/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:31014/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:31014/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:31014/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:31014/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:31014/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:31014/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:31014/\n"
Jun 10 16:11:50.629: INFO: stdout: "\naffinity-nodeport-n95ms\naffinity-nodeport-n95ms\naffinity-nodeport-n95ms\naffinity-nodeport-n95ms\naffinity-nodeport-n95ms\naffinity-nodeport-n95ms\naffinity-nodeport-n95ms\naffinity-nodeport-n95ms\naffinity-nodeport-n95ms\naffinity-nodeport-n95ms\naffinity-nodeport-n95ms\naffinity-nodeport-n95ms\naffinity-nodeport-n95ms\naffinity-nodeport-n95ms\naffinity-nodeport-n95ms\naffinity-nodeport-n95ms"
Jun 10 16:11:50.629: INFO: Received response from host: affinity-nodeport-n95ms
Jun 10 16:11:50.629: INFO: Received response from host: affinity-nodeport-n95ms
Jun 10 16:11:50.629: INFO: Received response from host: affinity-nodeport-n95ms
Jun 10 16:11:50.629: INFO: Received response from host: affinity-nodeport-n95ms
Jun 10 16:11:50.629: INFO: Received response from host: affinity-nodeport-n95ms
Jun 10 16:11:50.629: INFO: Received response from host: affinity-nodeport-n95ms
Jun 10 16:11:50.629: INFO: Received response from host: affinity-nodeport-n95ms
Jun 10 16:11:50.629: INFO: Received response from host: affinity-nodeport-n95ms
Jun 10 16:11:50.629: INFO: Received response from host: affinity-nodeport-n95ms
Jun 10 16:11:50.629: INFO: Received response from host: affinity-nodeport-n95ms
Jun 10 16:11:50.629: INFO: Received response from host: affinity-nodeport-n95ms
Jun 10 16:11:50.629: INFO: Received response from host: affinity-nodeport-n95ms
Jun 10 16:11:50.629: INFO: Received response from host: affinity-nodeport-n95ms
Jun 10 16:11:50.629: INFO: Received response from host: affinity-nodeport-n95ms
Jun 10 16:11:50.629: INFO: Received response from host: affinity-nodeport-n95ms
Jun 10 16:11:50.629: INFO: Received response from host: affinity-nodeport-n95ms
Jun 10 16:11:50.629: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-6509, will wait for the garbage collector to delete the pods
Jun 10 16:11:50.736: INFO: Deleting ReplicationController affinity-nodeport took: 27.923203ms
Jun 10 16:11:52.237: INFO: Terminating ReplicationController affinity-nodeport pods took: 1.500404306s
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:11:59.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6509" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:16.521 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":98,"skipped":1383,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:11:59.479: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jun 10 16:11:59.800: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1878 /api/v1/namespaces/watch-1878/configmaps/e2e-watch-test-watch-closed 7812e53d-6b32-49a5-8c3e-9010df6695b4 24182 0 2021-06-10 16:11:59 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-06-10 16:11:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 10 16:11:59.801: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1878 /api/v1/namespaces/watch-1878/configmaps/e2e-watch-test-watch-closed 7812e53d-6b32-49a5-8c3e-9010df6695b4 24184 0 2021-06-10 16:11:59 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-06-10 16:11:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jun 10 16:11:59.873: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1878 /api/v1/namespaces/watch-1878/configmaps/e2e-watch-test-watch-closed 7812e53d-6b32-49a5-8c3e-9010df6695b4 24186 0 2021-06-10 16:11:59 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-06-10 16:11:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 10 16:11:59.874: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1878 /api/v1/namespaces/watch-1878/configmaps/e2e-watch-test-watch-closed 7812e53d-6b32-49a5-8c3e-9010df6695b4 24189 0 2021-06-10 16:11:59 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-06-10 16:11:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:11:59.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1878" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":305,"completed":99,"skipped":1395,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:11:59.903: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 10 16:11:59.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-8049 version'
Jun 10 16:12:00.310: INFO: stderr: ""
Jun 10 16:12:00.310: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.11\", GitCommit:\"c6a2f08fc4378c5381dd948d9ad9d1080e3e6b33\", GitTreeState:\"clean\", BuildDate:\"2021-05-12T12:27:07Z\", GoVersion:\"go1.15.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.11\", GitCommit:\"c6a2f08fc4378c5381dd948d9ad9d1080e3e6b33\", GitTreeState:\"clean\", BuildDate:\"2021-05-27T23:47:11Z\", GoVersion:\"go1.15.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:12:00.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8049" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":305,"completed":100,"skipped":1424,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:12:00.347: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on tmpfs
Jun 10 16:12:00.507: INFO: Waiting up to 5m0s for pod "pod-94e843ae-4d53-404b-81a3-7a58685931f8" in namespace "emptydir-937" to be "Succeeded or Failed"
Jun 10 16:12:00.516: INFO: Pod "pod-94e843ae-4d53-404b-81a3-7a58685931f8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.069862ms
Jun 10 16:12:02.518: INFO: Pod "pod-94e843ae-4d53-404b-81a3-7a58685931f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010613233s
STEP: Saw pod success
Jun 10 16:12:02.518: INFO: Pod "pod-94e843ae-4d53-404b-81a3-7a58685931f8" satisfied condition "Succeeded or Failed"
Jun 10 16:12:02.525: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod pod-94e843ae-4d53-404b-81a3-7a58685931f8 container test-container: <nil>
STEP: delete the pod
Jun 10 16:12:02.544: INFO: Waiting for pod pod-94e843ae-4d53-404b-81a3-7a58685931f8 to disappear
Jun 10 16:12:02.547: INFO: Pod pod-94e843ae-4d53-404b-81a3-7a58685931f8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:12:02.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-937" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":101,"skipped":1436,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:12:02.554: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-5833
Jun 10 16:12:04.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=services-5833 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jun 10 16:12:04.992: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jun 10 16:12:04.992: INFO: stdout: "iptables"
Jun 10 16:12:04.992: INFO: proxyMode: iptables
Jun 10 16:12:05.012: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jun 10 16:12:05.022: INFO: Pod kube-proxy-mode-detector still exists
Jun 10 16:12:07.022: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jun 10 16:12:07.033: INFO: Pod kube-proxy-mode-detector still exists
Jun 10 16:12:09.022: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jun 10 16:12:09.027: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-5833
STEP: creating replication controller affinity-clusterip-timeout in namespace services-5833
I0610 16:12:09.105391      26 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-5833, replica count: 3
I0610 16:12:12.157115      26 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 10 16:12:12.187: INFO: Creating new exec pod
Jun 10 16:12:15.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=services-5833 exec execpod-affinityjv6z9 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Jun 10 16:12:15.503: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Jun 10 16:12:15.503: INFO: stdout: ""
Jun 10 16:12:15.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=services-5833 exec execpod-affinityjv6z9 -- /bin/sh -x -c nc -zv -t -w 2 10.132.16.235 80'
Jun 10 16:12:15.792: INFO: stderr: "+ nc -zv -t -w 2 10.132.16.235 80\nConnection to 10.132.16.235 80 port [tcp/http] succeeded!\n"
Jun 10 16:12:15.792: INFO: stdout: ""
Jun 10 16:12:15.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=services-5833 exec execpod-affinityjv6z9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.132.16.235:80/ ; done'
Jun 10 16:12:16.169: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.16.235:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.16.235:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.16.235:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.16.235:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.16.235:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.16.235:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.16.235:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.16.235:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.16.235:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.16.235:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.16.235:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.16.235:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.16.235:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.16.235:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.16.235:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.16.235:80/\n"
Jun 10 16:12:16.169: INFO: stdout: "\naffinity-clusterip-timeout-x692m\naffinity-clusterip-timeout-x692m\naffinity-clusterip-timeout-x692m\naffinity-clusterip-timeout-x692m\naffinity-clusterip-timeout-x692m\naffinity-clusterip-timeout-x692m\naffinity-clusterip-timeout-x692m\naffinity-clusterip-timeout-x692m\naffinity-clusterip-timeout-x692m\naffinity-clusterip-timeout-x692m\naffinity-clusterip-timeout-x692m\naffinity-clusterip-timeout-x692m\naffinity-clusterip-timeout-x692m\naffinity-clusterip-timeout-x692m\naffinity-clusterip-timeout-x692m\naffinity-clusterip-timeout-x692m"
Jun 10 16:12:16.169: INFO: Received response from host: affinity-clusterip-timeout-x692m
Jun 10 16:12:16.169: INFO: Received response from host: affinity-clusterip-timeout-x692m
Jun 10 16:12:16.169: INFO: Received response from host: affinity-clusterip-timeout-x692m
Jun 10 16:12:16.169: INFO: Received response from host: affinity-clusterip-timeout-x692m
Jun 10 16:12:16.170: INFO: Received response from host: affinity-clusterip-timeout-x692m
Jun 10 16:12:16.170: INFO: Received response from host: affinity-clusterip-timeout-x692m
Jun 10 16:12:16.170: INFO: Received response from host: affinity-clusterip-timeout-x692m
Jun 10 16:12:16.170: INFO: Received response from host: affinity-clusterip-timeout-x692m
Jun 10 16:12:16.170: INFO: Received response from host: affinity-clusterip-timeout-x692m
Jun 10 16:12:16.170: INFO: Received response from host: affinity-clusterip-timeout-x692m
Jun 10 16:12:16.170: INFO: Received response from host: affinity-clusterip-timeout-x692m
Jun 10 16:12:16.170: INFO: Received response from host: affinity-clusterip-timeout-x692m
Jun 10 16:12:16.170: INFO: Received response from host: affinity-clusterip-timeout-x692m
Jun 10 16:12:16.170: INFO: Received response from host: affinity-clusterip-timeout-x692m
Jun 10 16:12:16.170: INFO: Received response from host: affinity-clusterip-timeout-x692m
Jun 10 16:12:16.170: INFO: Received response from host: affinity-clusterip-timeout-x692m
Jun 10 16:12:16.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=services-5833 exec execpod-affinityjv6z9 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.132.16.235:80/'
Jun 10 16:12:16.414: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.132.16.235:80/\n"
Jun 10 16:12:16.414: INFO: stdout: "affinity-clusterip-timeout-x692m"
Jun 10 16:12:31.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=services-5833 exec execpod-affinityjv6z9 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.132.16.235:80/'
Jun 10 16:12:31.739: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.132.16.235:80/\n"
Jun 10 16:12:31.739: INFO: stdout: "affinity-clusterip-timeout-qhq7k"
Jun 10 16:12:31.739: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-5833, will wait for the garbage collector to delete the pods
Jun 10 16:12:31.847: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 25.409305ms
Jun 10 16:12:33.248: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 1.4005208s
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:12:45.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5833" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:43.197 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":102,"skipped":1445,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:12:45.768: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Jun 10 16:12:45.889: INFO: Created pod &Pod{ObjectMeta:{dns-5476  dns-5476 /api/v1/namespaces/dns-5476/pods/dns-5476 802f4bb9-fa93-45a8-9779-0dac8c5f77af 24653 0 2021-06-10 16:12:45 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2021-06-10 16:12:45 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vdkjn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vdkjn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vdkjn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 16:12:45.905: INFO: The status of Pod dns-5476 is Pending, waiting for it to be Running (with Ready = true)
Jun 10 16:12:47.909: INFO: The status of Pod dns-5476 is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Jun 10 16:12:47.909: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-5476 PodName:dns-5476 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 10 16:12:47.909: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Verifying customized DNS server is configured on pod...
Jun 10 16:12:48.017: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-5476 PodName:dns-5476 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 10 16:12:48.017: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
Jun 10 16:12:48.130: INFO: Deleting pod dns-5476...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:12:48.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5476" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":305,"completed":103,"skipped":1474,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:12:48.171: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-08862203-9f4b-43e8-ae65-25dfd6396487
STEP: Creating a pod to test consume configMaps
Jun 10 16:12:48.263: INFO: Waiting up to 5m0s for pod "pod-configmaps-65d9093a-1aa3-4f89-87c5-a4b250dd1e5d" in namespace "configmap-9890" to be "Succeeded or Failed"
Jun 10 16:12:48.281: INFO: Pod "pod-configmaps-65d9093a-1aa3-4f89-87c5-a4b250dd1e5d": Phase="Pending", Reason="", readiness=false. Elapsed: 17.5488ms
Jun 10 16:12:50.289: INFO: Pod "pod-configmaps-65d9093a-1aa3-4f89-87c5-a4b250dd1e5d": Phase="Running", Reason="", readiness=true. Elapsed: 2.025696394s
Jun 10 16:12:52.295: INFO: Pod "pod-configmaps-65d9093a-1aa3-4f89-87c5-a4b250dd1e5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031413455s
STEP: Saw pod success
Jun 10 16:12:52.295: INFO: Pod "pod-configmaps-65d9093a-1aa3-4f89-87c5-a4b250dd1e5d" satisfied condition "Succeeded or Failed"
Jun 10 16:12:52.299: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod pod-configmaps-65d9093a-1aa3-4f89-87c5-a4b250dd1e5d container configmap-volume-test: <nil>
STEP: delete the pod
Jun 10 16:12:52.328: INFO: Waiting for pod pod-configmaps-65d9093a-1aa3-4f89-87c5-a4b250dd1e5d to disappear
Jun 10 16:12:52.331: INFO: Pod pod-configmaps-65d9093a-1aa3-4f89-87c5-a4b250dd1e5d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:12:52.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9890" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":104,"skipped":1492,"failed":0}
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:12:52.341: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-j4xr
STEP: Creating a pod to test atomic-volume-subpath
Jun 10 16:12:52.414: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-j4xr" in namespace "subpath-2052" to be "Succeeded or Failed"
Jun 10 16:12:52.431: INFO: Pod "pod-subpath-test-configmap-j4xr": Phase="Pending", Reason="", readiness=false. Elapsed: 17.310026ms
Jun 10 16:12:54.434: INFO: Pod "pod-subpath-test-configmap-j4xr": Phase="Running", Reason="", readiness=true. Elapsed: 2.020155838s
Jun 10 16:12:56.438: INFO: Pod "pod-subpath-test-configmap-j4xr": Phase="Running", Reason="", readiness=true. Elapsed: 4.023582384s
Jun 10 16:12:58.441: INFO: Pod "pod-subpath-test-configmap-j4xr": Phase="Running", Reason="", readiness=true. Elapsed: 6.027043949s
Jun 10 16:13:00.445: INFO: Pod "pod-subpath-test-configmap-j4xr": Phase="Running", Reason="", readiness=true. Elapsed: 8.031198864s
Jun 10 16:13:02.454: INFO: Pod "pod-subpath-test-configmap-j4xr": Phase="Running", Reason="", readiness=true. Elapsed: 10.039671748s
Jun 10 16:13:04.456: INFO: Pod "pod-subpath-test-configmap-j4xr": Phase="Running", Reason="", readiness=true. Elapsed: 12.042080618s
Jun 10 16:13:06.461: INFO: Pod "pod-subpath-test-configmap-j4xr": Phase="Running", Reason="", readiness=true. Elapsed: 14.047170615s
Jun 10 16:13:08.466: INFO: Pod "pod-subpath-test-configmap-j4xr": Phase="Running", Reason="", readiness=true. Elapsed: 16.051720923s
Jun 10 16:13:10.474: INFO: Pod "pod-subpath-test-configmap-j4xr": Phase="Running", Reason="", readiness=true. Elapsed: 18.059567624s
Jun 10 16:13:12.476: INFO: Pod "pod-subpath-test-configmap-j4xr": Phase="Running", Reason="", readiness=true. Elapsed: 20.062084175s
Jun 10 16:13:14.480: INFO: Pod "pod-subpath-test-configmap-j4xr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.066091523s
STEP: Saw pod success
Jun 10 16:13:14.481: INFO: Pod "pod-subpath-test-configmap-j4xr" satisfied condition "Succeeded or Failed"
Jun 10 16:13:14.484: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod pod-subpath-test-configmap-j4xr container test-container-subpath-configmap-j4xr: <nil>
STEP: delete the pod
Jun 10 16:13:14.518: INFO: Waiting for pod pod-subpath-test-configmap-j4xr to disappear
Jun 10 16:13:14.523: INFO: Pod pod-subpath-test-configmap-j4xr no longer exists
STEP: Deleting pod pod-subpath-test-configmap-j4xr
Jun 10 16:13:14.523: INFO: Deleting pod "pod-subpath-test-configmap-j4xr" in namespace "subpath-2052"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:13:14.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2052" for this suite.

• [SLOW TEST:22.206 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":305,"completed":105,"skipped":1494,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:13:14.548: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:13:14.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4075" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":305,"completed":106,"skipped":1501,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:13:14.700: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jun 10 16:13:15.044: INFO: Pod name wrapped-volume-race-b618ba3b-85fc-44d1-b48e-8efbb2ae9733: Found 0 pods out of 5
Jun 10 16:13:20.063: INFO: Pod name wrapped-volume-race-b618ba3b-85fc-44d1-b48e-8efbb2ae9733: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-b618ba3b-85fc-44d1-b48e-8efbb2ae9733 in namespace emptydir-wrapper-6330, will wait for the garbage collector to delete the pods
Jun 10 16:13:32.225: INFO: Deleting ReplicationController wrapped-volume-race-b618ba3b-85fc-44d1-b48e-8efbb2ae9733 took: 41.497518ms
Jun 10 16:13:33.725: INFO: Terminating ReplicationController wrapped-volume-race-b618ba3b-85fc-44d1-b48e-8efbb2ae9733 pods took: 1.500464584s
STEP: Creating RC which spawns configmap-volume pods
Jun 10 16:13:45.761: INFO: Pod name wrapped-volume-race-8007a6b2-b317-4f3e-826d-ca23febd4db9: Found 0 pods out of 5
Jun 10 16:13:50.766: INFO: Pod name wrapped-volume-race-8007a6b2-b317-4f3e-826d-ca23febd4db9: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-8007a6b2-b317-4f3e-826d-ca23febd4db9 in namespace emptydir-wrapper-6330, will wait for the garbage collector to delete the pods
Jun 10 16:14:00.958: INFO: Deleting ReplicationController wrapped-volume-race-8007a6b2-b317-4f3e-826d-ca23febd4db9 took: 60.924726ms
Jun 10 16:14:02.458: INFO: Terminating ReplicationController wrapped-volume-race-8007a6b2-b317-4f3e-826d-ca23febd4db9 pods took: 1.500203527s
STEP: Creating RC which spawns configmap-volume pods
Jun 10 16:14:15.807: INFO: Pod name wrapped-volume-race-2ed483e8-0a9c-4223-84dd-538c2a44dff5: Found 0 pods out of 5
Jun 10 16:14:20.828: INFO: Pod name wrapped-volume-race-2ed483e8-0a9c-4223-84dd-538c2a44dff5: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-2ed483e8-0a9c-4223-84dd-538c2a44dff5 in namespace emptydir-wrapper-6330, will wait for the garbage collector to delete the pods
Jun 10 16:14:30.944: INFO: Deleting ReplicationController wrapped-volume-race-2ed483e8-0a9c-4223-84dd-538c2a44dff5 took: 22.22693ms
Jun 10 16:14:32.445: INFO: Terminating ReplicationController wrapped-volume-race-2ed483e8-0a9c-4223-84dd-538c2a44dff5 pods took: 1.500206806s
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:14:39.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6330" for this suite.

• [SLOW TEST:84.989 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":305,"completed":107,"skipped":1508,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:14:39.689: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 10 16:14:39.732: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jun 10 16:14:39.744: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun 10 16:14:44.748: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 10 16:14:44.748: INFO: Creating deployment "test-rolling-update-deployment"
Jun 10 16:14:44.756: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jun 10 16:14:44.762: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jun 10 16:14:46.773: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jun 10 16:14:46.775: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Jun 10 16:14:46.791: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-643 /apis/apps/v1/namespaces/deployment-643/deployments/test-rolling-update-deployment 40c36e46-02ab-468a-a03a-b1abd3b9e71a 26453 1 2021-06-10 16:14:44 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-06-10 16:14:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-06-10 16:14:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035d2ea8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-06-10 16:14:44 +0000 UTC,LastTransitionTime:2021-06-10 16:14:44 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" has successfully progressed.,LastUpdateTime:2021-06-10 16:14:46 +0000 UTC,LastTransitionTime:2021-06-10 16:14:44 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 10 16:14:46.796: INFO: New ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9  deployment-643 /apis/apps/v1/namespaces/deployment-643/replicasets/test-rolling-update-deployment-c4cb8d6d9 2d1739f6-6e96-414c-a9b1-71fe96493789 26442 1 2021-06-10 16:14:44 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 40c36e46-02ab-468a-a03a-b1abd3b9e71a 0xc002af89f0 0xc002af89f1}] []  [{kube-controller-manager Update apps/v1 2021-06-10 16:14:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40c36e46-02ab-468a-a03a-b1abd3b9e71a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: c4cb8d6d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002af8ad8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 10 16:14:46.796: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jun 10 16:14:46.796: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-643 /apis/apps/v1/namespaces/deployment-643/replicasets/test-rolling-update-controller ad3a744c-275d-41c3-80d1-93f2a4bcac95 26452 2 2021-06-10 16:14:39 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 40c36e46-02ab-468a-a03a-b1abd3b9e71a 0xc002af8867 0xc002af8868}] []  [{e2e.test Update apps/v1 2021-06-10 16:14:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-06-10 16:14:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40c36e46-02ab-468a-a03a-b1abd3b9e71a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002af8968 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 10 16:14:46.801: INFO: Pod "test-rolling-update-deployment-c4cb8d6d9-st7fh" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9-st7fh test-rolling-update-deployment-c4cb8d6d9- deployment-643 /api/v1/namespaces/deployment-643/pods/test-rolling-update-deployment-c4cb8d6d9-st7fh 78d55b56-ed70-4338-bcde-29aa0a8eb17f 26441 0 2021-06-10 16:14:44 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[cni.projectcalico.org/podIP:192.168.24.179/32 cni.projectcalico.org/podIPs:192.168.24.179/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-c4cb8d6d9 2d1739f6-6e96-414c-a9b1-71fe96493789 0xc002af90c0 0xc002af90c1}] []  [{kube-controller-manager Update v1 2021-06-10 16:14:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d1739f6-6e96-414c-a9b1-71fe96493789\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-10 16:14:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-06-10 16:14:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.24.179\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f7s7z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f7s7z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f7s7z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:target-cluster-md-0-6b59c4f65-5pcx5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 16:14:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 16:14:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 16:14:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 16:14:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.17.0.6,PodIP:192.168.24.179,StartTime:2021-06-10 16:14:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-10 16:14:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:containerd://faf38908b47386ddb71df7ec41297522a5d92048263c56c3881701ece5e3270c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.24.179,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:14:46.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-643" for this suite.

• [SLOW TEST:7.125 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":305,"completed":108,"skipped":1525,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:14:46.815: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 10 16:14:46.889: INFO: Waiting up to 5m0s for pod "downwardapi-volume-37a9fa9c-ccf7-44bc-a700-cbb5fa7ddb14" in namespace "downward-api-7441" to be "Succeeded or Failed"
Jun 10 16:14:46.894: INFO: Pod "downwardapi-volume-37a9fa9c-ccf7-44bc-a700-cbb5fa7ddb14": Phase="Pending", Reason="", readiness=false. Elapsed: 4.482458ms
Jun 10 16:14:48.897: INFO: Pod "downwardapi-volume-37a9fa9c-ccf7-44bc-a700-cbb5fa7ddb14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008254703s
STEP: Saw pod success
Jun 10 16:14:48.897: INFO: Pod "downwardapi-volume-37a9fa9c-ccf7-44bc-a700-cbb5fa7ddb14" satisfied condition "Succeeded or Failed"
Jun 10 16:14:48.900: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod downwardapi-volume-37a9fa9c-ccf7-44bc-a700-cbb5fa7ddb14 container client-container: <nil>
STEP: delete the pod
Jun 10 16:14:48.955: INFO: Waiting for pod downwardapi-volume-37a9fa9c-ccf7-44bc-a700-cbb5fa7ddb14 to disappear
Jun 10 16:14:48.966: INFO: Pod downwardapi-volume-37a9fa9c-ccf7-44bc-a700-cbb5fa7ddb14 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:14:48.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7441" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":305,"completed":109,"skipped":1560,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:14:48.999: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:15:05.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9565" for this suite.

• [SLOW TEST:16.296 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":305,"completed":110,"skipped":1572,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:15:05.301: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-9395ea33-db41-4c78-aa70-16a4d8388a17
STEP: Creating a pod to test consume secrets
Jun 10 16:15:05.538: INFO: Waiting up to 5m0s for pod "pod-secrets-8a6974e0-0942-482f-8802-0b3c4cdeaf5d" in namespace "secrets-3434" to be "Succeeded or Failed"
Jun 10 16:15:05.552: INFO: Pod "pod-secrets-8a6974e0-0942-482f-8802-0b3c4cdeaf5d": Phase="Pending", Reason="", readiness=false. Elapsed: 13.33073ms
Jun 10 16:15:07.565: INFO: Pod "pod-secrets-8a6974e0-0942-482f-8802-0b3c4cdeaf5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026671679s
STEP: Saw pod success
Jun 10 16:15:07.565: INFO: Pod "pod-secrets-8a6974e0-0942-482f-8802-0b3c4cdeaf5d" satisfied condition "Succeeded or Failed"
Jun 10 16:15:07.568: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod pod-secrets-8a6974e0-0942-482f-8802-0b3c4cdeaf5d container secret-volume-test: <nil>
STEP: delete the pod
Jun 10 16:15:07.598: INFO: Waiting for pod pod-secrets-8a6974e0-0942-482f-8802-0b3c4cdeaf5d to disappear
Jun 10 16:15:07.600: INFO: Pod pod-secrets-8a6974e0-0942-482f-8802-0b3c4cdeaf5d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:15:07.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3434" for this suite.
STEP: Destroying namespace "secret-namespace-9690" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":305,"completed":111,"skipped":1616,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:15:07.611: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 10 16:15:08.669: INFO: The status of Pod test-webserver-8bd54914-2500-4d1f-88a1-b9fb9e5564e5 is Pending, waiting for it to be Running (with Ready = true)
Jun 10 16:15:10.672: INFO: The status of Pod test-webserver-8bd54914-2500-4d1f-88a1-b9fb9e5564e5 is Running (Ready = false)
Jun 10 16:15:12.671: INFO: The status of Pod test-webserver-8bd54914-2500-4d1f-88a1-b9fb9e5564e5 is Running (Ready = false)
Jun 10 16:15:14.674: INFO: The status of Pod test-webserver-8bd54914-2500-4d1f-88a1-b9fb9e5564e5 is Running (Ready = false)
Jun 10 16:15:16.677: INFO: The status of Pod test-webserver-8bd54914-2500-4d1f-88a1-b9fb9e5564e5 is Running (Ready = false)
Jun 10 16:15:18.672: INFO: The status of Pod test-webserver-8bd54914-2500-4d1f-88a1-b9fb9e5564e5 is Running (Ready = false)
Jun 10 16:15:20.673: INFO: The status of Pod test-webserver-8bd54914-2500-4d1f-88a1-b9fb9e5564e5 is Running (Ready = false)
Jun 10 16:15:22.672: INFO: The status of Pod test-webserver-8bd54914-2500-4d1f-88a1-b9fb9e5564e5 is Running (Ready = false)
Jun 10 16:15:24.678: INFO: The status of Pod test-webserver-8bd54914-2500-4d1f-88a1-b9fb9e5564e5 is Running (Ready = false)
Jun 10 16:15:26.675: INFO: The status of Pod test-webserver-8bd54914-2500-4d1f-88a1-b9fb9e5564e5 is Running (Ready = false)
Jun 10 16:15:28.677: INFO: The status of Pod test-webserver-8bd54914-2500-4d1f-88a1-b9fb9e5564e5 is Running (Ready = false)
Jun 10 16:15:30.672: INFO: The status of Pod test-webserver-8bd54914-2500-4d1f-88a1-b9fb9e5564e5 is Running (Ready = false)
Jun 10 16:15:32.674: INFO: The status of Pod test-webserver-8bd54914-2500-4d1f-88a1-b9fb9e5564e5 is Running (Ready = true)
Jun 10 16:15:32.682: INFO: Container started at 2021-06-10 16:15:09 +0000 UTC, pod became ready at 2021-06-10 16:15:32 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:15:32.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1303" for this suite.

• [SLOW TEST:25.085 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":305,"completed":112,"skipped":1627,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:15:32.696: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 10 16:15:32.763: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a92e3fbb-a4fd-45e1-a2c3-670031024bca" in namespace "downward-api-1612" to be "Succeeded or Failed"
Jun 10 16:15:32.785: INFO: Pod "downwardapi-volume-a92e3fbb-a4fd-45e1-a2c3-670031024bca": Phase="Pending", Reason="", readiness=false. Elapsed: 21.442347ms
Jun 10 16:15:34.790: INFO: Pod "downwardapi-volume-a92e3fbb-a4fd-45e1-a2c3-670031024bca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026967543s
STEP: Saw pod success
Jun 10 16:15:34.791: INFO: Pod "downwardapi-volume-a92e3fbb-a4fd-45e1-a2c3-670031024bca" satisfied condition "Succeeded or Failed"
Jun 10 16:15:34.802: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-5pcx5 pod downwardapi-volume-a92e3fbb-a4fd-45e1-a2c3-670031024bca container client-container: <nil>
STEP: delete the pod
Jun 10 16:15:34.856: INFO: Waiting for pod downwardapi-volume-a92e3fbb-a4fd-45e1-a2c3-670031024bca to disappear
Jun 10 16:15:34.865: INFO: Pod downwardapi-volume-a92e3fbb-a4fd-45e1-a2c3-670031024bca no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:15:34.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1612" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":113,"skipped":1647,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:15:34.876: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:15:34.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4962" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":305,"completed":114,"skipped":1674,"failed":0}
SSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:15:35.004: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Jun 10 16:15:35.135: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 10 16:16:35.192: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:16:35.201: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:487
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Jun 10 16:16:37.294: INFO: found a healthy node: target-cluster-md-0-6b59c4f65-cqpjz
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 10 16:16:45.523: INFO: pods created so far: [1 1 1]
Jun 10 16:16:45.523: INFO: length of pods created so far: 3
Jun 10 16:17:01.539: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:17:08.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-710" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:461
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:17:08.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3339" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:93.626 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:450
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":305,"completed":115,"skipped":1679,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:17:08.635: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create deployment with httpd image
Jun 10 16:17:08.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-8158 create -f -'
Jun 10 16:17:09.341: INFO: stderr: ""
Jun 10 16:17:09.341: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Jun 10 16:17:09.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-8158 diff -f -'
Jun 10 16:17:10.269: INFO: rc: 1
Jun 10 16:17:10.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-8158 delete -f -'
Jun 10 16:17:10.630: INFO: stderr: ""
Jun 10 16:17:10.630: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:17:10.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8158" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":305,"completed":116,"skipped":1693,"failed":0}
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:17:10.656: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-9684
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 10 16:17:10.714: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun 10 16:17:10.804: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 10 16:17:12.809: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 10 16:17:14.808: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 16:17:16.811: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 16:17:18.814: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 16:17:20.808: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 16:17:22.807: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jun 10 16:17:22.811: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jun 10 16:17:24.814: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jun 10 16:17:24.821: INFO: The status of Pod netserver-2 is Running (Ready = false)
Jun 10 16:17:26.826: INFO: The status of Pod netserver-2 is Running (Ready = false)
Jun 10 16:17:28.824: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Jun 10 16:17:30.869: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.78.110:8080/dial?request=hostname&protocol=http&host=192.168.251.33&port=8080&tries=1'] Namespace:pod-network-test-9684 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 10 16:17:30.869: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
Jun 10 16:17:30.988: INFO: Waiting for responses: map[]
Jun 10 16:17:30.991: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.78.110:8080/dial?request=hostname&protocol=http&host=192.168.24.182&port=8080&tries=1'] Namespace:pod-network-test-9684 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 10 16:17:30.991: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
Jun 10 16:17:31.065: INFO: Waiting for responses: map[]
Jun 10 16:17:31.068: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.78.110:8080/dial?request=hostname&protocol=http&host=192.168.78.109&port=8080&tries=1'] Namespace:pod-network-test-9684 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 10 16:17:31.068: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
Jun 10 16:17:31.191: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:17:31.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9684" for this suite.

• [SLOW TEST:20.547 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":305,"completed":117,"skipped":1699,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:17:31.203: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod test-webserver-d2c952ed-15f1-4b95-955a-16ef7ef35cbd in namespace container-probe-9540
Jun 10 16:17:33.247: INFO: Started pod test-webserver-d2c952ed-15f1-4b95-955a-16ef7ef35cbd in namespace container-probe-9540
STEP: checking the pod's current state and verifying that restartCount is present
Jun 10 16:17:33.251: INFO: Initial restart count of pod test-webserver-d2c952ed-15f1-4b95-955a-16ef7ef35cbd is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:21:34.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9540" for this suite.

• [SLOW TEST:242.840 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":305,"completed":118,"skipped":1729,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:21:34.044: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2467.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2467.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2467.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2467.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2467.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2467.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2467.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2467.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2467.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2467.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2467.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 40.37.140.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.140.37.40_udp@PTR;check="$$(dig +tcp +noall +answer +search 40.37.140.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.140.37.40_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2467.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2467.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2467.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2467.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2467.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2467.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2467.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2467.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2467.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2467.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2467.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 40.37.140.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.140.37.40_udp@PTR;check="$$(dig +tcp +noall +answer +search 40.37.140.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.140.37.40_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 10 16:21:38.301: INFO: Unable to read wheezy_udp@dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:21:38.305: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:21:38.309: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:21:38.313: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:21:38.336: INFO: Unable to read jessie_udp@dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:21:38.341: INFO: Unable to read jessie_tcp@dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:21:38.345: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:21:38.348: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:21:38.368: INFO: Lookups using dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81 failed for: [wheezy_udp@dns-test-service.dns-2467.svc.cluster.local wheezy_tcp@dns-test-service.dns-2467.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local jessie_udp@dns-test-service.dns-2467.svc.cluster.local jessie_tcp@dns-test-service.dns-2467.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local]

Jun 10 16:21:43.371: INFO: Unable to read wheezy_udp@dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:21:43.376: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:21:43.380: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:21:43.383: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:21:43.409: INFO: Unable to read jessie_udp@dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:21:43.414: INFO: Unable to read jessie_tcp@dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:21:43.419: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:21:43.422: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:21:43.443: INFO: Lookups using dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81 failed for: [wheezy_udp@dns-test-service.dns-2467.svc.cluster.local wheezy_tcp@dns-test-service.dns-2467.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local jessie_udp@dns-test-service.dns-2467.svc.cluster.local jessie_tcp@dns-test-service.dns-2467.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local]

Jun 10 16:21:48.375: INFO: Unable to read wheezy_udp@dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:21:48.385: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:21:48.394: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:21:48.398: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:21:48.421: INFO: Unable to read jessie_udp@dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:21:48.424: INFO: Unable to read jessie_tcp@dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:21:48.427: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:21:48.429: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:21:48.449: INFO: Lookups using dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81 failed for: [wheezy_udp@dns-test-service.dns-2467.svc.cluster.local wheezy_tcp@dns-test-service.dns-2467.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local jessie_udp@dns-test-service.dns-2467.svc.cluster.local jessie_tcp@dns-test-service.dns-2467.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local]

Jun 10 16:21:53.372: INFO: Unable to read wheezy_udp@dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:21:53.375: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:21:53.381: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:21:53.397: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:21:53.427: INFO: Unable to read jessie_udp@dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:21:53.429: INFO: Unable to read jessie_tcp@dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:21:53.432: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:21:53.435: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:21:53.460: INFO: Lookups using dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81 failed for: [wheezy_udp@dns-test-service.dns-2467.svc.cluster.local wheezy_tcp@dns-test-service.dns-2467.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local jessie_udp@dns-test-service.dns-2467.svc.cluster.local jessie_tcp@dns-test-service.dns-2467.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local]

Jun 10 16:21:58.373: INFO: Unable to read wheezy_udp@dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:21:58.378: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:21:58.383: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:21:58.388: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:21:58.418: INFO: Unable to read jessie_udp@dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:21:58.424: INFO: Unable to read jessie_tcp@dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:21:58.428: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:21:58.433: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:21:58.462: INFO: Lookups using dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81 failed for: [wheezy_udp@dns-test-service.dns-2467.svc.cluster.local wheezy_tcp@dns-test-service.dns-2467.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local jessie_udp@dns-test-service.dns-2467.svc.cluster.local jessie_tcp@dns-test-service.dns-2467.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local]

Jun 10 16:22:03.373: INFO: Unable to read wheezy_udp@dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:22:03.378: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:22:03.382: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:22:03.385: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:22:03.409: INFO: Unable to read jessie_udp@dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:22:03.413: INFO: Unable to read jessie_tcp@dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:22:03.417: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:22:03.420: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local from pod dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81: the server could not find the requested resource (get pods dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81)
Jun 10 16:22:03.438: INFO: Lookups using dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81 failed for: [wheezy_udp@dns-test-service.dns-2467.svc.cluster.local wheezy_tcp@dns-test-service.dns-2467.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local jessie_udp@dns-test-service.dns-2467.svc.cluster.local jessie_tcp@dns-test-service.dns-2467.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2467.svc.cluster.local]

Jun 10 16:22:08.427: INFO: DNS probes using dns-2467/dns-test-0fd149b4-3286-4ea1-a6fc-8558ce695e81 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:22:08.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2467" for this suite.

• [SLOW TEST:34.632 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":305,"completed":119,"skipped":1760,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:22:08.677: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:22:19.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-516" for this suite.

• [SLOW TEST:11.158 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":305,"completed":120,"skipped":1797,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:22:19.836: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-0c0c5910-427c-47f3-b136-fe5551b186ac
STEP: Creating a pod to test consume secrets
Jun 10 16:22:19.881: INFO: Waiting up to 5m0s for pod "pod-secrets-c1410d3b-5546-4701-9b47-d185cae35ba3" in namespace "secrets-4881" to be "Succeeded or Failed"
Jun 10 16:22:19.885: INFO: Pod "pod-secrets-c1410d3b-5546-4701-9b47-d185cae35ba3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.17283ms
Jun 10 16:22:21.887: INFO: Pod "pod-secrets-c1410d3b-5546-4701-9b47-d185cae35ba3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0059681s
STEP: Saw pod success
Jun 10 16:22:21.887: INFO: Pod "pod-secrets-c1410d3b-5546-4701-9b47-d185cae35ba3" satisfied condition "Succeeded or Failed"
Jun 10 16:22:21.889: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod pod-secrets-c1410d3b-5546-4701-9b47-d185cae35ba3 container secret-env-test: <nil>
STEP: delete the pod
Jun 10 16:22:21.914: INFO: Waiting for pod pod-secrets-c1410d3b-5546-4701-9b47-d185cae35ba3 to disappear
Jun 10 16:22:21.917: INFO: Pod pod-secrets-c1410d3b-5546-4701-9b47-d185cae35ba3 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:22:21.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4881" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":305,"completed":121,"skipped":1863,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:22:21.931: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-041d0b87-c07e-404c-bcb3-69e53e943057
STEP: Creating a pod to test consume secrets
Jun 10 16:22:21.984: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b9682c79-3e4f-4cf5-9a42-66f13029b638" in namespace "projected-9122" to be "Succeeded or Failed"
Jun 10 16:22:21.989: INFO: Pod "pod-projected-secrets-b9682c79-3e4f-4cf5-9a42-66f13029b638": Phase="Pending", Reason="", readiness=false. Elapsed: 5.343956ms
Jun 10 16:22:24.002: INFO: Pod "pod-projected-secrets-b9682c79-3e4f-4cf5-9a42-66f13029b638": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018671886s
STEP: Saw pod success
Jun 10 16:22:24.003: INFO: Pod "pod-projected-secrets-b9682c79-3e4f-4cf5-9a42-66f13029b638" satisfied condition "Succeeded or Failed"
Jun 10 16:22:24.010: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod pod-projected-secrets-b9682c79-3e4f-4cf5-9a42-66f13029b638 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 10 16:22:24.036: INFO: Waiting for pod pod-projected-secrets-b9682c79-3e4f-4cf5-9a42-66f13029b638 to disappear
Jun 10 16:22:24.041: INFO: Pod pod-projected-secrets-b9682c79-3e4f-4cf5-9a42-66f13029b638 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:22:24.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9122" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":122,"skipped":1871,"failed":0}
SSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:22:24.048: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jun 10 16:22:24.940: INFO: starting watch
STEP: patching
STEP: updating
Jun 10 16:22:24.955: INFO: waiting for watch events with expected annotations
Jun 10 16:22:24.955: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:22:25.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-8530" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":305,"completed":123,"skipped":1879,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:22:25.092: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:22:36.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4217" for this suite.

• [SLOW TEST:11.142 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":305,"completed":124,"skipped":1914,"failed":0}
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:22:36.234: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 10 16:22:36.267: INFO: Creating deployment "test-recreate-deployment"
Jun 10 16:22:36.273: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jun 10 16:22:36.279: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Jun 10 16:22:38.283: INFO: Waiting deployment "test-recreate-deployment" to complete
Jun 10 16:22:38.285: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jun 10 16:22:38.291: INFO: Updating deployment test-recreate-deployment
Jun 10 16:22:38.291: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Jun 10 16:22:38.431: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-573 /apis/apps/v1/namespaces/deployment-573/deployments/test-recreate-deployment 875d3307-5037-4e3e-b121-0247618bafba 29891 2 2021-06-10 16:22:36 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-06-10 16:22:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-06-10 16:22:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00369c4e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-06-10 16:22:38 +0000 UTC,LastTransitionTime:2021-06-10 16:22:38 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f79dd4667" is progressing.,LastUpdateTime:2021-06-10 16:22:38 +0000 UTC,LastTransitionTime:2021-06-10 16:22:36 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jun 10 16:22:38.442: INFO: New ReplicaSet "test-recreate-deployment-f79dd4667" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f79dd4667  deployment-573 /apis/apps/v1/namespaces/deployment-573/replicasets/test-recreate-deployment-f79dd4667 04ad88cc-cad3-46e0-bcfc-417470e1ab7f 29890 1 2021-06-10 16:22:38 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 875d3307-5037-4e3e-b121-0247618bafba 0xc008073a80 0xc008073a81}] []  [{kube-controller-manager Update apps/v1 2021-06-10 16:22:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"875d3307-5037-4e3e-b121-0247618bafba\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f79dd4667,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc008073af8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 10 16:22:38.442: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jun 10 16:22:38.443: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-c96cf48f  deployment-573 /apis/apps/v1/namespaces/deployment-573/replicasets/test-recreate-deployment-c96cf48f b35c4f06-8aa5-48cb-b8f9-db9c812aa36a 29879 2 2021-06-10 16:22:36 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 875d3307-5037-4e3e-b121-0247618bafba 0xc00807398f 0xc0080739a0}] []  [{kube-controller-manager Update apps/v1 2021-06-10 16:22:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"875d3307-5037-4e3e-b121-0247618bafba\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c96cf48f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc008073a18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 10 16:22:38.447: INFO: Pod "test-recreate-deployment-f79dd4667-n6jnm" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f79dd4667-n6jnm test-recreate-deployment-f79dd4667- deployment-573 /api/v1/namespaces/deployment-573/pods/test-recreate-deployment-f79dd4667-n6jnm ccfc9120-1aff-40a7-8b12-4b806e0c6719 29888 0 2021-06-10 16:22:38 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [{apps/v1 ReplicaSet test-recreate-deployment-f79dd4667 04ad88cc-cad3-46e0-bcfc-417470e1ab7f 0xc003147280 0xc003147281}] []  [{kube-controller-manager Update v1 2021-06-10 16:22:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"04ad88cc-cad3-46e0-bcfc-417470e1ab7f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-10 16:22:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zrw8v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zrw8v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zrw8v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:target-cluster-md-0-6b59c4f65-cqpjz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 16:22:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 16:22:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 16:22:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 16:22:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.17.0.5,PodIP:,StartTime:2021-06-10 16:22:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:22:38.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-573" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":305,"completed":125,"skipped":1914,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:22:38.459: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Jun 10 16:22:38.554: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 10 16:23:38.598: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
Jun 10 16:23:38.647: INFO: Created pod: pod0-sched-preemption-low-priority
Jun 10 16:23:38.668: INFO: Created pod: pod1-sched-preemption-medium-priority
Jun 10 16:23:38.708: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:24:10.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1469" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:92.485 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":305,"completed":126,"skipped":2006,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:24:10.945: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 10 16:26:11.077: INFO: Deleting pod "var-expansion-9ac88330-f6a9-40ba-b974-b0286f285a20" in namespace "var-expansion-3845"
Jun 10 16:26:11.090: INFO: Wait up to 5m0s for pod "var-expansion-9ac88330-f6a9-40ba-b974-b0286f285a20" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:26:13.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3845" for this suite.

• [SLOW TEST:122.169 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":305,"completed":127,"skipped":2028,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:26:13.114: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 10 16:26:13.204: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:26:14.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8449" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":305,"completed":128,"skipped":2034,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:26:14.448: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 10 16:26:14.503: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c5a60c87-f293-4068-8b4d-6d9679f96e51" in namespace "downward-api-48" to be "Succeeded or Failed"
Jun 10 16:26:14.509: INFO: Pod "downwardapi-volume-c5a60c87-f293-4068-8b4d-6d9679f96e51": Phase="Pending", Reason="", readiness=false. Elapsed: 5.842821ms
Jun 10 16:26:16.529: INFO: Pod "downwardapi-volume-c5a60c87-f293-4068-8b4d-6d9679f96e51": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025419353s
Jun 10 16:26:18.539: INFO: Pod "downwardapi-volume-c5a60c87-f293-4068-8b4d-6d9679f96e51": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0360531s
STEP: Saw pod success
Jun 10 16:26:18.540: INFO: Pod "downwardapi-volume-c5a60c87-f293-4068-8b4d-6d9679f96e51" satisfied condition "Succeeded or Failed"
Jun 10 16:26:18.549: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod downwardapi-volume-c5a60c87-f293-4068-8b4d-6d9679f96e51 container client-container: <nil>
STEP: delete the pod
Jun 10 16:26:18.651: INFO: Waiting for pod downwardapi-volume-c5a60c87-f293-4068-8b4d-6d9679f96e51 to disappear
Jun 10 16:26:18.663: INFO: Pod downwardapi-volume-c5a60c87-f293-4068-8b4d-6d9679f96e51 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:26:18.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-48" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":305,"completed":129,"skipped":2063,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:26:18.678: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Jun 10 16:26:18.733: INFO: Waiting up to 5m0s for pod "downward-api-2fa28635-97e6-41c5-893c-31ab6248bb03" in namespace "downward-api-8862" to be "Succeeded or Failed"
Jun 10 16:26:18.755: INFO: Pod "downward-api-2fa28635-97e6-41c5-893c-31ab6248bb03": Phase="Pending", Reason="", readiness=false. Elapsed: 21.868392ms
Jun 10 16:26:20.761: INFO: Pod "downward-api-2fa28635-97e6-41c5-893c-31ab6248bb03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027783382s
STEP: Saw pod success
Jun 10 16:26:20.761: INFO: Pod "downward-api-2fa28635-97e6-41c5-893c-31ab6248bb03" satisfied condition "Succeeded or Failed"
Jun 10 16:26:20.767: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod downward-api-2fa28635-97e6-41c5-893c-31ab6248bb03 container dapi-container: <nil>
STEP: delete the pod
Jun 10 16:26:20.818: INFO: Waiting for pod downward-api-2fa28635-97e6-41c5-893c-31ab6248bb03 to disappear
Jun 10 16:26:20.827: INFO: Pod downward-api-2fa28635-97e6-41c5-893c-31ab6248bb03 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:26:20.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8862" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":305,"completed":130,"skipped":2086,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:26:20.849: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jun 10 16:26:20.989: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2178 /api/v1/namespaces/watch-2178/configmaps/e2e-watch-test-resource-version 8c59e474-5f3d-42f3-88bf-1c827f99f5cb 31408 0 2021-06-10 16:26:20 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-06-10 16:26:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 10 16:26:20.990: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2178 /api/v1/namespaces/watch-2178/configmaps/e2e-watch-test-resource-version 8c59e474-5f3d-42f3-88bf-1c827f99f5cb 31409 0 2021-06-10 16:26:20 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-06-10 16:26:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:26:20.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2178" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":305,"completed":131,"skipped":2108,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:26:21.026: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Jun 10 16:26:21.071: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Jun 10 16:27:21.298: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
Jun 10 16:27:35.681: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:28:31.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5856" for this suite.

• [SLOW TEST:130.915 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":305,"completed":132,"skipped":2114,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:28:31.944: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 10 16:28:35.051: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:28:35.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2792" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":305,"completed":133,"skipped":2159,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:28:35.079: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Jun 10 16:28:35.137: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Jun 10 16:28:35.145: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jun 10 16:28:35.145: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Jun 10 16:28:35.164: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jun 10 16:28:35.164: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Jun 10 16:28:35.209: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jun 10 16:28:35.209: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Jun 10 16:28:42.303: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:28:42.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-4171" for this suite.

• [SLOW TEST:7.244 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":305,"completed":134,"skipped":2174,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:28:42.324: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-8e42541e-d3dd-43c2-9d62-4ae0af9e9077
STEP: Creating a pod to test consume configMaps
Jun 10 16:28:42.398: INFO: Waiting up to 5m0s for pod "pod-configmaps-e8d1f652-e374-4e8b-806e-4522dc532e52" in namespace "configmap-7316" to be "Succeeded or Failed"
Jun 10 16:28:42.402: INFO: Pod "pod-configmaps-e8d1f652-e374-4e8b-806e-4522dc532e52": Phase="Pending", Reason="", readiness=false. Elapsed: 3.621341ms
Jun 10 16:28:44.405: INFO: Pod "pod-configmaps-e8d1f652-e374-4e8b-806e-4522dc532e52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006283152s
STEP: Saw pod success
Jun 10 16:28:44.405: INFO: Pod "pod-configmaps-e8d1f652-e374-4e8b-806e-4522dc532e52" satisfied condition "Succeeded or Failed"
Jun 10 16:28:44.409: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod pod-configmaps-e8d1f652-e374-4e8b-806e-4522dc532e52 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 10 16:28:44.450: INFO: Waiting for pod pod-configmaps-e8d1f652-e374-4e8b-806e-4522dc532e52 to disappear
Jun 10 16:28:44.453: INFO: Pod pod-configmaps-e8d1f652-e374-4e8b-806e-4522dc532e52 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:28:44.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7316" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":135,"skipped":2214,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:28:44.466: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Jun 10 16:28:46.533: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-2235 PodName:pod-sharedvolume-482ca276-662f-4d48-bfba-af8eed4bfffd ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 10 16:28:46.533: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
Jun 10 16:28:46.635: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:28:46.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2235" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":305,"completed":136,"skipped":2237,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:28:46.643: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jun 10 16:28:46.675: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 10 16:28:46.681: INFO: Waiting for terminating namespaces to be deleted...
Jun 10 16:28:46.685: INFO: 
Logging pods the apiserver thinks is on node target-cluster-control-plane-8m52s before test
Jun 10 16:28:46.691: INFO: calico-kube-controllers-7c5d656c49-x6mc4 from calico-system started at 2021-06-10 15:29:10 +0000 UTC (1 container statuses recorded)
Jun 10 16:28:46.691: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 10 16:28:46.691: INFO: calico-node-phlfp from calico-system started at 2021-06-10 15:28:49 +0000 UTC (1 container statuses recorded)
Jun 10 16:28:46.691: INFO: 	Container calico-node ready: true, restart count 0
Jun 10 16:28:46.691: INFO: calico-typha-5d4587dc9c-7g8gs from calico-system started at 2021-06-10 15:28:49 +0000 UTC (1 container statuses recorded)
Jun 10 16:28:46.691: INFO: 	Container calico-typha ready: true, restart count 0
Jun 10 16:28:46.691: INFO: capd-controller-manager-7c89f6ddd7-dc686 from capd-system started at 2021-06-10 15:31:47 +0000 UTC (2 container statuses recorded)
Jun 10 16:28:46.691: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 10 16:28:46.691: INFO: 	Container manager ready: true, restart count 0
Jun 10 16:28:46.691: INFO: capi-kubeadm-bootstrap-controller-manager-6f669ccd7c-gtbcn from capi-kubeadm-bootstrap-system started at 2021-06-10 15:31:34 +0000 UTC (2 container statuses recorded)
Jun 10 16:28:46.691: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 10 16:28:46.691: INFO: 	Container manager ready: true, restart count 0
Jun 10 16:28:46.691: INFO: capi-kubeadm-control-plane-controller-manager-5c95f59c5c-l4jvx from capi-kubeadm-control-plane-system started at 2021-06-10 15:31:42 +0000 UTC (2 container statuses recorded)
Jun 10 16:28:46.691: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 10 16:28:46.691: INFO: 	Container manager ready: true, restart count 0
Jun 10 16:28:46.691: INFO: capi-controller-manager-5f677d7d65-tzt7h from capi-system started at 2021-06-10 15:31:31 +0000 UTC (2 container statuses recorded)
Jun 10 16:28:46.691: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 10 16:28:46.691: INFO: 	Container manager ready: true, restart count 0
Jun 10 16:28:46.691: INFO: capi-controller-manager-745689557d-77m69 from capi-webhook-system started at 2021-06-10 15:31:29 +0000 UTC (2 container statuses recorded)
Jun 10 16:28:46.691: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 10 16:28:46.691: INFO: 	Container manager ready: true, restart count 0
Jun 10 16:28:46.691: INFO: capi-kubeadm-bootstrap-controller-manager-6949f44db8-hn6kq from capi-webhook-system started at 2021-06-10 15:31:31 +0000 UTC (2 container statuses recorded)
Jun 10 16:28:46.691: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 10 16:28:46.691: INFO: 	Container manager ready: true, restart count 0
Jun 10 16:28:46.691: INFO: capi-kubeadm-control-plane-controller-manager-7b6c4bf48d-x8pzk from capi-webhook-system started at 2021-06-10 15:31:35 +0000 UTC (2 container statuses recorded)
Jun 10 16:28:46.691: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 10 16:28:46.691: INFO: 	Container manager ready: true, restart count 0
Jun 10 16:28:46.691: INFO: cert-manager-768bf64dd4-4lhmj from cert-manager started at 2021-06-10 15:29:10 +0000 UTC (1 container statuses recorded)
Jun 10 16:28:46.691: INFO: 	Container cert-manager ready: true, restart count 0
Jun 10 16:28:46.691: INFO: cert-manager-cainjector-646879549c-pbn4t from cert-manager started at 2021-06-10 15:29:10 +0000 UTC (1 container statuses recorded)
Jun 10 16:28:46.691: INFO: 	Container cert-manager ready: true, restart count 0
Jun 10 16:28:46.691: INFO: cert-manager-webhook-6dc9ccc9fb-6gg4f from cert-manager started at 2021-06-10 15:29:10 +0000 UTC (1 container statuses recorded)
Jun 10 16:28:46.691: INFO: 	Container cert-manager ready: true, restart count 0
Jun 10 16:28:46.691: INFO: coredns-f9fd979d6-2dcsm from kube-system started at 2021-06-10 15:29:10 +0000 UTC (1 container statuses recorded)
Jun 10 16:28:46.691: INFO: 	Container coredns ready: true, restart count 0
Jun 10 16:28:46.691: INFO: coredns-f9fd979d6-zsjpx from kube-system started at 2021-06-10 15:29:21 +0000 UTC (1 container statuses recorded)
Jun 10 16:28:46.691: INFO: 	Container coredns ready: true, restart count 0
Jun 10 16:28:46.691: INFO: etcd-target-cluster-control-plane-8m52s from kube-system started at 2021-06-10 15:22:59 +0000 UTC (1 container statuses recorded)
Jun 10 16:28:46.691: INFO: 	Container etcd ready: true, restart count 0
Jun 10 16:28:46.691: INFO: kube-apiserver-target-cluster-control-plane-8m52s from kube-system started at 2021-06-10 15:22:59 +0000 UTC (1 container statuses recorded)
Jun 10 16:28:46.691: INFO: 	Container kube-apiserver ready: true, restart count 0
Jun 10 16:28:46.691: INFO: kube-controller-manager-target-cluster-control-plane-8m52s from kube-system started at 2021-06-10 15:22:59 +0000 UTC (1 container statuses recorded)
Jun 10 16:28:46.691: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jun 10 16:28:46.691: INFO: kube-proxy-hz9zf from kube-system started at 2021-06-10 15:23:04 +0000 UTC (1 container statuses recorded)
Jun 10 16:28:46.691: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 10 16:28:46.691: INFO: kube-scheduler-target-cluster-control-plane-8m52s from kube-system started at 2021-06-10 15:22:59 +0000 UTC (1 container statuses recorded)
Jun 10 16:28:46.691: INFO: 	Container kube-scheduler ready: true, restart count 0
Jun 10 16:28:46.691: INFO: sonobuoy-systemd-logs-daemon-set-0467f197b1c74b53-bs8vn from sonobuoy started at 2021-06-10 15:38:55 +0000 UTC (2 container statuses recorded)
Jun 10 16:28:46.691: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 10 16:28:46.691: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 10 16:28:46.691: INFO: tigera-operator-5b76777d49-9zvzd from tigera-operator started at 2021-06-10 15:28:37 +0000 UTC (1 container statuses recorded)
Jun 10 16:28:46.691: INFO: 	Container tigera-operator ready: true, restart count 0
Jun 10 16:28:46.691: INFO: 
Logging pods the apiserver thinks is on node target-cluster-md-0-6b59c4f65-5pcx5 before test
Jun 10 16:28:46.701: INFO: calico-node-dvbdj from calico-system started at 2021-06-10 15:34:05 +0000 UTC (1 container statuses recorded)
Jun 10 16:28:46.701: INFO: 	Container calico-node ready: true, restart count 0
Jun 10 16:28:46.701: INFO: calico-typha-5d4587dc9c-lq9tq from calico-system started at 2021-06-10 15:34:47 +0000 UTC (1 container statuses recorded)
Jun 10 16:28:46.701: INFO: 	Container calico-typha ready: true, restart count 0
Jun 10 16:28:46.701: INFO: kube-proxy-dklkq from kube-system started at 2021-06-10 15:34:05 +0000 UTC (1 container statuses recorded)
Jun 10 16:28:46.701: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 10 16:28:46.701: INFO: pfpod from limitrange-4171 started at 2021-06-10 16:28:37 +0000 UTC (1 container statuses recorded)
Jun 10 16:28:46.701: INFO: 	Container pause ready: true, restart count 0
Jun 10 16:28:46.701: INFO: pod-partial-resources from limitrange-4171 started at 2021-06-10 16:28:35 +0000 UTC (1 container statuses recorded)
Jun 10 16:28:46.701: INFO: 	Container pause ready: true, restart count 0
Jun 10 16:28:46.701: INFO: sonobuoy-e2e-job-04c20f7a477e40c3 from sonobuoy started at 2021-06-10 15:38:55 +0000 UTC (2 container statuses recorded)
Jun 10 16:28:46.701: INFO: 	Container e2e ready: true, restart count 0
Jun 10 16:28:46.701: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 10 16:28:46.701: INFO: sonobuoy-systemd-logs-daemon-set-0467f197b1c74b53-gj6jj from sonobuoy started at 2021-06-10 15:38:55 +0000 UTC (2 container statuses recorded)
Jun 10 16:28:46.701: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 10 16:28:46.701: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 10 16:28:46.701: INFO: 
Logging pods the apiserver thinks is on node target-cluster-md-0-6b59c4f65-cqpjz before test
Jun 10 16:28:46.708: INFO: calico-node-j7mxn from calico-system started at 2021-06-10 15:34:05 +0000 UTC (1 container statuses recorded)
Jun 10 16:28:46.708: INFO: 	Container calico-node ready: true, restart count 0
Jun 10 16:28:46.708: INFO: calico-typha-5d4587dc9c-ckl68 from calico-system started at 2021-06-10 15:34:47 +0000 UTC (1 container statuses recorded)
Jun 10 16:28:46.708: INFO: 	Container calico-typha ready: true, restart count 0
Jun 10 16:28:46.708: INFO: pod-sharedvolume-482ca276-662f-4d48-bfba-af8eed4bfffd from emptydir-2235 started at 2021-06-10 16:28:44 +0000 UTC (2 container statuses recorded)
Jun 10 16:28:46.708: INFO: 	Container busybox-main-container ready: true, restart count 0
Jun 10 16:28:46.708: INFO: 	Container busybox-sub-container ready: false, restart count 0
Jun 10 16:28:46.708: INFO: kube-proxy-mc9rx from kube-system started at 2021-06-10 15:34:05 +0000 UTC (1 container statuses recorded)
Jun 10 16:28:46.708: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 10 16:28:46.708: INFO: pod-no-resources from limitrange-4171 started at 2021-06-10 16:28:35 +0000 UTC (1 container statuses recorded)
Jun 10 16:28:46.708: INFO: 	Container pause ready: true, restart count 0
Jun 10 16:28:46.708: INFO: sonobuoy from sonobuoy started at 2021-06-10 15:38:49 +0000 UTC (1 container statuses recorded)
Jun 10 16:28:46.708: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 10 16:28:46.708: INFO: sonobuoy-systemd-logs-daemon-set-0467f197b1c74b53-cf9lb from sonobuoy started at 2021-06-10 15:38:55 +0000 UTC (2 container statuses recorded)
Jun 10 16:28:46.708: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 10 16:28:46.708: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-a213b453-a852-4c6c-a1d2-3d27d93ea5cd 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-a213b453-a852-4c6c-a1d2-3d27d93ea5cd off the node target-cluster-md-0-6b59c4f65-cqpjz
STEP: verifying the node doesn't have the label kubernetes.io/e2e-a213b453-a852-4c6c-a1d2-3d27d93ea5cd
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:28:52.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9636" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:6.364 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":305,"completed":137,"skipped":2251,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:28:53.008: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 10 16:28:53.067: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-ed9f40c0-4bf8-4297-8c18-4e496578c230" in namespace "security-context-test-8425" to be "Succeeded or Failed"
Jun 10 16:28:53.095: INFO: Pod "busybox-readonly-false-ed9f40c0-4bf8-4297-8c18-4e496578c230": Phase="Pending", Reason="", readiness=false. Elapsed: 28.816495ms
Jun 10 16:28:55.099: INFO: Pod "busybox-readonly-false-ed9f40c0-4bf8-4297-8c18-4e496578c230": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032554553s
Jun 10 16:28:57.102: INFO: Pod "busybox-readonly-false-ed9f40c0-4bf8-4297-8c18-4e496578c230": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0355552s
Jun 10 16:28:57.102: INFO: Pod "busybox-readonly-false-ed9f40c0-4bf8-4297-8c18-4e496578c230" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:28:57.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8425" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":305,"completed":138,"skipped":2264,"failed":0}
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:28:57.112: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-2180
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Jun 10 16:28:57.168: INFO: Found 0 stateful pods, waiting for 3
Jun 10 16:29:07.172: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 10 16:29:07.172: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 10 16:29:07.172: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jun 10 16:29:07.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-2180 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 10 16:29:07.643: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 10 16:29:07.643: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 10 16:29:07.643: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jun 10 16:29:17.671: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jun 10 16:29:27.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-2180 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 16:29:27.979: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 10 16:29:27.980: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 10 16:29:27.980: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 10 16:29:38.012: INFO: Waiting for StatefulSet statefulset-2180/ss2 to complete update
Jun 10 16:29:38.012: INFO: Waiting for Pod statefulset-2180/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 10 16:29:38.012: INFO: Waiting for Pod statefulset-2180/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 10 16:29:38.012: INFO: Waiting for Pod statefulset-2180/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 10 16:29:48.022: INFO: Waiting for StatefulSet statefulset-2180/ss2 to complete update
Jun 10 16:29:48.022: INFO: Waiting for Pod statefulset-2180/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
Jun 10 16:29:58.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-2180 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 10 16:29:58.291: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 10 16:29:58.291: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 10 16:29:58.291: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 10 16:30:08.320: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jun 10 16:30:18.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-2180 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 16:30:18.562: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 10 16:30:18.562: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 10 16:30:18.562: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 10 16:30:28.581: INFO: Waiting for StatefulSet statefulset-2180/ss2 to complete update
Jun 10 16:30:28.581: INFO: Waiting for Pod statefulset-2180/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jun 10 16:30:28.581: INFO: Waiting for Pod statefulset-2180/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jun 10 16:30:28.581: INFO: Waiting for Pod statefulset-2180/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jun 10 16:30:38.588: INFO: Waiting for StatefulSet statefulset-2180/ss2 to complete update
Jun 10 16:30:38.588: INFO: Waiting for Pod statefulset-2180/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jun 10 16:30:48.587: INFO: Deleting all statefulset in ns statefulset-2180
Jun 10 16:30:48.591: INFO: Scaling statefulset ss2 to 0
Jun 10 16:31:08.621: INFO: Waiting for statefulset status.replicas updated to 0
Jun 10 16:31:08.624: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:31:08.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2180" for this suite.

• [SLOW TEST:131.539 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":305,"completed":139,"skipped":2270,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:31:08.651: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-befc1316-2983-47e1-80cb-10d05d02c16a
STEP: Creating configMap with name cm-test-opt-upd-26e4b7e3-60d4-4a52-ba7b-0bfd957c3f69
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-befc1316-2983-47e1-80cb-10d05d02c16a
STEP: Updating configmap cm-test-opt-upd-26e4b7e3-60d4-4a52-ba7b-0bfd957c3f69
STEP: Creating configMap with name cm-test-opt-create-af84b231-9a32-49fb-8e37-0431cdb12f47
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:31:14.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1884" for this suite.

• [SLOW TEST:6.114 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":140,"skipped":2312,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:31:14.766: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:31:16.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-916" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":305,"completed":141,"skipped":2359,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:31:16.853: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Jun 10 16:31:19.463: INFO: Successfully updated pod "labelsupdate8a3e6d26-9a6b-4143-bf03-26fc94544991"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:31:23.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5103" for this suite.

• [SLOW TEST:6.642 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":305,"completed":142,"skipped":2361,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:31:23.495: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun 10 16:31:23.549: INFO: Waiting up to 5m0s for pod "pod-8c674523-0e23-4075-86c2-7ad044803a38" in namespace "emptydir-4449" to be "Succeeded or Failed"
Jun 10 16:31:23.563: INFO: Pod "pod-8c674523-0e23-4075-86c2-7ad044803a38": Phase="Pending", Reason="", readiness=false. Elapsed: 14.7614ms
Jun 10 16:31:25.596: INFO: Pod "pod-8c674523-0e23-4075-86c2-7ad044803a38": Phase="Running", Reason="", readiness=true. Elapsed: 2.046996281s
Jun 10 16:31:27.614: INFO: Pod "pod-8c674523-0e23-4075-86c2-7ad044803a38": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065371262s
STEP: Saw pod success
Jun 10 16:31:27.614: INFO: Pod "pod-8c674523-0e23-4075-86c2-7ad044803a38" satisfied condition "Succeeded or Failed"
Jun 10 16:31:27.633: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod pod-8c674523-0e23-4075-86c2-7ad044803a38 container test-container: <nil>
STEP: delete the pod
Jun 10 16:31:27.661: INFO: Waiting for pod pod-8c674523-0e23-4075-86c2-7ad044803a38 to disappear
Jun 10 16:31:27.666: INFO: Pod pod-8c674523-0e23-4075-86c2-7ad044803a38 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:31:27.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4449" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":143,"skipped":2388,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:31:27.674: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:31:32.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5968" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":305,"completed":144,"skipped":2395,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:31:32.608: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-29765218-769d-4e5e-a672-3f1d01ce825c
STEP: Creating a pod to test consume configMaps
Jun 10 16:31:32.719: INFO: Waiting up to 5m0s for pod "pod-configmaps-3dfee0e7-807f-496f-821a-699be6db84fd" in namespace "configmap-6912" to be "Succeeded or Failed"
Jun 10 16:31:32.737: INFO: Pod "pod-configmaps-3dfee0e7-807f-496f-821a-699be6db84fd": Phase="Pending", Reason="", readiness=false. Elapsed: 18.208366ms
Jun 10 16:31:34.740: INFO: Pod "pod-configmaps-3dfee0e7-807f-496f-821a-699be6db84fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021670197s
Jun 10 16:31:36.744: INFO: Pod "pod-configmaps-3dfee0e7-807f-496f-821a-699be6db84fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024852207s
STEP: Saw pod success
Jun 10 16:31:36.744: INFO: Pod "pod-configmaps-3dfee0e7-807f-496f-821a-699be6db84fd" satisfied condition "Succeeded or Failed"
Jun 10 16:31:36.746: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod pod-configmaps-3dfee0e7-807f-496f-821a-699be6db84fd container configmap-volume-test: <nil>
STEP: delete the pod
Jun 10 16:31:36.773: INFO: Waiting for pod pod-configmaps-3dfee0e7-807f-496f-821a-699be6db84fd to disappear
Jun 10 16:31:36.776: INFO: Pod pod-configmaps-3dfee0e7-807f-496f-821a-699be6db84fd no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:31:36.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6912" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":305,"completed":145,"skipped":2406,"failed":0}
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:31:36.787: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-e5d8ef7f-3674-4bac-9d0e-a3585cd35312
STEP: Creating a pod to test consume secrets
Jun 10 16:31:36.906: INFO: Waiting up to 5m0s for pod "pod-secrets-550c173a-9e9e-41f4-ba24-2aeb1a0d1e56" in namespace "secrets-8572" to be "Succeeded or Failed"
Jun 10 16:31:36.940: INFO: Pod "pod-secrets-550c173a-9e9e-41f4-ba24-2aeb1a0d1e56": Phase="Pending", Reason="", readiness=false. Elapsed: 34.613936ms
Jun 10 16:31:38.949: INFO: Pod "pod-secrets-550c173a-9e9e-41f4-ba24-2aeb1a0d1e56": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043576386s
Jun 10 16:31:40.953: INFO: Pod "pod-secrets-550c173a-9e9e-41f4-ba24-2aeb1a0d1e56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047324581s
STEP: Saw pod success
Jun 10 16:31:40.953: INFO: Pod "pod-secrets-550c173a-9e9e-41f4-ba24-2aeb1a0d1e56" satisfied condition "Succeeded or Failed"
Jun 10 16:31:40.955: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod pod-secrets-550c173a-9e9e-41f4-ba24-2aeb1a0d1e56 container secret-volume-test: <nil>
STEP: delete the pod
Jun 10 16:31:40.992: INFO: Waiting for pod pod-secrets-550c173a-9e9e-41f4-ba24-2aeb1a0d1e56 to disappear
Jun 10 16:31:41.001: INFO: Pod pod-secrets-550c173a-9e9e-41f4-ba24-2aeb1a0d1e56 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:31:41.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8572" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":305,"completed":146,"skipped":2411,"failed":0}
SSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:31:41.019: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 10 16:31:41.068: INFO: Waiting up to 5m0s for pod "busybox-user-65534-c5ac28d8-9805-4e34-9ae3-1096420ca116" in namespace "security-context-test-7989" to be "Succeeded or Failed"
Jun 10 16:31:41.081: INFO: Pod "busybox-user-65534-c5ac28d8-9805-4e34-9ae3-1096420ca116": Phase="Pending", Reason="", readiness=false. Elapsed: 12.060977ms
Jun 10 16:31:43.084: INFO: Pod "busybox-user-65534-c5ac28d8-9805-4e34-9ae3-1096420ca116": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0152457s
Jun 10 16:31:43.084: INFO: Pod "busybox-user-65534-c5ac28d8-9805-4e34-9ae3-1096420ca116" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:31:43.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7989" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":147,"skipped":2416,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:31:43.101: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5152.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-5152.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5152.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5152.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-5152.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5152.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 10 16:31:45.308: INFO: DNS probes using dns-5152/dns-test-08dc0ae5-1ebc-42bb-b19a-87181875f99a succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:31:45.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5152" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":305,"completed":148,"skipped":2425,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:31:45.416: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-de00430f-33ca-4162-9519-f7393067776b
STEP: Creating a pod to test consume secrets
Jun 10 16:31:45.602: INFO: Waiting up to 5m0s for pod "pod-secrets-cf7bb2a2-49f6-4c60-af9e-fc9c46202891" in namespace "secrets-6911" to be "Succeeded or Failed"
Jun 10 16:31:45.622: INFO: Pod "pod-secrets-cf7bb2a2-49f6-4c60-af9e-fc9c46202891": Phase="Pending", Reason="", readiness=false. Elapsed: 20.018793ms
Jun 10 16:31:47.625: INFO: Pod "pod-secrets-cf7bb2a2-49f6-4c60-af9e-fc9c46202891": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023702236s
Jun 10 16:31:49.629: INFO: Pod "pod-secrets-cf7bb2a2-49f6-4c60-af9e-fc9c46202891": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027697854s
STEP: Saw pod success
Jun 10 16:31:49.630: INFO: Pod "pod-secrets-cf7bb2a2-49f6-4c60-af9e-fc9c46202891" satisfied condition "Succeeded or Failed"
Jun 10 16:31:49.636: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod pod-secrets-cf7bb2a2-49f6-4c60-af9e-fc9c46202891 container secret-volume-test: <nil>
STEP: delete the pod
Jun 10 16:31:49.667: INFO: Waiting for pod pod-secrets-cf7bb2a2-49f6-4c60-af9e-fc9c46202891 to disappear
Jun 10 16:31:49.671: INFO: Pod pod-secrets-cf7bb2a2-49f6-4c60-af9e-fc9c46202891 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:31:49.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6911" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":149,"skipped":2435,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:31:49.686: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jun 10 16:31:49.747: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3332 /api/v1/namespaces/watch-3332/configmaps/e2e-watch-test-configmap-a 79703809-1884-4c6d-9c27-32845beb9059 34446 0 2021-06-10 16:31:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-06-10 16:31:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 10 16:31:49.747: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3332 /api/v1/namespaces/watch-3332/configmaps/e2e-watch-test-configmap-a 79703809-1884-4c6d-9c27-32845beb9059 34446 0 2021-06-10 16:31:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-06-10 16:31:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jun 10 16:31:59.756: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3332 /api/v1/namespaces/watch-3332/configmaps/e2e-watch-test-configmap-a 79703809-1884-4c6d-9c27-32845beb9059 34529 0 2021-06-10 16:31:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-06-10 16:31:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 10 16:31:59.756: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3332 /api/v1/namespaces/watch-3332/configmaps/e2e-watch-test-configmap-a 79703809-1884-4c6d-9c27-32845beb9059 34529 0 2021-06-10 16:31:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-06-10 16:31:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jun 10 16:32:09.762: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3332 /api/v1/namespaces/watch-3332/configmaps/e2e-watch-test-configmap-a 79703809-1884-4c6d-9c27-32845beb9059 34585 0 2021-06-10 16:31:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-06-10 16:31:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 10 16:32:09.763: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3332 /api/v1/namespaces/watch-3332/configmaps/e2e-watch-test-configmap-a 79703809-1884-4c6d-9c27-32845beb9059 34585 0 2021-06-10 16:31:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-06-10 16:31:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jun 10 16:32:19.786: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3332 /api/v1/namespaces/watch-3332/configmaps/e2e-watch-test-configmap-a 79703809-1884-4c6d-9c27-32845beb9059 34640 0 2021-06-10 16:31:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-06-10 16:31:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 10 16:32:19.787: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3332 /api/v1/namespaces/watch-3332/configmaps/e2e-watch-test-configmap-a 79703809-1884-4c6d-9c27-32845beb9059 34640 0 2021-06-10 16:31:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-06-10 16:31:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jun 10 16:32:29.832: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3332 /api/v1/namespaces/watch-3332/configmaps/e2e-watch-test-configmap-b 10dccd0c-589d-477b-935e-315701ee392f 34697 0 2021-06-10 16:32:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-06-10 16:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 10 16:32:29.832: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3332 /api/v1/namespaces/watch-3332/configmaps/e2e-watch-test-configmap-b 10dccd0c-589d-477b-935e-315701ee392f 34697 0 2021-06-10 16:32:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-06-10 16:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jun 10 16:32:39.839: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3332 /api/v1/namespaces/watch-3332/configmaps/e2e-watch-test-configmap-b 10dccd0c-589d-477b-935e-315701ee392f 34753 0 2021-06-10 16:32:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-06-10 16:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 10 16:32:39.839: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3332 /api/v1/namespaces/watch-3332/configmaps/e2e-watch-test-configmap-b 10dccd0c-589d-477b-935e-315701ee392f 34753 0 2021-06-10 16:32:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-06-10 16:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:32:49.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3332" for this suite.

• [SLOW TEST:60.162 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":305,"completed":150,"skipped":2456,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:32:49.849: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2409.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2409.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2409.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2409.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 10 16:32:53.937: INFO: DNS probes using dns-test-0be46221-4316-490d-af15-85b99cf27912 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2409.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2409.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2409.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2409.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 10 16:32:58.016: INFO: File wheezy_udp@dns-test-service-3.dns-2409.svc.cluster.local from pod  dns-2409/dns-test-f9d90062-af91-4a86-ac8a-58d7b62d3943 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 10 16:32:58.020: INFO: File jessie_udp@dns-test-service-3.dns-2409.svc.cluster.local from pod  dns-2409/dns-test-f9d90062-af91-4a86-ac8a-58d7b62d3943 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 10 16:32:58.020: INFO: Lookups using dns-2409/dns-test-f9d90062-af91-4a86-ac8a-58d7b62d3943 failed for: [wheezy_udp@dns-test-service-3.dns-2409.svc.cluster.local jessie_udp@dns-test-service-3.dns-2409.svc.cluster.local]

Jun 10 16:33:03.025: INFO: File wheezy_udp@dns-test-service-3.dns-2409.svc.cluster.local from pod  dns-2409/dns-test-f9d90062-af91-4a86-ac8a-58d7b62d3943 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 10 16:33:03.027: INFO: File jessie_udp@dns-test-service-3.dns-2409.svc.cluster.local from pod  dns-2409/dns-test-f9d90062-af91-4a86-ac8a-58d7b62d3943 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 10 16:33:03.027: INFO: Lookups using dns-2409/dns-test-f9d90062-af91-4a86-ac8a-58d7b62d3943 failed for: [wheezy_udp@dns-test-service-3.dns-2409.svc.cluster.local jessie_udp@dns-test-service-3.dns-2409.svc.cluster.local]

Jun 10 16:33:08.023: INFO: File wheezy_udp@dns-test-service-3.dns-2409.svc.cluster.local from pod  dns-2409/dns-test-f9d90062-af91-4a86-ac8a-58d7b62d3943 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 10 16:33:08.025: INFO: File jessie_udp@dns-test-service-3.dns-2409.svc.cluster.local from pod  dns-2409/dns-test-f9d90062-af91-4a86-ac8a-58d7b62d3943 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 10 16:33:08.025: INFO: Lookups using dns-2409/dns-test-f9d90062-af91-4a86-ac8a-58d7b62d3943 failed for: [wheezy_udp@dns-test-service-3.dns-2409.svc.cluster.local jessie_udp@dns-test-service-3.dns-2409.svc.cluster.local]

Jun 10 16:33:13.023: INFO: File wheezy_udp@dns-test-service-3.dns-2409.svc.cluster.local from pod  dns-2409/dns-test-f9d90062-af91-4a86-ac8a-58d7b62d3943 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 10 16:33:13.025: INFO: File jessie_udp@dns-test-service-3.dns-2409.svc.cluster.local from pod  dns-2409/dns-test-f9d90062-af91-4a86-ac8a-58d7b62d3943 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 10 16:33:13.025: INFO: Lookups using dns-2409/dns-test-f9d90062-af91-4a86-ac8a-58d7b62d3943 failed for: [wheezy_udp@dns-test-service-3.dns-2409.svc.cluster.local jessie_udp@dns-test-service-3.dns-2409.svc.cluster.local]

Jun 10 16:33:18.024: INFO: File wheezy_udp@dns-test-service-3.dns-2409.svc.cluster.local from pod  dns-2409/dns-test-f9d90062-af91-4a86-ac8a-58d7b62d3943 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 10 16:33:18.027: INFO: File jessie_udp@dns-test-service-3.dns-2409.svc.cluster.local from pod  dns-2409/dns-test-f9d90062-af91-4a86-ac8a-58d7b62d3943 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 10 16:33:18.027: INFO: Lookups using dns-2409/dns-test-f9d90062-af91-4a86-ac8a-58d7b62d3943 failed for: [wheezy_udp@dns-test-service-3.dns-2409.svc.cluster.local jessie_udp@dns-test-service-3.dns-2409.svc.cluster.local]

Jun 10 16:33:23.024: INFO: File wheezy_udp@dns-test-service-3.dns-2409.svc.cluster.local from pod  dns-2409/dns-test-f9d90062-af91-4a86-ac8a-58d7b62d3943 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 10 16:33:23.030: INFO: File jessie_udp@dns-test-service-3.dns-2409.svc.cluster.local from pod  dns-2409/dns-test-f9d90062-af91-4a86-ac8a-58d7b62d3943 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 10 16:33:23.031: INFO: Lookups using dns-2409/dns-test-f9d90062-af91-4a86-ac8a-58d7b62d3943 failed for: [wheezy_udp@dns-test-service-3.dns-2409.svc.cluster.local jessie_udp@dns-test-service-3.dns-2409.svc.cluster.local]

Jun 10 16:33:28.036: INFO: DNS probes using dns-test-f9d90062-af91-4a86-ac8a-58d7b62d3943 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2409.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-2409.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2409.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-2409.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 10 16:33:32.206: INFO: DNS probes using dns-test-ba711b19-7e67-4c02-826d-c3ede7ff9414 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:33:32.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2409" for this suite.

• [SLOW TEST:42.451 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":305,"completed":151,"skipped":2480,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:33:32.303: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-872a6d20-6f4e-4c9f-bfa2-119f5af29f3d
STEP: Creating a pod to test consume configMaps
Jun 10 16:33:32.562: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d0efa988-69cd-43d0-a836-9aed80024612" in namespace "projected-7492" to be "Succeeded or Failed"
Jun 10 16:33:32.565: INFO: Pod "pod-projected-configmaps-d0efa988-69cd-43d0-a836-9aed80024612": Phase="Pending", Reason="", readiness=false. Elapsed: 3.185764ms
Jun 10 16:33:34.569: INFO: Pod "pod-projected-configmaps-d0efa988-69cd-43d0-a836-9aed80024612": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007436972s
STEP: Saw pod success
Jun 10 16:33:34.569: INFO: Pod "pod-projected-configmaps-d0efa988-69cd-43d0-a836-9aed80024612" satisfied condition "Succeeded or Failed"
Jun 10 16:33:34.574: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod pod-projected-configmaps-d0efa988-69cd-43d0-a836-9aed80024612 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 10 16:33:34.625: INFO: Waiting for pod pod-projected-configmaps-d0efa988-69cd-43d0-a836-9aed80024612 to disappear
Jun 10 16:33:34.661: INFO: Pod pod-projected-configmaps-d0efa988-69cd-43d0-a836-9aed80024612 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:33:34.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7492" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":305,"completed":152,"skipped":2482,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:33:34.694: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-e07d9d65-6f7d-457d-8f60-d908758a078f
STEP: Creating a pod to test consume configMaps
Jun 10 16:33:34.834: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7af79ef5-6ecc-453a-bf92-73fdf6beb810" in namespace "projected-1577" to be "Succeeded or Failed"
Jun 10 16:33:34.837: INFO: Pod "pod-projected-configmaps-7af79ef5-6ecc-453a-bf92-73fdf6beb810": Phase="Pending", Reason="", readiness=false. Elapsed: 3.372222ms
Jun 10 16:33:36.841: INFO: Pod "pod-projected-configmaps-7af79ef5-6ecc-453a-bf92-73fdf6beb810": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007374108s
STEP: Saw pod success
Jun 10 16:33:36.841: INFO: Pod "pod-projected-configmaps-7af79ef5-6ecc-453a-bf92-73fdf6beb810" satisfied condition "Succeeded or Failed"
Jun 10 16:33:36.851: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod pod-projected-configmaps-7af79ef5-6ecc-453a-bf92-73fdf6beb810 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 10 16:33:36.878: INFO: Waiting for pod pod-projected-configmaps-7af79ef5-6ecc-453a-bf92-73fdf6beb810 to disappear
Jun 10 16:33:36.886: INFO: Pod pod-projected-configmaps-7af79ef5-6ecc-453a-bf92-73fdf6beb810 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:33:36.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1577" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":153,"skipped":2487,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:33:36.903: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 10 16:33:38.531: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 10 16:33:40.554: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758939618, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758939618, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758939618, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758939618, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 10 16:33:43.589: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:33:53.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5226" for this suite.
STEP: Destroying namespace "webhook-5226-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:17.034 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":305,"completed":154,"skipped":2518,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:33:53.948: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-2baeedff-e9a6-4d2d-aa7a-4a1d5647c224
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:33:56.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3486" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":155,"skipped":2519,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:33:56.141: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:34:00.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7167" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":305,"completed":156,"skipped":2529,"failed":0}
SSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:34:00.223: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-17859986-a6be-4b42-bdc8-38f657a8cb9a in namespace container-probe-9384
Jun 10 16:34:04.312: INFO: Started pod busybox-17859986-a6be-4b42-bdc8-38f657a8cb9a in namespace container-probe-9384
STEP: checking the pod's current state and verifying that restartCount is present
Jun 10 16:34:04.316: INFO: Initial restart count of pod busybox-17859986-a6be-4b42-bdc8-38f657a8cb9a is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:38:04.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9384" for this suite.

• [SLOW TEST:244.746 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":305,"completed":157,"skipped":2535,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:38:04.970: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting the proxy server
Jun 10 16:38:05.021: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-4227 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:38:05.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4227" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":305,"completed":158,"skipped":2569,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:38:05.170: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with configMap that has name projected-configmap-test-upd-39e9d584-84b3-4d32-a803-194b72bd79e8
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-39e9d584-84b3-4d32-a803-194b72bd79e8
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:38:11.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8357" for this suite.

• [SLOW TEST:6.179 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":159,"skipped":2572,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:38:11.351: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 10 16:38:12.273: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 10 16:38:14.280: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758939892, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758939892, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758939892, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758939892, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 10 16:38:17.292: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 10 16:38:17.303: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:38:18.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9754" for this suite.
STEP: Destroying namespace "webhook-9754-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.410 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":305,"completed":160,"skipped":2600,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:38:18.763: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jun 10 16:38:21.886: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:38:22.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1071" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":305,"completed":161,"skipped":2611,"failed":0}

------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:38:23.115: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
Jun 10 16:38:30.679: INFO: 0 pods remaining
Jun 10 16:38:30.679: INFO: 0 pods has nil DeletionTimestamp
Jun 10 16:38:30.679: INFO: 
STEP: Gathering metrics
W0610 16:38:31.953498      26 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jun 10 16:39:34.010: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:39:34.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1031" for this suite.

• [SLOW TEST:70.905 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":305,"completed":162,"skipped":2611,"failed":0}
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:39:34.020: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jun 10 16:39:38.113: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9430 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 10 16:39:38.113: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
Jun 10 16:39:38.247: INFO: Exec stderr: ""
Jun 10 16:39:38.248: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9430 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 10 16:39:38.248: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
Jun 10 16:39:38.385: INFO: Exec stderr: ""
Jun 10 16:39:38.385: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9430 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 10 16:39:38.386: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
Jun 10 16:39:38.563: INFO: Exec stderr: ""
Jun 10 16:39:38.563: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9430 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 10 16:39:38.563: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
Jun 10 16:39:38.685: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jun 10 16:39:38.685: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9430 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 10 16:39:38.685: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
Jun 10 16:39:38.784: INFO: Exec stderr: ""
Jun 10 16:39:38.784: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9430 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 10 16:39:38.784: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
Jun 10 16:39:38.852: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jun 10 16:39:38.852: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9430 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 10 16:39:38.853: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
Jun 10 16:39:38.938: INFO: Exec stderr: ""
Jun 10 16:39:38.939: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9430 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 10 16:39:38.939: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
Jun 10 16:39:39.023: INFO: Exec stderr: ""
Jun 10 16:39:39.023: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9430 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 10 16:39:39.023: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
Jun 10 16:39:39.118: INFO: Exec stderr: ""
Jun 10 16:39:39.118: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9430 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 10 16:39:39.118: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
Jun 10 16:39:39.231: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:39:39.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-9430" for this suite.

• [SLOW TEST:5.224 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":163,"skipped":2611,"failed":0}
SSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:39:39.248: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-projected-all-test-volume-0f61f486-fa4e-41af-8208-85fd2c0f2d68
STEP: Creating secret with name secret-projected-all-test-volume-b26a9ccb-db0e-451d-a9f5-66e33cda8df1
STEP: Creating a pod to test Check all projections for projected volume plugin
Jun 10 16:39:39.411: INFO: Waiting up to 5m0s for pod "projected-volume-4a1865cf-7c38-401b-aa29-46881ca3adba" in namespace "projected-3938" to be "Succeeded or Failed"
Jun 10 16:39:39.430: INFO: Pod "projected-volume-4a1865cf-7c38-401b-aa29-46881ca3adba": Phase="Pending", Reason="", readiness=false. Elapsed: 18.906921ms
Jun 10 16:39:41.447: INFO: Pod "projected-volume-4a1865cf-7c38-401b-aa29-46881ca3adba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.035312814s
STEP: Saw pod success
Jun 10 16:39:41.447: INFO: Pod "projected-volume-4a1865cf-7c38-401b-aa29-46881ca3adba" satisfied condition "Succeeded or Failed"
Jun 10 16:39:41.451: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-5pcx5 pod projected-volume-4a1865cf-7c38-401b-aa29-46881ca3adba container projected-all-volume-test: <nil>
STEP: delete the pod
Jun 10 16:39:41.488: INFO: Waiting for pod projected-volume-4a1865cf-7c38-401b-aa29-46881ca3adba to disappear
Jun 10 16:39:41.510: INFO: Pod projected-volume-4a1865cf-7c38-401b-aa29-46881ca3adba no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:39:41.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3938" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":305,"completed":164,"skipped":2614,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:39:41.518: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jun 10 16:39:41.561: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 10 16:39:41.570: INFO: Waiting for terminating namespaces to be deleted...
Jun 10 16:39:41.577: INFO: 
Logging pods the apiserver thinks is on node target-cluster-control-plane-8m52s before test
Jun 10 16:39:41.590: INFO: calico-kube-controllers-7c5d656c49-x6mc4 from calico-system started at 2021-06-10 15:29:10 +0000 UTC (1 container statuses recorded)
Jun 10 16:39:41.590: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 10 16:39:41.590: INFO: calico-node-phlfp from calico-system started at 2021-06-10 15:28:49 +0000 UTC (1 container statuses recorded)
Jun 10 16:39:41.590: INFO: 	Container calico-node ready: true, restart count 0
Jun 10 16:39:41.590: INFO: calico-typha-5d4587dc9c-7g8gs from calico-system started at 2021-06-10 15:28:49 +0000 UTC (1 container statuses recorded)
Jun 10 16:39:41.591: INFO: 	Container calico-typha ready: true, restart count 0
Jun 10 16:39:41.591: INFO: capd-controller-manager-7c89f6ddd7-dc686 from capd-system started at 2021-06-10 15:31:47 +0000 UTC (2 container statuses recorded)
Jun 10 16:39:41.591: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 10 16:39:41.591: INFO: 	Container manager ready: true, restart count 0
Jun 10 16:39:41.591: INFO: capi-kubeadm-bootstrap-controller-manager-6f669ccd7c-gtbcn from capi-kubeadm-bootstrap-system started at 2021-06-10 15:31:34 +0000 UTC (2 container statuses recorded)
Jun 10 16:39:41.591: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 10 16:39:41.591: INFO: 	Container manager ready: true, restart count 0
Jun 10 16:39:41.591: INFO: capi-kubeadm-control-plane-controller-manager-5c95f59c5c-l4jvx from capi-kubeadm-control-plane-system started at 2021-06-10 15:31:42 +0000 UTC (2 container statuses recorded)
Jun 10 16:39:41.591: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 10 16:39:41.591: INFO: 	Container manager ready: true, restart count 0
Jun 10 16:39:41.591: INFO: capi-controller-manager-5f677d7d65-tzt7h from capi-system started at 2021-06-10 15:31:31 +0000 UTC (2 container statuses recorded)
Jun 10 16:39:41.591: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 10 16:39:41.591: INFO: 	Container manager ready: true, restart count 0
Jun 10 16:39:41.591: INFO: capi-controller-manager-745689557d-77m69 from capi-webhook-system started at 2021-06-10 15:31:29 +0000 UTC (2 container statuses recorded)
Jun 10 16:39:41.591: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 10 16:39:41.591: INFO: 	Container manager ready: true, restart count 0
Jun 10 16:39:41.591: INFO: capi-kubeadm-bootstrap-controller-manager-6949f44db8-hn6kq from capi-webhook-system started at 2021-06-10 15:31:31 +0000 UTC (2 container statuses recorded)
Jun 10 16:39:41.591: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 10 16:39:41.592: INFO: 	Container manager ready: true, restart count 0
Jun 10 16:39:41.592: INFO: capi-kubeadm-control-plane-controller-manager-7b6c4bf48d-x8pzk from capi-webhook-system started at 2021-06-10 15:31:35 +0000 UTC (2 container statuses recorded)
Jun 10 16:39:41.592: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 10 16:39:41.592: INFO: 	Container manager ready: true, restart count 0
Jun 10 16:39:41.592: INFO: cert-manager-768bf64dd4-4lhmj from cert-manager started at 2021-06-10 15:29:10 +0000 UTC (1 container statuses recorded)
Jun 10 16:39:41.592: INFO: 	Container cert-manager ready: true, restart count 0
Jun 10 16:39:41.592: INFO: cert-manager-cainjector-646879549c-pbn4t from cert-manager started at 2021-06-10 15:29:10 +0000 UTC (1 container statuses recorded)
Jun 10 16:39:41.592: INFO: 	Container cert-manager ready: true, restart count 0
Jun 10 16:39:41.592: INFO: cert-manager-webhook-6dc9ccc9fb-6gg4f from cert-manager started at 2021-06-10 15:29:10 +0000 UTC (1 container statuses recorded)
Jun 10 16:39:41.592: INFO: 	Container cert-manager ready: true, restart count 0
Jun 10 16:39:41.592: INFO: coredns-f9fd979d6-2dcsm from kube-system started at 2021-06-10 15:29:10 +0000 UTC (1 container statuses recorded)
Jun 10 16:39:41.592: INFO: 	Container coredns ready: true, restart count 0
Jun 10 16:39:41.592: INFO: coredns-f9fd979d6-zsjpx from kube-system started at 2021-06-10 15:29:21 +0000 UTC (1 container statuses recorded)
Jun 10 16:39:41.592: INFO: 	Container coredns ready: true, restart count 0
Jun 10 16:39:41.592: INFO: etcd-target-cluster-control-plane-8m52s from kube-system started at 2021-06-10 15:22:59 +0000 UTC (1 container statuses recorded)
Jun 10 16:39:41.592: INFO: 	Container etcd ready: true, restart count 0
Jun 10 16:39:41.592: INFO: kube-apiserver-target-cluster-control-plane-8m52s from kube-system started at 2021-06-10 15:22:59 +0000 UTC (1 container statuses recorded)
Jun 10 16:39:41.593: INFO: 	Container kube-apiserver ready: true, restart count 0
Jun 10 16:39:41.593: INFO: kube-controller-manager-target-cluster-control-plane-8m52s from kube-system started at 2021-06-10 15:22:59 +0000 UTC (1 container statuses recorded)
Jun 10 16:39:41.593: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jun 10 16:39:41.593: INFO: kube-proxy-hz9zf from kube-system started at 2021-06-10 15:23:04 +0000 UTC (1 container statuses recorded)
Jun 10 16:39:41.593: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 10 16:39:41.593: INFO: kube-scheduler-target-cluster-control-plane-8m52s from kube-system started at 2021-06-10 15:22:59 +0000 UTC (1 container statuses recorded)
Jun 10 16:39:41.593: INFO: 	Container kube-scheduler ready: true, restart count 0
Jun 10 16:39:41.593: INFO: sonobuoy-systemd-logs-daemon-set-0467f197b1c74b53-bs8vn from sonobuoy started at 2021-06-10 15:38:55 +0000 UTC (2 container statuses recorded)
Jun 10 16:39:41.593: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 10 16:39:41.593: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 10 16:39:41.593: INFO: tigera-operator-5b76777d49-9zvzd from tigera-operator started at 2021-06-10 15:28:37 +0000 UTC (1 container statuses recorded)
Jun 10 16:39:41.593: INFO: 	Container tigera-operator ready: true, restart count 0
Jun 10 16:39:41.593: INFO: 
Logging pods the apiserver thinks is on node target-cluster-md-0-6b59c4f65-5pcx5 before test
Jun 10 16:39:41.610: INFO: calico-node-dvbdj from calico-system started at 2021-06-10 15:34:05 +0000 UTC (1 container statuses recorded)
Jun 10 16:39:41.610: INFO: 	Container calico-node ready: true, restart count 0
Jun 10 16:39:41.610: INFO: calico-typha-5d4587dc9c-lq9tq from calico-system started at 2021-06-10 15:34:47 +0000 UTC (1 container statuses recorded)
Jun 10 16:39:41.610: INFO: 	Container calico-typha ready: true, restart count 0
Jun 10 16:39:41.610: INFO: test-host-network-pod from e2e-kubelet-etc-hosts-9430 started at 2021-06-10 16:39:36 +0000 UTC (2 container statuses recorded)
Jun 10 16:39:41.610: INFO: 	Container busybox-1 ready: true, restart count 0
Jun 10 16:39:41.610: INFO: 	Container busybox-2 ready: true, restart count 0
Jun 10 16:39:41.610: INFO: kube-proxy-dklkq from kube-system started at 2021-06-10 15:34:05 +0000 UTC (1 container statuses recorded)
Jun 10 16:39:41.610: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 10 16:39:41.610: INFO: sonobuoy-e2e-job-04c20f7a477e40c3 from sonobuoy started at 2021-06-10 15:38:55 +0000 UTC (2 container statuses recorded)
Jun 10 16:39:41.610: INFO: 	Container e2e ready: true, restart count 0
Jun 10 16:39:41.610: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 10 16:39:41.610: INFO: sonobuoy-systemd-logs-daemon-set-0467f197b1c74b53-gj6jj from sonobuoy started at 2021-06-10 15:38:55 +0000 UTC (2 container statuses recorded)
Jun 10 16:39:41.610: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 10 16:39:41.610: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 10 16:39:41.610: INFO: 
Logging pods the apiserver thinks is on node target-cluster-md-0-6b59c4f65-cqpjz before test
Jun 10 16:39:41.616: INFO: calico-node-j7mxn from calico-system started at 2021-06-10 15:34:05 +0000 UTC (1 container statuses recorded)
Jun 10 16:39:41.617: INFO: 	Container calico-node ready: true, restart count 0
Jun 10 16:39:41.617: INFO: calico-typha-5d4587dc9c-ckl68 from calico-system started at 2021-06-10 15:34:47 +0000 UTC (1 container statuses recorded)
Jun 10 16:39:41.617: INFO: 	Container calico-typha ready: true, restart count 0
Jun 10 16:39:41.617: INFO: test-pod from e2e-kubelet-etc-hosts-9430 started at 2021-06-10 16:39:34 +0000 UTC (3 container statuses recorded)
Jun 10 16:39:41.617: INFO: 	Container busybox-1 ready: true, restart count 0
Jun 10 16:39:41.617: INFO: 	Container busybox-2 ready: true, restart count 0
Jun 10 16:39:41.617: INFO: 	Container busybox-3 ready: true, restart count 0
Jun 10 16:39:41.617: INFO: kube-proxy-mc9rx from kube-system started at 2021-06-10 15:34:05 +0000 UTC (1 container statuses recorded)
Jun 10 16:39:41.617: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 10 16:39:41.617: INFO: sonobuoy from sonobuoy started at 2021-06-10 15:38:49 +0000 UTC (1 container statuses recorded)
Jun 10 16:39:41.617: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 10 16:39:41.618: INFO: sonobuoy-systemd-logs-daemon-set-0467f197b1c74b53-cf9lb from sonobuoy started at 2021-06-10 15:38:55 +0000 UTC (2 container statuses recorded)
Jun 10 16:39:41.618: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 10 16:39:41.618: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: verifying the node has the label node target-cluster-control-plane-8m52s
STEP: verifying the node has the label node target-cluster-md-0-6b59c4f65-5pcx5
STEP: verifying the node has the label node target-cluster-md-0-6b59c4f65-cqpjz
Jun 10 16:39:41.749: INFO: Pod calico-kube-controllers-7c5d656c49-x6mc4 requesting resource cpu=0m on Node target-cluster-control-plane-8m52s
Jun 10 16:39:41.749: INFO: Pod calico-node-dvbdj requesting resource cpu=0m on Node target-cluster-md-0-6b59c4f65-5pcx5
Jun 10 16:39:41.749: INFO: Pod calico-node-j7mxn requesting resource cpu=0m on Node target-cluster-md-0-6b59c4f65-cqpjz
Jun 10 16:39:41.749: INFO: Pod calico-node-phlfp requesting resource cpu=0m on Node target-cluster-control-plane-8m52s
Jun 10 16:39:41.749: INFO: Pod calico-typha-5d4587dc9c-7g8gs requesting resource cpu=0m on Node target-cluster-control-plane-8m52s
Jun 10 16:39:41.749: INFO: Pod calico-typha-5d4587dc9c-ckl68 requesting resource cpu=0m on Node target-cluster-md-0-6b59c4f65-cqpjz
Jun 10 16:39:41.749: INFO: Pod calico-typha-5d4587dc9c-lq9tq requesting resource cpu=0m on Node target-cluster-md-0-6b59c4f65-5pcx5
Jun 10 16:39:41.749: INFO: Pod capd-controller-manager-7c89f6ddd7-dc686 requesting resource cpu=0m on Node target-cluster-control-plane-8m52s
Jun 10 16:39:41.749: INFO: Pod capi-kubeadm-bootstrap-controller-manager-6f669ccd7c-gtbcn requesting resource cpu=0m on Node target-cluster-control-plane-8m52s
Jun 10 16:39:41.749: INFO: Pod capi-kubeadm-control-plane-controller-manager-5c95f59c5c-l4jvx requesting resource cpu=0m on Node target-cluster-control-plane-8m52s
Jun 10 16:39:41.749: INFO: Pod capi-controller-manager-5f677d7d65-tzt7h requesting resource cpu=0m on Node target-cluster-control-plane-8m52s
Jun 10 16:39:41.749: INFO: Pod capi-controller-manager-745689557d-77m69 requesting resource cpu=0m on Node target-cluster-control-plane-8m52s
Jun 10 16:39:41.749: INFO: Pod capi-kubeadm-bootstrap-controller-manager-6949f44db8-hn6kq requesting resource cpu=0m on Node target-cluster-control-plane-8m52s
Jun 10 16:39:41.749: INFO: Pod capi-kubeadm-control-plane-controller-manager-7b6c4bf48d-x8pzk requesting resource cpu=0m on Node target-cluster-control-plane-8m52s
Jun 10 16:39:41.749: INFO: Pod cert-manager-768bf64dd4-4lhmj requesting resource cpu=0m on Node target-cluster-control-plane-8m52s
Jun 10 16:39:41.749: INFO: Pod cert-manager-cainjector-646879549c-pbn4t requesting resource cpu=0m on Node target-cluster-control-plane-8m52s
Jun 10 16:39:41.749: INFO: Pod cert-manager-webhook-6dc9ccc9fb-6gg4f requesting resource cpu=0m on Node target-cluster-control-plane-8m52s
Jun 10 16:39:41.749: INFO: Pod test-host-network-pod requesting resource cpu=0m on Node target-cluster-md-0-6b59c4f65-5pcx5
Jun 10 16:39:41.749: INFO: Pod test-pod requesting resource cpu=0m on Node target-cluster-md-0-6b59c4f65-cqpjz
Jun 10 16:39:41.749: INFO: Pod coredns-f9fd979d6-2dcsm requesting resource cpu=100m on Node target-cluster-control-plane-8m52s
Jun 10 16:39:41.749: INFO: Pod coredns-f9fd979d6-zsjpx requesting resource cpu=100m on Node target-cluster-control-plane-8m52s
Jun 10 16:39:41.749: INFO: Pod etcd-target-cluster-control-plane-8m52s requesting resource cpu=0m on Node target-cluster-control-plane-8m52s
Jun 10 16:39:41.749: INFO: Pod kube-apiserver-target-cluster-control-plane-8m52s requesting resource cpu=250m on Node target-cluster-control-plane-8m52s
Jun 10 16:39:41.749: INFO: Pod kube-controller-manager-target-cluster-control-plane-8m52s requesting resource cpu=200m on Node target-cluster-control-plane-8m52s
Jun 10 16:39:41.749: INFO: Pod kube-proxy-dklkq requesting resource cpu=0m on Node target-cluster-md-0-6b59c4f65-5pcx5
Jun 10 16:39:41.749: INFO: Pod kube-proxy-hz9zf requesting resource cpu=0m on Node target-cluster-control-plane-8m52s
Jun 10 16:39:41.749: INFO: Pod kube-proxy-mc9rx requesting resource cpu=0m on Node target-cluster-md-0-6b59c4f65-cqpjz
Jun 10 16:39:41.749: INFO: Pod kube-scheduler-target-cluster-control-plane-8m52s requesting resource cpu=100m on Node target-cluster-control-plane-8m52s
Jun 10 16:39:41.749: INFO: Pod sonobuoy requesting resource cpu=0m on Node target-cluster-md-0-6b59c4f65-cqpjz
Jun 10 16:39:41.749: INFO: Pod sonobuoy-e2e-job-04c20f7a477e40c3 requesting resource cpu=0m on Node target-cluster-md-0-6b59c4f65-5pcx5
Jun 10 16:39:41.749: INFO: Pod sonobuoy-systemd-logs-daemon-set-0467f197b1c74b53-bs8vn requesting resource cpu=0m on Node target-cluster-control-plane-8m52s
Jun 10 16:39:41.749: INFO: Pod sonobuoy-systemd-logs-daemon-set-0467f197b1c74b53-cf9lb requesting resource cpu=0m on Node target-cluster-md-0-6b59c4f65-cqpjz
Jun 10 16:39:41.749: INFO: Pod sonobuoy-systemd-logs-daemon-set-0467f197b1c74b53-gj6jj requesting resource cpu=0m on Node target-cluster-md-0-6b59c4f65-5pcx5
Jun 10 16:39:41.749: INFO: Pod tigera-operator-5b76777d49-9zvzd requesting resource cpu=0m on Node target-cluster-control-plane-8m52s
STEP: Starting Pods to consume most of the cluster CPU.
Jun 10 16:39:41.749: INFO: Creating a pod which consumes cpu=2275m on Node target-cluster-control-plane-8m52s
Jun 10 16:39:41.756: INFO: Creating a pod which consumes cpu=2800m on Node target-cluster-md-0-6b59c4f65-5pcx5
Jun 10 16:39:41.761: INFO: Creating a pod which consumes cpu=2800m on Node target-cluster-md-0-6b59c4f65-cqpjz
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1a33d28c-8a9d-4b83-a099-714d865525b0.16874605e40b8360], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9108/filler-pod-1a33d28c-8a9d-4b83-a099-714d865525b0 to target-cluster-md-0-6b59c4f65-cqpjz]
STEP: Considering event: 
Type = [Warning], Name = [filler-pod-1a33d28c-8a9d-4b83-a099-714d865525b0.1687460629cdd841], Reason = [FailedMount], Message = [MountVolume.SetUp failed for volume "default-token-hm44t" : failed to sync secret cache: timed out waiting for the condition]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1a33d28c-8a9d-4b83-a099-714d865525b0.168746066cf708c9], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1a33d28c-8a9d-4b83-a099-714d865525b0.168746067085560d], Reason = [Created], Message = [Created container filler-pod-1a33d28c-8a9d-4b83-a099-714d865525b0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1a33d28c-8a9d-4b83-a099-714d865525b0.16874606790d6c51], Reason = [Started], Message = [Started container filler-pod-1a33d28c-8a9d-4b83-a099-714d865525b0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bd4c5174-6fbb-4a6d-8c76-32933a01ff3a.16874605e282bf8e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9108/filler-pod-bd4c5174-6fbb-4a6d-8c76-32933a01ff3a to target-cluster-md-0-6b59c4f65-5pcx5]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bd4c5174-6fbb-4a6d-8c76-32933a01ff3a.16874606216c3449], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bd4c5174-6fbb-4a6d-8c76-32933a01ff3a.16874606240c3c7f], Reason = [Created], Message = [Created container filler-pod-bd4c5174-6fbb-4a6d-8c76-32933a01ff3a]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bd4c5174-6fbb-4a6d-8c76-32933a01ff3a.168746062d2d53fa], Reason = [Started], Message = [Started container filler-pod-bd4c5174-6fbb-4a6d-8c76-32933a01ff3a]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e37137cd-5602-4388-a3db-1efdd088c27f.16874605e1af59c4], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9108/filler-pod-e37137cd-5602-4388-a3db-1efdd088c27f to target-cluster-control-plane-8m52s]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e37137cd-5602-4388-a3db-1efdd088c27f.16874606261ae7d2], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e37137cd-5602-4388-a3db-1efdd088c27f.16874606289213bc], Reason = [Created], Message = [Created container filler-pod-e37137cd-5602-4388-a3db-1efdd088c27f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e37137cd-5602-4388-a3db-1efdd088c27f.1687460632889df1], Reason = [Started], Message = [Started container filler-pod-e37137cd-5602-4388-a3db-1efdd088c27f]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16874606d4f7766f], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node target-cluster-md-0-6b59c4f65-5pcx5
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node target-cluster-md-0-6b59c4f65-cqpjz
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node target-cluster-control-plane-8m52s
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:39:46.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9108" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:5.435 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":305,"completed":165,"skipped":2620,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:39:46.954: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-97b455fc-2b59-44e3-95c0-e1b77d4b440e
STEP: Creating a pod to test consume configMaps
Jun 10 16:39:47.009: INFO: Waiting up to 5m0s for pod "pod-configmaps-5d720c61-4423-4e7c-ba61-686b60bb1305" in namespace "configmap-9259" to be "Succeeded or Failed"
Jun 10 16:39:47.011: INFO: Pod "pod-configmaps-5d720c61-4423-4e7c-ba61-686b60bb1305": Phase="Pending", Reason="", readiness=false. Elapsed: 2.862265ms
Jun 10 16:39:49.014: INFO: Pod "pod-configmaps-5d720c61-4423-4e7c-ba61-686b60bb1305": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005201131s
STEP: Saw pod success
Jun 10 16:39:49.014: INFO: Pod "pod-configmaps-5d720c61-4423-4e7c-ba61-686b60bb1305" satisfied condition "Succeeded or Failed"
Jun 10 16:39:49.016: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod pod-configmaps-5d720c61-4423-4e7c-ba61-686b60bb1305 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 10 16:39:49.066: INFO: Waiting for pod pod-configmaps-5d720c61-4423-4e7c-ba61-686b60bb1305 to disappear
Jun 10 16:39:49.078: INFO: Pod pod-configmaps-5d720c61-4423-4e7c-ba61-686b60bb1305 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:39:49.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9259" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":166,"skipped":2621,"failed":0}
SSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:39:49.111: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Jun 10 16:39:49.175: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 10 16:40:49.230: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 10 16:40:49.235: INFO: Starting informer...
STEP: Starting pods...
Jun 10 16:40:49.509: INFO: Pod1 is running on target-cluster-md-0-6b59c4f65-cqpjz. Tainting Node
Jun 10 16:40:51.757: INFO: Pod2 is running on target-cluster-md-0-6b59c4f65-cqpjz. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Jun 10 16:41:05.442: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jun 10 16:41:25.466: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:41:25.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-2594" for this suite.

• [SLOW TEST:96.391 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":305,"completed":167,"skipped":2624,"failed":0}
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:41:25.503: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 10 16:41:25.642: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jun 10 16:41:25.655: INFO: Number of nodes with available pods: 0
Jun 10 16:41:25.655: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jun 10 16:41:25.688: INFO: Number of nodes with available pods: 0
Jun 10 16:41:25.688: INFO: Node target-cluster-md-0-6b59c4f65-cqpjz is running more than one daemon pod
Jun 10 16:41:26.721: INFO: Number of nodes with available pods: 0
Jun 10 16:41:26.721: INFO: Node target-cluster-md-0-6b59c4f65-cqpjz is running more than one daemon pod
Jun 10 16:41:27.698: INFO: Number of nodes with available pods: 1
Jun 10 16:41:27.698: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jun 10 16:41:27.738: INFO: Number of nodes with available pods: 1
Jun 10 16:41:27.738: INFO: Number of running nodes: 0, number of available pods: 1
Jun 10 16:41:28.745: INFO: Number of nodes with available pods: 0
Jun 10 16:41:28.745: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jun 10 16:41:28.763: INFO: Number of nodes with available pods: 0
Jun 10 16:41:28.763: INFO: Node target-cluster-md-0-6b59c4f65-cqpjz is running more than one daemon pod
Jun 10 16:41:29.766: INFO: Number of nodes with available pods: 0
Jun 10 16:41:29.767: INFO: Node target-cluster-md-0-6b59c4f65-cqpjz is running more than one daemon pod
Jun 10 16:41:30.768: INFO: Number of nodes with available pods: 0
Jun 10 16:41:30.768: INFO: Node target-cluster-md-0-6b59c4f65-cqpjz is running more than one daemon pod
Jun 10 16:41:31.776: INFO: Number of nodes with available pods: 0
Jun 10 16:41:31.776: INFO: Node target-cluster-md-0-6b59c4f65-cqpjz is running more than one daemon pod
Jun 10 16:41:32.768: INFO: Number of nodes with available pods: 0
Jun 10 16:41:32.768: INFO: Node target-cluster-md-0-6b59c4f65-cqpjz is running more than one daemon pod
Jun 10 16:41:33.767: INFO: Number of nodes with available pods: 0
Jun 10 16:41:33.767: INFO: Node target-cluster-md-0-6b59c4f65-cqpjz is running more than one daemon pod
Jun 10 16:41:34.768: INFO: Number of nodes with available pods: 0
Jun 10 16:41:34.768: INFO: Node target-cluster-md-0-6b59c4f65-cqpjz is running more than one daemon pod
Jun 10 16:41:35.766: INFO: Number of nodes with available pods: 0
Jun 10 16:41:35.766: INFO: Node target-cluster-md-0-6b59c4f65-cqpjz is running more than one daemon pod
Jun 10 16:41:36.787: INFO: Number of nodes with available pods: 0
Jun 10 16:41:36.787: INFO: Node target-cluster-md-0-6b59c4f65-cqpjz is running more than one daemon pod
Jun 10 16:41:37.769: INFO: Number of nodes with available pods: 1
Jun 10 16:41:37.769: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1970, will wait for the garbage collector to delete the pods
Jun 10 16:41:37.837: INFO: Deleting DaemonSet.extensions daemon-set took: 8.173683ms
Jun 10 16:41:39.239: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.401907292s
Jun 10 16:41:45.542: INFO: Number of nodes with available pods: 0
Jun 10 16:41:45.542: INFO: Number of running nodes: 0, number of available pods: 0
Jun 10 16:41:45.545: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1970/daemonsets","resourceVersion":"39049"},"items":null}

Jun 10 16:41:45.548: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1970/pods","resourceVersion":"39049"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:41:45.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1970" for this suite.

• [SLOW TEST:20.082 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":305,"completed":168,"skipped":2625,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:41:45.585: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-ffb117c3-e733-432e-895c-8e16ab653944
STEP: Creating secret with name s-test-opt-upd-6dc805e0-5500-40eb-9a06-63dd01ca12ae
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-ffb117c3-e733-432e-895c-8e16ab653944
STEP: Updating secret s-test-opt-upd-6dc805e0-5500-40eb-9a06-63dd01ca12ae
STEP: Creating secret with name s-test-opt-create-86e4298d-6dc3-4b85-a6d9-47c7a9d5ca0e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:41:49.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7008" for this suite.
•{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":169,"skipped":2640,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:41:49.834: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jun 10 16:41:49.894: INFO: starting watch
STEP: patching
STEP: updating
Jun 10 16:41:49.905: INFO: waiting for watch events with expected annotations
Jun 10 16:41:49.905: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:41:49.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-3410" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":305,"completed":170,"skipped":2668,"failed":0}
SSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:41:49.942: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
Jun 10 16:41:50.599: INFO: created pod pod-service-account-defaultsa
Jun 10 16:41:50.599: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jun 10 16:41:50.603: INFO: created pod pod-service-account-mountsa
Jun 10 16:41:50.603: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jun 10 16:41:50.635: INFO: created pod pod-service-account-nomountsa
Jun 10 16:41:50.635: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jun 10 16:41:50.679: INFO: created pod pod-service-account-defaultsa-mountspec
Jun 10 16:41:50.679: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jun 10 16:41:50.691: INFO: created pod pod-service-account-mountsa-mountspec
Jun 10 16:41:50.691: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jun 10 16:41:50.701: INFO: created pod pod-service-account-nomountsa-mountspec
Jun 10 16:41:50.701: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jun 10 16:41:50.717: INFO: created pod pod-service-account-defaultsa-nomountspec
Jun 10 16:41:50.718: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jun 10 16:41:50.831: INFO: created pod pod-service-account-mountsa-nomountspec
Jun 10 16:41:50.831: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jun 10 16:41:50.850: INFO: created pod pod-service-account-nomountsa-nomountspec
Jun 10 16:41:50.851: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:41:50.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5431" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":305,"completed":171,"skipped":2671,"failed":0}
SSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:41:50.963: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override arguments
Jun 10 16:41:51.275: INFO: Waiting up to 5m0s for pod "client-containers-6b656a43-a75e-42a5-93e1-c9ae627e81e1" in namespace "containers-4213" to be "Succeeded or Failed"
Jun 10 16:41:51.288: INFO: Pod "client-containers-6b656a43-a75e-42a5-93e1-c9ae627e81e1": Phase="Pending", Reason="", readiness=false. Elapsed: 13.520376ms
Jun 10 16:41:53.305: INFO: Pod "client-containers-6b656a43-a75e-42a5-93e1-c9ae627e81e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030364188s
Jun 10 16:41:55.339: INFO: Pod "client-containers-6b656a43-a75e-42a5-93e1-c9ae627e81e1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.064282699s
Jun 10 16:41:57.345: INFO: Pod "client-containers-6b656a43-a75e-42a5-93e1-c9ae627e81e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.070285267s
STEP: Saw pod success
Jun 10 16:41:57.345: INFO: Pod "client-containers-6b656a43-a75e-42a5-93e1-c9ae627e81e1" satisfied condition "Succeeded or Failed"
Jun 10 16:41:57.358: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-5pcx5 pod client-containers-6b656a43-a75e-42a5-93e1-c9ae627e81e1 container test-container: <nil>
STEP: delete the pod
Jun 10 16:41:57.504: INFO: Waiting for pod client-containers-6b656a43-a75e-42a5-93e1-c9ae627e81e1 to disappear
Jun 10 16:41:57.542: INFO: Pod client-containers-6b656a43-a75e-42a5-93e1-c9ae627e81e1 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:41:57.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4213" for this suite.

• [SLOW TEST:6.659 seconds]
[k8s.io] Docker Containers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":305,"completed":172,"skipped":2674,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:41:57.623: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Jun 10 16:41:57.869: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:43:25.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9054" for this suite.

• [SLOW TEST:88.075 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":305,"completed":173,"skipped":2684,"failed":0}
S
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:43:25.698: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jun 10 16:43:25.792: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2509 /api/v1/namespaces/watch-2509/configmaps/e2e-watch-test-label-changed 1624b997-dad1-4c9a-b9d1-cd7132bf3b65 39934 0 2021-06-10 16:43:25 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-06-10 16:43:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 10 16:43:25.793: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2509 /api/v1/namespaces/watch-2509/configmaps/e2e-watch-test-label-changed 1624b997-dad1-4c9a-b9d1-cd7132bf3b65 39935 0 2021-06-10 16:43:25 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-06-10 16:43:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 10 16:43:25.793: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2509 /api/v1/namespaces/watch-2509/configmaps/e2e-watch-test-label-changed 1624b997-dad1-4c9a-b9d1-cd7132bf3b65 39936 0 2021-06-10 16:43:25 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-06-10 16:43:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jun 10 16:43:35.824: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2509 /api/v1/namespaces/watch-2509/configmaps/e2e-watch-test-label-changed 1624b997-dad1-4c9a-b9d1-cd7132bf3b65 39996 0 2021-06-10 16:43:25 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-06-10 16:43:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 10 16:43:35.824: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2509 /api/v1/namespaces/watch-2509/configmaps/e2e-watch-test-label-changed 1624b997-dad1-4c9a-b9d1-cd7132bf3b65 39997 0 2021-06-10 16:43:25 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-06-10 16:43:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 10 16:43:35.825: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2509 /api/v1/namespaces/watch-2509/configmaps/e2e-watch-test-label-changed 1624b997-dad1-4c9a-b9d1-cd7132bf3b65 39998 0 2021-06-10 16:43:25 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-06-10 16:43:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:43:35.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2509" for this suite.

• [SLOW TEST:10.137 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":305,"completed":174,"skipped":2685,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:43:35.835: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in volume subpath
Jun 10 16:43:35.914: INFO: Waiting up to 5m0s for pod "var-expansion-8c667ce2-8910-4f31-bc9d-c25eb33fd5d0" in namespace "var-expansion-1356" to be "Succeeded or Failed"
Jun 10 16:43:35.928: INFO: Pod "var-expansion-8c667ce2-8910-4f31-bc9d-c25eb33fd5d0": Phase="Pending", Reason="", readiness=false. Elapsed: 13.58937ms
Jun 10 16:43:37.932: INFO: Pod "var-expansion-8c667ce2-8910-4f31-bc9d-c25eb33fd5d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017582235s
Jun 10 16:43:39.940: INFO: Pod "var-expansion-8c667ce2-8910-4f31-bc9d-c25eb33fd5d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025373421s
STEP: Saw pod success
Jun 10 16:43:39.940: INFO: Pod "var-expansion-8c667ce2-8910-4f31-bc9d-c25eb33fd5d0" satisfied condition "Succeeded or Failed"
Jun 10 16:43:39.952: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod var-expansion-8c667ce2-8910-4f31-bc9d-c25eb33fd5d0 container dapi-container: <nil>
STEP: delete the pod
Jun 10 16:43:39.983: INFO: Waiting for pod var-expansion-8c667ce2-8910-4f31-bc9d-c25eb33fd5d0 to disappear
Jun 10 16:43:39.986: INFO: Pod var-expansion-8c667ce2-8910-4f31-bc9d-c25eb33fd5d0 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:43:39.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1356" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":305,"completed":175,"skipped":2736,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:43:39.995: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 10 16:43:40.040: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7499aff1-316a-4112-a44e-c6dfdc73b0da" in namespace "downward-api-7584" to be "Succeeded or Failed"
Jun 10 16:43:40.048: INFO: Pod "downwardapi-volume-7499aff1-316a-4112-a44e-c6dfdc73b0da": Phase="Pending", Reason="", readiness=false. Elapsed: 8.133412ms
Jun 10 16:43:42.053: INFO: Pod "downwardapi-volume-7499aff1-316a-4112-a44e-c6dfdc73b0da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013018809s
STEP: Saw pod success
Jun 10 16:43:42.053: INFO: Pod "downwardapi-volume-7499aff1-316a-4112-a44e-c6dfdc73b0da" satisfied condition "Succeeded or Failed"
Jun 10 16:43:42.057: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod downwardapi-volume-7499aff1-316a-4112-a44e-c6dfdc73b0da container client-container: <nil>
STEP: delete the pod
Jun 10 16:43:42.117: INFO: Waiting for pod downwardapi-volume-7499aff1-316a-4112-a44e-c6dfdc73b0da to disappear
Jun 10 16:43:42.141: INFO: Pod downwardapi-volume-7499aff1-316a-4112-a44e-c6dfdc73b0da no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:43:42.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7584" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":176,"skipped":2803,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:43:42.178: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Jun 10 16:43:42.298: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:43:42.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8354" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":305,"completed":177,"skipped":2820,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:43:42.372: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Jun 10 16:43:46.988: INFO: Successfully updated pod "adopt-release-8ws75"
STEP: Checking that the Job readopts the Pod
Jun 10 16:43:46.988: INFO: Waiting up to 15m0s for pod "adopt-release-8ws75" in namespace "job-9901" to be "adopted"
Jun 10 16:43:46.995: INFO: Pod "adopt-release-8ws75": Phase="Running", Reason="", readiness=true. Elapsed: 7.209474ms
Jun 10 16:43:49.003: INFO: Pod "adopt-release-8ws75": Phase="Running", Reason="", readiness=true. Elapsed: 2.015042775s
Jun 10 16:43:49.003: INFO: Pod "adopt-release-8ws75" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Jun 10 16:43:49.545: INFO: Successfully updated pod "adopt-release-8ws75"
STEP: Checking that the Job releases the Pod
Jun 10 16:43:49.545: INFO: Waiting up to 15m0s for pod "adopt-release-8ws75" in namespace "job-9901" to be "released"
Jun 10 16:43:49.552: INFO: Pod "adopt-release-8ws75": Phase="Running", Reason="", readiness=true. Elapsed: 7.607101ms
Jun 10 16:43:51.557: INFO: Pod "adopt-release-8ws75": Phase="Running", Reason="", readiness=true. Elapsed: 2.011805202s
Jun 10 16:43:51.557: INFO: Pod "adopt-release-8ws75" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:43:51.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9901" for this suite.

• [SLOW TEST:9.193 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":305,"completed":178,"skipped":2834,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:43:51.565: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 10 16:43:53.714: INFO: Waiting up to 5m0s for pod "client-envvars-2fde0e03-4206-47d8-804e-c324b67d30f2" in namespace "pods-5586" to be "Succeeded or Failed"
Jun 10 16:43:53.723: INFO: Pod "client-envvars-2fde0e03-4206-47d8-804e-c324b67d30f2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.852915ms
Jun 10 16:43:55.726: INFO: Pod "client-envvars-2fde0e03-4206-47d8-804e-c324b67d30f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012097473s
STEP: Saw pod success
Jun 10 16:43:55.726: INFO: Pod "client-envvars-2fde0e03-4206-47d8-804e-c324b67d30f2" satisfied condition "Succeeded or Failed"
Jun 10 16:43:55.730: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod client-envvars-2fde0e03-4206-47d8-804e-c324b67d30f2 container env3cont: <nil>
STEP: delete the pod
Jun 10 16:43:55.748: INFO: Waiting for pod client-envvars-2fde0e03-4206-47d8-804e-c324b67d30f2 to disappear
Jun 10 16:43:55.758: INFO: Pod client-envvars-2fde0e03-4206-47d8-804e-c324b67d30f2 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:43:55.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5586" for this suite.
•{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":305,"completed":179,"skipped":2846,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:43:55.769: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service endpoint-test2 in namespace services-6744
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6744 to expose endpoints map[]
Jun 10 16:43:55.866: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Jun 10 16:43:56.895: INFO: successfully validated that service endpoint-test2 in namespace services-6744 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-6744
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6744 to expose endpoints map[pod1:[80]]
Jun 10 16:43:58.940: INFO: successfully validated that service endpoint-test2 in namespace services-6744 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-6744
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6744 to expose endpoints map[pod1:[80] pod2:[80]]
Jun 10 16:44:01.040: INFO: successfully validated that service endpoint-test2 in namespace services-6744 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-6744
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6744 to expose endpoints map[pod2:[80]]
Jun 10 16:44:01.185: INFO: successfully validated that service endpoint-test2 in namespace services-6744 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-6744
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6744 to expose endpoints map[]
Jun 10 16:44:01.353: INFO: successfully validated that service endpoint-test2 in namespace services-6744 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:44:01.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6744" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:5.823 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":305,"completed":180,"skipped":2901,"failed":0}
SSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:44:01.594: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:44:01.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3061" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":305,"completed":181,"skipped":2911,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:44:02.008: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating replication controller my-hostname-basic-43347950-a7be-4e69-928b-2819be457ca1
Jun 10 16:44:02.160: INFO: Pod name my-hostname-basic-43347950-a7be-4e69-928b-2819be457ca1: Found 0 pods out of 1
Jun 10 16:44:07.169: INFO: Pod name my-hostname-basic-43347950-a7be-4e69-928b-2819be457ca1: Found 1 pods out of 1
Jun 10 16:44:07.170: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-43347950-a7be-4e69-928b-2819be457ca1" are running
Jun 10 16:44:07.176: INFO: Pod "my-hostname-basic-43347950-a7be-4e69-928b-2819be457ca1-bk7nm" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-06-10 16:44:02 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-06-10 16:44:05 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-06-10 16:44:05 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-06-10 16:44:02 +0000 UTC Reason: Message:}])
Jun 10 16:44:07.176: INFO: Trying to dial the pod
Jun 10 16:44:12.186: INFO: Controller my-hostname-basic-43347950-a7be-4e69-928b-2819be457ca1: Got expected result from replica 1 [my-hostname-basic-43347950-a7be-4e69-928b-2819be457ca1-bk7nm]: "my-hostname-basic-43347950-a7be-4e69-928b-2819be457ca1-bk7nm", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:44:12.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5916" for this suite.

• [SLOW TEST:10.195 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":305,"completed":182,"skipped":2947,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:44:12.204: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-pj48
STEP: Creating a pod to test atomic-volume-subpath
Jun 10 16:44:12.300: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-pj48" in namespace "subpath-6683" to be "Succeeded or Failed"
Jun 10 16:44:12.307: INFO: Pod "pod-subpath-test-configmap-pj48": Phase="Pending", Reason="", readiness=false. Elapsed: 7.126687ms
Jun 10 16:44:14.333: INFO: Pod "pod-subpath-test-configmap-pj48": Phase="Running", Reason="", readiness=true. Elapsed: 2.033045504s
Jun 10 16:44:16.341: INFO: Pod "pod-subpath-test-configmap-pj48": Phase="Running", Reason="", readiness=true. Elapsed: 4.041737575s
Jun 10 16:44:18.345: INFO: Pod "pod-subpath-test-configmap-pj48": Phase="Running", Reason="", readiness=true. Elapsed: 6.045204531s
Jun 10 16:44:20.351: INFO: Pod "pod-subpath-test-configmap-pj48": Phase="Running", Reason="", readiness=true. Elapsed: 8.051340663s
Jun 10 16:44:22.371: INFO: Pod "pod-subpath-test-configmap-pj48": Phase="Running", Reason="", readiness=true. Elapsed: 10.071258107s
Jun 10 16:44:24.376: INFO: Pod "pod-subpath-test-configmap-pj48": Phase="Running", Reason="", readiness=true. Elapsed: 12.075881227s
Jun 10 16:44:26.380: INFO: Pod "pod-subpath-test-configmap-pj48": Phase="Running", Reason="", readiness=true. Elapsed: 14.080713239s
Jun 10 16:44:28.384: INFO: Pod "pod-subpath-test-configmap-pj48": Phase="Running", Reason="", readiness=true. Elapsed: 16.084377663s
Jun 10 16:44:30.397: INFO: Pod "pod-subpath-test-configmap-pj48": Phase="Running", Reason="", readiness=true. Elapsed: 18.097491425s
Jun 10 16:44:32.406: INFO: Pod "pod-subpath-test-configmap-pj48": Phase="Running", Reason="", readiness=true. Elapsed: 20.106042987s
Jun 10 16:44:34.409: INFO: Pod "pod-subpath-test-configmap-pj48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.109546574s
STEP: Saw pod success
Jun 10 16:44:34.409: INFO: Pod "pod-subpath-test-configmap-pj48" satisfied condition "Succeeded or Failed"
Jun 10 16:44:34.412: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod pod-subpath-test-configmap-pj48 container test-container-subpath-configmap-pj48: <nil>
STEP: delete the pod
Jun 10 16:44:34.447: INFO: Waiting for pod pod-subpath-test-configmap-pj48 to disappear
Jun 10 16:44:34.450: INFO: Pod pod-subpath-test-configmap-pj48 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-pj48
Jun 10 16:44:34.450: INFO: Deleting pod "pod-subpath-test-configmap-pj48" in namespace "subpath-6683"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:44:34.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6683" for this suite.

• [SLOW TEST:22.305 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":305,"completed":183,"skipped":3021,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:44:34.509: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-3160
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 10 16:44:34.565: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun 10 16:44:34.674: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 10 16:44:36.691: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 16:44:38.677: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 16:44:40.690: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 16:44:42.680: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 16:44:44.680: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 16:44:46.682: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 16:44:48.676: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 16:44:50.677: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 16:44:52.677: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jun 10 16:44:52.680: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jun 10 16:44:52.685: INFO: The status of Pod netserver-2 is Running (Ready = false)
Jun 10 16:44:54.688: INFO: The status of Pod netserver-2 is Running (Ready = false)
Jun 10 16:44:56.688: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Jun 10 16:44:58.777: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.251.41:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3160 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 10 16:44:58.777: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
Jun 10 16:44:58.887: INFO: Found all expected endpoints: [netserver-0]
Jun 10 16:44:58.890: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.24.147:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3160 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 10 16:44:58.890: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
Jun 10 16:44:59.007: INFO: Found all expected endpoints: [netserver-1]
Jun 10 16:44:59.012: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.78.109:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3160 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 10 16:44:59.012: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
Jun 10 16:44:59.112: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:44:59.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3160" for this suite.

• [SLOW TEST:24.611 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":184,"skipped":3047,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:44:59.120: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:44:59.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2481" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":305,"completed":185,"skipped":3088,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:44:59.214: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun 10 16:44:59.271: INFO: Waiting up to 5m0s for pod "pod-86fb5a9f-d0bc-4d02-894b-fc602f551b95" in namespace "emptydir-9922" to be "Succeeded or Failed"
Jun 10 16:44:59.283: INFO: Pod "pod-86fb5a9f-d0bc-4d02-894b-fc602f551b95": Phase="Pending", Reason="", readiness=false. Elapsed: 11.978937ms
Jun 10 16:45:01.291: INFO: Pod "pod-86fb5a9f-d0bc-4d02-894b-fc602f551b95": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019995952s
Jun 10 16:45:03.294: INFO: Pod "pod-86fb5a9f-d0bc-4d02-894b-fc602f551b95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023238824s
STEP: Saw pod success
Jun 10 16:45:03.295: INFO: Pod "pod-86fb5a9f-d0bc-4d02-894b-fc602f551b95" satisfied condition "Succeeded or Failed"
Jun 10 16:45:03.296: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod pod-86fb5a9f-d0bc-4d02-894b-fc602f551b95 container test-container: <nil>
STEP: delete the pod
Jun 10 16:45:03.323: INFO: Waiting for pod pod-86fb5a9f-d0bc-4d02-894b-fc602f551b95 to disappear
Jun 10 16:45:03.330: INFO: Pod pod-86fb5a9f-d0bc-4d02-894b-fc602f551b95 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:45:03.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9922" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":186,"skipped":3118,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:45:03.340: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 10 16:45:03.403: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3f19bafa-b6c4-4048-8555-d12e07e8988d" in namespace "downward-api-6180" to be "Succeeded or Failed"
Jun 10 16:45:03.423: INFO: Pod "downwardapi-volume-3f19bafa-b6c4-4048-8555-d12e07e8988d": Phase="Pending", Reason="", readiness=false. Elapsed: 19.111008ms
Jun 10 16:45:05.541: INFO: Pod "downwardapi-volume-3f19bafa-b6c4-4048-8555-d12e07e8988d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.137840781s
Jun 10 16:45:07.551: INFO: Pod "downwardapi-volume-3f19bafa-b6c4-4048-8555-d12e07e8988d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.14795887s
STEP: Saw pod success
Jun 10 16:45:07.551: INFO: Pod "downwardapi-volume-3f19bafa-b6c4-4048-8555-d12e07e8988d" satisfied condition "Succeeded or Failed"
Jun 10 16:45:07.562: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod downwardapi-volume-3f19bafa-b6c4-4048-8555-d12e07e8988d container client-container: <nil>
STEP: delete the pod
Jun 10 16:45:07.618: INFO: Waiting for pod downwardapi-volume-3f19bafa-b6c4-4048-8555-d12e07e8988d to disappear
Jun 10 16:45:07.621: INFO: Pod downwardapi-volume-3f19bafa-b6c4-4048-8555-d12e07e8988d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:45:07.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6180" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":187,"skipped":3143,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:45:07.660: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-152c0bf7-c8cf-4cb3-9142-a83dbe91f507
STEP: Creating a pod to test consume secrets
Jun 10 16:45:07.808: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fe8ba36f-8c15-45c4-b830-f19eeac068ee" in namespace "projected-2474" to be "Succeeded or Failed"
Jun 10 16:45:07.815: INFO: Pod "pod-projected-secrets-fe8ba36f-8c15-45c4-b830-f19eeac068ee": Phase="Pending", Reason="", readiness=false. Elapsed: 7.614443ms
Jun 10 16:45:09.829: INFO: Pod "pod-projected-secrets-fe8ba36f-8c15-45c4-b830-f19eeac068ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020963553s
STEP: Saw pod success
Jun 10 16:45:09.829: INFO: Pod "pod-projected-secrets-fe8ba36f-8c15-45c4-b830-f19eeac068ee" satisfied condition "Succeeded or Failed"
Jun 10 16:45:09.831: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod pod-projected-secrets-fe8ba36f-8c15-45c4-b830-f19eeac068ee container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 10 16:45:09.867: INFO: Waiting for pod pod-projected-secrets-fe8ba36f-8c15-45c4-b830-f19eeac068ee to disappear
Jun 10 16:45:09.872: INFO: Pod pod-projected-secrets-fe8ba36f-8c15-45c4-b830-f19eeac068ee no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:45:09.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2474" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":188,"skipped":3187,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:45:09.887: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 10 16:45:09.957: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Jun 10 16:45:24.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=crd-publish-openapi-4274 --namespace=crd-publish-openapi-4274 create -f -'
Jun 10 16:45:25.658: INFO: stderr: ""
Jun 10 16:45:25.658: INFO: stdout: "e2e-test-crd-publish-openapi-6004-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jun 10 16:45:25.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=crd-publish-openapi-4274 --namespace=crd-publish-openapi-4274 delete e2e-test-crd-publish-openapi-6004-crds test-foo'
Jun 10 16:45:25.895: INFO: stderr: ""
Jun 10 16:45:25.895: INFO: stdout: "e2e-test-crd-publish-openapi-6004-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jun 10 16:45:25.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=crd-publish-openapi-4274 --namespace=crd-publish-openapi-4274 apply -f -'
Jun 10 16:45:26.389: INFO: stderr: ""
Jun 10 16:45:26.389: INFO: stdout: "e2e-test-crd-publish-openapi-6004-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jun 10 16:45:26.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=crd-publish-openapi-4274 --namespace=crd-publish-openapi-4274 delete e2e-test-crd-publish-openapi-6004-crds test-foo'
Jun 10 16:45:26.553: INFO: stderr: ""
Jun 10 16:45:26.553: INFO: stdout: "e2e-test-crd-publish-openapi-6004-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Jun 10 16:45:26.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=crd-publish-openapi-4274 --namespace=crd-publish-openapi-4274 create -f -'
Jun 10 16:45:26.995: INFO: rc: 1
Jun 10 16:45:26.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=crd-publish-openapi-4274 --namespace=crd-publish-openapi-4274 apply -f -'
Jun 10 16:45:27.424: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Jun 10 16:45:27.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=crd-publish-openapi-4274 --namespace=crd-publish-openapi-4274 create -f -'
Jun 10 16:45:27.805: INFO: rc: 1
Jun 10 16:45:27.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=crd-publish-openapi-4274 --namespace=crd-publish-openapi-4274 apply -f -'
Jun 10 16:45:28.245: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Jun 10 16:45:28.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=crd-publish-openapi-4274 explain e2e-test-crd-publish-openapi-6004-crds'
Jun 10 16:45:29.030: INFO: stderr: ""
Jun 10 16:45:29.030: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6004-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Jun 10 16:45:29.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=crd-publish-openapi-4274 explain e2e-test-crd-publish-openapi-6004-crds.metadata'
Jun 10 16:45:29.526: INFO: stderr: ""
Jun 10 16:45:29.526: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6004-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jun 10 16:45:29.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=crd-publish-openapi-4274 explain e2e-test-crd-publish-openapi-6004-crds.spec'
Jun 10 16:45:30.059: INFO: stderr: ""
Jun 10 16:45:30.059: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6004-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jun 10 16:45:30.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=crd-publish-openapi-4274 explain e2e-test-crd-publish-openapi-6004-crds.spec.bars'
Jun 10 16:45:30.583: INFO: stderr: ""
Jun 10 16:45:30.583: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6004-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Jun 10 16:45:30.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=crd-publish-openapi-4274 explain e2e-test-crd-publish-openapi-6004-crds.spec.bars2'
Jun 10 16:45:31.227: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:45:46.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4274" for this suite.

• [SLOW TEST:36.538 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":305,"completed":189,"skipped":3222,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:45:46.426: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name secret-emptykey-test-40a11c99-5805-4e5d-a84a-05afe8f8b9c9
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:45:46.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7106" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":305,"completed":190,"skipped":3233,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:45:46.527: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-4af8747e-17d9-4dbf-8b1d-47fa60ebf857 in namespace container-probe-7342
Jun 10 16:45:48.648: INFO: Started pod liveness-4af8747e-17d9-4dbf-8b1d-47fa60ebf857 in namespace container-probe-7342
STEP: checking the pod's current state and verifying that restartCount is present
Jun 10 16:45:48.650: INFO: Initial restart count of pod liveness-4af8747e-17d9-4dbf-8b1d-47fa60ebf857 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:49:49.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7342" for this suite.

• [SLOW TEST:242.841 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":305,"completed":191,"skipped":3249,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:49:49.369: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name projected-secret-test-5d33a17b-d83a-45ae-bdd1-fed18f2ae1d0
STEP: Creating a pod to test consume secrets
Jun 10 16:49:49.470: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6ce65916-ecd0-422c-8c4b-bffc0228acb3" in namespace "projected-8716" to be "Succeeded or Failed"
Jun 10 16:49:49.479: INFO: Pod "pod-projected-secrets-6ce65916-ecd0-422c-8c4b-bffc0228acb3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.871008ms
Jun 10 16:49:51.484: INFO: Pod "pod-projected-secrets-6ce65916-ecd0-422c-8c4b-bffc0228acb3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013315008s
STEP: Saw pod success
Jun 10 16:49:51.484: INFO: Pod "pod-projected-secrets-6ce65916-ecd0-422c-8c4b-bffc0228acb3" satisfied condition "Succeeded or Failed"
Jun 10 16:49:51.488: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod pod-projected-secrets-6ce65916-ecd0-422c-8c4b-bffc0228acb3 container secret-volume-test: <nil>
STEP: delete the pod
Jun 10 16:49:51.576: INFO: Waiting for pod pod-projected-secrets-6ce65916-ecd0-422c-8c4b-bffc0228acb3 to disappear
Jun 10 16:49:51.579: INFO: Pod pod-projected-secrets-6ce65916-ecd0-422c-8c4b-bffc0228acb3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:49:51.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8716" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":305,"completed":192,"skipped":3269,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:49:51.602: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 10 16:49:51.720: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-e83cdd55-7c55-47f9-9e85-5bfcccf2055b" in namespace "security-context-test-7928" to be "Succeeded or Failed"
Jun 10 16:49:51.734: INFO: Pod "alpine-nnp-false-e83cdd55-7c55-47f9-9e85-5bfcccf2055b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.334816ms
Jun 10 16:49:53.739: INFO: Pod "alpine-nnp-false-e83cdd55-7c55-47f9-9e85-5bfcccf2055b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019066374s
Jun 10 16:49:55.748: INFO: Pod "alpine-nnp-false-e83cdd55-7c55-47f9-9e85-5bfcccf2055b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028387113s
Jun 10 16:49:57.752: INFO: Pod "alpine-nnp-false-e83cdd55-7c55-47f9-9e85-5bfcccf2055b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031951889s
Jun 10 16:49:57.752: INFO: Pod "alpine-nnp-false-e83cdd55-7c55-47f9-9e85-5bfcccf2055b" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:49:57.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7928" for this suite.

• [SLOW TEST:6.168 seconds]
[k8s.io] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when creating containers with AllowPrivilegeEscalation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:291
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":193,"skipped":3326,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:49:57.771: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 10 16:49:58.720: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 10 16:50:00.728: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758940598, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758940598, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758940598, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758940598, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 10 16:50:03.744: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:50:04.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1406" for this suite.
STEP: Destroying namespace "webhook-1406-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.599 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":305,"completed":194,"skipped":3337,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:50:04.370: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Jun 10 16:50:04.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-1869 create -f -'
Jun 10 16:50:05.227: INFO: stderr: ""
Jun 10 16:50:05.227: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 10 16:50:05.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-1869 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 10 16:50:05.681: INFO: stderr: ""
Jun 10 16:50:05.681: INFO: stdout: "update-demo-nautilus-9ttvl update-demo-nautilus-g8hvj "
Jun 10 16:50:05.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-1869 get pods update-demo-nautilus-9ttvl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 10 16:50:06.266: INFO: stderr: ""
Jun 10 16:50:06.266: INFO: stdout: ""
Jun 10 16:50:06.266: INFO: update-demo-nautilus-9ttvl is created but not running
Jun 10 16:50:11.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-1869 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 10 16:50:11.398: INFO: stderr: ""
Jun 10 16:50:11.398: INFO: stdout: "update-demo-nautilus-9ttvl update-demo-nautilus-g8hvj "
Jun 10 16:50:11.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-1869 get pods update-demo-nautilus-9ttvl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 10 16:50:11.525: INFO: stderr: ""
Jun 10 16:50:11.525: INFO: stdout: "true"
Jun 10 16:50:11.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-1869 get pods update-demo-nautilus-9ttvl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 10 16:50:11.650: INFO: stderr: ""
Jun 10 16:50:11.650: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 10 16:50:11.651: INFO: validating pod update-demo-nautilus-9ttvl
Jun 10 16:50:11.654: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 10 16:50:11.655: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 10 16:50:11.655: INFO: update-demo-nautilus-9ttvl is verified up and running
Jun 10 16:50:11.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-1869 get pods update-demo-nautilus-g8hvj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 10 16:50:11.779: INFO: stderr: ""
Jun 10 16:50:11.779: INFO: stdout: "true"
Jun 10 16:50:11.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-1869 get pods update-demo-nautilus-g8hvj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 10 16:50:11.898: INFO: stderr: ""
Jun 10 16:50:11.898: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 10 16:50:11.898: INFO: validating pod update-demo-nautilus-g8hvj
Jun 10 16:50:11.901: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 10 16:50:11.902: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 10 16:50:11.902: INFO: update-demo-nautilus-g8hvj is verified up and running
STEP: using delete to clean up resources
Jun 10 16:50:11.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-1869 delete --grace-period=0 --force -f -'
Jun 10 16:50:12.083: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 10 16:50:12.083: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun 10 16:50:12.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-1869 get rc,svc -l name=update-demo --no-headers'
Jun 10 16:50:12.387: INFO: stderr: "No resources found in kubectl-1869 namespace.\n"
Jun 10 16:50:12.387: INFO: stdout: ""
Jun 10 16:50:12.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-1869 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 10 16:50:12.734: INFO: stderr: ""
Jun 10 16:50:12.734: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:50:12.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1869" for this suite.

• [SLOW TEST:8.376 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":305,"completed":195,"skipped":3349,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:50:12.747: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-4155
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 10 16:50:12.854: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun 10 16:50:12.960: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 10 16:50:14.970: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 10 16:50:16.965: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 16:50:18.963: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 16:50:20.963: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 16:50:22.963: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 16:50:24.975: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jun 10 16:50:24.988: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jun 10 16:50:26.993: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jun 10 16:50:28.996: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jun 10 16:50:29.005: INFO: The status of Pod netserver-2 is Running (Ready = false)
Jun 10 16:50:31.010: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Jun 10 16:50:33.044: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.78.120:8080/dial?request=hostname&protocol=udp&host=192.168.251.42&port=8081&tries=1'] Namespace:pod-network-test-4155 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 10 16:50:33.044: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
Jun 10 16:50:33.143: INFO: Waiting for responses: map[]
Jun 10 16:50:33.145: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.78.120:8080/dial?request=hostname&protocol=udp&host=192.168.24.150&port=8081&tries=1'] Namespace:pod-network-test-4155 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 10 16:50:33.145: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
Jun 10 16:50:33.235: INFO: Waiting for responses: map[]
Jun 10 16:50:33.239: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.78.120:8080/dial?request=hostname&protocol=udp&host=192.168.78.124&port=8081&tries=1'] Namespace:pod-network-test-4155 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 10 16:50:33.239: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
Jun 10 16:50:33.375: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:50:33.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4155" for this suite.

• [SLOW TEST:20.646 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":305,"completed":196,"skipped":3357,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:50:33.394: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating cluster-info
Jun 10 16:50:33.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-9962 cluster-info'
Jun 10 16:50:33.563: INFO: stderr: ""
Jun 10 16:50:33.563: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.128.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:50:33.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9962" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":305,"completed":197,"skipped":3364,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:50:33.585: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:50:41.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1582" for this suite.

• [SLOW TEST:8.065 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":305,"completed":198,"skipped":3394,"failed":0}
SS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:50:41.651: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-1936/configmap-test-452ad717-e84e-44bd-be7b-61aa2fc078d2
STEP: Creating a pod to test consume configMaps
Jun 10 16:50:41.745: INFO: Waiting up to 5m0s for pod "pod-configmaps-341d8c46-43a5-4987-8d2c-329131d53000" in namespace "configmap-1936" to be "Succeeded or Failed"
Jun 10 16:50:41.757: INFO: Pod "pod-configmaps-341d8c46-43a5-4987-8d2c-329131d53000": Phase="Pending", Reason="", readiness=false. Elapsed: 12.419982ms
Jun 10 16:50:43.766: INFO: Pod "pod-configmaps-341d8c46-43a5-4987-8d2c-329131d53000": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020698914s
STEP: Saw pod success
Jun 10 16:50:43.766: INFO: Pod "pod-configmaps-341d8c46-43a5-4987-8d2c-329131d53000" satisfied condition "Succeeded or Failed"
Jun 10 16:50:43.772: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-5pcx5 pod pod-configmaps-341d8c46-43a5-4987-8d2c-329131d53000 container env-test: <nil>
STEP: delete the pod
Jun 10 16:50:43.809: INFO: Waiting for pod pod-configmaps-341d8c46-43a5-4987-8d2c-329131d53000 to disappear
Jun 10 16:50:43.811: INFO: Pod pod-configmaps-341d8c46-43a5-4987-8d2c-329131d53000 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:50:43.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1936" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":305,"completed":199,"skipped":3396,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:50:43.825: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-secret-pl2p
STEP: Creating a pod to test atomic-volume-subpath
Jun 10 16:50:43.923: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-pl2p" in namespace "subpath-9841" to be "Succeeded or Failed"
Jun 10 16:50:43.929: INFO: Pod "pod-subpath-test-secret-pl2p": Phase="Pending", Reason="", readiness=false. Elapsed: 5.871843ms
Jun 10 16:50:45.937: INFO: Pod "pod-subpath-test-secret-pl2p": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013624981s
Jun 10 16:50:47.949: INFO: Pod "pod-subpath-test-secret-pl2p": Phase="Running", Reason="", readiness=true. Elapsed: 4.025788511s
Jun 10 16:50:49.959: INFO: Pod "pod-subpath-test-secret-pl2p": Phase="Running", Reason="", readiness=true. Elapsed: 6.035772126s
Jun 10 16:50:51.962: INFO: Pod "pod-subpath-test-secret-pl2p": Phase="Running", Reason="", readiness=true. Elapsed: 8.038789486s
Jun 10 16:50:53.965: INFO: Pod "pod-subpath-test-secret-pl2p": Phase="Running", Reason="", readiness=true. Elapsed: 10.042145135s
Jun 10 16:50:55.968: INFO: Pod "pod-subpath-test-secret-pl2p": Phase="Running", Reason="", readiness=true. Elapsed: 12.045287532s
Jun 10 16:50:57.972: INFO: Pod "pod-subpath-test-secret-pl2p": Phase="Running", Reason="", readiness=true. Elapsed: 14.049195947s
Jun 10 16:50:59.977: INFO: Pod "pod-subpath-test-secret-pl2p": Phase="Running", Reason="", readiness=true. Elapsed: 16.053894436s
Jun 10 16:51:01.979: INFO: Pod "pod-subpath-test-secret-pl2p": Phase="Running", Reason="", readiness=true. Elapsed: 18.05615002s
Jun 10 16:51:03.983: INFO: Pod "pod-subpath-test-secret-pl2p": Phase="Running", Reason="", readiness=true. Elapsed: 20.059961554s
Jun 10 16:51:05.989: INFO: Pod "pod-subpath-test-secret-pl2p": Phase="Running", Reason="", readiness=true. Elapsed: 22.065544555s
Jun 10 16:51:07.992: INFO: Pod "pod-subpath-test-secret-pl2p": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.068856616s
STEP: Saw pod success
Jun 10 16:51:07.992: INFO: Pod "pod-subpath-test-secret-pl2p" satisfied condition "Succeeded or Failed"
Jun 10 16:51:07.994: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-5pcx5 pod pod-subpath-test-secret-pl2p container test-container-subpath-secret-pl2p: <nil>
STEP: delete the pod
Jun 10 16:51:08.014: INFO: Waiting for pod pod-subpath-test-secret-pl2p to disappear
Jun 10 16:51:08.018: INFO: Pod pod-subpath-test-secret-pl2p no longer exists
STEP: Deleting pod pod-subpath-test-secret-pl2p
Jun 10 16:51:08.018: INFO: Deleting pod "pod-subpath-test-secret-pl2p" in namespace "subpath-9841"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:51:08.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9841" for this suite.

• [SLOW TEST:24.203 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":305,"completed":200,"skipped":3421,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:51:08.028: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:51:08.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7296" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":305,"completed":201,"skipped":3424,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:51:08.079: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:51:12.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1462" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":305,"completed":202,"skipped":3427,"failed":0}
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:51:12.243: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 10 16:51:16.331: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:51:16.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9753" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":203,"skipped":3431,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:51:16.417: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 10 16:51:17.227: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 10 16:51:19.234: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758940677, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758940677, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758940677, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758940677, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 10 16:51:22.253: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:51:22.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6646" for this suite.
STEP: Destroying namespace "webhook-6646-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.059 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":305,"completed":204,"skipped":3436,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:51:22.478: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-68e0574a-d040-42a5-906a-5af67f7fdee2
STEP: Creating secret with name s-test-opt-upd-eb4db1b4-2c45-49fb-b311-843e43635813
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-68e0574a-d040-42a5-906a-5af67f7fdee2
STEP: Updating secret s-test-opt-upd-eb4db1b4-2c45-49fb-b311-843e43635813
STEP: Creating secret with name s-test-opt-create-7906babb-56b9-4aab-bfd8-642ae4977197
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:52:57.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5899" for this suite.

• [SLOW TEST:94.549 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":205,"skipped":3468,"failed":0}
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:52:57.027: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jun 10 16:52:57.067: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 10 16:52:57.074: INFO: Waiting for terminating namespaces to be deleted...
Jun 10 16:52:57.076: INFO: 
Logging pods the apiserver thinks is on node target-cluster-control-plane-8m52s before test
Jun 10 16:52:57.093: INFO: calico-kube-controllers-7c5d656c49-x6mc4 from calico-system started at 2021-06-10 15:29:10 +0000 UTC (1 container statuses recorded)
Jun 10 16:52:57.093: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 10 16:52:57.093: INFO: calico-node-phlfp from calico-system started at 2021-06-10 15:28:49 +0000 UTC (1 container statuses recorded)
Jun 10 16:52:57.093: INFO: 	Container calico-node ready: true, restart count 0
Jun 10 16:52:57.093: INFO: calico-typha-5d4587dc9c-7g8gs from calico-system started at 2021-06-10 15:28:49 +0000 UTC (1 container statuses recorded)
Jun 10 16:52:57.093: INFO: 	Container calico-typha ready: true, restart count 0
Jun 10 16:52:57.093: INFO: capd-controller-manager-7c89f6ddd7-dc686 from capd-system started at 2021-06-10 15:31:47 +0000 UTC (2 container statuses recorded)
Jun 10 16:52:57.093: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 10 16:52:57.093: INFO: 	Container manager ready: true, restart count 0
Jun 10 16:52:57.093: INFO: capi-kubeadm-bootstrap-controller-manager-6f669ccd7c-gtbcn from capi-kubeadm-bootstrap-system started at 2021-06-10 15:31:34 +0000 UTC (2 container statuses recorded)
Jun 10 16:52:57.093: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 10 16:52:57.093: INFO: 	Container manager ready: true, restart count 0
Jun 10 16:52:57.093: INFO: capi-kubeadm-control-plane-controller-manager-5c95f59c5c-l4jvx from capi-kubeadm-control-plane-system started at 2021-06-10 15:31:42 +0000 UTC (2 container statuses recorded)
Jun 10 16:52:57.094: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 10 16:52:57.094: INFO: 	Container manager ready: true, restart count 0
Jun 10 16:52:57.094: INFO: capi-controller-manager-5f677d7d65-tzt7h from capi-system started at 2021-06-10 15:31:31 +0000 UTC (2 container statuses recorded)
Jun 10 16:52:57.094: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 10 16:52:57.094: INFO: 	Container manager ready: true, restart count 0
Jun 10 16:52:57.094: INFO: capi-controller-manager-745689557d-77m69 from capi-webhook-system started at 2021-06-10 15:31:29 +0000 UTC (2 container statuses recorded)
Jun 10 16:52:57.094: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 10 16:52:57.094: INFO: 	Container manager ready: true, restart count 0
Jun 10 16:52:57.094: INFO: capi-kubeadm-bootstrap-controller-manager-6949f44db8-hn6kq from capi-webhook-system started at 2021-06-10 15:31:31 +0000 UTC (2 container statuses recorded)
Jun 10 16:52:57.094: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 10 16:52:57.094: INFO: 	Container manager ready: true, restart count 0
Jun 10 16:52:57.094: INFO: capi-kubeadm-control-plane-controller-manager-7b6c4bf48d-x8pzk from capi-webhook-system started at 2021-06-10 15:31:35 +0000 UTC (2 container statuses recorded)
Jun 10 16:52:57.094: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 10 16:52:57.094: INFO: 	Container manager ready: true, restart count 0
Jun 10 16:52:57.094: INFO: cert-manager-768bf64dd4-4lhmj from cert-manager started at 2021-06-10 15:29:10 +0000 UTC (1 container statuses recorded)
Jun 10 16:52:57.094: INFO: 	Container cert-manager ready: true, restart count 0
Jun 10 16:52:57.094: INFO: cert-manager-cainjector-646879549c-pbn4t from cert-manager started at 2021-06-10 15:29:10 +0000 UTC (1 container statuses recorded)
Jun 10 16:52:57.094: INFO: 	Container cert-manager ready: true, restart count 0
Jun 10 16:52:57.094: INFO: cert-manager-webhook-6dc9ccc9fb-6gg4f from cert-manager started at 2021-06-10 15:29:10 +0000 UTC (1 container statuses recorded)
Jun 10 16:52:57.094: INFO: 	Container cert-manager ready: true, restart count 0
Jun 10 16:52:57.094: INFO: coredns-f9fd979d6-2dcsm from kube-system started at 2021-06-10 15:29:10 +0000 UTC (1 container statuses recorded)
Jun 10 16:52:57.094: INFO: 	Container coredns ready: true, restart count 0
Jun 10 16:52:57.094: INFO: coredns-f9fd979d6-zsjpx from kube-system started at 2021-06-10 15:29:21 +0000 UTC (1 container statuses recorded)
Jun 10 16:52:57.094: INFO: 	Container coredns ready: true, restart count 0
Jun 10 16:52:57.094: INFO: etcd-target-cluster-control-plane-8m52s from kube-system started at 2021-06-10 15:22:59 +0000 UTC (1 container statuses recorded)
Jun 10 16:52:57.094: INFO: 	Container etcd ready: true, restart count 0
Jun 10 16:52:57.094: INFO: kube-apiserver-target-cluster-control-plane-8m52s from kube-system started at 2021-06-10 15:22:59 +0000 UTC (1 container statuses recorded)
Jun 10 16:52:57.094: INFO: 	Container kube-apiserver ready: true, restart count 0
Jun 10 16:52:57.094: INFO: kube-controller-manager-target-cluster-control-plane-8m52s from kube-system started at 2021-06-10 15:22:59 +0000 UTC (1 container statuses recorded)
Jun 10 16:52:57.094: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jun 10 16:52:57.094: INFO: kube-proxy-hz9zf from kube-system started at 2021-06-10 15:23:04 +0000 UTC (1 container statuses recorded)
Jun 10 16:52:57.094: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 10 16:52:57.094: INFO: kube-scheduler-target-cluster-control-plane-8m52s from kube-system started at 2021-06-10 15:22:59 +0000 UTC (1 container statuses recorded)
Jun 10 16:52:57.094: INFO: 	Container kube-scheduler ready: true, restart count 0
Jun 10 16:52:57.094: INFO: sonobuoy-systemd-logs-daemon-set-0467f197b1c74b53-bs8vn from sonobuoy started at 2021-06-10 15:38:55 +0000 UTC (2 container statuses recorded)
Jun 10 16:52:57.094: INFO: 	Container sonobuoy-worker ready: false, restart count 7
Jun 10 16:52:57.094: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 10 16:52:57.094: INFO: tigera-operator-5b76777d49-9zvzd from tigera-operator started at 2021-06-10 15:28:37 +0000 UTC (1 container statuses recorded)
Jun 10 16:52:57.094: INFO: 	Container tigera-operator ready: true, restart count 0
Jun 10 16:52:57.094: INFO: 
Logging pods the apiserver thinks is on node target-cluster-md-0-6b59c4f65-5pcx5 before test
Jun 10 16:52:57.099: INFO: calico-node-dvbdj from calico-system started at 2021-06-10 15:34:05 +0000 UTC (1 container statuses recorded)
Jun 10 16:52:57.099: INFO: 	Container calico-node ready: true, restart count 0
Jun 10 16:52:57.099: INFO: calico-typha-5d4587dc9c-lq9tq from calico-system started at 2021-06-10 15:34:47 +0000 UTC (1 container statuses recorded)
Jun 10 16:52:57.099: INFO: 	Container calico-typha ready: true, restart count 0
Jun 10 16:52:57.099: INFO: kube-proxy-dklkq from kube-system started at 2021-06-10 15:34:05 +0000 UTC (1 container statuses recorded)
Jun 10 16:52:57.099: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 10 16:52:57.099: INFO: sonobuoy-e2e-job-04c20f7a477e40c3 from sonobuoy started at 2021-06-10 15:38:55 +0000 UTC (2 container statuses recorded)
Jun 10 16:52:57.099: INFO: 	Container e2e ready: true, restart count 0
Jun 10 16:52:57.099: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 10 16:52:57.099: INFO: sonobuoy-systemd-logs-daemon-set-0467f197b1c74b53-gj6jj from sonobuoy started at 2021-06-10 15:38:55 +0000 UTC (2 container statuses recorded)
Jun 10 16:52:57.099: INFO: 	Container sonobuoy-worker ready: false, restart count 7
Jun 10 16:52:57.099: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 10 16:52:57.099: INFO: 
Logging pods the apiserver thinks is on node target-cluster-md-0-6b59c4f65-cqpjz before test
Jun 10 16:52:57.106: INFO: calico-node-j7mxn from calico-system started at 2021-06-10 15:34:05 +0000 UTC (1 container statuses recorded)
Jun 10 16:52:57.106: INFO: 	Container calico-node ready: true, restart count 0
Jun 10 16:52:57.106: INFO: calico-typha-5d4587dc9c-ckl68 from calico-system started at 2021-06-10 15:34:47 +0000 UTC (1 container statuses recorded)
Jun 10 16:52:57.106: INFO: 	Container calico-typha ready: true, restart count 0
Jun 10 16:52:57.106: INFO: kube-proxy-mc9rx from kube-system started at 2021-06-10 15:34:05 +0000 UTC (1 container statuses recorded)
Jun 10 16:52:57.106: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 10 16:52:57.106: INFO: pod-projected-secrets-a3770ae0-f4af-4e75-a465-feff33d0bcf7 from projected-5899 started at 2021-06-10 16:51:22 +0000 UTC (3 container statuses recorded)
Jun 10 16:52:57.106: INFO: 	Container creates-volume-test ready: true, restart count 0
Jun 10 16:52:57.106: INFO: 	Container dels-volume-test ready: true, restart count 0
Jun 10 16:52:57.106: INFO: 	Container upds-volume-test ready: true, restart count 0
Jun 10 16:52:57.106: INFO: sonobuoy from sonobuoy started at 2021-06-10 15:38:49 +0000 UTC (1 container statuses recorded)
Jun 10 16:52:57.106: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 10 16:52:57.106: INFO: sonobuoy-systemd-logs-daemon-set-0467f197b1c74b53-cf9lb from sonobuoy started at 2021-06-10 15:38:55 +0000 UTC (2 container statuses recorded)
Jun 10 16:52:57.106: INFO: 	Container sonobuoy-worker ready: false, restart count 7
Jun 10 16:52:57.106: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-18753067-431b-484f-83ef-227fe311116b 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-18753067-431b-484f-83ef-227fe311116b off the node target-cluster-md-0-6b59c4f65-5pcx5
STEP: verifying the node doesn't have the label kubernetes.io/e2e-18753067-431b-484f-83ef-227fe311116b
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:53:05.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3846" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:8.315 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":305,"completed":206,"skipped":3477,"failed":0}
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:53:05.342: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-9af1bba9-1f34-486f-a480-c636b299df38
STEP: Creating a pod to test consume configMaps
Jun 10 16:53:05.418: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fd6aad3f-92e5-49e2-9925-be8d7b2b3138" in namespace "projected-4445" to be "Succeeded or Failed"
Jun 10 16:53:05.438: INFO: Pod "pod-projected-configmaps-fd6aad3f-92e5-49e2-9925-be8d7b2b3138": Phase="Pending", Reason="", readiness=false. Elapsed: 19.301904ms
Jun 10 16:53:07.441: INFO: Pod "pod-projected-configmaps-fd6aad3f-92e5-49e2-9925-be8d7b2b3138": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022956259s
STEP: Saw pod success
Jun 10 16:53:07.441: INFO: Pod "pod-projected-configmaps-fd6aad3f-92e5-49e2-9925-be8d7b2b3138" satisfied condition "Succeeded or Failed"
Jun 10 16:53:07.444: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod pod-projected-configmaps-fd6aad3f-92e5-49e2-9925-be8d7b2b3138 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 10 16:53:07.463: INFO: Waiting for pod pod-projected-configmaps-fd6aad3f-92e5-49e2-9925-be8d7b2b3138 to disappear
Jun 10 16:53:07.470: INFO: Pod pod-projected-configmaps-fd6aad3f-92e5-49e2-9925-be8d7b2b3138 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:53:07.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4445" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":207,"skipped":3477,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:53:07.495: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 10 16:53:08.304: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758940788, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758940788, loc:(*time.Location)(0x770e980)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-cbccbf6bb\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758940788, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758940788, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 10 16:53:11.344: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:53:11.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2238" for this suite.
STEP: Destroying namespace "webhook-2238-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":305,"completed":208,"skipped":3482,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:53:11.931: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 10 16:53:12.073: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jun 10 16:53:28.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=crd-publish-openapi-3055 --namespace=crd-publish-openapi-3055 create -f -'
Jun 10 16:53:30.057: INFO: stderr: ""
Jun 10 16:53:30.057: INFO: stdout: "e2e-test-crd-publish-openapi-604-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jun 10 16:53:30.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=crd-publish-openapi-3055 --namespace=crd-publish-openapi-3055 delete e2e-test-crd-publish-openapi-604-crds test-cr'
Jun 10 16:53:30.243: INFO: stderr: ""
Jun 10 16:53:30.243: INFO: stdout: "e2e-test-crd-publish-openapi-604-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jun 10 16:53:30.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=crd-publish-openapi-3055 --namespace=crd-publish-openapi-3055 apply -f -'
Jun 10 16:53:30.821: INFO: stderr: ""
Jun 10 16:53:30.821: INFO: stdout: "e2e-test-crd-publish-openapi-604-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jun 10 16:53:30.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=crd-publish-openapi-3055 --namespace=crd-publish-openapi-3055 delete e2e-test-crd-publish-openapi-604-crds test-cr'
Jun 10 16:53:31.031: INFO: stderr: ""
Jun 10 16:53:31.031: INFO: stdout: "e2e-test-crd-publish-openapi-604-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Jun 10 16:53:31.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=crd-publish-openapi-3055 explain e2e-test-crd-publish-openapi-604-crds'
Jun 10 16:53:31.520: INFO: stderr: ""
Jun 10 16:53:31.520: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-604-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:53:46.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3055" for this suite.

• [SLOW TEST:34.336 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":305,"completed":209,"skipped":3490,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:53:46.269: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun 10 16:53:46.325: INFO: Waiting up to 5m0s for pod "pod-9eb1ae14-08db-47af-8cb9-8b7d1404f0a5" in namespace "emptydir-7748" to be "Succeeded or Failed"
Jun 10 16:53:46.339: INFO: Pod "pod-9eb1ae14-08db-47af-8cb9-8b7d1404f0a5": Phase="Pending", Reason="", readiness=false. Elapsed: 13.224014ms
Jun 10 16:53:48.343: INFO: Pod "pod-9eb1ae14-08db-47af-8cb9-8b7d1404f0a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017578125s
STEP: Saw pod success
Jun 10 16:53:48.343: INFO: Pod "pod-9eb1ae14-08db-47af-8cb9-8b7d1404f0a5" satisfied condition "Succeeded or Failed"
Jun 10 16:53:48.347: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod pod-9eb1ae14-08db-47af-8cb9-8b7d1404f0a5 container test-container: <nil>
STEP: delete the pod
Jun 10 16:53:48.375: INFO: Waiting for pod pod-9eb1ae14-08db-47af-8cb9-8b7d1404f0a5 to disappear
Jun 10 16:53:48.392: INFO: Pod pod-9eb1ae14-08db-47af-8cb9-8b7d1404f0a5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:53:48.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7748" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":210,"skipped":3583,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:53:48.403: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:53:48.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4286" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":305,"completed":211,"skipped":3613,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:53:48.508: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1971.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1971.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1971.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1971.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1971.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1971.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 10 16:53:50.606: INFO: DNS probes using dns-1971/dns-test-ea729915-b568-4e0d-b1de-49fcbddb84b3 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:53:50.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1971" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":305,"completed":212,"skipped":3666,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:53:50.705: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 10 16:53:50.894: INFO: Waiting up to 5m0s for pod "downwardapi-volume-56b10ba6-5ada-4cbd-9811-a19637987344" in namespace "projected-1903" to be "Succeeded or Failed"
Jun 10 16:53:50.924: INFO: Pod "downwardapi-volume-56b10ba6-5ada-4cbd-9811-a19637987344": Phase="Pending", Reason="", readiness=false. Elapsed: 30.224501ms
Jun 10 16:53:52.931: INFO: Pod "downwardapi-volume-56b10ba6-5ada-4cbd-9811-a19637987344": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036890236s
Jun 10 16:53:54.952: INFO: Pod "downwardapi-volume-56b10ba6-5ada-4cbd-9811-a19637987344": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.057850662s
STEP: Saw pod success
Jun 10 16:53:54.952: INFO: Pod "downwardapi-volume-56b10ba6-5ada-4cbd-9811-a19637987344" satisfied condition "Succeeded or Failed"
Jun 10 16:53:54.960: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod downwardapi-volume-56b10ba6-5ada-4cbd-9811-a19637987344 container client-container: <nil>
STEP: delete the pod
Jun 10 16:53:55.002: INFO: Waiting for pod downwardapi-volume-56b10ba6-5ada-4cbd-9811-a19637987344 to disappear
Jun 10 16:53:55.004: INFO: Pod downwardapi-volume-56b10ba6-5ada-4cbd-9811-a19637987344 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:53:55.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1903" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":213,"skipped":3679,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:53:55.013: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 10 16:53:55.137: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"93a6cdfa-e9fa-4ac5-aee4-06023085470e", Controller:(*bool)(0xc00945177a), BlockOwnerDeletion:(*bool)(0xc00945177b)}}
Jun 10 16:53:55.161: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"db67fa99-bd12-4c3b-a97b-74142b2c8f7f", Controller:(*bool)(0xc009406406), BlockOwnerDeletion:(*bool)(0xc009406407)}}
Jun 10 16:53:55.191: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"dd7a599b-c29a-4a0b-9c56-c95aff83259f", Controller:(*bool)(0xc00950211a), BlockOwnerDeletion:(*bool)(0xc00950211b)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:54:00.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8167" for this suite.

• [SLOW TEST:5.221 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":305,"completed":214,"skipped":3684,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:54:00.238: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Jun 10 16:54:00.312: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 10 16:55:00.369: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 10 16:55:00.372: INFO: Starting informer...
STEP: Starting pod...
Jun 10 16:55:00.585: INFO: Pod is running on target-cluster-md-0-6b59c4f65-cqpjz. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Jun 10 16:55:00.631: INFO: Pod wasn't evicted. Proceeding
Jun 10 16:55:00.631: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Jun 10 16:56:15.675: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:56:15.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-2132" for this suite.

• [SLOW TEST:135.467 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":305,"completed":215,"skipped":3716,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:56:15.705: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-262
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-262
I0610 16:56:15.833963      26 runners.go:190] Created replication controller with name: externalname-service, namespace: services-262, replica count: 2
Jun 10 16:56:18.884: INFO: Creating new exec pod
I0610 16:56:18.884460      26 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 10 16:56:21.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=services-262 exec execpodnsvcs -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jun 10 16:56:22.286: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun 10 16:56:22.286: INFO: stdout: ""
Jun 10 16:56:22.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=services-262 exec execpodnsvcs -- /bin/sh -x -c nc -zv -t -w 2 10.128.42.39 80'
Jun 10 16:56:22.512: INFO: stderr: "+ nc -zv -t -w 2 10.128.42.39 80\nConnection to 10.128.42.39 80 port [tcp/http] succeeded!\n"
Jun 10 16:56:22.512: INFO: stdout: ""
Jun 10 16:56:22.512: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:56:22.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-262" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:6.905 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":305,"completed":216,"skipped":3750,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:56:22.610: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Jun 10 16:56:22.699: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 10 16:57:22.734: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
Jun 10 16:57:22.759: INFO: Created pod: pod0-sched-preemption-low-priority
Jun 10 16:57:22.788: INFO: Created pod: pod1-sched-preemption-medium-priority
Jun 10 16:57:22.827: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:57:40.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3239" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:78.404 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":305,"completed":217,"skipped":3766,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:57:41.021: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:57:43.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2731" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":218,"skipped":3779,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:57:43.175: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:58:43.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8300" for this suite.

• [SLOW TEST:60.101 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":305,"completed":219,"skipped":3829,"failed":0}
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:58:43.276: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun 10 16:58:49.462: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 10 16:58:49.467: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 10 16:58:51.470: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 10 16:58:51.474: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 10 16:58:53.468: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 10 16:58:53.475: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:58:53.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9920" for this suite.

• [SLOW TEST:10.214 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":305,"completed":220,"skipped":3829,"failed":0}
SSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:58:53.491: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5956 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5956;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5956 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5956;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5956.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5956.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5956.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5956.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5956.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5956.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5956.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5956.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5956.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5956.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5956.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5956.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5956.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 137.228.134.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.134.228.137_udp@PTR;check="$$(dig +tcp +noall +answer +search 137.228.134.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.134.228.137_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5956 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5956;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5956 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5956;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5956.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5956.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5956.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5956.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5956.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5956.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5956.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5956.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5956.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5956.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5956.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5956.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5956.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 137.228.134.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.134.228.137_udp@PTR;check="$$(dig +tcp +noall +answer +search 137.228.134.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.134.228.137_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 10 16:59:09.647: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:09.651: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:09.655: INFO: Unable to read wheezy_udp@dns-test-service.dns-5956 from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:09.658: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5956 from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:09.661: INFO: Unable to read wheezy_udp@dns-test-service.dns-5956.svc from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:09.664: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5956.svc from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:09.668: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5956.svc from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:09.670: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5956.svc from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:09.673: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.dns-5956.svc from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:09.675: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.dns-5956.svc from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:09.677: INFO: Unable to read wheezy_udp@PodARecord from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:09.679: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:09.682: INFO: Unable to read 10.134.228.137_udp@PTR from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:09.685: INFO: Unable to read 10.134.228.137_tcp@PTR from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:09.688: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:09.695: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:09.699: INFO: Unable to read jessie_udp@dns-test-service.dns-5956 from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:09.702: INFO: Unable to read jessie_tcp@dns-test-service.dns-5956 from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:09.707: INFO: Unable to read jessie_udp@dns-test-service.dns-5956.svc from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:09.710: INFO: Unable to read jessie_tcp@dns-test-service.dns-5956.svc from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:09.715: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5956.svc from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:09.719: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5956.svc from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:09.723: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.dns-5956.svc from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:09.732: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.dns-5956.svc from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:09.736: INFO: Unable to read jessie_udp@PodARecord from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:09.739: INFO: Unable to read jessie_tcp@PodARecord from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:09.743: INFO: Unable to read 10.134.228.137_udp@PTR from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:09.746: INFO: Unable to read 10.134.228.137_tcp@PTR from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:09.746: INFO: Lookups using dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5956 wheezy_tcp@dns-test-service.dns-5956 wheezy_udp@dns-test-service.dns-5956.svc wheezy_tcp@dns-test-service.dns-5956.svc wheezy_udp@_http._tcp.dns-test-service.dns-5956.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5956.svc wheezy_udp@_http._tcp.test-service-2.dns-5956.svc wheezy_tcp@_http._tcp.test-service-2.dns-5956.svc wheezy_udp@PodARecord wheezy_tcp@PodARecord 10.134.228.137_udp@PTR 10.134.228.137_tcp@PTR jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5956 jessie_tcp@dns-test-service.dns-5956 jessie_udp@dns-test-service.dns-5956.svc jessie_tcp@dns-test-service.dns-5956.svc jessie_udp@_http._tcp.dns-test-service.dns-5956.svc jessie_tcp@_http._tcp.dns-test-service.dns-5956.svc jessie_udp@_http._tcp.test-service-2.dns-5956.svc jessie_tcp@_http._tcp.test-service-2.dns-5956.svc jessie_udp@PodARecord jessie_tcp@PodARecord 10.134.228.137_udp@PTR 10.134.228.137_tcp@PTR]

Jun 10 16:59:14.753: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:14.761: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:14.765: INFO: Unable to read wheezy_udp@dns-test-service.dns-5956 from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:14.769: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5956 from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:14.774: INFO: Unable to read wheezy_udp@dns-test-service.dns-5956.svc from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:14.779: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5956.svc from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:14.828: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:14.836: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:14.840: INFO: Unable to read jessie_udp@dns-test-service.dns-5956 from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:14.845: INFO: Unable to read jessie_tcp@dns-test-service.dns-5956 from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:14.850: INFO: Unable to read jessie_udp@dns-test-service.dns-5956.svc from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:14.857: INFO: Unable to read jessie_tcp@dns-test-service.dns-5956.svc from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:14.864: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5956.svc from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:14.868: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5956.svc from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:14.875: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.dns-5956.svc from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:14.881: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.dns-5956.svc from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:14.886: INFO: Unable to read jessie_udp@PodARecord from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:14.894: INFO: Unable to read jessie_tcp@PodARecord from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:14.905: INFO: Lookups using dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5956 wheezy_tcp@dns-test-service.dns-5956 wheezy_udp@dns-test-service.dns-5956.svc wheezy_tcp@dns-test-service.dns-5956.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5956 jessie_tcp@dns-test-service.dns-5956 jessie_udp@dns-test-service.dns-5956.svc jessie_tcp@dns-test-service.dns-5956.svc jessie_udp@_http._tcp.dns-test-service.dns-5956.svc jessie_tcp@_http._tcp.dns-test-service.dns-5956.svc jessie_udp@_http._tcp.test-service-2.dns-5956.svc jessie_tcp@_http._tcp.test-service-2.dns-5956.svc jessie_udp@PodARecord jessie_tcp@PodARecord]

Jun 10 16:59:19.751: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:19.754: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:19.759: INFO: Unable to read wheezy_udp@dns-test-service.dns-5956 from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:19.764: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5956 from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:19.767: INFO: Unable to read wheezy_udp@dns-test-service.dns-5956.svc from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:19.770: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5956.svc from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:19.800: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:19.803: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:19.806: INFO: Unable to read jessie_udp@dns-test-service.dns-5956 from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:19.811: INFO: Unable to read jessie_tcp@dns-test-service.dns-5956 from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:19.815: INFO: Unable to read jessie_udp@dns-test-service.dns-5956.svc from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:19.826: INFO: Unable to read jessie_tcp@dns-test-service.dns-5956.svc from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:19.831: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5956.svc from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:19.835: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5956.svc from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:19.839: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.dns-5956.svc from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:19.842: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.dns-5956.svc from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:19.846: INFO: Unable to read jessie_udp@PodARecord from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:19.850: INFO: Unable to read jessie_tcp@PodARecord from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:19.857: INFO: Lookups using dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5956 wheezy_tcp@dns-test-service.dns-5956 wheezy_udp@dns-test-service.dns-5956.svc wheezy_tcp@dns-test-service.dns-5956.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5956 jessie_tcp@dns-test-service.dns-5956 jessie_udp@dns-test-service.dns-5956.svc jessie_tcp@dns-test-service.dns-5956.svc jessie_udp@_http._tcp.dns-test-service.dns-5956.svc jessie_tcp@_http._tcp.dns-test-service.dns-5956.svc jessie_udp@_http._tcp.test-service-2.dns-5956.svc jessie_tcp@_http._tcp.test-service-2.dns-5956.svc jessie_udp@PodARecord jessie_tcp@PodARecord]

Jun 10 16:59:24.751: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:24.772: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:24.777: INFO: Unable to read wheezy_udp@dns-test-service.dns-5956 from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:24.781: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5956 from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:24.784: INFO: Unable to read wheezy_udp@dns-test-service.dns-5956.svc from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:24.787: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5956.svc from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:24.833: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:24.840: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:24.846: INFO: Unable to read jessie_udp@dns-test-service.dns-5956 from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:24.850: INFO: Unable to read jessie_tcp@dns-test-service.dns-5956 from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:24.860: INFO: Unable to read jessie_udp@dns-test-service.dns-5956.svc from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:24.865: INFO: Unable to read jessie_tcp@dns-test-service.dns-5956.svc from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:24.909: INFO: Lookups using dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5956 wheezy_tcp@dns-test-service.dns-5956 wheezy_udp@dns-test-service.dns-5956.svc wheezy_tcp@dns-test-service.dns-5956.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5956 jessie_tcp@dns-test-service.dns-5956 jessie_udp@dns-test-service.dns-5956.svc jessie_tcp@dns-test-service.dns-5956.svc]

Jun 10 16:59:29.760: INFO: Unable to read wheezy_udp@dns-test-service.dns-5956.svc from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:29.793: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:29.808: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:29.813: INFO: Unable to read jessie_udp@dns-test-service.dns-5956 from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:29.816: INFO: Unable to read jessie_tcp@dns-test-service.dns-5956 from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:29.819: INFO: Unable to read jessie_udp@dns-test-service.dns-5956.svc from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:29.822: INFO: Unable to read jessie_tcp@dns-test-service.dns-5956.svc from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:29.862: INFO: Lookups using dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467 failed for: [wheezy_udp@dns-test-service.dns-5956.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5956 jessie_tcp@dns-test-service.dns-5956 jessie_udp@dns-test-service.dns-5956.svc jessie_tcp@dns-test-service.dns-5956.svc]

Jun 10 16:59:34.764: INFO: Unable to read wheezy_udp@dns-test-service.dns-5956.svc from pod dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467: the server could not find the requested resource (get pods dns-test-22984125-beff-4d77-b005-503c1d4e4467)
Jun 10 16:59:34.864: INFO: Lookups using dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467 failed for: [wheezy_udp@dns-test-service.dns-5956.svc]

Jun 10 16:59:39.859: INFO: DNS probes using dns-5956/dns-test-22984125-beff-4d77-b005-503c1d4e4467 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:59:40.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5956" for this suite.

• [SLOW TEST:46.622 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":305,"completed":221,"skipped":3833,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:59:40.113: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 10 16:59:41.068: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 10 16:59:44.130: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:59:45.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7627" for this suite.
STEP: Destroying namespace "webhook-7627-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.273 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":305,"completed":222,"skipped":3912,"failed":0}
SS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:59:45.387: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Jun 10 16:59:45.521: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:59:49.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9654" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":305,"completed":223,"skipped":3914,"failed":0}
SSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:59:49.473: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
STEP: reading a file in the container
Jun 10 16:59:52.066: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3233 pod-service-account-a949709a-c74d-4f36-89af-3afb40e419f8 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jun 10 16:59:52.301: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3233 pod-service-account-a949709a-c74d-4f36-89af-3afb40e419f8 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jun 10 16:59:52.552: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3233 pod-service-account-a949709a-c74d-4f36-89af-3afb40e419f8 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:59:52.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3233" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":305,"completed":224,"skipped":3918,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:59:52.784: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 16:59:52.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-2957" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":305,"completed":225,"skipped":3960,"failed":0}
S
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 16:59:52.832: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-90
STEP: creating service affinity-nodeport-transition in namespace services-90
STEP: creating replication controller affinity-nodeport-transition in namespace services-90
I0610 16:59:52.922139      26 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-90, replica count: 3
I0610 16:59:55.976134      26 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 10 16:59:55.987: INFO: Creating new exec pod
Jun 10 16:59:59.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=services-90 exec execpod-affinity2nkqb -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
Jun 10 16:59:59.351: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jun 10 16:59:59.351: INFO: stdout: ""
Jun 10 16:59:59.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=services-90 exec execpod-affinity2nkqb -- /bin/sh -x -c nc -zv -t -w 2 10.137.143.97 80'
Jun 10 16:59:59.649: INFO: stderr: "+ nc -zv -t -w 2 10.137.143.97 80\nConnection to 10.137.143.97 80 port [tcp/http] succeeded!\n"
Jun 10 16:59:59.649: INFO: stdout: ""
Jun 10 16:59:59.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=services-90 exec execpod-affinity2nkqb -- /bin/sh -x -c nc -zv -t -w 2 172.17.0.4 32258'
Jun 10 16:59:59.923: INFO: stderr: "+ nc -zv -t -w 2 172.17.0.4 32258\nConnection to 172.17.0.4 32258 port [tcp/32258] succeeded!\n"
Jun 10 16:59:59.923: INFO: stdout: ""
Jun 10 16:59:59.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=services-90 exec execpod-affinity2nkqb -- /bin/sh -x -c nc -zv -t -w 2 172.17.0.6 32258'
Jun 10 17:00:00.215: INFO: stderr: "+ nc -zv -t -w 2 172.17.0.6 32258\nConnection to 172.17.0.6 32258 port [tcp/32258] succeeded!\n"
Jun 10 17:00:00.216: INFO: stdout: ""
Jun 10 17:00:00.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=services-90 exec execpod-affinity2nkqb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.17.0.4:32258/ ; done'
Jun 10 17:00:01.014: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32258/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32258/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32258/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32258/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32258/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32258/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32258/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32258/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32258/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32258/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32258/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32258/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32258/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32258/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32258/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32258/\n"
Jun 10 17:00:01.014: INFO: stdout: "\naffinity-nodeport-transition-58h7g\naffinity-nodeport-transition-l724s\naffinity-nodeport-transition-58h7g\naffinity-nodeport-transition-58h7g\naffinity-nodeport-transition-db4fz\naffinity-nodeport-transition-db4fz\naffinity-nodeport-transition-db4fz\naffinity-nodeport-transition-58h7g\naffinity-nodeport-transition-58h7g\naffinity-nodeport-transition-l724s\naffinity-nodeport-transition-58h7g\naffinity-nodeport-transition-58h7g\naffinity-nodeport-transition-db4fz\naffinity-nodeport-transition-58h7g\naffinity-nodeport-transition-db4fz\naffinity-nodeport-transition-l724s"
Jun 10 17:00:01.014: INFO: Received response from host: affinity-nodeport-transition-58h7g
Jun 10 17:00:01.014: INFO: Received response from host: affinity-nodeport-transition-l724s
Jun 10 17:00:01.014: INFO: Received response from host: affinity-nodeport-transition-58h7g
Jun 10 17:00:01.014: INFO: Received response from host: affinity-nodeport-transition-58h7g
Jun 10 17:00:01.014: INFO: Received response from host: affinity-nodeport-transition-db4fz
Jun 10 17:00:01.014: INFO: Received response from host: affinity-nodeport-transition-db4fz
Jun 10 17:00:01.014: INFO: Received response from host: affinity-nodeport-transition-db4fz
Jun 10 17:00:01.014: INFO: Received response from host: affinity-nodeport-transition-58h7g
Jun 10 17:00:01.014: INFO: Received response from host: affinity-nodeport-transition-58h7g
Jun 10 17:00:01.014: INFO: Received response from host: affinity-nodeport-transition-l724s
Jun 10 17:00:01.014: INFO: Received response from host: affinity-nodeport-transition-58h7g
Jun 10 17:00:01.014: INFO: Received response from host: affinity-nodeport-transition-58h7g
Jun 10 17:00:01.014: INFO: Received response from host: affinity-nodeport-transition-db4fz
Jun 10 17:00:01.014: INFO: Received response from host: affinity-nodeport-transition-58h7g
Jun 10 17:00:01.014: INFO: Received response from host: affinity-nodeport-transition-db4fz
Jun 10 17:00:01.014: INFO: Received response from host: affinity-nodeport-transition-l724s
Jun 10 17:00:01.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=services-90 exec execpod-affinity2nkqb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.17.0.4:32258/ ; done'
Jun 10 17:00:01.629: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32258/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32258/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32258/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32258/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32258/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32258/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32258/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32258/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32258/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32258/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32258/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32258/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32258/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32258/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32258/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.17.0.4:32258/\n"
Jun 10 17:00:01.630: INFO: stdout: "\naffinity-nodeport-transition-58h7g\naffinity-nodeport-transition-58h7g\naffinity-nodeport-transition-58h7g\naffinity-nodeport-transition-58h7g\naffinity-nodeport-transition-58h7g\naffinity-nodeport-transition-58h7g\naffinity-nodeport-transition-58h7g\naffinity-nodeport-transition-58h7g\naffinity-nodeport-transition-58h7g\naffinity-nodeport-transition-58h7g\naffinity-nodeport-transition-58h7g\naffinity-nodeport-transition-58h7g\naffinity-nodeport-transition-58h7g\naffinity-nodeport-transition-58h7g\naffinity-nodeport-transition-58h7g\naffinity-nodeport-transition-58h7g"
Jun 10 17:00:01.630: INFO: Received response from host: affinity-nodeport-transition-58h7g
Jun 10 17:00:01.630: INFO: Received response from host: affinity-nodeport-transition-58h7g
Jun 10 17:00:01.630: INFO: Received response from host: affinity-nodeport-transition-58h7g
Jun 10 17:00:01.630: INFO: Received response from host: affinity-nodeport-transition-58h7g
Jun 10 17:00:01.630: INFO: Received response from host: affinity-nodeport-transition-58h7g
Jun 10 17:00:01.630: INFO: Received response from host: affinity-nodeport-transition-58h7g
Jun 10 17:00:01.630: INFO: Received response from host: affinity-nodeport-transition-58h7g
Jun 10 17:00:01.630: INFO: Received response from host: affinity-nodeport-transition-58h7g
Jun 10 17:00:01.630: INFO: Received response from host: affinity-nodeport-transition-58h7g
Jun 10 17:00:01.630: INFO: Received response from host: affinity-nodeport-transition-58h7g
Jun 10 17:00:01.630: INFO: Received response from host: affinity-nodeport-transition-58h7g
Jun 10 17:00:01.630: INFO: Received response from host: affinity-nodeport-transition-58h7g
Jun 10 17:00:01.630: INFO: Received response from host: affinity-nodeport-transition-58h7g
Jun 10 17:00:01.630: INFO: Received response from host: affinity-nodeport-transition-58h7g
Jun 10 17:00:01.630: INFO: Received response from host: affinity-nodeport-transition-58h7g
Jun 10 17:00:01.630: INFO: Received response from host: affinity-nodeport-transition-58h7g
Jun 10 17:00:01.630: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-90, will wait for the garbage collector to delete the pods
Jun 10 17:00:01.713: INFO: Deleting ReplicationController affinity-nodeport-transition took: 6.312697ms
Jun 10 17:00:03.114: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 1.400960381s
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:00:19.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-90" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:26.503 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":226,"skipped":3961,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:00:19.336: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Jun 10 17:00:19.451: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:01:36.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1130" for this suite.

• [SLOW TEST:77.474 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":305,"completed":227,"skipped":3972,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:01:36.811: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating api versions
Jun 10 17:01:36.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-9507 api-versions'
Jun 10 17:01:37.057: INFO: stderr: ""
Jun 10 17:01:37.057: INFO: stdout: "acme.cert-manager.io/v1\nacme.cert-manager.io/v1alpha2\nacme.cert-manager.io/v1alpha3\nacme.cert-manager.io/v1beta1\naddons.cluster.x-k8s.io/v1alpha3\nadmissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbootstrap.cluster.x-k8s.io/v1alpha2\nbootstrap.cluster.x-k8s.io/v1alpha3\ncert-manager.io/v1\ncert-manager.io/v1alpha2\ncert-manager.io/v1alpha3\ncert-manager.io/v1beta1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncluster.x-k8s.io/v1alpha2\ncluster.x-k8s.io/v1alpha3\nclusterctl.cluster.x-k8s.io/v1alpha3\ncontrolplane.cluster.x-k8s.io/v1alpha3\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nexp.cluster.x-k8s.io/v1alpha3\nexp.infrastructure.cluster.x-k8s.io/v1alpha3\nextensions/v1beta1\ninfrastructure.cluster.x-k8s.io/v1alpha3\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\noperator.tigera.io/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:01:37.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9507" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":305,"completed":228,"skipped":3984,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:01:37.079: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1512
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun 10 17:01:37.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-9306 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine'
Jun 10 17:01:37.319: INFO: stderr: ""
Jun 10 17:01:37.319: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1516
Jun 10 17:01:37.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-9306 delete pods e2e-test-httpd-pod'
Jun 10 17:01:39.713: INFO: stderr: ""
Jun 10 17:01:39.713: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:01:39.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9306" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":305,"completed":229,"skipped":4006,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:01:39.738: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service nodeport-service with the type=NodePort in namespace services-4442
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-4442
STEP: creating replication controller externalsvc in namespace services-4442
I0610 17:01:39.915060      26 runners.go:190] Created replication controller with name: externalsvc, namespace: services-4442, replica count: 2
I0610 17:01:42.966685      26 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Jun 10 17:01:43.014: INFO: Creating new exec pod
Jun 10 17:01:45.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=services-4442 exec execpodjlbdz -- /bin/sh -x -c nslookup nodeport-service.services-4442.svc.cluster.local'
Jun 10 17:01:45.360: INFO: stderr: "+ nslookup nodeport-service.services-4442.svc.cluster.local\n"
Jun 10 17:01:45.360: INFO: stdout: "Server:\t\t10.128.0.10\nAddress:\t10.128.0.10#53\n\nnodeport-service.services-4442.svc.cluster.local\tcanonical name = externalsvc.services-4442.svc.cluster.local.\nName:\texternalsvc.services-4442.svc.cluster.local\nAddress: 10.130.182.103\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4442, will wait for the garbage collector to delete the pods
Jun 10 17:01:45.436: INFO: Deleting ReplicationController externalsvc took: 22.523924ms
Jun 10 17:01:46.836: INFO: Terminating ReplicationController externalsvc pods took: 1.400164938s
Jun 10 17:01:55.621: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:01:55.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4442" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:15.965 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":305,"completed":230,"skipped":4031,"failed":0}
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:01:55.703: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-5125
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating stateful set ss in namespace statefulset-5125
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5125
Jun 10 17:01:55.872: INFO: Found 0 stateful pods, waiting for 1
Jun 10 17:02:05.876: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jun 10 17:02:05.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 10 17:02:06.146: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 10 17:02:06.146: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 10 17:02:06.146: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 10 17:02:06.156: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun 10 17:02:16.160: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 10 17:02:16.161: INFO: Waiting for statefulset status.replicas updated to 0
Jun 10 17:02:16.173: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Jun 10 17:02:16.174: INFO: ss-0  target-cluster-md-0-6b59c4f65-5pcx5  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:01:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:01:55 +0000 UTC  }]
Jun 10 17:02:16.174: INFO: 
Jun 10 17:02:16.174: INFO: StatefulSet ss has not reached scale 3, at 1
Jun 10 17:02:17.177: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995700777s
Jun 10 17:02:18.181: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.991664412s
Jun 10 17:02:19.185: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.987990786s
Jun 10 17:02:20.189: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.984297858s
Jun 10 17:02:21.212: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.979987125s
Jun 10 17:02:22.219: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.956626785s
Jun 10 17:02:23.224: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.949606775s
Jun 10 17:02:24.228: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.944506859s
Jun 10 17:02:25.233: INFO: Verifying statefulset ss doesn't scale past 3 for another 939.671304ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5125
Jun 10 17:02:26.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 17:02:26.468: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 10 17:02:26.468: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 10 17:02:26.468: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 10 17:02:26.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 17:02:26.720: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun 10 17:02:26.720: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 10 17:02:26.720: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 10 17:02:26.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 17:02:27.078: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun 10 17:02:27.078: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 10 17:02:27.078: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 10 17:02:27.097: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 10 17:02:27.097: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 10 17:02:27.097: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jun 10 17:02:27.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 10 17:02:27.356: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 10 17:02:27.356: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 10 17:02:27.356: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 10 17:02:27.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 10 17:02:27.638: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 10 17:02:27.638: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 10 17:02:27.638: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 10 17:02:27.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 10 17:02:27.859: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 10 17:02:27.859: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 10 17:02:27.859: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 10 17:02:27.859: INFO: Waiting for statefulset status.replicas updated to 0
Jun 10 17:02:27.863: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jun 10 17:02:37.868: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 10 17:02:37.868: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun 10 17:02:37.868: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun 10 17:02:37.878: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Jun 10 17:02:37.878: INFO: ss-0  target-cluster-md-0-6b59c4f65-5pcx5  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:01:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:01:55 +0000 UTC  }]
Jun 10 17:02:37.878: INFO: ss-1  target-cluster-md-0-6b59c4f65-cqpjz  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:16 +0000 UTC  }]
Jun 10 17:02:37.878: INFO: ss-2  target-cluster-control-plane-8m52s   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:16 +0000 UTC  }]
Jun 10 17:02:37.878: INFO: 
Jun 10 17:02:37.878: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 10 17:02:38.890: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Jun 10 17:02:38.890: INFO: ss-0  target-cluster-md-0-6b59c4f65-5pcx5  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:01:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:01:55 +0000 UTC  }]
Jun 10 17:02:38.890: INFO: ss-1  target-cluster-md-0-6b59c4f65-cqpjz  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:16 +0000 UTC  }]
Jun 10 17:02:38.890: INFO: ss-2  target-cluster-control-plane-8m52s   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:16 +0000 UTC  }]
Jun 10 17:02:38.890: INFO: 
Jun 10 17:02:38.890: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 10 17:02:39.906: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Jun 10 17:02:39.906: INFO: ss-0  target-cluster-md-0-6b59c4f65-5pcx5  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:01:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:01:55 +0000 UTC  }]
Jun 10 17:02:39.907: INFO: ss-1  target-cluster-md-0-6b59c4f65-cqpjz  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:16 +0000 UTC  }]
Jun 10 17:02:39.907: INFO: ss-2  target-cluster-control-plane-8m52s   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:16 +0000 UTC  }]
Jun 10 17:02:39.907: INFO: 
Jun 10 17:02:39.908: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 10 17:02:40.948: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Jun 10 17:02:40.948: INFO: ss-0  target-cluster-md-0-6b59c4f65-5pcx5  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:01:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:01:55 +0000 UTC  }]
Jun 10 17:02:40.948: INFO: ss-1  target-cluster-md-0-6b59c4f65-cqpjz  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:16 +0000 UTC  }]
Jun 10 17:02:40.948: INFO: ss-2  target-cluster-control-plane-8m52s   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:16 +0000 UTC  }]
Jun 10 17:02:40.948: INFO: 
Jun 10 17:02:40.948: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 10 17:02:41.951: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Jun 10 17:02:41.951: INFO: ss-0  target-cluster-md-0-6b59c4f65-5pcx5  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:01:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:01:55 +0000 UTC  }]
Jun 10 17:02:41.951: INFO: ss-1  target-cluster-md-0-6b59c4f65-cqpjz  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:16 +0000 UTC  }]
Jun 10 17:02:41.951: INFO: ss-2  target-cluster-control-plane-8m52s   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:16 +0000 UTC  }]
Jun 10 17:02:41.951: INFO: 
Jun 10 17:02:41.951: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 10 17:02:42.955: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Jun 10 17:02:42.955: INFO: ss-0  target-cluster-md-0-6b59c4f65-5pcx5  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:01:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:01:55 +0000 UTC  }]
Jun 10 17:02:42.955: INFO: ss-1  target-cluster-md-0-6b59c4f65-cqpjz  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:16 +0000 UTC  }]
Jun 10 17:02:42.955: INFO: ss-2  target-cluster-control-plane-8m52s   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:16 +0000 UTC  }]
Jun 10 17:02:42.955: INFO: 
Jun 10 17:02:42.955: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 10 17:02:43.959: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Jun 10 17:02:43.959: INFO: ss-0  target-cluster-md-0-6b59c4f65-5pcx5  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:01:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:01:55 +0000 UTC  }]
Jun 10 17:02:43.959: INFO: ss-1  target-cluster-md-0-6b59c4f65-cqpjz  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:16 +0000 UTC  }]
Jun 10 17:02:43.959: INFO: ss-2  target-cluster-control-plane-8m52s   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:16 +0000 UTC  }]
Jun 10 17:02:43.959: INFO: 
Jun 10 17:02:43.959: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 10 17:02:44.964: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Jun 10 17:02:44.964: INFO: ss-0  target-cluster-md-0-6b59c4f65-5pcx5  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:01:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:01:55 +0000 UTC  }]
Jun 10 17:02:44.964: INFO: ss-1  target-cluster-md-0-6b59c4f65-cqpjz  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:16 +0000 UTC  }]
Jun 10 17:02:44.964: INFO: ss-2  target-cluster-control-plane-8m52s   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:16 +0000 UTC  }]
Jun 10 17:02:44.964: INFO: 
Jun 10 17:02:44.964: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 10 17:02:45.968: INFO: POD   NODE                                PHASE    GRACE  CONDITIONS
Jun 10 17:02:45.968: INFO: ss-2  target-cluster-control-plane-8m52s  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:16 +0000 UTC  }]
Jun 10 17:02:45.968: INFO: 
Jun 10 17:02:45.968: INFO: StatefulSet ss has not reached scale 0, at 1
Jun 10 17:02:46.974: INFO: POD   NODE                                PHASE    GRACE  CONDITIONS
Jun 10 17:02:46.974: INFO: ss-2  target-cluster-control-plane-8m52s  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 17:02:16 +0000 UTC  }]
Jun 10 17:02:46.974: INFO: 
Jun 10 17:02:46.974: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5125
Jun 10 17:02:47.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 17:02:48.127: INFO: rc: 1
Jun 10 17:02:48.127: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Jun 10 17:02:58.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 17:02:58.281: INFO: rc: 1
Jun 10 17:02:58.281: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 10 17:03:08.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 17:03:08.417: INFO: rc: 1
Jun 10 17:03:08.417: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 10 17:03:18.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 17:03:18.551: INFO: rc: 1
Jun 10 17:03:18.551: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 10 17:03:28.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 17:03:28.683: INFO: rc: 1
Jun 10 17:03:28.684: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 10 17:03:38.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 17:03:39.117: INFO: rc: 1
Jun 10 17:03:39.117: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 10 17:03:49.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 17:03:49.261: INFO: rc: 1
Jun 10 17:03:49.261: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 10 17:03:59.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 17:03:59.428: INFO: rc: 1
Jun 10 17:03:59.428: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 10 17:04:09.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 17:04:09.572: INFO: rc: 1
Jun 10 17:04:09.572: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 10 17:04:19.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 17:04:19.722: INFO: rc: 1
Jun 10 17:04:19.722: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 10 17:04:29.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 17:04:29.889: INFO: rc: 1
Jun 10 17:04:29.889: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 10 17:04:39.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 17:04:40.040: INFO: rc: 1
Jun 10 17:04:40.040: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 10 17:04:50.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 17:04:50.175: INFO: rc: 1
Jun 10 17:04:50.175: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 10 17:05:00.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 17:05:00.336: INFO: rc: 1
Jun 10 17:05:00.336: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 10 17:05:10.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 17:05:10.448: INFO: rc: 1
Jun 10 17:05:10.448: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 10 17:05:20.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 17:05:20.567: INFO: rc: 1
Jun 10 17:05:20.567: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 10 17:05:30.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 17:05:30.720: INFO: rc: 1
Jun 10 17:05:30.720: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 10 17:05:40.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 17:05:40.859: INFO: rc: 1
Jun 10 17:05:40.859: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 10 17:05:50.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 17:05:50.962: INFO: rc: 1
Jun 10 17:05:50.962: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 10 17:06:00.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 17:06:01.139: INFO: rc: 1
Jun 10 17:06:01.139: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 10 17:06:11.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 17:06:11.291: INFO: rc: 1
Jun 10 17:06:11.291: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 10 17:06:21.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 17:06:21.444: INFO: rc: 1
Jun 10 17:06:21.444: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 10 17:06:31.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 17:06:31.617: INFO: rc: 1
Jun 10 17:06:31.617: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 10 17:06:41.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 17:06:41.749: INFO: rc: 1
Jun 10 17:06:41.749: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 10 17:06:51.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 17:06:51.883: INFO: rc: 1
Jun 10 17:06:51.883: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 10 17:07:01.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 17:07:02.033: INFO: rc: 1
Jun 10 17:07:02.033: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 10 17:07:12.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 17:07:12.173: INFO: rc: 1
Jun 10 17:07:12.173: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 10 17:07:22.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 17:07:22.315: INFO: rc: 1
Jun 10 17:07:22.315: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 10 17:07:32.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 17:07:32.438: INFO: rc: 1
Jun 10 17:07:32.438: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 10 17:07:42.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 17:07:42.647: INFO: rc: 1
Jun 10 17:07:42.647: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun 10 17:07:52.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-5125 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 17:07:52.826: INFO: rc: 1
Jun 10 17:07:52.826: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
Jun 10 17:07:52.826: INFO: Scaling statefulset ss to 0
Jun 10 17:07:52.835: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jun 10 17:07:52.836: INFO: Deleting all statefulset in ns statefulset-5125
Jun 10 17:07:52.841: INFO: Scaling statefulset ss to 0
Jun 10 17:07:52.849: INFO: Waiting for statefulset status.replicas updated to 0
Jun 10 17:07:52.851: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:07:52.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5125" for this suite.

• [SLOW TEST:357.169 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":305,"completed":231,"skipped":4037,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:07:52.873: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 10 17:07:52.916: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jun 10 17:08:08.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=crd-publish-openapi-3934 --namespace=crd-publish-openapi-3934 create -f -'
Jun 10 17:08:09.179: INFO: stderr: ""
Jun 10 17:08:09.179: INFO: stdout: "e2e-test-crd-publish-openapi-6737-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jun 10 17:08:09.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=crd-publish-openapi-3934 --namespace=crd-publish-openapi-3934 delete e2e-test-crd-publish-openapi-6737-crds test-cr'
Jun 10 17:08:09.387: INFO: stderr: ""
Jun 10 17:08:09.387: INFO: stdout: "e2e-test-crd-publish-openapi-6737-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jun 10 17:08:09.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=crd-publish-openapi-3934 --namespace=crd-publish-openapi-3934 apply -f -'
Jun 10 17:08:09.868: INFO: stderr: ""
Jun 10 17:08:09.868: INFO: stdout: "e2e-test-crd-publish-openapi-6737-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jun 10 17:08:09.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=crd-publish-openapi-3934 --namespace=crd-publish-openapi-3934 delete e2e-test-crd-publish-openapi-6737-crds test-cr'
Jun 10 17:08:10.015: INFO: stderr: ""
Jun 10 17:08:10.015: INFO: stdout: "e2e-test-crd-publish-openapi-6737-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jun 10 17:08:10.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=crd-publish-openapi-3934 explain e2e-test-crd-publish-openapi-6737-crds'
Jun 10 17:08:10.493: INFO: stderr: ""
Jun 10 17:08:10.493: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6737-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:08:26.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3934" for this suite.

• [SLOW TEST:33.158 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":305,"completed":232,"skipped":4041,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:08:26.031: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:08:28.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7439" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":305,"completed":233,"skipped":4048,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:08:28.199: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun 10 17:08:32.307: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 10 17:08:32.314: INFO: Pod pod-with-poststart-http-hook still exists
Jun 10 17:08:34.314: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 10 17:08:34.320: INFO: Pod pod-with-poststart-http-hook still exists
Jun 10 17:08:36.314: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 10 17:08:36.319: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:08:36.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9297" for this suite.

• [SLOW TEST:8.152 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":305,"completed":234,"skipped":4077,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:08:36.351: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 10 17:08:36.439: INFO: Waiting up to 5m0s for pod "downwardapi-volume-52e680c1-4dc7-432b-86ae-3c7b2cf172f1" in namespace "projected-1694" to be "Succeeded or Failed"
Jun 10 17:08:36.461: INFO: Pod "downwardapi-volume-52e680c1-4dc7-432b-86ae-3c7b2cf172f1": Phase="Pending", Reason="", readiness=false. Elapsed: 22.335896ms
Jun 10 17:08:38.479: INFO: Pod "downwardapi-volume-52e680c1-4dc7-432b-86ae-3c7b2cf172f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.040031552s
STEP: Saw pod success
Jun 10 17:08:38.479: INFO: Pod "downwardapi-volume-52e680c1-4dc7-432b-86ae-3c7b2cf172f1" satisfied condition "Succeeded or Failed"
Jun 10 17:08:38.487: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod downwardapi-volume-52e680c1-4dc7-432b-86ae-3c7b2cf172f1 container client-container: <nil>
STEP: delete the pod
Jun 10 17:08:38.533: INFO: Waiting for pod downwardapi-volume-52e680c1-4dc7-432b-86ae-3c7b2cf172f1 to disappear
Jun 10 17:08:38.536: INFO: Pod downwardapi-volume-52e680c1-4dc7-432b-86ae-3c7b2cf172f1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:08:38.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1694" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":305,"completed":235,"skipped":4079,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:08:38.543: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-888eefa8-af96-401e-9822-b6365bba03c5
STEP: Creating a pod to test consume secrets
Jun 10 17:08:38.608: INFO: Waiting up to 5m0s for pod "pod-secrets-69da212d-eb07-4863-856e-09c9ba7bd54f" in namespace "secrets-7894" to be "Succeeded or Failed"
Jun 10 17:08:38.618: INFO: Pod "pod-secrets-69da212d-eb07-4863-856e-09c9ba7bd54f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.394635ms
Jun 10 17:08:40.623: INFO: Pod "pod-secrets-69da212d-eb07-4863-856e-09c9ba7bd54f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014992998s
STEP: Saw pod success
Jun 10 17:08:40.623: INFO: Pod "pod-secrets-69da212d-eb07-4863-856e-09c9ba7bd54f" satisfied condition "Succeeded or Failed"
Jun 10 17:08:40.626: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod pod-secrets-69da212d-eb07-4863-856e-09c9ba7bd54f container secret-volume-test: <nil>
STEP: delete the pod
Jun 10 17:08:40.657: INFO: Waiting for pod pod-secrets-69da212d-eb07-4863-856e-09c9ba7bd54f to disappear
Jun 10 17:08:40.661: INFO: Pod pod-secrets-69da212d-eb07-4863-856e-09c9ba7bd54f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:08:40.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7894" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":236,"skipped":4086,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:08:40.674: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 10 17:08:40.721: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fc870c21-9163-463f-b713-ee33b1a03c5a" in namespace "projected-1892" to be "Succeeded or Failed"
Jun 10 17:08:40.731: INFO: Pod "downwardapi-volume-fc870c21-9163-463f-b713-ee33b1a03c5a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.502714ms
Jun 10 17:08:42.735: INFO: Pod "downwardapi-volume-fc870c21-9163-463f-b713-ee33b1a03c5a": Phase="Running", Reason="", readiness=true. Elapsed: 2.013613088s
Jun 10 17:08:44.738: INFO: Pod "downwardapi-volume-fc870c21-9163-463f-b713-ee33b1a03c5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017032504s
STEP: Saw pod success
Jun 10 17:08:44.738: INFO: Pod "downwardapi-volume-fc870c21-9163-463f-b713-ee33b1a03c5a" satisfied condition "Succeeded or Failed"
Jun 10 17:08:44.741: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod downwardapi-volume-fc870c21-9163-463f-b713-ee33b1a03c5a container client-container: <nil>
STEP: delete the pod
Jun 10 17:08:44.772: INFO: Waiting for pod downwardapi-volume-fc870c21-9163-463f-b713-ee33b1a03c5a to disappear
Jun 10 17:08:44.776: INFO: Pod downwardapi-volume-fc870c21-9163-463f-b713-ee33b1a03c5a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:08:44.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1892" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":237,"skipped":4111,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:08:44.785: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 10 17:08:45.568: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 10 17:08:48.603: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Jun 10 17:08:50.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=webhook-8460 attach --namespace=webhook-8460 to-be-attached-pod -i -c=container1'
Jun 10 17:08:50.827: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:08:50.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8460" for this suite.
STEP: Destroying namespace "webhook-8460-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.251 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":305,"completed":238,"skipped":4127,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:08:51.037: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pod templates
Jun 10 17:08:51.103: INFO: created test-podtemplate-1
Jun 10 17:08:51.109: INFO: created test-podtemplate-2
Jun 10 17:08:51.116: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Jun 10 17:08:51.120: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Jun 10 17:08:51.140: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:08:51.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-9466" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":305,"completed":239,"skipped":4140,"failed":0}
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:08:51.153: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:09:11.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5008" for this suite.

• [SLOW TEST:20.439 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":305,"completed":240,"skipped":4144,"failed":0}
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:09:11.593: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Jun 10 17:09:11.671: INFO: Waiting up to 5m0s for pod "downward-api-5468c9fa-5ac3-4efb-b4ec-17ccbb8d8e2f" in namespace "downward-api-8271" to be "Succeeded or Failed"
Jun 10 17:09:11.696: INFO: Pod "downward-api-5468c9fa-5ac3-4efb-b4ec-17ccbb8d8e2f": Phase="Pending", Reason="", readiness=false. Elapsed: 24.632175ms
Jun 10 17:09:13.717: INFO: Pod "downward-api-5468c9fa-5ac3-4efb-b4ec-17ccbb8d8e2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.045775623s
STEP: Saw pod success
Jun 10 17:09:13.717: INFO: Pod "downward-api-5468c9fa-5ac3-4efb-b4ec-17ccbb8d8e2f" satisfied condition "Succeeded or Failed"
Jun 10 17:09:13.724: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-5pcx5 pod downward-api-5468c9fa-5ac3-4efb-b4ec-17ccbb8d8e2f container dapi-container: <nil>
STEP: delete the pod
Jun 10 17:09:13.761: INFO: Waiting for pod downward-api-5468c9fa-5ac3-4efb-b4ec-17ccbb8d8e2f to disappear
Jun 10 17:09:13.773: INFO: Pod downward-api-5468c9fa-5ac3-4efb-b4ec-17ccbb8d8e2f no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:09:13.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8271" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":305,"completed":241,"skipped":4144,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:09:13.803: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-f003a31e-148b-4511-a554-90cc5d45217a in namespace container-probe-2979
Jun 10 17:09:15.968: INFO: Started pod liveness-f003a31e-148b-4511-a554-90cc5d45217a in namespace container-probe-2979
STEP: checking the pod's current state and verifying that restartCount is present
Jun 10 17:09:15.973: INFO: Initial restart count of pod liveness-f003a31e-148b-4511-a554-90cc5d45217a is 0
Jun 10 17:09:38.034: INFO: Restart count of pod container-probe-2979/liveness-f003a31e-148b-4511-a554-90cc5d45217a is now 1 (22.061014837s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:09:38.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2979" for this suite.

• [SLOW TEST:24.272 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":305,"completed":242,"skipped":4187,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:09:38.075: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun 10 17:09:38.179: INFO: Waiting up to 5m0s for pod "pod-ecd46c13-fc85-4cc2-9307-82108fea78f8" in namespace "emptydir-4888" to be "Succeeded or Failed"
Jun 10 17:09:38.204: INFO: Pod "pod-ecd46c13-fc85-4cc2-9307-82108fea78f8": Phase="Pending", Reason="", readiness=false. Elapsed: 25.26603ms
Jun 10 17:09:40.208: INFO: Pod "pod-ecd46c13-fc85-4cc2-9307-82108fea78f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029386471s
Jun 10 17:09:42.211: INFO: Pod "pod-ecd46c13-fc85-4cc2-9307-82108fea78f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032191466s
STEP: Saw pod success
Jun 10 17:09:42.211: INFO: Pod "pod-ecd46c13-fc85-4cc2-9307-82108fea78f8" satisfied condition "Succeeded or Failed"
Jun 10 17:09:42.214: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod pod-ecd46c13-fc85-4cc2-9307-82108fea78f8 container test-container: <nil>
STEP: delete the pod
Jun 10 17:09:42.238: INFO: Waiting for pod pod-ecd46c13-fc85-4cc2-9307-82108fea78f8 to disappear
Jun 10 17:09:42.240: INFO: Pod pod-ecd46c13-fc85-4cc2-9307-82108fea78f8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:09:42.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4888" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":243,"skipped":4209,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:09:42.248: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun 10 17:09:42.310: INFO: Waiting up to 5m0s for pod "pod-98382835-4fe5-4b16-92fa-85ec84adda3b" in namespace "emptydir-1548" to be "Succeeded or Failed"
Jun 10 17:09:42.326: INFO: Pod "pod-98382835-4fe5-4b16-92fa-85ec84adda3b": Phase="Pending", Reason="", readiness=false. Elapsed: 15.461514ms
Jun 10 17:09:44.333: INFO: Pod "pod-98382835-4fe5-4b16-92fa-85ec84adda3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0228291s
Jun 10 17:09:46.336: INFO: Pod "pod-98382835-4fe5-4b16-92fa-85ec84adda3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025587971s
STEP: Saw pod success
Jun 10 17:09:46.336: INFO: Pod "pod-98382835-4fe5-4b16-92fa-85ec84adda3b" satisfied condition "Succeeded or Failed"
Jun 10 17:09:46.338: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod pod-98382835-4fe5-4b16-92fa-85ec84adda3b container test-container: <nil>
STEP: delete the pod
Jun 10 17:09:46.360: INFO: Waiting for pod pod-98382835-4fe5-4b16-92fa-85ec84adda3b to disappear
Jun 10 17:09:46.365: INFO: Pod pod-98382835-4fe5-4b16-92fa-85ec84adda3b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:09:46.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1548" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":244,"skipped":4214,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:09:46.381: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Jun 10 17:09:46.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-303 create -f -'
Jun 10 17:09:47.037: INFO: stderr: ""
Jun 10 17:09:47.037: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 10 17:09:47.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-303 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 10 17:09:47.235: INFO: stderr: ""
Jun 10 17:09:47.235: INFO: stdout: "update-demo-nautilus-b4zt5 update-demo-nautilus-qn6wz "
Jun 10 17:09:47.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-303 get pods update-demo-nautilus-b4zt5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 10 17:09:47.509: INFO: stderr: ""
Jun 10 17:09:47.509: INFO: stdout: ""
Jun 10 17:09:47.509: INFO: update-demo-nautilus-b4zt5 is created but not running
Jun 10 17:09:52.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-303 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 10 17:09:52.645: INFO: stderr: ""
Jun 10 17:09:52.645: INFO: stdout: "update-demo-nautilus-b4zt5 update-demo-nautilus-qn6wz "
Jun 10 17:09:52.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-303 get pods update-demo-nautilus-b4zt5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 10 17:09:52.788: INFO: stderr: ""
Jun 10 17:09:52.788: INFO: stdout: "true"
Jun 10 17:09:52.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-303 get pods update-demo-nautilus-b4zt5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 10 17:09:52.939: INFO: stderr: ""
Jun 10 17:09:52.939: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 10 17:09:52.939: INFO: validating pod update-demo-nautilus-b4zt5
Jun 10 17:09:52.944: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 10 17:09:52.944: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 10 17:09:52.944: INFO: update-demo-nautilus-b4zt5 is verified up and running
Jun 10 17:09:52.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-303 get pods update-demo-nautilus-qn6wz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 10 17:09:53.111: INFO: stderr: ""
Jun 10 17:09:53.111: INFO: stdout: "true"
Jun 10 17:09:53.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-303 get pods update-demo-nautilus-qn6wz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 10 17:09:53.261: INFO: stderr: ""
Jun 10 17:09:53.261: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 10 17:09:53.261: INFO: validating pod update-demo-nautilus-qn6wz
Jun 10 17:09:53.265: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 10 17:09:53.265: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 10 17:09:53.265: INFO: update-demo-nautilus-qn6wz is verified up and running
STEP: scaling down the replication controller
Jun 10 17:09:53.268: INFO: scanned /root for discovery docs: <nil>
Jun 10 17:09:53.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-303 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jun 10 17:09:53.450: INFO: stderr: ""
Jun 10 17:09:53.450: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 10 17:09:53.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-303 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 10 17:09:53.687: INFO: stderr: ""
Jun 10 17:09:53.687: INFO: stdout: "update-demo-nautilus-b4zt5 update-demo-nautilus-qn6wz "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jun 10 17:09:58.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-303 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 10 17:09:58.864: INFO: stderr: ""
Jun 10 17:09:58.864: INFO: stdout: "update-demo-nautilus-b4zt5 update-demo-nautilus-qn6wz "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jun 10 17:10:03.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-303 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 10 17:10:03.999: INFO: stderr: ""
Jun 10 17:10:03.999: INFO: stdout: "update-demo-nautilus-b4zt5 update-demo-nautilus-qn6wz "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jun 10 17:10:09.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-303 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 10 17:10:09.159: INFO: stderr: ""
Jun 10 17:10:09.159: INFO: stdout: "update-demo-nautilus-qn6wz "
Jun 10 17:10:09.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-303 get pods update-demo-nautilus-qn6wz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 10 17:10:09.321: INFO: stderr: ""
Jun 10 17:10:09.321: INFO: stdout: "true"
Jun 10 17:10:09.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-303 get pods update-demo-nautilus-qn6wz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 10 17:10:09.506: INFO: stderr: ""
Jun 10 17:10:09.506: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 10 17:10:09.506: INFO: validating pod update-demo-nautilus-qn6wz
Jun 10 17:10:09.511: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 10 17:10:09.511: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 10 17:10:09.511: INFO: update-demo-nautilus-qn6wz is verified up and running
STEP: scaling up the replication controller
Jun 10 17:10:09.514: INFO: scanned /root for discovery docs: <nil>
Jun 10 17:10:09.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-303 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jun 10 17:10:10.727: INFO: stderr: ""
Jun 10 17:10:10.727: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 10 17:10:10.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-303 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 10 17:10:10.928: INFO: stderr: ""
Jun 10 17:10:10.928: INFO: stdout: "update-demo-nautilus-qn6wz update-demo-nautilus-x9jjj "
Jun 10 17:10:10.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-303 get pods update-demo-nautilus-qn6wz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 10 17:10:11.120: INFO: stderr: ""
Jun 10 17:10:11.120: INFO: stdout: "true"
Jun 10 17:10:11.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-303 get pods update-demo-nautilus-qn6wz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 10 17:10:11.356: INFO: stderr: ""
Jun 10 17:10:11.356: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 10 17:10:11.356: INFO: validating pod update-demo-nautilus-qn6wz
Jun 10 17:10:11.360: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 10 17:10:11.360: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 10 17:10:11.360: INFO: update-demo-nautilus-qn6wz is verified up and running
Jun 10 17:10:11.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-303 get pods update-demo-nautilus-x9jjj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 10 17:10:11.586: INFO: stderr: ""
Jun 10 17:10:11.586: INFO: stdout: ""
Jun 10 17:10:11.586: INFO: update-demo-nautilus-x9jjj is created but not running
Jun 10 17:10:16.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-303 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 10 17:10:16.836: INFO: stderr: ""
Jun 10 17:10:16.836: INFO: stdout: "update-demo-nautilus-qn6wz update-demo-nautilus-x9jjj "
Jun 10 17:10:16.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-303 get pods update-demo-nautilus-qn6wz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 10 17:10:17.092: INFO: stderr: ""
Jun 10 17:10:17.092: INFO: stdout: "true"
Jun 10 17:10:17.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-303 get pods update-demo-nautilus-qn6wz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 10 17:10:17.342: INFO: stderr: ""
Jun 10 17:10:17.342: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 10 17:10:17.342: INFO: validating pod update-demo-nautilus-qn6wz
Jun 10 17:10:17.350: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 10 17:10:17.350: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 10 17:10:17.350: INFO: update-demo-nautilus-qn6wz is verified up and running
Jun 10 17:10:17.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-303 get pods update-demo-nautilus-x9jjj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 10 17:10:17.656: INFO: stderr: ""
Jun 10 17:10:17.656: INFO: stdout: "true"
Jun 10 17:10:17.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-303 get pods update-demo-nautilus-x9jjj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 10 17:10:17.876: INFO: stderr: ""
Jun 10 17:10:17.877: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 10 17:10:17.877: INFO: validating pod update-demo-nautilus-x9jjj
Jun 10 17:10:17.882: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 10 17:10:17.882: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 10 17:10:17.882: INFO: update-demo-nautilus-x9jjj is verified up and running
STEP: using delete to clean up resources
Jun 10 17:10:17.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-303 delete --grace-period=0 --force -f -'
Jun 10 17:10:18.125: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 10 17:10:18.125: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun 10 17:10:18.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-303 get rc,svc -l name=update-demo --no-headers'
Jun 10 17:10:18.361: INFO: stderr: "No resources found in kubectl-303 namespace.\n"
Jun 10 17:10:18.361: INFO: stdout: ""
Jun 10 17:10:18.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-303 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 10 17:10:18.566: INFO: stderr: ""
Jun 10 17:10:18.566: INFO: stdout: "update-demo-nautilus-qn6wz\nupdate-demo-nautilus-x9jjj\n"
Jun 10 17:10:19.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-303 get rc,svc -l name=update-demo --no-headers'
Jun 10 17:10:19.328: INFO: stderr: "No resources found in kubectl-303 namespace.\n"
Jun 10 17:10:19.329: INFO: stdout: ""
Jun 10 17:10:19.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-303 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 10 17:10:19.754: INFO: stderr: ""
Jun 10 17:10:19.755: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:10:19.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-303" for this suite.

• [SLOW TEST:33.507 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":305,"completed":245,"skipped":4216,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:10:19.888: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 10 17:10:20.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-1801 create -f -'
Jun 10 17:10:21.148: INFO: stderr: ""
Jun 10 17:10:21.148: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jun 10 17:10:21.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-1801 create -f -'
Jun 10 17:10:21.830: INFO: stderr: ""
Jun 10 17:10:21.830: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jun 10 17:10:22.836: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 10 17:10:22.836: INFO: Found 0 / 1
Jun 10 17:10:23.834: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 10 17:10:23.834: INFO: Found 0 / 1
Jun 10 17:10:24.833: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 10 17:10:24.834: INFO: Found 1 / 1
Jun 10 17:10:24.834: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 10 17:10:24.840: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 10 17:10:24.840: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 10 17:10:24.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-1801 describe pod agnhost-primary-psvsv'
Jun 10 17:10:25.126: INFO: stderr: ""
Jun 10 17:10:25.126: INFO: stdout: "Name:         agnhost-primary-psvsv\nNamespace:    kubectl-1801\nPriority:     0\nNode:         target-cluster-md-0-6b59c4f65-cqpjz/172.17.0.5\nStart Time:   Thu, 10 Jun 2021 17:10:21 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/podIP: 192.168.78.101/32\n              cni.projectcalico.org/podIPs: 192.168.78.101/32\nStatus:       Running\nIP:           192.168.78.101\nIPs:\n  IP:           192.168.78.101\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://02edcfdeb003a17573e599b2b46d63195306f541422fa3da54163bc35fec794a\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 10 Jun 2021 17:10:23 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-8htm5 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-8htm5:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-8htm5\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type     Reason       Age   From               Message\n  ----     ------       ----  ----               -------\n  Normal   Scheduled    4s    default-scheduler  Successfully assigned kubectl-1801/agnhost-primary-psvsv to target-cluster-md-0-6b59c4f65-cqpjz\n  Warning  FailedMount  3s    kubelet            MountVolume.SetUp failed for volume \"default-token-8htm5\" : failed to sync secret cache: timed out waiting for the condition\n  Normal   Pulled       2s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.20\" already present on machine\n  Normal   Created      2s    kubelet            Created container agnhost-primary\n  Normal   Started      2s    kubelet            Started container agnhost-primary\n"
Jun 10 17:10:25.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-1801 describe rc agnhost-primary'
Jun 10 17:10:25.410: INFO: stderr: ""
Jun 10 17:10:25.410: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-1801\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: agnhost-primary-psvsv\n"
Jun 10 17:10:25.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-1801 describe service agnhost-primary'
Jun 10 17:10:25.771: INFO: stderr: ""
Jun 10 17:10:25.771: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-1801\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP:                10.135.119.11\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.78.101:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jun 10 17:10:25.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-1801 describe node target-cluster-control-plane-8m52s'
Jun 10 17:10:26.060: INFO: stderr: ""
Jun 10 17:10:26.060: INFO: stdout: "Name:               target-cluster-control-plane-8m52s\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=target-cluster-control-plane-8m52s\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 172.17.0.4/16\n                    projectcalico.org/IPv4IPIPTunnelAddr: 192.168.251.0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 10 Jun 2021 15:22:51 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  target-cluster-control-plane-8m52s\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 10 Jun 2021 17:10:17 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Thu, 10 Jun 2021 15:29:19 +0000   Thu, 10 Jun 2021 15:29:19 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Thu, 10 Jun 2021 17:07:43 +0000   Thu, 10 Jun 2021 15:22:50 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 10 Jun 2021 17:07:43 +0000   Thu, 10 Jun 2021 15:22:50 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 10 Jun 2021 17:07:43 +0000   Thu, 10 Jun 2021 15:22:50 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 10 Jun 2021 17:07:43 +0000   Thu, 10 Jun 2021 15:29:10 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  172.17.0.4\n  Hostname:    target-cluster-control-plane-8m52s\nCapacity:\n  cpu:                4\n  ephemeral-storage:  226639372Ki\n  hugepages-2Mi:      0\n  memory:             21338804Ki\n  pods:               110\nAllocatable:\n  cpu:                4\n  ephemeral-storage:  226639372Ki\n  hugepages-2Mi:      0\n  memory:             21338804Ki\n  pods:               110\nSystem Info:\n  Machine ID:                        d30e9b7c0e7045fdab94807756260371\n  System UUID:                       4f39faf9-98c8-4720-b909-0fb9c504ca59\n  Boot ID:                           355d45da-d33b-4aef-b3d1-e187a719fd08\n  Kernel Version:                    5.4.0-73-generic\n  OS Image:                          Ubuntu 21.04\n  Operating System:                  linux\n  Architecture:                      amd64\n  Container Runtime Version:         containerd://1.5.2\n  Kubelet Version:                   v1.19.11\n  Kube-Proxy Version:                v1.19.11\nPodCIDR:                             192.168.0.0/24\nPodCIDRs:                            192.168.0.0/24\nProviderID:                          docker:////target-cluster-control-plane-8m52s\nNon-terminated Pods:                 (22 in total)\n  Namespace                          Name                                                              CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                          ----                                                              ------------  ----------  ---------------  -------------  ---\n  calico-system                      calico-kube-controllers-7c5d656c49-x6mc4                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         101m\n  calico-system                      calico-node-phlfp                                                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         101m\n  calico-system                      calico-typha-5d4587dc9c-7g8gs                                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         101m\n  capd-system                        capd-controller-manager-7c89f6ddd7-dc686                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         98m\n  capi-kubeadm-bootstrap-system      capi-kubeadm-bootstrap-controller-manager-6f669ccd7c-gtbcn        0 (0%)        0 (0%)      0 (0%)           0 (0%)         98m\n  capi-kubeadm-control-plane-system  capi-kubeadm-control-plane-controller-manager-5c95f59c5c-l4jvx    0 (0%)        0 (0%)      0 (0%)           0 (0%)         98m\n  capi-system                        capi-controller-manager-5f677d7d65-tzt7h                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         98m\n  capi-webhook-system                capi-controller-manager-745689557d-77m69                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         98m\n  capi-webhook-system                capi-kubeadm-bootstrap-controller-manager-6949f44db8-hn6kq        0 (0%)        0 (0%)      0 (0%)           0 (0%)         98m\n  capi-webhook-system                capi-kubeadm-control-plane-controller-manager-7b6c4bf48d-x8pzk    0 (0%)        0 (0%)      0 (0%)           0 (0%)         98m\n  cert-manager                       cert-manager-768bf64dd4-4lhmj                                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         101m\n  cert-manager                       cert-manager-cainjector-646879549c-pbn4t                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         101m\n  cert-manager                       cert-manager-webhook-6dc9ccc9fb-6gg4f                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         101m\n  kube-system                        coredns-f9fd979d6-2dcsm                                           100m (2%)     0 (0%)      70Mi (0%)        170Mi (0%)     107m\n  kube-system                        coredns-f9fd979d6-zsjpx                                           100m (2%)     0 (0%)      70Mi (0%)        170Mi (0%)     107m\n  kube-system                        etcd-target-cluster-control-plane-8m52s                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         107m\n  kube-system                        kube-apiserver-target-cluster-control-plane-8m52s                 250m (6%)     0 (0%)      0 (0%)           0 (0%)         107m\n  kube-system                        kube-controller-manager-target-cluster-control-plane-8m52s        200m (5%)     0 (0%)      0 (0%)           0 (0%)         107m\n  kube-system                        kube-proxy-hz9zf                                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         107m\n  kube-system                        kube-scheduler-target-cluster-control-plane-8m52s                 100m (2%)     0 (0%)      0 (0%)           0 (0%)         107m\n  sonobuoy                           sonobuoy-systemd-logs-daemon-set-0467f197b1c74b53-bs8vn           0 (0%)        0 (0%)      0 (0%)           0 (0%)         91m\n  tigera-operator                    tigera-operator-5b76777d49-9zvzd                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         101m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                750m (18%)  0 (0%)\n  memory             140Mi (0%)  340Mi (1%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
Jun 10 17:10:26.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=kubectl-1801 describe namespace kubectl-1801'
Jun 10 17:10:26.311: INFO: stderr: ""
Jun 10 17:10:26.312: INFO: stdout: "Name:         kubectl-1801\nLabels:       e2e-framework=kubectl\n              e2e-run=4f102d75-248c-4634-8f62-c18949c3d47f\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:10:26.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1801" for this suite.

• [SLOW TEST:6.448 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1083
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":305,"completed":246,"skipped":4217,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:10:26.337: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 10 17:10:26.498: INFO: Waiting up to 5m0s for pod "downwardapi-volume-80aefb5e-bec6-41e2-a904-100ade5b2695" in namespace "downward-api-8375" to be "Succeeded or Failed"
Jun 10 17:10:26.508: INFO: Pod "downwardapi-volume-80aefb5e-bec6-41e2-a904-100ade5b2695": Phase="Pending", Reason="", readiness=false. Elapsed: 9.294603ms
Jun 10 17:10:28.513: INFO: Pod "downwardapi-volume-80aefb5e-bec6-41e2-a904-100ade5b2695": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014890262s
Jun 10 17:10:30.518: INFO: Pod "downwardapi-volume-80aefb5e-bec6-41e2-a904-100ade5b2695": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019944369s
STEP: Saw pod success
Jun 10 17:10:30.518: INFO: Pod "downwardapi-volume-80aefb5e-bec6-41e2-a904-100ade5b2695" satisfied condition "Succeeded or Failed"
Jun 10 17:10:30.521: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod downwardapi-volume-80aefb5e-bec6-41e2-a904-100ade5b2695 container client-container: <nil>
STEP: delete the pod
Jun 10 17:10:30.549: INFO: Waiting for pod downwardapi-volume-80aefb5e-bec6-41e2-a904-100ade5b2695 to disappear
Jun 10 17:10:30.553: INFO: Pod downwardapi-volume-80aefb5e-bec6-41e2-a904-100ade5b2695 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:10:30.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8375" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":305,"completed":247,"skipped":4231,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:10:30.569: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun 10 17:10:30.653: INFO: Waiting up to 5m0s for pod "pod-77055d51-f586-4ef0-ba5c-ea826c1b316e" in namespace "emptydir-571" to be "Succeeded or Failed"
Jun 10 17:10:30.658: INFO: Pod "pod-77055d51-f586-4ef0-ba5c-ea826c1b316e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.949758ms
Jun 10 17:10:32.674: INFO: Pod "pod-77055d51-f586-4ef0-ba5c-ea826c1b316e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020517019s
Jun 10 17:10:34.680: INFO: Pod "pod-77055d51-f586-4ef0-ba5c-ea826c1b316e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026936157s
STEP: Saw pod success
Jun 10 17:10:34.680: INFO: Pod "pod-77055d51-f586-4ef0-ba5c-ea826c1b316e" satisfied condition "Succeeded or Failed"
Jun 10 17:10:34.685: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-5pcx5 pod pod-77055d51-f586-4ef0-ba5c-ea826c1b316e container test-container: <nil>
STEP: delete the pod
Jun 10 17:10:34.754: INFO: Waiting for pod pod-77055d51-f586-4ef0-ba5c-ea826c1b316e to disappear
Jun 10 17:10:34.764: INFO: Pod pod-77055d51-f586-4ef0-ba5c-ea826c1b316e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:10:34.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-571" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":248,"skipped":4253,"failed":0}
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:10:34.809: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun 10 17:10:34.994: INFO: Number of nodes with available pods: 0
Jun 10 17:10:34.994: INFO: Node target-cluster-control-plane-8m52s is running more than one daemon pod
Jun 10 17:10:36.036: INFO: Number of nodes with available pods: 0
Jun 10 17:10:36.036: INFO: Node target-cluster-control-plane-8m52s is running more than one daemon pod
Jun 10 17:10:37.001: INFO: Number of nodes with available pods: 0
Jun 10 17:10:37.001: INFO: Node target-cluster-control-plane-8m52s is running more than one daemon pod
Jun 10 17:10:38.014: INFO: Number of nodes with available pods: 2
Jun 10 17:10:38.014: INFO: Node target-cluster-md-0-6b59c4f65-5pcx5 is running more than one daemon pod
Jun 10 17:10:39.008: INFO: Number of nodes with available pods: 3
Jun 10 17:10:39.008: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jun 10 17:10:39.090: INFO: Number of nodes with available pods: 2
Jun 10 17:10:39.090: INFO: Node target-cluster-md-0-6b59c4f65-cqpjz is running more than one daemon pod
Jun 10 17:10:40.105: INFO: Number of nodes with available pods: 2
Jun 10 17:10:40.105: INFO: Node target-cluster-md-0-6b59c4f65-cqpjz is running more than one daemon pod
Jun 10 17:10:41.097: INFO: Number of nodes with available pods: 2
Jun 10 17:10:41.097: INFO: Node target-cluster-md-0-6b59c4f65-cqpjz is running more than one daemon pod
Jun 10 17:10:42.104: INFO: Number of nodes with available pods: 3
Jun 10 17:10:42.104: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3926, will wait for the garbage collector to delete the pods
Jun 10 17:10:42.179: INFO: Deleting DaemonSet.extensions daemon-set took: 9.060972ms
Jun 10 17:10:42.280: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.557435ms
Jun 10 17:10:55.589: INFO: Number of nodes with available pods: 0
Jun 10 17:10:55.589: INFO: Number of running nodes: 0, number of available pods: 0
Jun 10 17:10:55.592: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3926/daemonsets","resourceVersion":"52933"},"items":null}

Jun 10 17:10:55.595: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3926/pods","resourceVersion":"52933"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:10:55.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3926" for this suite.

• [SLOW TEST:20.821 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":305,"completed":249,"skipped":4259,"failed":0}
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:10:55.631: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4392.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4392.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 10 17:10:57.721: INFO: DNS probes using dns-4392/dns-test-47a911f3-709e-4e61-9f2b-b6897ebed177 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:10:57.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4392" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":305,"completed":250,"skipped":4259,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:10:57.763: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 10 17:10:57.838: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jun 10 17:11:02.843: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 10 17:11:02.843: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Jun 10 17:11:02.863: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-474 /apis/apps/v1/namespaces/deployment-474/deployments/test-cleanup-deployment 39169a12-ffc2-4b3c-a4b9-59761f8a4651 53057 1 2021-06-10 17:11:02 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2021-06-10 17:11:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005312be8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Jun 10 17:11:02.867: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:11:02.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-474" for this suite.

• [SLOW TEST:5.118 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":305,"completed":251,"skipped":4270,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:11:02.881: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 10 17:11:02.946: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d79608a7-58e8-44c9-9d39-a98181a9d772" in namespace "downward-api-4425" to be "Succeeded or Failed"
Jun 10 17:11:02.955: INFO: Pod "downwardapi-volume-d79608a7-58e8-44c9-9d39-a98181a9d772": Phase="Pending", Reason="", readiness=false. Elapsed: 8.55182ms
Jun 10 17:11:04.959: INFO: Pod "downwardapi-volume-d79608a7-58e8-44c9-9d39-a98181a9d772": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012175682s
Jun 10 17:11:06.962: INFO: Pod "downwardapi-volume-d79608a7-58e8-44c9-9d39-a98181a9d772": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015702689s
STEP: Saw pod success
Jun 10 17:11:06.962: INFO: Pod "downwardapi-volume-d79608a7-58e8-44c9-9d39-a98181a9d772" satisfied condition "Succeeded or Failed"
Jun 10 17:11:06.966: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod downwardapi-volume-d79608a7-58e8-44c9-9d39-a98181a9d772 container client-container: <nil>
STEP: delete the pod
Jun 10 17:11:07.011: INFO: Waiting for pod downwardapi-volume-d79608a7-58e8-44c9-9d39-a98181a9d772 to disappear
Jun 10 17:11:07.029: INFO: Pod downwardapi-volume-d79608a7-58e8-44c9-9d39-a98181a9d772 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:11:07.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4425" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":305,"completed":252,"skipped":4274,"failed":0}
SS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:11:07.052: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 10 17:11:07.139: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-4031235e-04c7-4ce0-b1ad-c4e084dd372a" in namespace "security-context-test-6887" to be "Succeeded or Failed"
Jun 10 17:11:07.167: INFO: Pod "busybox-privileged-false-4031235e-04c7-4ce0-b1ad-c4e084dd372a": Phase="Pending", Reason="", readiness=false. Elapsed: 28.359335ms
Jun 10 17:11:09.181: INFO: Pod "busybox-privileged-false-4031235e-04c7-4ce0-b1ad-c4e084dd372a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.042507733s
Jun 10 17:11:09.181: INFO: Pod "busybox-privileged-false-4031235e-04c7-4ce0-b1ad-c4e084dd372a" satisfied condition "Succeeded or Failed"
Jun 10 17:11:09.219: INFO: Got logs for pod "busybox-privileged-false-4031235e-04c7-4ce0-b1ad-c4e084dd372a": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:11:09.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6887" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":253,"skipped":4276,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:11:09.254: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 10 17:11:09.347: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:11:11.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-362" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":305,"completed":254,"skipped":4286,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:11:11.387: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jun 10 17:11:11.437: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 10 17:11:11.447: INFO: Waiting for terminating namespaces to be deleted...
Jun 10 17:11:11.451: INFO: 
Logging pods the apiserver thinks is on node target-cluster-control-plane-8m52s before test
Jun 10 17:11:11.459: INFO: calico-kube-controllers-7c5d656c49-x6mc4 from calico-system started at 2021-06-10 15:29:10 +0000 UTC (1 container statuses recorded)
Jun 10 17:11:11.459: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 10 17:11:11.459: INFO: calico-node-phlfp from calico-system started at 2021-06-10 15:28:49 +0000 UTC (1 container statuses recorded)
Jun 10 17:11:11.459: INFO: 	Container calico-node ready: true, restart count 0
Jun 10 17:11:11.459: INFO: calico-typha-5d4587dc9c-7g8gs from calico-system started at 2021-06-10 15:28:49 +0000 UTC (1 container statuses recorded)
Jun 10 17:11:11.459: INFO: 	Container calico-typha ready: true, restart count 0
Jun 10 17:11:11.459: INFO: capd-controller-manager-7c89f6ddd7-dc686 from capd-system started at 2021-06-10 15:31:47 +0000 UTC (2 container statuses recorded)
Jun 10 17:11:11.459: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 10 17:11:11.459: INFO: 	Container manager ready: true, restart count 0
Jun 10 17:11:11.459: INFO: capi-kubeadm-bootstrap-controller-manager-6f669ccd7c-gtbcn from capi-kubeadm-bootstrap-system started at 2021-06-10 15:31:34 +0000 UTC (2 container statuses recorded)
Jun 10 17:11:11.459: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 10 17:11:11.459: INFO: 	Container manager ready: true, restart count 0
Jun 10 17:11:11.459: INFO: capi-kubeadm-control-plane-controller-manager-5c95f59c5c-l4jvx from capi-kubeadm-control-plane-system started at 2021-06-10 15:31:42 +0000 UTC (2 container statuses recorded)
Jun 10 17:11:11.459: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 10 17:11:11.459: INFO: 	Container manager ready: true, restart count 0
Jun 10 17:11:11.459: INFO: capi-controller-manager-5f677d7d65-tzt7h from capi-system started at 2021-06-10 15:31:31 +0000 UTC (2 container statuses recorded)
Jun 10 17:11:11.459: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 10 17:11:11.459: INFO: 	Container manager ready: true, restart count 0
Jun 10 17:11:11.459: INFO: capi-controller-manager-745689557d-77m69 from capi-webhook-system started at 2021-06-10 15:31:29 +0000 UTC (2 container statuses recorded)
Jun 10 17:11:11.459: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 10 17:11:11.459: INFO: 	Container manager ready: true, restart count 0
Jun 10 17:11:11.459: INFO: capi-kubeadm-bootstrap-controller-manager-6949f44db8-hn6kq from capi-webhook-system started at 2021-06-10 15:31:31 +0000 UTC (2 container statuses recorded)
Jun 10 17:11:11.459: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 10 17:11:11.459: INFO: 	Container manager ready: true, restart count 0
Jun 10 17:11:11.459: INFO: capi-kubeadm-control-plane-controller-manager-7b6c4bf48d-x8pzk from capi-webhook-system started at 2021-06-10 15:31:35 +0000 UTC (2 container statuses recorded)
Jun 10 17:11:11.459: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 10 17:11:11.459: INFO: 	Container manager ready: true, restart count 0
Jun 10 17:11:11.459: INFO: cert-manager-768bf64dd4-4lhmj from cert-manager started at 2021-06-10 15:29:10 +0000 UTC (1 container statuses recorded)
Jun 10 17:11:11.459: INFO: 	Container cert-manager ready: true, restart count 0
Jun 10 17:11:11.459: INFO: cert-manager-cainjector-646879549c-pbn4t from cert-manager started at 2021-06-10 15:29:10 +0000 UTC (1 container statuses recorded)
Jun 10 17:11:11.459: INFO: 	Container cert-manager ready: true, restart count 0
Jun 10 17:11:11.459: INFO: cert-manager-webhook-6dc9ccc9fb-6gg4f from cert-manager started at 2021-06-10 15:29:10 +0000 UTC (1 container statuses recorded)
Jun 10 17:11:11.459: INFO: 	Container cert-manager ready: true, restart count 0
Jun 10 17:11:11.459: INFO: coredns-f9fd979d6-2dcsm from kube-system started at 2021-06-10 15:29:10 +0000 UTC (1 container statuses recorded)
Jun 10 17:11:11.459: INFO: 	Container coredns ready: true, restart count 0
Jun 10 17:11:11.459: INFO: coredns-f9fd979d6-zsjpx from kube-system started at 2021-06-10 15:29:21 +0000 UTC (1 container statuses recorded)
Jun 10 17:11:11.459: INFO: 	Container coredns ready: true, restart count 0
Jun 10 17:11:11.460: INFO: etcd-target-cluster-control-plane-8m52s from kube-system started at 2021-06-10 15:22:59 +0000 UTC (1 container statuses recorded)
Jun 10 17:11:11.460: INFO: 	Container etcd ready: true, restart count 0
Jun 10 17:11:11.460: INFO: kube-apiserver-target-cluster-control-plane-8m52s from kube-system started at 2021-06-10 15:22:59 +0000 UTC (1 container statuses recorded)
Jun 10 17:11:11.460: INFO: 	Container kube-apiserver ready: true, restart count 0
Jun 10 17:11:11.460: INFO: kube-controller-manager-target-cluster-control-plane-8m52s from kube-system started at 2021-06-10 15:22:59 +0000 UTC (1 container statuses recorded)
Jun 10 17:11:11.460: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jun 10 17:11:11.460: INFO: kube-proxy-hz9zf from kube-system started at 2021-06-10 15:23:04 +0000 UTC (1 container statuses recorded)
Jun 10 17:11:11.460: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 10 17:11:11.460: INFO: kube-scheduler-target-cluster-control-plane-8m52s from kube-system started at 2021-06-10 15:22:59 +0000 UTC (1 container statuses recorded)
Jun 10 17:11:11.460: INFO: 	Container kube-scheduler ready: true, restart count 0
Jun 10 17:11:11.460: INFO: sonobuoy-systemd-logs-daemon-set-0467f197b1c74b53-bs8vn from sonobuoy started at 2021-06-10 15:38:55 +0000 UTC (2 container statuses recorded)
Jun 10 17:11:11.460: INFO: 	Container sonobuoy-worker ready: false, restart count 10
Jun 10 17:11:11.460: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 10 17:11:11.460: INFO: tigera-operator-5b76777d49-9zvzd from tigera-operator started at 2021-06-10 15:28:37 +0000 UTC (1 container statuses recorded)
Jun 10 17:11:11.460: INFO: 	Container tigera-operator ready: true, restart count 0
Jun 10 17:11:11.460: INFO: 
Logging pods the apiserver thinks is on node target-cluster-md-0-6b59c4f65-5pcx5 before test
Jun 10 17:11:11.467: INFO: calico-node-dvbdj from calico-system started at 2021-06-10 15:34:05 +0000 UTC (1 container statuses recorded)
Jun 10 17:11:11.467: INFO: 	Container calico-node ready: true, restart count 0
Jun 10 17:11:11.467: INFO: calico-typha-5d4587dc9c-lq9tq from calico-system started at 2021-06-10 15:34:47 +0000 UTC (1 container statuses recorded)
Jun 10 17:11:11.467: INFO: 	Container calico-typha ready: true, restart count 0
Jun 10 17:11:11.467: INFO: kube-proxy-dklkq from kube-system started at 2021-06-10 15:34:05 +0000 UTC (1 container statuses recorded)
Jun 10 17:11:11.467: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 10 17:11:11.467: INFO: sonobuoy-e2e-job-04c20f7a477e40c3 from sonobuoy started at 2021-06-10 15:38:55 +0000 UTC (2 container statuses recorded)
Jun 10 17:11:11.467: INFO: 	Container e2e ready: true, restart count 0
Jun 10 17:11:11.467: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 10 17:11:11.467: INFO: sonobuoy-systemd-logs-daemon-set-0467f197b1c74b53-gj6jj from sonobuoy started at 2021-06-10 15:38:55 +0000 UTC (2 container statuses recorded)
Jun 10 17:11:11.467: INFO: 	Container sonobuoy-worker ready: false, restart count 10
Jun 10 17:11:11.467: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 10 17:11:11.467: INFO: 
Logging pods the apiserver thinks is on node target-cluster-md-0-6b59c4f65-cqpjz before test
Jun 10 17:11:11.476: INFO: calico-node-j7mxn from calico-system started at 2021-06-10 15:34:05 +0000 UTC (1 container statuses recorded)
Jun 10 17:11:11.476: INFO: 	Container calico-node ready: true, restart count 0
Jun 10 17:11:11.476: INFO: calico-typha-5d4587dc9c-ckl68 from calico-system started at 2021-06-10 15:34:47 +0000 UTC (1 container statuses recorded)
Jun 10 17:11:11.476: INFO: 	Container calico-typha ready: true, restart count 0
Jun 10 17:11:11.476: INFO: kube-proxy-mc9rx from kube-system started at 2021-06-10 15:34:05 +0000 UTC (1 container statuses recorded)
Jun 10 17:11:11.476: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 10 17:11:11.476: INFO: pod-logs-websocket-5311de78-c5d0-429a-a575-de96e94d13b9 from pods-362 started at 2021-06-10 17:11:09 +0000 UTC (1 container statuses recorded)
Jun 10 17:11:11.476: INFO: 	Container main ready: true, restart count 0
Jun 10 17:11:11.476: INFO: busybox-privileged-false-4031235e-04c7-4ce0-b1ad-c4e084dd372a from security-context-test-6887 started at 2021-06-10 17:11:07 +0000 UTC (1 container statuses recorded)
Jun 10 17:11:11.476: INFO: 	Container busybox-privileged-false-4031235e-04c7-4ce0-b1ad-c4e084dd372a ready: false, restart count 0
Jun 10 17:11:11.476: INFO: sonobuoy from sonobuoy started at 2021-06-10 15:38:49 +0000 UTC (1 container statuses recorded)
Jun 10 17:11:11.476: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 10 17:11:11.476: INFO: sonobuoy-systemd-logs-daemon-set-0467f197b1c74b53-cf9lb from sonobuoy started at 2021-06-10 15:38:55 +0000 UTC (2 container statuses recorded)
Jun 10 17:11:11.476: INFO: 	Container sonobuoy-worker ready: false, restart count 10
Jun 10 17:11:11.476: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-c5205840-3413-42b0-940e-da8fc301dc04 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-c5205840-3413-42b0-940e-da8fc301dc04 off the node target-cluster-md-0-6b59c4f65-5pcx5
STEP: verifying the node doesn't have the label kubernetes.io/e2e-c5205840-3413-42b0-940e-da8fc301dc04
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:16:17.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4985" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:306.238 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":305,"completed":255,"skipped":4316,"failed":0}
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:16:17.625: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on node default medium
Jun 10 17:16:17.693: INFO: Waiting up to 5m0s for pod "pod-f65d24bc-8432-4295-990a-55abfd9fe59d" in namespace "emptydir-3834" to be "Succeeded or Failed"
Jun 10 17:16:17.702: INFO: Pod "pod-f65d24bc-8432-4295-990a-55abfd9fe59d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.707469ms
Jun 10 17:16:19.704: INFO: Pod "pod-f65d24bc-8432-4295-990a-55abfd9fe59d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010713502s
STEP: Saw pod success
Jun 10 17:16:19.704: INFO: Pod "pod-f65d24bc-8432-4295-990a-55abfd9fe59d" satisfied condition "Succeeded or Failed"
Jun 10 17:16:19.705: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod pod-f65d24bc-8432-4295-990a-55abfd9fe59d container test-container: <nil>
STEP: delete the pod
Jun 10 17:16:19.739: INFO: Waiting for pod pod-f65d24bc-8432-4295-990a-55abfd9fe59d to disappear
Jun 10 17:16:19.742: INFO: Pod pod-f65d24bc-8432-4295-990a-55abfd9fe59d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:16:19.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3834" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":256,"skipped":4316,"failed":0}
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:16:19.760: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jun 10 17:16:23.905: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 10 17:16:23.911: INFO: Pod pod-with-prestop-http-hook still exists
Jun 10 17:16:25.911: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 10 17:16:25.921: INFO: Pod pod-with-prestop-http-hook still exists
Jun 10 17:16:27.912: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 10 17:16:27.920: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:16:27.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5404" for this suite.

• [SLOW TEST:8.193 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":305,"completed":257,"skipped":4317,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:16:27.954: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 10 17:16:28.787: INFO: Checking APIGroup: apiregistration.k8s.io
Jun 10 17:16:28.788: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jun 10 17:16:28.788: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Jun 10 17:16:28.788: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jun 10 17:16:28.788: INFO: Checking APIGroup: extensions
Jun 10 17:16:28.790: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Jun 10 17:16:28.790: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Jun 10 17:16:28.790: INFO: extensions/v1beta1 matches extensions/v1beta1
Jun 10 17:16:28.790: INFO: Checking APIGroup: apps
Jun 10 17:16:28.791: INFO: PreferredVersion.GroupVersion: apps/v1
Jun 10 17:16:28.791: INFO: Versions found [{apps/v1 v1}]
Jun 10 17:16:28.791: INFO: apps/v1 matches apps/v1
Jun 10 17:16:28.791: INFO: Checking APIGroup: events.k8s.io
Jun 10 17:16:28.792: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jun 10 17:16:28.792: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Jun 10 17:16:28.792: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jun 10 17:16:28.792: INFO: Checking APIGroup: authentication.k8s.io
Jun 10 17:16:28.793: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jun 10 17:16:28.793: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Jun 10 17:16:28.793: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jun 10 17:16:28.793: INFO: Checking APIGroup: authorization.k8s.io
Jun 10 17:16:28.793: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jun 10 17:16:28.793: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Jun 10 17:16:28.793: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jun 10 17:16:28.793: INFO: Checking APIGroup: autoscaling
Jun 10 17:16:28.794: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Jun 10 17:16:28.794: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Jun 10 17:16:28.794: INFO: autoscaling/v1 matches autoscaling/v1
Jun 10 17:16:28.794: INFO: Checking APIGroup: batch
Jun 10 17:16:28.795: INFO: PreferredVersion.GroupVersion: batch/v1
Jun 10 17:16:28.795: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Jun 10 17:16:28.795: INFO: batch/v1 matches batch/v1
Jun 10 17:16:28.795: INFO: Checking APIGroup: certificates.k8s.io
Jun 10 17:16:28.796: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jun 10 17:16:28.796: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Jun 10 17:16:28.796: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jun 10 17:16:28.796: INFO: Checking APIGroup: networking.k8s.io
Jun 10 17:16:28.797: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jun 10 17:16:28.797: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Jun 10 17:16:28.797: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jun 10 17:16:28.797: INFO: Checking APIGroup: policy
Jun 10 17:16:28.797: INFO: PreferredVersion.GroupVersion: policy/v1beta1
Jun 10 17:16:28.797: INFO: Versions found [{policy/v1beta1 v1beta1}]
Jun 10 17:16:28.797: INFO: policy/v1beta1 matches policy/v1beta1
Jun 10 17:16:28.797: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jun 10 17:16:28.798: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jun 10 17:16:28.798: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Jun 10 17:16:28.798: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jun 10 17:16:28.798: INFO: Checking APIGroup: storage.k8s.io
Jun 10 17:16:28.799: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jun 10 17:16:28.799: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jun 10 17:16:28.799: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jun 10 17:16:28.799: INFO: Checking APIGroup: admissionregistration.k8s.io
Jun 10 17:16:28.800: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jun 10 17:16:28.800: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Jun 10 17:16:28.800: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jun 10 17:16:28.800: INFO: Checking APIGroup: apiextensions.k8s.io
Jun 10 17:16:28.801: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jun 10 17:16:28.801: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Jun 10 17:16:28.801: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jun 10 17:16:28.801: INFO: Checking APIGroup: scheduling.k8s.io
Jun 10 17:16:28.802: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jun 10 17:16:28.802: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
Jun 10 17:16:28.802: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jun 10 17:16:28.802: INFO: Checking APIGroup: coordination.k8s.io
Jun 10 17:16:28.802: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jun 10 17:16:28.802: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Jun 10 17:16:28.802: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jun 10 17:16:28.802: INFO: Checking APIGroup: node.k8s.io
Jun 10 17:16:28.803: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1beta1
Jun 10 17:16:28.803: INFO: Versions found [{node.k8s.io/v1beta1 v1beta1}]
Jun 10 17:16:28.803: INFO: node.k8s.io/v1beta1 matches node.k8s.io/v1beta1
Jun 10 17:16:28.803: INFO: Checking APIGroup: discovery.k8s.io
Jun 10 17:16:28.804: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
Jun 10 17:16:28.804: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
Jun 10 17:16:28.804: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
Jun 10 17:16:28.804: INFO: Checking APIGroup: acme.cert-manager.io
Jun 10 17:16:28.805: INFO: PreferredVersion.GroupVersion: acme.cert-manager.io/v1
Jun 10 17:16:28.806: INFO: Versions found [{acme.cert-manager.io/v1 v1} {acme.cert-manager.io/v1beta1 v1beta1} {acme.cert-manager.io/v1alpha3 v1alpha3} {acme.cert-manager.io/v1alpha2 v1alpha2}]
Jun 10 17:16:28.806: INFO: acme.cert-manager.io/v1 matches acme.cert-manager.io/v1
Jun 10 17:16:28.806: INFO: Checking APIGroup: cert-manager.io
Jun 10 17:16:28.806: INFO: PreferredVersion.GroupVersion: cert-manager.io/v1
Jun 10 17:16:28.806: INFO: Versions found [{cert-manager.io/v1 v1} {cert-manager.io/v1beta1 v1beta1} {cert-manager.io/v1alpha3 v1alpha3} {cert-manager.io/v1alpha2 v1alpha2}]
Jun 10 17:16:28.806: INFO: cert-manager.io/v1 matches cert-manager.io/v1
Jun 10 17:16:28.806: INFO: Checking APIGroup: crd.projectcalico.org
Jun 10 17:16:28.807: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Jun 10 17:16:28.807: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Jun 10 17:16:28.807: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Jun 10 17:16:28.807: INFO: Checking APIGroup: operator.tigera.io
Jun 10 17:16:28.808: INFO: PreferredVersion.GroupVersion: operator.tigera.io/v1
Jun 10 17:16:28.808: INFO: Versions found [{operator.tigera.io/v1 v1}]
Jun 10 17:16:28.808: INFO: operator.tigera.io/v1 matches operator.tigera.io/v1
Jun 10 17:16:28.808: INFO: Checking APIGroup: bootstrap.cluster.x-k8s.io
Jun 10 17:16:28.809: INFO: PreferredVersion.GroupVersion: bootstrap.cluster.x-k8s.io/v1alpha3
Jun 10 17:16:28.809: INFO: Versions found [{bootstrap.cluster.x-k8s.io/v1alpha3 v1alpha3} {bootstrap.cluster.x-k8s.io/v1alpha2 v1alpha2}]
Jun 10 17:16:28.809: INFO: bootstrap.cluster.x-k8s.io/v1alpha3 matches bootstrap.cluster.x-k8s.io/v1alpha3
Jun 10 17:16:28.809: INFO: Checking APIGroup: cluster.x-k8s.io
Jun 10 17:16:28.809: INFO: PreferredVersion.GroupVersion: cluster.x-k8s.io/v1alpha3
Jun 10 17:16:28.809: INFO: Versions found [{cluster.x-k8s.io/v1alpha3 v1alpha3} {cluster.x-k8s.io/v1alpha2 v1alpha2}]
Jun 10 17:16:28.809: INFO: cluster.x-k8s.io/v1alpha3 matches cluster.x-k8s.io/v1alpha3
Jun 10 17:16:28.809: INFO: Checking APIGroup: addons.cluster.x-k8s.io
Jun 10 17:16:28.810: INFO: PreferredVersion.GroupVersion: addons.cluster.x-k8s.io/v1alpha3
Jun 10 17:16:28.810: INFO: Versions found [{addons.cluster.x-k8s.io/v1alpha3 v1alpha3}]
Jun 10 17:16:28.810: INFO: addons.cluster.x-k8s.io/v1alpha3 matches addons.cluster.x-k8s.io/v1alpha3
Jun 10 17:16:28.810: INFO: Checking APIGroup: clusterctl.cluster.x-k8s.io
Jun 10 17:16:28.811: INFO: PreferredVersion.GroupVersion: clusterctl.cluster.x-k8s.io/v1alpha3
Jun 10 17:16:28.811: INFO: Versions found [{clusterctl.cluster.x-k8s.io/v1alpha3 v1alpha3}]
Jun 10 17:16:28.811: INFO: clusterctl.cluster.x-k8s.io/v1alpha3 matches clusterctl.cluster.x-k8s.io/v1alpha3
Jun 10 17:16:28.811: INFO: Checking APIGroup: controlplane.cluster.x-k8s.io
Jun 10 17:16:28.811: INFO: PreferredVersion.GroupVersion: controlplane.cluster.x-k8s.io/v1alpha3
Jun 10 17:16:28.811: INFO: Versions found [{controlplane.cluster.x-k8s.io/v1alpha3 v1alpha3}]
Jun 10 17:16:28.811: INFO: controlplane.cluster.x-k8s.io/v1alpha3 matches controlplane.cluster.x-k8s.io/v1alpha3
Jun 10 17:16:28.811: INFO: Checking APIGroup: exp.cluster.x-k8s.io
Jun 10 17:16:28.812: INFO: PreferredVersion.GroupVersion: exp.cluster.x-k8s.io/v1alpha3
Jun 10 17:16:28.812: INFO: Versions found [{exp.cluster.x-k8s.io/v1alpha3 v1alpha3}]
Jun 10 17:16:28.812: INFO: exp.cluster.x-k8s.io/v1alpha3 matches exp.cluster.x-k8s.io/v1alpha3
Jun 10 17:16:28.812: INFO: Checking APIGroup: exp.infrastructure.cluster.x-k8s.io
Jun 10 17:16:28.813: INFO: PreferredVersion.GroupVersion: exp.infrastructure.cluster.x-k8s.io/v1alpha3
Jun 10 17:16:28.813: INFO: Versions found [{exp.infrastructure.cluster.x-k8s.io/v1alpha3 v1alpha3}]
Jun 10 17:16:28.813: INFO: exp.infrastructure.cluster.x-k8s.io/v1alpha3 matches exp.infrastructure.cluster.x-k8s.io/v1alpha3
Jun 10 17:16:28.813: INFO: Checking APIGroup: infrastructure.cluster.x-k8s.io
Jun 10 17:16:28.814: INFO: PreferredVersion.GroupVersion: infrastructure.cluster.x-k8s.io/v1alpha3
Jun 10 17:16:28.814: INFO: Versions found [{infrastructure.cluster.x-k8s.io/v1alpha3 v1alpha3}]
Jun 10 17:16:28.814: INFO: infrastructure.cluster.x-k8s.io/v1alpha3 matches infrastructure.cluster.x-k8s.io/v1alpha3
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:16:28.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-7223" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":305,"completed":258,"skipped":4375,"failed":0}
S
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:16:28.838: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Jun 10 17:16:28.884: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the sample API server.
Jun 10 17:16:30.626: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jun 10 17:16:32.732: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942190, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942190, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942190, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942190, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 10 17:16:34.736: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942190, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942190, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942190, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942190, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 10 17:16:36.739: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942190, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942190, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942190, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942190, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 10 17:16:38.745: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942190, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942190, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942190, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942190, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 10 17:16:40.736: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942190, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942190, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942190, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942190, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 10 17:16:42.739: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942190, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942190, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942190, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942190, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 10 17:16:47.962: INFO: Waited 3.212096803s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:16:49.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-2359" for this suite.

• [SLOW TEST:20.767 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":305,"completed":259,"skipped":4376,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:16:49.606: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-81093af8-6ea4-46e4-85cc-19541f5a09e0 in namespace container-probe-802
Jun 10 17:16:53.776: INFO: Started pod liveness-81093af8-6ea4-46e4-85cc-19541f5a09e0 in namespace container-probe-802
STEP: checking the pod's current state and verifying that restartCount is present
Jun 10 17:16:53.781: INFO: Initial restart count of pod liveness-81093af8-6ea4-46e4-85cc-19541f5a09e0 is 0
Jun 10 17:17:05.812: INFO: Restart count of pod container-probe-802/liveness-81093af8-6ea4-46e4-85cc-19541f5a09e0 is now 1 (12.030966854s elapsed)
Jun 10 17:17:25.877: INFO: Restart count of pod container-probe-802/liveness-81093af8-6ea4-46e4-85cc-19541f5a09e0 is now 2 (32.095817256s elapsed)
Jun 10 17:17:45.913: INFO: Restart count of pod container-probe-802/liveness-81093af8-6ea4-46e4-85cc-19541f5a09e0 is now 3 (52.131786696s elapsed)
Jun 10 17:18:05.960: INFO: Restart count of pod container-probe-802/liveness-81093af8-6ea4-46e4-85cc-19541f5a09e0 is now 4 (1m12.179539649s elapsed)
Jun 10 17:19:06.090: INFO: Restart count of pod container-probe-802/liveness-81093af8-6ea4-46e4-85cc-19541f5a09e0 is now 5 (2m12.309597888s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:19:06.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-802" for this suite.

• [SLOW TEST:136.527 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":305,"completed":260,"skipped":4409,"failed":0}
SSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:19:06.134: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun 10 17:19:08.738: INFO: Successfully updated pod "pod-update-activedeadlineseconds-39a0dbb5-2568-451b-afc9-da955a0b0d31"
Jun 10 17:19:08.738: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-39a0dbb5-2568-451b-afc9-da955a0b0d31" in namespace "pods-6632" to be "terminated due to deadline exceeded"
Jun 10 17:19:08.743: INFO: Pod "pod-update-activedeadlineseconds-39a0dbb5-2568-451b-afc9-da955a0b0d31": Phase="Running", Reason="", readiness=true. Elapsed: 4.364662ms
Jun 10 17:19:10.746: INFO: Pod "pod-update-activedeadlineseconds-39a0dbb5-2568-451b-afc9-da955a0b0d31": Phase="Running", Reason="", readiness=true. Elapsed: 2.00739906s
Jun 10 17:19:12.752: INFO: Pod "pod-update-activedeadlineseconds-39a0dbb5-2568-451b-afc9-da955a0b0d31": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.014356679s
Jun 10 17:19:12.753: INFO: Pod "pod-update-activedeadlineseconds-39a0dbb5-2568-451b-afc9-da955a0b0d31" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:19:12.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6632" for this suite.

• [SLOW TEST:6.636 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":305,"completed":261,"skipped":4413,"failed":0}
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:19:12.770: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 10 17:19:12.841: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0dea56ee-ffcf-47eb-a6d6-73ff29d12c54" in namespace "projected-3392" to be "Succeeded or Failed"
Jun 10 17:19:12.849: INFO: Pod "downwardapi-volume-0dea56ee-ffcf-47eb-a6d6-73ff29d12c54": Phase="Pending", Reason="", readiness=false. Elapsed: 8.61434ms
Jun 10 17:19:14.854: INFO: Pod "downwardapi-volume-0dea56ee-ffcf-47eb-a6d6-73ff29d12c54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013668846s
STEP: Saw pod success
Jun 10 17:19:14.854: INFO: Pod "downwardapi-volume-0dea56ee-ffcf-47eb-a6d6-73ff29d12c54" satisfied condition "Succeeded or Failed"
Jun 10 17:19:14.867: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod downwardapi-volume-0dea56ee-ffcf-47eb-a6d6-73ff29d12c54 container client-container: <nil>
STEP: delete the pod
Jun 10 17:19:14.891: INFO: Waiting for pod downwardapi-volume-0dea56ee-ffcf-47eb-a6d6-73ff29d12c54 to disappear
Jun 10 17:19:14.896: INFO: Pod downwardapi-volume-0dea56ee-ffcf-47eb-a6d6-73ff29d12c54 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:19:14.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3392" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":262,"skipped":4413,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:19:14.906: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-c9fe8300-8ee9-47cd-b6ad-9958481893b4
STEP: Creating a pod to test consume secrets
Jun 10 17:19:14.951: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-32a22717-04d2-4510-b257-fdb1f94cede0" in namespace "projected-4497" to be "Succeeded or Failed"
Jun 10 17:19:14.963: INFO: Pod "pod-projected-secrets-32a22717-04d2-4510-b257-fdb1f94cede0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.594104ms
Jun 10 17:19:16.968: INFO: Pod "pod-projected-secrets-32a22717-04d2-4510-b257-fdb1f94cede0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017064214s
STEP: Saw pod success
Jun 10 17:19:16.969: INFO: Pod "pod-projected-secrets-32a22717-04d2-4510-b257-fdb1f94cede0" satisfied condition "Succeeded or Failed"
Jun 10 17:19:16.971: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod pod-projected-secrets-32a22717-04d2-4510-b257-fdb1f94cede0 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 10 17:19:17.004: INFO: Waiting for pod pod-projected-secrets-32a22717-04d2-4510-b257-fdb1f94cede0 to disappear
Jun 10 17:19:17.006: INFO: Pod pod-projected-secrets-32a22717-04d2-4510-b257-fdb1f94cede0 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:19:17.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4497" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":263,"skipped":4435,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:19:17.012: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-73de1321-a5ee-4ad4-b632-6eb9363ab24a
STEP: Creating a pod to test consume configMaps
Jun 10 17:19:17.065: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fc738dc2-0168-41a6-8853-a9af49a1bdd0" in namespace "projected-9896" to be "Succeeded or Failed"
Jun 10 17:19:17.071: INFO: Pod "pod-projected-configmaps-fc738dc2-0168-41a6-8853-a9af49a1bdd0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.199617ms
Jun 10 17:19:19.078: INFO: Pod "pod-projected-configmaps-fc738dc2-0168-41a6-8853-a9af49a1bdd0": Phase="Running", Reason="", readiness=true. Elapsed: 2.01333129s
Jun 10 17:19:21.085: INFO: Pod "pod-projected-configmaps-fc738dc2-0168-41a6-8853-a9af49a1bdd0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019778122s
STEP: Saw pod success
Jun 10 17:19:21.085: INFO: Pod "pod-projected-configmaps-fc738dc2-0168-41a6-8853-a9af49a1bdd0" satisfied condition "Succeeded or Failed"
Jun 10 17:19:21.093: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod pod-projected-configmaps-fc738dc2-0168-41a6-8853-a9af49a1bdd0 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 10 17:19:21.122: INFO: Waiting for pod pod-projected-configmaps-fc738dc2-0168-41a6-8853-a9af49a1bdd0 to disappear
Jun 10 17:19:21.126: INFO: Pod pod-projected-configmaps-fc738dc2-0168-41a6-8853-a9af49a1bdd0 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:19:21.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9896" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":305,"completed":264,"skipped":4437,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:19:21.141: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating secret secrets-7182/secret-test-32ec777a-36dd-4e60-bba9-0e946bdad360
STEP: Creating a pod to test consume secrets
Jun 10 17:19:21.237: INFO: Waiting up to 5m0s for pod "pod-configmaps-6838fa10-2105-44f5-a334-d04feb37408a" in namespace "secrets-7182" to be "Succeeded or Failed"
Jun 10 17:19:21.246: INFO: Pod "pod-configmaps-6838fa10-2105-44f5-a334-d04feb37408a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.187613ms
Jun 10 17:19:23.249: INFO: Pod "pod-configmaps-6838fa10-2105-44f5-a334-d04feb37408a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012412892s
STEP: Saw pod success
Jun 10 17:19:23.249: INFO: Pod "pod-configmaps-6838fa10-2105-44f5-a334-d04feb37408a" satisfied condition "Succeeded or Failed"
Jun 10 17:19:23.251: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod pod-configmaps-6838fa10-2105-44f5-a334-d04feb37408a container env-test: <nil>
STEP: delete the pod
Jun 10 17:19:23.306: INFO: Waiting for pod pod-configmaps-6838fa10-2105-44f5-a334-d04feb37408a to disappear
Jun 10 17:19:23.310: INFO: Pod pod-configmaps-6838fa10-2105-44f5-a334-d04feb37408a no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:19:23.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7182" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":305,"completed":265,"skipped":4444,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:19:23.337: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-3863
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 10 17:19:23.404: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun 10 17:19:23.500: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 10 17:19:25.512: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 10 17:19:27.518: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 17:19:29.503: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 17:19:31.515: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 17:19:33.503: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 17:19:35.507: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 17:19:37.504: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 17:19:39.503: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 17:19:41.503: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 17:19:43.502: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jun 10 17:19:43.506: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jun 10 17:19:43.510: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Jun 10 17:19:45.543: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.251.48 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3863 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 10 17:19:45.543: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
Jun 10 17:19:46.678: INFO: Found all expected endpoints: [netserver-0]
Jun 10 17:19:46.682: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.24.179 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3863 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 10 17:19:46.682: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
Jun 10 17:19:47.765: INFO: Found all expected endpoints: [netserver-1]
Jun 10 17:19:47.767: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.78.119 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3863 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 10 17:19:47.767: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
Jun 10 17:19:48.842: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:19:48.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3863" for this suite.

• [SLOW TEST:25.516 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":266,"skipped":4454,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:19:48.853: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-fd93d750-7586-4c30-a68e-0a13d498acf4
STEP: Creating a pod to test consume secrets
Jun 10 17:19:48.933: INFO: Waiting up to 5m0s for pod "pod-secrets-fecca424-9300-4a01-8a33-0be9da78d9b9" in namespace "secrets-3997" to be "Succeeded or Failed"
Jun 10 17:19:48.937: INFO: Pod "pod-secrets-fecca424-9300-4a01-8a33-0be9da78d9b9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.763992ms
Jun 10 17:19:50.941: INFO: Pod "pod-secrets-fecca424-9300-4a01-8a33-0be9da78d9b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007692722s
STEP: Saw pod success
Jun 10 17:19:50.941: INFO: Pod "pod-secrets-fecca424-9300-4a01-8a33-0be9da78d9b9" satisfied condition "Succeeded or Failed"
Jun 10 17:19:50.943: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-5pcx5 pod pod-secrets-fecca424-9300-4a01-8a33-0be9da78d9b9 container secret-volume-test: <nil>
STEP: delete the pod
Jun 10 17:19:50.983: INFO: Waiting for pod pod-secrets-fecca424-9300-4a01-8a33-0be9da78d9b9 to disappear
Jun 10 17:19:50.993: INFO: Pod pod-secrets-fecca424-9300-4a01-8a33-0be9da78d9b9 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:19:50.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3997" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":267,"skipped":4496,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:19:51.007: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
Jun 10 17:19:51.128: INFO: created test-event-1
Jun 10 17:19:51.133: INFO: created test-event-2
Jun 10 17:19:51.141: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Jun 10 17:19:51.144: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Jun 10 17:19:51.177: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:19:51.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7400" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":305,"completed":268,"skipped":4514,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:19:51.190: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:19:51.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8020" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":305,"completed":269,"skipped":4528,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:19:51.272: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:19:51.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-38" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":305,"completed":270,"skipped":4547,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:19:51.406: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 10 17:19:51.471: INFO: Waiting up to 5m0s for pod "downwardapi-volume-85204ac1-7f1c-432c-960e-445d9fc084cf" in namespace "downward-api-5733" to be "Succeeded or Failed"
Jun 10 17:19:51.482: INFO: Pod "downwardapi-volume-85204ac1-7f1c-432c-960e-445d9fc084cf": Phase="Pending", Reason="", readiness=false. Elapsed: 11.406065ms
Jun 10 17:19:53.503: INFO: Pod "downwardapi-volume-85204ac1-7f1c-432c-960e-445d9fc084cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032520133s
STEP: Saw pod success
Jun 10 17:19:53.503: INFO: Pod "downwardapi-volume-85204ac1-7f1c-432c-960e-445d9fc084cf" satisfied condition "Succeeded or Failed"
Jun 10 17:19:53.516: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-5pcx5 pod downwardapi-volume-85204ac1-7f1c-432c-960e-445d9fc084cf container client-container: <nil>
STEP: delete the pod
Jun 10 17:19:53.541: INFO: Waiting for pod downwardapi-volume-85204ac1-7f1c-432c-960e-445d9fc084cf to disappear
Jun 10 17:19:53.545: INFO: Pod downwardapi-volume-85204ac1-7f1c-432c-960e-445d9fc084cf no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:19:53.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5733" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":305,"completed":271,"skipped":4565,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:19:53.558: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:20:09.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-186" for this suite.

• [SLOW TEST:16.142 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":305,"completed":272,"skipped":4591,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:20:09.701: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's command
Jun 10 17:20:09.751: INFO: Waiting up to 5m0s for pod "var-expansion-150fb0f7-4689-494a-916d-70934f71b03b" in namespace "var-expansion-3787" to be "Succeeded or Failed"
Jun 10 17:20:09.774: INFO: Pod "var-expansion-150fb0f7-4689-494a-916d-70934f71b03b": Phase="Pending", Reason="", readiness=false. Elapsed: 23.365905ms
Jun 10 17:20:11.778: INFO: Pod "var-expansion-150fb0f7-4689-494a-916d-70934f71b03b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027008336s
STEP: Saw pod success
Jun 10 17:20:11.778: INFO: Pod "var-expansion-150fb0f7-4689-494a-916d-70934f71b03b" satisfied condition "Succeeded or Failed"
Jun 10 17:20:11.781: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod var-expansion-150fb0f7-4689-494a-916d-70934f71b03b container dapi-container: <nil>
STEP: delete the pod
Jun 10 17:20:11.805: INFO: Waiting for pod var-expansion-150fb0f7-4689-494a-916d-70934f71b03b to disappear
Jun 10 17:20:11.809: INFO: Pod var-expansion-150fb0f7-4689-494a-916d-70934f71b03b no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:20:11.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3787" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":305,"completed":273,"skipped":4619,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:20:11.819: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:20:11.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7083" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":305,"completed":274,"skipped":4638,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:20:11.914: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:20:13.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-451" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":275,"skipped":4645,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:20:13.992: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 10 17:20:14.973: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jun 10 17:20:17.138: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942414, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942414, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942415, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942414, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 10 17:20:20.167: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 10 17:20:20.177: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9436-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:20:21.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9691" for this suite.
STEP: Destroying namespace "webhook-9691-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.481 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":305,"completed":276,"skipped":4668,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:20:21.473: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Jun 10 17:20:21.538: INFO: Waiting up to 5m0s for pod "downward-api-4c11aef3-1ae3-4963-b367-4fc0a145a337" in namespace "downward-api-3588" to be "Succeeded or Failed"
Jun 10 17:20:21.541: INFO: Pod "downward-api-4c11aef3-1ae3-4963-b367-4fc0a145a337": Phase="Pending", Reason="", readiness=false. Elapsed: 2.978412ms
Jun 10 17:20:23.549: INFO: Pod "downward-api-4c11aef3-1ae3-4963-b367-4fc0a145a337": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010210144s
Jun 10 17:20:25.564: INFO: Pod "downward-api-4c11aef3-1ae3-4963-b367-4fc0a145a337": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025704442s
STEP: Saw pod success
Jun 10 17:20:25.564: INFO: Pod "downward-api-4c11aef3-1ae3-4963-b367-4fc0a145a337" satisfied condition "Succeeded or Failed"
Jun 10 17:20:25.570: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-5pcx5 pod downward-api-4c11aef3-1ae3-4963-b367-4fc0a145a337 container dapi-container: <nil>
STEP: delete the pod
Jun 10 17:20:25.620: INFO: Waiting for pod downward-api-4c11aef3-1ae3-4963-b367-4fc0a145a337 to disappear
Jun 10 17:20:25.667: INFO: Pod downward-api-4c11aef3-1ae3-4963-b367-4fc0a145a337 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:20:25.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3588" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":305,"completed":277,"skipped":4701,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:20:25.724: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jun 10 17:20:31.939: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 10 17:20:31.956: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 10 17:20:33.957: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 10 17:20:33.961: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 10 17:20:35.957: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 10 17:20:35.970: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:20:35.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2328" for this suite.

• [SLOW TEST:10.294 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":305,"completed":278,"skipped":4735,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:20:36.018: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:20:52.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2113" for this suite.

• [SLOW TEST:16.273 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":305,"completed":279,"skipped":4753,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:20:52.293: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap that has name configmap-test-emptyKey-99d0e761-c04e-436e-95f0-985628d42490
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:20:52.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5762" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":305,"completed":280,"skipped":4779,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:20:52.403: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 10 17:20:54.494: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:20:54.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2580" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":281,"skipped":4800,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:20:54.535: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:21:11.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6412" for this suite.

• [SLOW TEST:17.117 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":305,"completed":282,"skipped":4801,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:21:11.654: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 10 17:21:11.699: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a02bef8d-f2f3-4566-b6f4-dce3299ea8a0" in namespace "projected-8805" to be "Succeeded or Failed"
Jun 10 17:21:11.701: INFO: Pod "downwardapi-volume-a02bef8d-f2f3-4566-b6f4-dce3299ea8a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00988ms
Jun 10 17:21:13.703: INFO: Pod "downwardapi-volume-a02bef8d-f2f3-4566-b6f4-dce3299ea8a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004537969s
STEP: Saw pod success
Jun 10 17:21:13.703: INFO: Pod "downwardapi-volume-a02bef8d-f2f3-4566-b6f4-dce3299ea8a0" satisfied condition "Succeeded or Failed"
Jun 10 17:21:13.706: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod downwardapi-volume-a02bef8d-f2f3-4566-b6f4-dce3299ea8a0 container client-container: <nil>
STEP: delete the pod
Jun 10 17:21:13.726: INFO: Waiting for pod downwardapi-volume-a02bef8d-f2f3-4566-b6f4-dce3299ea8a0 to disappear
Jun 10 17:21:13.735: INFO: Pod downwardapi-volume-a02bef8d-f2f3-4566-b6f4-dce3299ea8a0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:21:13.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8805" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":305,"completed":283,"skipped":4888,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:21:13.752: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 10 17:21:14.450: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 10 17:21:16.491: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942474, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942474, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942474, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942474, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 10 17:21:19.507: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:21:19.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2953" for this suite.
STEP: Destroying namespace "webhook-2953-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.012 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":305,"completed":284,"skipped":4890,"failed":0}
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:21:19.764: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Jun 10 17:21:19.817: INFO: PodSpec: initContainers in spec.initContainers
Jun 10 17:22:04.855: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-170dff9b-d3ad-4d1e-bfaf-52b7f6771641", GenerateName:"", Namespace:"init-container-6177", SelfLink:"/api/v1/namespaces/init-container-6177/pods/pod-init-170dff9b-d3ad-4d1e-bfaf-52b7f6771641", UID:"5ea5a466-97ee-4eef-b025-06ddbc9b84a6", ResourceVersion:"58041", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63758942479, loc:(*time.Location)(0x770e980)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"817080509"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"192.168.78.66/32", "cni.projectcalico.org/podIPs":"192.168.78.66/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc003c32800), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003c32820)}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc003c32840), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003c32860)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc003c32880), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003c328a0)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-2qzt9", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc002d69500), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-2qzt9", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-2qzt9", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-2qzt9", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc008af9008), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"target-cluster-md-0-6b59c4f65-cqpjz", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002dd7c70), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc008af9090)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc008af90b0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc008af90b8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc008af90bc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc005f6ab30), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942479, loc:(*time.Location)(0x770e980)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942479, loc:(*time.Location)(0x770e980)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942479, loc:(*time.Location)(0x770e980)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942479, loc:(*time.Location)(0x770e980)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.17.0.5", PodIP:"192.168.78.66", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.78.66"}}, StartTime:(*v1.Time)(0xc003c328c0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002dd7dc0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002dd7e30)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"containerd://5b275a4d2275719e31db8c88e7207114ff5fd42c305b4ad27c64ca77640044aa", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003c32900), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003c328e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc008af914f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:22:04.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6177" for this suite.

• [SLOW TEST:45.115 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":305,"completed":285,"skipped":4891,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:22:04.880: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-c8611b7d-0da1-415f-b6bd-de4a1170f87b
STEP: Creating a pod to test consume configMaps
Jun 10 17:22:04.967: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-31af3233-1653-4128-b26a-a6f78dc6f675" in namespace "projected-6918" to be "Succeeded or Failed"
Jun 10 17:22:04.982: INFO: Pod "pod-projected-configmaps-31af3233-1653-4128-b26a-a6f78dc6f675": Phase="Pending", Reason="", readiness=false. Elapsed: 15.012971ms
Jun 10 17:22:06.998: INFO: Pod "pod-projected-configmaps-31af3233-1653-4128-b26a-a6f78dc6f675": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031529401s
Jun 10 17:22:09.003: INFO: Pod "pod-projected-configmaps-31af3233-1653-4128-b26a-a6f78dc6f675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036554907s
STEP: Saw pod success
Jun 10 17:22:09.003: INFO: Pod "pod-projected-configmaps-31af3233-1653-4128-b26a-a6f78dc6f675" satisfied condition "Succeeded or Failed"
Jun 10 17:22:09.008: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod pod-projected-configmaps-31af3233-1653-4128-b26a-a6f78dc6f675 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 10 17:22:09.052: INFO: Waiting for pod pod-projected-configmaps-31af3233-1653-4128-b26a-a6f78dc6f675 to disappear
Jun 10 17:22:09.061: INFO: Pod pod-projected-configmaps-31af3233-1653-4128-b26a-a6f78dc6f675 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:22:09.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6918" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":286,"skipped":4900,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:22:09.101: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Jun 10 17:22:09.224: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:22:12.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-46" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":305,"completed":287,"skipped":4919,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:22:12.625: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 10 17:22:13.590: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 10 17:22:15.639: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942533, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942533, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942533, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942533, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 10 17:22:18.657: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:22:18.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4914" for this suite.
STEP: Destroying namespace "webhook-4914-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.444 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":305,"completed":288,"skipped":4936,"failed":0}
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:22:19.070: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-8731
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-8731
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8731
Jun 10 17:22:19.218: INFO: Found 0 stateful pods, waiting for 1
Jun 10 17:22:29.221: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jun 10 17:22:29.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-8731 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 10 17:22:29.696: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 10 17:22:29.696: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 10 17:22:29.696: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 10 17:22:29.700: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun 10 17:22:39.704: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 10 17:22:39.704: INFO: Waiting for statefulset status.replicas updated to 0
Jun 10 17:22:39.716: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999707s
Jun 10 17:22:40.719: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995825729s
Jun 10 17:22:41.724: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.992745062s
Jun 10 17:22:42.732: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.987789303s
Jun 10 17:22:43.734: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.980502315s
Jun 10 17:22:44.738: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.977144084s
Jun 10 17:22:45.771: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.961647934s
Jun 10 17:22:46.774: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.9411704s
Jun 10 17:22:47.778: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.93736215s
Jun 10 17:22:48.780: INFO: Verifying statefulset ss doesn't scale past 1 for another 934.32436ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8731
Jun 10 17:22:49.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-8731 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 17:22:50.007: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 10 17:22:50.007: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 10 17:22:50.007: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 10 17:22:50.010: INFO: Found 1 stateful pods, waiting for 3
Jun 10 17:23:00.017: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 10 17:23:00.017: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 10 17:23:00.017: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jun 10 17:23:00.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-8731 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 10 17:23:00.388: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 10 17:23:00.388: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 10 17:23:00.388: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 10 17:23:00.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-8731 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 10 17:23:00.780: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 10 17:23:00.780: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 10 17:23:00.780: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 10 17:23:00.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-8731 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 10 17:23:01.411: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 10 17:23:01.411: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 10 17:23:01.411: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 10 17:23:01.411: INFO: Waiting for statefulset status.replicas updated to 0
Jun 10 17:23:01.420: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Jun 10 17:23:11.429: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 10 17:23:11.429: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun 10 17:23:11.429: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun 10 17:23:11.471: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999728s
Jun 10 17:23:12.486: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992964873s
Jun 10 17:23:13.494: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.977908551s
Jun 10 17:23:14.498: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.969800247s
Jun 10 17:23:15.502: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.966336471s
Jun 10 17:23:16.507: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.961280556s
Jun 10 17:23:17.517: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.957626645s
Jun 10 17:23:18.524: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.944577426s
Jun 10 17:23:19.530: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.940354401s
Jun 10 17:23:20.539: INFO: Verifying statefulset ss doesn't scale past 3 for another 928.575542ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8731
Jun 10 17:23:21.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-8731 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 17:23:21.815: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 10 17:23:21.815: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 10 17:23:21.815: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 10 17:23:21.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-8731 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 17:23:22.163: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 10 17:23:22.163: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 10 17:23:22.163: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 10 17:23:22.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=statefulset-8731 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 17:23:22.425: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 10 17:23:22.425: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 10 17:23:22.425: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 10 17:23:22.425: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jun 10 17:23:52.451: INFO: Deleting all statefulset in ns statefulset-8731
Jun 10 17:23:52.458: INFO: Scaling statefulset ss to 0
Jun 10 17:23:52.474: INFO: Waiting for statefulset status.replicas updated to 0
Jun 10 17:23:52.477: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:23:52.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8731" for this suite.

• [SLOW TEST:93.427 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":305,"completed":289,"skipped":4942,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:23:52.497: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 10 17:23:53.130: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 10 17:23:56.149: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 10 17:23:56.157: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6154-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:23:57.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1688" for this suite.
STEP: Destroying namespace "webhook-1688-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.234 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":305,"completed":290,"skipped":4960,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:23:57.733: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-29239e5b-9fd0-4846-9ec5-a7a7d3cb5595
STEP: Creating a pod to test consume configMaps
Jun 10 17:23:57.893: INFO: Waiting up to 5m0s for pod "pod-configmaps-b3a6114a-2570-4649-a457-4e229bb62b21" in namespace "configmap-2993" to be "Succeeded or Failed"
Jun 10 17:23:57.906: INFO: Pod "pod-configmaps-b3a6114a-2570-4649-a457-4e229bb62b21": Phase="Pending", Reason="", readiness=false. Elapsed: 12.618118ms
Jun 10 17:23:59.915: INFO: Pod "pod-configmaps-b3a6114a-2570-4649-a457-4e229bb62b21": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021934151s
Jun 10 17:24:01.920: INFO: Pod "pod-configmaps-b3a6114a-2570-4649-a457-4e229bb62b21": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02606126s
STEP: Saw pod success
Jun 10 17:24:01.920: INFO: Pod "pod-configmaps-b3a6114a-2570-4649-a457-4e229bb62b21" satisfied condition "Succeeded or Failed"
Jun 10 17:24:01.937: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod pod-configmaps-b3a6114a-2570-4649-a457-4e229bb62b21 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 10 17:24:01.997: INFO: Waiting for pod pod-configmaps-b3a6114a-2570-4649-a457-4e229bb62b21 to disappear
Jun 10 17:24:02.008: INFO: Pod pod-configmaps-b3a6114a-2570-4649-a457-4e229bb62b21 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:24:02.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2993" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":305,"completed":291,"skipped":5000,"failed":0}
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:24:02.022: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Jun 10 17:24:02.100: INFO: Waiting up to 5m0s for pod "downward-api-d417abde-4fb7-4fe4-b931-9f2e99847be8" in namespace "downward-api-4888" to be "Succeeded or Failed"
Jun 10 17:24:02.118: INFO: Pod "downward-api-d417abde-4fb7-4fe4-b931-9f2e99847be8": Phase="Pending", Reason="", readiness=false. Elapsed: 17.928103ms
Jun 10 17:24:04.122: INFO: Pod "downward-api-d417abde-4fb7-4fe4-b931-9f2e99847be8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022538955s
Jun 10 17:24:06.130: INFO: Pod "downward-api-d417abde-4fb7-4fe4-b931-9f2e99847be8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030345637s
STEP: Saw pod success
Jun 10 17:24:06.130: INFO: Pod "downward-api-d417abde-4fb7-4fe4-b931-9f2e99847be8" satisfied condition "Succeeded or Failed"
Jun 10 17:24:06.136: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod downward-api-d417abde-4fb7-4fe4-b931-9f2e99847be8 container dapi-container: <nil>
STEP: delete the pod
Jun 10 17:24:06.203: INFO: Waiting for pod downward-api-d417abde-4fb7-4fe4-b931-9f2e99847be8 to disappear
Jun 10 17:24:06.208: INFO: Pod downward-api-d417abde-4fb7-4fe4-b931-9f2e99847be8 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:24:06.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4888" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":305,"completed":292,"skipped":5007,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:24:06.230: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0610 17:24:06.918181      26 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jun 10 17:25:08.944: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:25:08.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1327" for this suite.

• [SLOW TEST:62.736 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":305,"completed":293,"skipped":5013,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:25:08.966: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 10 17:25:09.002: INFO: Creating deployment "webserver-deployment"
Jun 10 17:25:09.006: INFO: Waiting for observed generation 1
Jun 10 17:25:11.104: INFO: Waiting for all required pods to come up
Jun 10 17:25:11.248: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Jun 10 17:25:15.330: INFO: Waiting for deployment "webserver-deployment" to complete
Jun 10 17:25:15.337: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jun 10 17:25:15.351: INFO: Updating deployment webserver-deployment
Jun 10 17:25:15.352: INFO: Waiting for observed generation 2
Jun 10 17:25:17.415: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jun 10 17:25:17.454: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jun 10 17:25:17.458: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jun 10 17:25:17.499: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jun 10 17:25:17.499: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jun 10 17:25:17.517: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jun 10 17:25:17.588: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jun 10 17:25:17.588: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jun 10 17:25:17.622: INFO: Updating deployment webserver-deployment
Jun 10 17:25:17.623: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jun 10 17:25:17.678: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jun 10 17:25:17.701: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Jun 10 17:25:18.412: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-1367 /apis/apps/v1/namespaces/deployment-1367/deployments/webserver-deployment 1a8ac1d5-2740-4b96-9935-9cc8cd9574e5 59926 3 2021-06-10 17:25:09 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-06-10 17:25:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-06-10 17:25:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0052e9c98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-06-10 17:25:16 +0000 UTC,LastTransitionTime:2021-06-10 17:25:09 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-06-10 17:25:17 +0000 UTC,LastTransitionTime:2021-06-10 17:25:17 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jun 10 17:25:18.897: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-1367 /apis/apps/v1/namespaces/deployment-1367/replicasets/webserver-deployment-795d758f88 9c654d3d-faef-4ba0-a0cf-4daa01193bec 59961 3 2021-06-10 17:25:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 1a8ac1d5-2740-4b96-9935-9cc8cd9574e5 0xc009541207 0xc009541208}] []  [{kube-controller-manager Update apps/v1 2021-06-10 17:25:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a8ac1d5-2740-4b96-9935-9cc8cd9574e5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009541288 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 10 17:25:18.897: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jun 10 17:25:18.897: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-dd94f59b7  deployment-1367 /apis/apps/v1/namespaces/deployment-1367/replicasets/webserver-deployment-dd94f59b7 fec51ba3-b6ab-478b-a26f-959358cf739e 59960 3 2021-06-10 17:25:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 1a8ac1d5-2740-4b96-9935-9cc8cd9574e5 0xc0095412e7 0xc0095412e8}] []  [{kube-controller-manager Update apps/v1 2021-06-10 17:25:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a8ac1d5-2740-4b96-9935-9cc8cd9574e5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: dd94f59b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009541378 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jun 10 17:25:19.024: INFO: Pod "webserver-deployment-795d758f88-4pgkc" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-4pgkc webserver-deployment-795d758f88- deployment-1367 /api/v1/namespaces/deployment-1367/pods/webserver-deployment-795d758f88-4pgkc 0a672b54-749f-463a-9757-dfc036c069b6 59935 0 2021-06-10 17:25:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 9c654d3d-faef-4ba0-a0cf-4daa01193bec 0xc00938e147 0xc00938e148}] []  [{kube-controller-manager Update v1 2021-06-10 17:25:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9c654d3d-faef-4ba0-a0cf-4daa01193bec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6b228,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6b228,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6b228,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:target-cluster-md-0-6b59c4f65-cqpjz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 17:25:19.024: INFO: Pod "webserver-deployment-795d758f88-7ltbx" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-7ltbx webserver-deployment-795d758f88- deployment-1367 /api/v1/namespaces/deployment-1367/pods/webserver-deployment-795d758f88-7ltbx bac8a40a-7607-4ae9-a56d-b760401a8b53 59945 0 2021-06-10 17:25:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 9c654d3d-faef-4ba0-a0cf-4daa01193bec 0xc00938e270 0xc00938e271}] []  [{kube-controller-manager Update v1 2021-06-10 17:25:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9c654d3d-faef-4ba0-a0cf-4daa01193bec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6b228,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6b228,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6b228,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:target-cluster-md-0-6b59c4f65-cqpjz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 17:25:19.025: INFO: Pod "webserver-deployment-795d758f88-7smtw" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-7smtw webserver-deployment-795d758f88- deployment-1367 /api/v1/namespaces/deployment-1367/pods/webserver-deployment-795d758f88-7smtw 01da7cf1-17fa-48dd-b40e-7cb8594dfa2e 59889 0 2021-06-10 17:25:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:192.168.251.51/32 cni.projectcalico.org/podIPs:192.168.251.51/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 9c654d3d-faef-4ba0-a0cf-4daa01193bec 0xc00938e3a0 0xc00938e3a1}] []  [{kube-controller-manager Update v1 2021-06-10 17:25:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9c654d3d-faef-4ba0-a0cf-4daa01193bec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-10 17:25:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-06-10 17:25:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6b228,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6b228,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6b228,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:target-cluster-control-plane-8m52s,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.17.0.4,PodIP:,StartTime:2021-06-10 17:25:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 17:25:19.025: INFO: Pod "webserver-deployment-795d758f88-886qm" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-886qm webserver-deployment-795d758f88- deployment-1367 /api/v1/namespaces/deployment-1367/pods/webserver-deployment-795d758f88-886qm 63df141f-58d3-40b1-88c2-2ca422ce286e 59901 0 2021-06-10 17:25:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:192.168.78.76/32 cni.projectcalico.org/podIPs:192.168.78.76/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 9c654d3d-faef-4ba0-a0cf-4daa01193bec 0xc00938e570 0xc00938e571}] []  [{kube-controller-manager Update v1 2021-06-10 17:25:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9c654d3d-faef-4ba0-a0cf-4daa01193bec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-10 17:25:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-06-10 17:25:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6b228,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6b228,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6b228,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:target-cluster-md-0-6b59c4f65-cqpjz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.17.0.5,PodIP:,StartTime:2021-06-10 17:25:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 17:25:19.025: INFO: Pod "webserver-deployment-795d758f88-8z47p" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-8z47p webserver-deployment-795d758f88- deployment-1367 /api/v1/namespaces/deployment-1367/pods/webserver-deployment-795d758f88-8z47p 30b97666-9591-493a-9535-a21d714bce75 59947 0 2021-06-10 17:25:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 9c654d3d-faef-4ba0-a0cf-4daa01193bec 0xc00938e730 0xc00938e731}] []  [{kube-controller-manager Update v1 2021-06-10 17:25:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9c654d3d-faef-4ba0-a0cf-4daa01193bec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6b228,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6b228,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6b228,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:target-cluster-control-plane-8m52s,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 17:25:19.025: INFO: Pod "webserver-deployment-795d758f88-cphnf" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-cphnf webserver-deployment-795d758f88- deployment-1367 /api/v1/namespaces/deployment-1367/pods/webserver-deployment-795d758f88-cphnf 704851f4-e453-4203-a49a-66da2316117d 59970 0 2021-06-10 17:25:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 9c654d3d-faef-4ba0-a0cf-4daa01193bec 0xc00938e860 0xc00938e861}] []  [{kube-controller-manager Update v1 2021-06-10 17:25:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9c654d3d-faef-4ba0-a0cf-4daa01193bec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-10 17:25:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6b228,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6b228,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6b228,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:target-cluster-md-0-6b59c4f65-cqpjz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.17.0.5,PodIP:,StartTime:2021-06-10 17:25:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 17:25:19.025: INFO: Pod "webserver-deployment-795d758f88-hj7sk" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-hj7sk webserver-deployment-795d758f88- deployment-1367 /api/v1/namespaces/deployment-1367/pods/webserver-deployment-795d758f88-hj7sk 727eaa44-fa54-4d4d-9ec2-377e5edf6f63 59949 0 2021-06-10 17:25:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 9c654d3d-faef-4ba0-a0cf-4daa01193bec 0xc00938e9f0 0xc00938e9f1}] []  [{kube-controller-manager Update v1 2021-06-10 17:25:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9c654d3d-faef-4ba0-a0cf-4daa01193bec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6b228,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6b228,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6b228,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:target-cluster-md-0-6b59c4f65-5pcx5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 17:25:19.025: INFO: Pod "webserver-deployment-795d758f88-k26qg" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-k26qg webserver-deployment-795d758f88- deployment-1367 /api/v1/namespaces/deployment-1367/pods/webserver-deployment-795d758f88-k26qg cdb36a24-afe5-4b7c-9e8c-182eb1aa51a2 59929 0 2021-06-10 17:25:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 9c654d3d-faef-4ba0-a0cf-4daa01193bec 0xc00938eb20 0xc00938eb21}] []  [{kube-controller-manager Update v1 2021-06-10 17:25:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9c654d3d-faef-4ba0-a0cf-4daa01193bec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6b228,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6b228,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6b228,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:target-cluster-md-0-6b59c4f65-5pcx5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 17:25:19.026: INFO: Pod "webserver-deployment-795d758f88-n6vm2" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-n6vm2 webserver-deployment-795d758f88- deployment-1367 /api/v1/namespaces/deployment-1367/pods/webserver-deployment-795d758f88-n6vm2 ccfa9d36-2f4c-4374-b461-054a7f1df415 59940 0 2021-06-10 17:25:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 9c654d3d-faef-4ba0-a0cf-4daa01193bec 0xc00938ec50 0xc00938ec51}] []  [{kube-controller-manager Update v1 2021-06-10 17:25:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9c654d3d-faef-4ba0-a0cf-4daa01193bec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6b228,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6b228,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6b228,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:target-cluster-md-0-6b59c4f65-5pcx5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 17:25:19.026: INFO: Pod "webserver-deployment-795d758f88-nr8hp" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-nr8hp webserver-deployment-795d758f88- deployment-1367 /api/v1/namespaces/deployment-1367/pods/webserver-deployment-795d758f88-nr8hp 22e57011-4bc6-4d41-a6ec-b5d80e8d06ac 59898 0 2021-06-10 17:25:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:192.168.24.130/32 cni.projectcalico.org/podIPs:192.168.24.130/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 9c654d3d-faef-4ba0-a0cf-4daa01193bec 0xc00938ed80 0xc00938ed81}] []  [{kube-controller-manager Update v1 2021-06-10 17:25:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9c654d3d-faef-4ba0-a0cf-4daa01193bec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-10 17:25:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-06-10 17:25:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6b228,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6b228,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6b228,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:target-cluster-md-0-6b59c4f65-5pcx5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.17.0.6,PodIP:,StartTime:2021-06-10 17:25:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 17:25:19.026: INFO: Pod "webserver-deployment-795d758f88-nr9cd" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-nr9cd webserver-deployment-795d758f88- deployment-1367 /api/v1/namespaces/deployment-1367/pods/webserver-deployment-795d758f88-nr9cd d2d4a784-a4b6-470b-aa47-d43b0980ff5d 59893 0 2021-06-10 17:25:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:192.168.78.77/32 cni.projectcalico.org/podIPs:192.168.78.77/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 9c654d3d-faef-4ba0-a0cf-4daa01193bec 0xc00938ef50 0xc00938ef51}] []  [{kube-controller-manager Update v1 2021-06-10 17:25:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9c654d3d-faef-4ba0-a0cf-4daa01193bec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-10 17:25:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-06-10 17:25:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6b228,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6b228,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6b228,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:target-cluster-md-0-6b59c4f65-cqpjz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.17.0.5,PodIP:,StartTime:2021-06-10 17:25:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 17:25:19.026: INFO: Pod "webserver-deployment-795d758f88-rmq4p" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-rmq4p webserver-deployment-795d758f88- deployment-1367 /api/v1/namespaces/deployment-1367/pods/webserver-deployment-795d758f88-rmq4p 8481580d-7a56-43b9-9a36-30ed3d8d0247 59969 0 2021-06-10 17:25:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 9c654d3d-faef-4ba0-a0cf-4daa01193bec 0xc00938f110 0xc00938f111}] []  [{kube-controller-manager Update v1 2021-06-10 17:25:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9c654d3d-faef-4ba0-a0cf-4daa01193bec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6b228,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6b228,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6b228,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:target-cluster-md-0-6b59c4f65-5pcx5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 17:25:19.026: INFO: Pod "webserver-deployment-795d758f88-wx6pt" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-wx6pt webserver-deployment-795d758f88- deployment-1367 /api/v1/namespaces/deployment-1367/pods/webserver-deployment-795d758f88-wx6pt e11dfebb-befe-4fa4-8f33-5fc738d0922b 59890 0 2021-06-10 17:25:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:192.168.24.191/32 cni.projectcalico.org/podIPs:192.168.24.191/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 9c654d3d-faef-4ba0-a0cf-4daa01193bec 0xc00938f240 0xc00938f241}] []  [{kube-controller-manager Update v1 2021-06-10 17:25:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9c654d3d-faef-4ba0-a0cf-4daa01193bec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-10 17:25:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-06-10 17:25:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6b228,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6b228,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6b228,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:target-cluster-md-0-6b59c4f65-5pcx5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.17.0.6,PodIP:,StartTime:2021-06-10 17:25:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 17:25:19.027: INFO: Pod "webserver-deployment-dd94f59b7-4ks5b" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-4ks5b webserver-deployment-dd94f59b7- deployment-1367 /api/v1/namespaces/deployment-1367/pods/webserver-deployment-dd94f59b7-4ks5b ee6fe46f-ae94-48ad-b100-fb1ec8867d2a 59740 0 2021-06-10 17:25:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:192.168.24.187/32 cni.projectcalico.org/podIPs:192.168.24.187/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 fec51ba3-b6ab-478b-a26f-959358cf739e 0xc00938f3f0 0xc00938f3f1}] []  [{kube-controller-manager Update v1 2021-06-10 17:25:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fec51ba3-b6ab-478b-a26f-959358cf739e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-10 17:25:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-06-10 17:25:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.24.187\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6b228,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6b228,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6b228,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:target-cluster-md-0-6b59c4f65-5pcx5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.17.0.6,PodIP:192.168.24.187,StartTime:2021-06-10 17:25:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-10 17:25:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://c869b18a3c6026e8f76f1ef2c792e10e3848748290397d6fc6639586f5d56e49,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.24.187,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 17:25:19.027: INFO: Pod "webserver-deployment-dd94f59b7-5qdwl" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-5qdwl webserver-deployment-dd94f59b7- deployment-1367 /api/v1/namespaces/deployment-1367/pods/webserver-deployment-dd94f59b7-5qdwl 3c5fade4-3940-4bb1-9caf-8ed9ae86a766 59944 0 2021-06-10 17:25:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 fec51ba3-b6ab-478b-a26f-959358cf739e 0xc00938f5a0 0xc00938f5a1}] []  [{kube-controller-manager Update v1 2021-06-10 17:25:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fec51ba3-b6ab-478b-a26f-959358cf739e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6b228,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6b228,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6b228,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:target-cluster-md-0-6b59c4f65-cqpjz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 17:25:19.027: INFO: Pod "webserver-deployment-dd94f59b7-9c4cw" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-9c4cw webserver-deployment-dd94f59b7- deployment-1367 /api/v1/namespaces/deployment-1367/pods/webserver-deployment-dd94f59b7-9c4cw c0d6bdd6-6bf1-46d8-9be2-a2413ea9d319 59958 0 2021-06-10 17:25:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 fec51ba3-b6ab-478b-a26f-959358cf739e 0xc00938f6c0 0xc00938f6c1}] []  [{kube-controller-manager Update v1 2021-06-10 17:25:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fec51ba3-b6ab-478b-a26f-959358cf739e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6b228,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6b228,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6b228,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:target-cluster-md-0-6b59c4f65-cqpjz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 17:25:19.027: INFO: Pod "webserver-deployment-dd94f59b7-bd2zg" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-bd2zg webserver-deployment-dd94f59b7- deployment-1367 /api/v1/namespaces/deployment-1367/pods/webserver-deployment-dd94f59b7-bd2zg 8aa2cf67-2287-4656-ad19-9426d72eb34e 59813 0 2021-06-10 17:25:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:192.168.78.78/32 cni.projectcalico.org/podIPs:192.168.78.78/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 fec51ba3-b6ab-478b-a26f-959358cf739e 0xc00938f800 0xc00938f801}] []  [{kube-controller-manager Update v1 2021-06-10 17:25:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fec51ba3-b6ab-478b-a26f-959358cf739e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-10 17:25:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-06-10 17:25:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.78.78\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6b228,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6b228,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6b228,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:target-cluster-md-0-6b59c4f65-cqpjz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.17.0.5,PodIP:192.168.78.78,StartTime:2021-06-10 17:25:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-10 17:25:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://67d9a2caf0f4d1efc51fa10d6b81202c358001d5cdee28ca58867018469f403d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.78.78,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 17:25:19.028: INFO: Pod "webserver-deployment-dd94f59b7-fwxwk" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-fwxwk webserver-deployment-dd94f59b7- deployment-1367 /api/v1/namespaces/deployment-1367/pods/webserver-deployment-dd94f59b7-fwxwk 8762b50b-5cb3-42ec-bc77-37fc593e95e8 59786 0 2021-06-10 17:25:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:192.168.78.73/32 cni.projectcalico.org/podIPs:192.168.78.73/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 fec51ba3-b6ab-478b-a26f-959358cf739e 0xc00938f9d0 0xc00938f9d1}] []  [{kube-controller-manager Update v1 2021-06-10 17:25:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fec51ba3-b6ab-478b-a26f-959358cf739e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-10 17:25:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-06-10 17:25:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.78.73\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6b228,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6b228,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6b228,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:target-cluster-md-0-6b59c4f65-cqpjz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.17.0.5,PodIP:192.168.78.73,StartTime:2021-06-10 17:25:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-10 17:25:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://06ba6d47fe93117af8d44397411f13d37d966c73918cfc39f76b9e483041a05d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.78.73,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 17:25:19.028: INFO: Pod "webserver-deployment-dd94f59b7-h48xh" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-h48xh webserver-deployment-dd94f59b7- deployment-1367 /api/v1/namespaces/deployment-1367/pods/webserver-deployment-dd94f59b7-h48xh bb4ad64e-505c-4cd5-9401-1c5eb2dad02d 59943 0 2021-06-10 17:25:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 fec51ba3-b6ab-478b-a26f-959358cf739e 0xc00938fb80 0xc00938fb81}] []  [{kube-controller-manager Update v1 2021-06-10 17:25:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fec51ba3-b6ab-478b-a26f-959358cf739e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-10 17:25:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6b228,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6b228,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6b228,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:target-cluster-control-plane-8m52s,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.17.0.4,PodIP:,StartTime:2021-06-10 17:25:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 17:25:19.028: INFO: Pod "webserver-deployment-dd94f59b7-hlrkh" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-hlrkh webserver-deployment-dd94f59b7- deployment-1367 /api/v1/namespaces/deployment-1367/pods/webserver-deployment-dd94f59b7-hlrkh fa12822b-d53d-4bbf-899e-742285587f6b 59956 0 2021-06-10 17:25:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 fec51ba3-b6ab-478b-a26f-959358cf739e 0xc00938fcf0 0xc00938fcf1}] []  [{kube-controller-manager Update v1 2021-06-10 17:25:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fec51ba3-b6ab-478b-a26f-959358cf739e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6b228,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6b228,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6b228,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 17:25:19.028: INFO: Pod "webserver-deployment-dd94f59b7-hqlv2" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-hqlv2 webserver-deployment-dd94f59b7- deployment-1367 /api/v1/namespaces/deployment-1367/pods/webserver-deployment-dd94f59b7-hqlv2 eaff043f-86d2-4551-a5a0-f2c7d98fc72d 59952 0 2021-06-10 17:25:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 fec51ba3-b6ab-478b-a26f-959358cf739e 0xc00938fdf7 0xc00938fdf8}] []  [{kube-controller-manager Update v1 2021-06-10 17:25:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fec51ba3-b6ab-478b-a26f-959358cf739e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6b228,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6b228,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6b228,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:target-cluster-md-0-6b59c4f65-5pcx5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 17:25:19.037: INFO: Pod "webserver-deployment-dd94f59b7-ll9s5" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-ll9s5 webserver-deployment-dd94f59b7- deployment-1367 /api/v1/namespaces/deployment-1367/pods/webserver-deployment-dd94f59b7-ll9s5 08f55658-c911-4021-876c-a41007f3b4bf 59771 0 2021-06-10 17:25:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:192.168.24.189/32 cni.projectcalico.org/podIPs:192.168.24.189/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 fec51ba3-b6ab-478b-a26f-959358cf739e 0xc00938ff10 0xc00938ff11}] []  [{kube-controller-manager Update v1 2021-06-10 17:25:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fec51ba3-b6ab-478b-a26f-959358cf739e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-10 17:25:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-06-10 17:25:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.24.189\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6b228,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6b228,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6b228,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:target-cluster-md-0-6b59c4f65-5pcx5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.17.0.6,PodIP:192.168.24.189,StartTime:2021-06-10 17:25:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-10 17:25:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://856f3fb709d69adeff39af0a31f5ec6639ed5deb3893dddfdab625ebfd383f81,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.24.189,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 17:25:19.037: INFO: Pod "webserver-deployment-dd94f59b7-lnn8q" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-lnn8q webserver-deployment-dd94f59b7- deployment-1367 /api/v1/namespaces/deployment-1367/pods/webserver-deployment-dd94f59b7-lnn8q 794ba0d9-6e3a-4cc4-8030-626ce52e98c8 59769 0 2021-06-10 17:25:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:192.168.24.188/32 cni.projectcalico.org/podIPs:192.168.24.188/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 fec51ba3-b6ab-478b-a26f-959358cf739e 0xc0056aa0b0 0xc0056aa0b1}] []  [{kube-controller-manager Update v1 2021-06-10 17:25:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fec51ba3-b6ab-478b-a26f-959358cf739e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-10 17:25:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-06-10 17:25:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.24.188\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6b228,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6b228,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6b228,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:target-cluster-md-0-6b59c4f65-5pcx5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.17.0.6,PodIP:192.168.24.188,StartTime:2021-06-10 17:25:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-10 17:25:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://e77a42c3b59a428e08ceced917c21ca936f7a2a1e7bb750788b3c890b3f56c59,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.24.188,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 17:25:19.037: INFO: Pod "webserver-deployment-dd94f59b7-mg2md" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-mg2md webserver-deployment-dd94f59b7- deployment-1367 /api/v1/namespaces/deployment-1367/pods/webserver-deployment-dd94f59b7-mg2md a569065b-598f-4c65-ad89-0aef492f722e 59966 0 2021-06-10 17:25:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 fec51ba3-b6ab-478b-a26f-959358cf739e 0xc0056aa280 0xc0056aa281}] []  [{kube-controller-manager Update v1 2021-06-10 17:25:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fec51ba3-b6ab-478b-a26f-959358cf739e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6b228,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6b228,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6b228,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:target-cluster-md-0-6b59c4f65-5pcx5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 17:25:19.045: INFO: Pod "webserver-deployment-dd94f59b7-mksvk" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-mksvk webserver-deployment-dd94f59b7- deployment-1367 /api/v1/namespaces/deployment-1367/pods/webserver-deployment-dd94f59b7-mksvk f1ae6c50-00e7-410d-a231-fc1360bfb68f 59946 0 2021-06-10 17:25:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 fec51ba3-b6ab-478b-a26f-959358cf739e 0xc0056aa3a0 0xc0056aa3a1}] []  [{kube-controller-manager Update v1 2021-06-10 17:25:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fec51ba3-b6ab-478b-a26f-959358cf739e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6b228,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6b228,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6b228,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:target-cluster-md-0-6b59c4f65-cqpjz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 17:25:19.045: INFO: Pod "webserver-deployment-dd94f59b7-mp6jw" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-mp6jw webserver-deployment-dd94f59b7- deployment-1367 /api/v1/namespaces/deployment-1367/pods/webserver-deployment-dd94f59b7-mp6jw 211757fb-e84a-412e-a802-fb84ef543ffe 59972 0 2021-06-10 17:25:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 fec51ba3-b6ab-478b-a26f-959358cf739e 0xc0056aa4c0 0xc0056aa4c1}] []  [{kube-controller-manager Update v1 2021-06-10 17:25:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fec51ba3-b6ab-478b-a26f-959358cf739e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6b228,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6b228,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6b228,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:target-cluster-md-0-6b59c4f65-cqpjz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 17:25:19.046: INFO: Pod "webserver-deployment-dd94f59b7-qdwnf" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-qdwnf webserver-deployment-dd94f59b7- deployment-1367 /api/v1/namespaces/deployment-1367/pods/webserver-deployment-dd94f59b7-qdwnf 8beed225-f20e-4dd4-bec9-918fc4db6d93 59760 0 2021-06-10 17:25:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:192.168.251.50/32 cni.projectcalico.org/podIPs:192.168.251.50/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 fec51ba3-b6ab-478b-a26f-959358cf739e 0xc0056aa5e0 0xc0056aa5e1}] []  [{kube-controller-manager Update v1 2021-06-10 17:25:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fec51ba3-b6ab-478b-a26f-959358cf739e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-10 17:25:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-06-10 17:25:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.251.50\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6b228,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6b228,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6b228,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:target-cluster-control-plane-8m52s,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.17.0.4,PodIP:192.168.251.50,StartTime:2021-06-10 17:25:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-10 17:25:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://6225fb85d5b0ec655d84f936b48ac7fcab85e1c85e5463a38749f9f6a237230d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.251.50,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 17:25:19.046: INFO: Pod "webserver-deployment-dd94f59b7-qtxw2" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-qtxw2 webserver-deployment-dd94f59b7- deployment-1367 /api/v1/namespaces/deployment-1367/pods/webserver-deployment-dd94f59b7-qtxw2 b614de42-4f4f-4033-b7a9-8fb8310c767f 59939 0 2021-06-10 17:25:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 fec51ba3-b6ab-478b-a26f-959358cf739e 0xc0056aa790 0xc0056aa791}] []  [{kube-controller-manager Update v1 2021-06-10 17:25:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fec51ba3-b6ab-478b-a26f-959358cf739e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-10 17:25:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6b228,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6b228,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6b228,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:target-cluster-md-0-6b59c4f65-cqpjz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.17.0.5,PodIP:,StartTime:2021-06-10 17:25:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 17:25:19.046: INFO: Pod "webserver-deployment-dd94f59b7-s7xjj" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-s7xjj webserver-deployment-dd94f59b7- deployment-1367 /api/v1/namespaces/deployment-1367/pods/webserver-deployment-dd94f59b7-s7xjj 3c098ac0-f33d-4a0d-9fb7-59092590afb5 59955 0 2021-06-10 17:25:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 fec51ba3-b6ab-478b-a26f-959358cf739e 0xc0056aa900 0xc0056aa901}] []  [{kube-controller-manager Update v1 2021-06-10 17:25:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fec51ba3-b6ab-478b-a26f-959358cf739e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6b228,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6b228,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6b228,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 17:25:19.047: INFO: Pod "webserver-deployment-dd94f59b7-vlf7r" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-vlf7r webserver-deployment-dd94f59b7- deployment-1367 /api/v1/namespaces/deployment-1367/pods/webserver-deployment-dd94f59b7-vlf7r f5072d83-4fd3-429d-9510-27fe9dcf1f90 59777 0 2021-06-10 17:25:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:192.168.24.190/32 cni.projectcalico.org/podIPs:192.168.24.190/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 fec51ba3-b6ab-478b-a26f-959358cf739e 0xc0056aaa07 0xc0056aaa08}] []  [{kube-controller-manager Update v1 2021-06-10 17:25:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fec51ba3-b6ab-478b-a26f-959358cf739e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-10 17:25:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-06-10 17:25:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.24.190\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6b228,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6b228,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6b228,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:target-cluster-md-0-6b59c4f65-5pcx5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.17.0.6,PodIP:192.168.24.190,StartTime:2021-06-10 17:25:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-10 17:25:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://3df65d1ddcb1539dc071fbcee82b188643720f56933451187a3203c07b65b1fb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.24.190,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 17:25:19.047: INFO: Pod "webserver-deployment-dd94f59b7-xnvhp" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-xnvhp webserver-deployment-dd94f59b7- deployment-1367 /api/v1/namespaces/deployment-1367/pods/webserver-deployment-dd94f59b7-xnvhp 5a0f0128-daae-4f61-ac57-f951b0480341 59962 0 2021-06-10 17:25:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 fec51ba3-b6ab-478b-a26f-959358cf739e 0xc0056aabb0 0xc0056aabb1}] []  [{kube-controller-manager Update v1 2021-06-10 17:25:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fec51ba3-b6ab-478b-a26f-959358cf739e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-10 17:25:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6b228,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6b228,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6b228,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:target-cluster-md-0-6b59c4f65-5pcx5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.17.0.6,PodIP:,StartTime:2021-06-10 17:25:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 17:25:19.047: INFO: Pod "webserver-deployment-dd94f59b7-z2wsb" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-z2wsb webserver-deployment-dd94f59b7- deployment-1367 /api/v1/namespaces/deployment-1367/pods/webserver-deployment-dd94f59b7-z2wsb fd575ef3-c1cc-4f8e-b47a-ff7d6003995e 59790 0 2021-06-10 17:25:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:192.168.78.72/32 cni.projectcalico.org/podIPs:192.168.78.72/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 fec51ba3-b6ab-478b-a26f-959358cf739e 0xc0056aad60 0xc0056aad61}] []  [{kube-controller-manager Update v1 2021-06-10 17:25:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fec51ba3-b6ab-478b-a26f-959358cf739e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-10 17:25:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-06-10 17:25:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.78.72\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6b228,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6b228,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6b228,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:target-cluster-md-0-6b59c4f65-cqpjz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.17.0.5,PodIP:192.168.78.72,StartTime:2021-06-10 17:25:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-10 17:25:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://defefb1d5a34a158c201754cb619046f9148c4b64774b8bec7b397070ddee836,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.78.72,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 17:25:19.047: INFO: Pod "webserver-deployment-dd94f59b7-z8bwg" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-z8bwg webserver-deployment-dd94f59b7- deployment-1367 /api/v1/namespaces/deployment-1367/pods/webserver-deployment-dd94f59b7-z8bwg be2bfbd4-714a-408b-ad3a-e903a7b75475 59971 0 2021-06-10 17:25:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 fec51ba3-b6ab-478b-a26f-959358cf739e 0xc0056aaf30 0xc0056aaf31}] []  [{kube-controller-manager Update v1 2021-06-10 17:25:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fec51ba3-b6ab-478b-a26f-959358cf739e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6b228,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6b228,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6b228,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:target-cluster-control-plane-8m52s,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:25:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:25:19.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1367" for this suite.

• [SLOW TEST:10.871 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":305,"completed":294,"skipped":5028,"failed":0}
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:25:19.837: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jun 10 17:25:20.516: INFO: Waiting up to 5m0s for pod "downwardapi-volume-866077e1-5033-4475-ba0b-3208837653d8" in namespace "downward-api-2392" to be "Succeeded or Failed"
Jun 10 17:25:20.641: INFO: Pod "downwardapi-volume-866077e1-5033-4475-ba0b-3208837653d8": Phase="Pending", Reason="", readiness=false. Elapsed: 124.516422ms
Jun 10 17:25:22.728: INFO: Pod "downwardapi-volume-866077e1-5033-4475-ba0b-3208837653d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.211493044s
Jun 10 17:25:24.764: INFO: Pod "downwardapi-volume-866077e1-5033-4475-ba0b-3208837653d8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.247657203s
Jun 10 17:25:26.906: INFO: Pod "downwardapi-volume-866077e1-5033-4475-ba0b-3208837653d8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.389441889s
Jun 10 17:25:29.221: INFO: Pod "downwardapi-volume-866077e1-5033-4475-ba0b-3208837653d8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.704438659s
Jun 10 17:25:31.704: INFO: Pod "downwardapi-volume-866077e1-5033-4475-ba0b-3208837653d8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.187797997s
Jun 10 17:25:33.803: INFO: Pod "downwardapi-volume-866077e1-5033-4475-ba0b-3208837653d8": Phase="Pending", Reason="", readiness=false. Elapsed: 13.286860764s
Jun 10 17:25:37.112: INFO: Pod "downwardapi-volume-866077e1-5033-4475-ba0b-3208837653d8": Phase="Pending", Reason="", readiness=false. Elapsed: 16.59578138s
Jun 10 17:25:39.489: INFO: Pod "downwardapi-volume-866077e1-5033-4475-ba0b-3208837653d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 18.972769144s
STEP: Saw pod success
Jun 10 17:25:39.489: INFO: Pod "downwardapi-volume-866077e1-5033-4475-ba0b-3208837653d8" satisfied condition "Succeeded or Failed"
Jun 10 17:25:39.671: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod downwardapi-volume-866077e1-5033-4475-ba0b-3208837653d8 container client-container: <nil>
STEP: delete the pod
Jun 10 17:25:40.521: INFO: Waiting for pod downwardapi-volume-866077e1-5033-4475-ba0b-3208837653d8 to disappear
Jun 10 17:25:40.546: INFO: Pod downwardapi-volume-866077e1-5033-4475-ba0b-3208837653d8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:25:40.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2392" for this suite.

• [SLOW TEST:20.890 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":295,"skipped":5028,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:25:40.728: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-a0768456-f449-4081-ab84-318a147775d7
STEP: Creating a pod to test consume configMaps
Jun 10 17:25:41.404: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0b2a9705-a8f9-4491-9cb0-454e6e917e02" in namespace "projected-804" to be "Succeeded or Failed"
Jun 10 17:25:41.447: INFO: Pod "pod-projected-configmaps-0b2a9705-a8f9-4491-9cb0-454e6e917e02": Phase="Pending", Reason="", readiness=false. Elapsed: 43.042713ms
Jun 10 17:25:43.464: INFO: Pod "pod-projected-configmaps-0b2a9705-a8f9-4491-9cb0-454e6e917e02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059855827s
Jun 10 17:25:45.488: INFO: Pod "pod-projected-configmaps-0b2a9705-a8f9-4491-9cb0-454e6e917e02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.084064048s
STEP: Saw pod success
Jun 10 17:25:45.488: INFO: Pod "pod-projected-configmaps-0b2a9705-a8f9-4491-9cb0-454e6e917e02" satisfied condition "Succeeded or Failed"
Jun 10 17:25:45.493: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod pod-projected-configmaps-0b2a9705-a8f9-4491-9cb0-454e6e917e02 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 10 17:25:45.737: INFO: Waiting for pod pod-projected-configmaps-0b2a9705-a8f9-4491-9cb0-454e6e917e02 to disappear
Jun 10 17:25:45.759: INFO: Pod pod-projected-configmaps-0b2a9705-a8f9-4491-9cb0-454e6e917e02 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:25:45.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-804" for this suite.

• [SLOW TEST:5.071 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":305,"completed":296,"skipped":5029,"failed":0}
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:25:45.799: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun 10 17:25:46.029: INFO: Waiting up to 5m0s for pod "pod-0681257c-16a0-4471-9b29-8b29637886d3" in namespace "emptydir-4583" to be "Succeeded or Failed"
Jun 10 17:25:46.057: INFO: Pod "pod-0681257c-16a0-4471-9b29-8b29637886d3": Phase="Pending", Reason="", readiness=false. Elapsed: 28.772779ms
Jun 10 17:25:48.067: INFO: Pod "pod-0681257c-16a0-4471-9b29-8b29637886d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038026174s
Jun 10 17:25:50.071: INFO: Pod "pod-0681257c-16a0-4471-9b29-8b29637886d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042709304s
STEP: Saw pod success
Jun 10 17:25:50.071: INFO: Pod "pod-0681257c-16a0-4471-9b29-8b29637886d3" satisfied condition "Succeeded or Failed"
Jun 10 17:25:50.073: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod pod-0681257c-16a0-4471-9b29-8b29637886d3 container test-container: <nil>
STEP: delete the pod
Jun 10 17:25:50.092: INFO: Waiting for pod pod-0681257c-16a0-4471-9b29-8b29637886d3 to disappear
Jun 10 17:25:50.096: INFO: Pod pod-0681257c-16a0-4471-9b29-8b29637886d3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:25:50.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4583" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":297,"skipped":5029,"failed":0}

------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:25:50.116: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override command
Jun 10 17:25:50.180: INFO: Waiting up to 5m0s for pod "client-containers-a78a8f6d-e3c9-4c97-b2ba-1274dff79581" in namespace "containers-9815" to be "Succeeded or Failed"
Jun 10 17:25:50.185: INFO: Pod "client-containers-a78a8f6d-e3c9-4c97-b2ba-1274dff79581": Phase="Pending", Reason="", readiness=false. Elapsed: 4.973292ms
Jun 10 17:25:52.188: INFO: Pod "client-containers-a78a8f6d-e3c9-4c97-b2ba-1274dff79581": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00824651s
Jun 10 17:25:54.193: INFO: Pod "client-containers-a78a8f6d-e3c9-4c97-b2ba-1274dff79581": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012748789s
STEP: Saw pod success
Jun 10 17:25:54.193: INFO: Pod "client-containers-a78a8f6d-e3c9-4c97-b2ba-1274dff79581" satisfied condition "Succeeded or Failed"
Jun 10 17:25:54.199: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod client-containers-a78a8f6d-e3c9-4c97-b2ba-1274dff79581 container test-container: <nil>
STEP: delete the pod
Jun 10 17:25:54.224: INFO: Waiting for pod client-containers-a78a8f6d-e3c9-4c97-b2ba-1274dff79581 to disappear
Jun 10 17:25:54.227: INFO: Pod client-containers-a78a8f6d-e3c9-4c97-b2ba-1274dff79581 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:25:54.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9815" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":305,"completed":298,"skipped":5029,"failed":0}
S
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:25:54.239: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-4283
STEP: creating service affinity-clusterip in namespace services-4283
STEP: creating replication controller affinity-clusterip in namespace services-4283
I0610 17:25:54.354653      26 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-4283, replica count: 3
I0610 17:25:57.407190      26 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 10 17:25:57.416: INFO: Creating new exec pod
Jun 10 17:26:00.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=services-4283 exec execpod-affinityq2fsd -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Jun 10 17:26:00.715: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jun 10 17:26:00.715: INFO: stdout: ""
Jun 10 17:26:00.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=services-4283 exec execpod-affinityq2fsd -- /bin/sh -x -c nc -zv -t -w 2 10.134.132.15 80'
Jun 10 17:26:01.041: INFO: stderr: "+ nc -zv -t -w 2 10.134.132.15 80\nConnection to 10.134.132.15 80 port [tcp/http] succeeded!\n"
Jun 10 17:26:01.041: INFO: stdout: ""
Jun 10 17:26:01.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-615187751 --namespace=services-4283 exec execpod-affinityq2fsd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.134.132.15:80/ ; done'
Jun 10 17:26:01.623: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.132.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.132.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.132.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.132.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.132.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.132.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.132.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.132.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.132.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.132.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.132.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.132.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.132.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.132.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.132.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.132.15:80/\n"
Jun 10 17:26:01.623: INFO: stdout: "\naffinity-clusterip-gs2qk\naffinity-clusterip-gs2qk\naffinity-clusterip-gs2qk\naffinity-clusterip-gs2qk\naffinity-clusterip-gs2qk\naffinity-clusterip-gs2qk\naffinity-clusterip-gs2qk\naffinity-clusterip-gs2qk\naffinity-clusterip-gs2qk\naffinity-clusterip-gs2qk\naffinity-clusterip-gs2qk\naffinity-clusterip-gs2qk\naffinity-clusterip-gs2qk\naffinity-clusterip-gs2qk\naffinity-clusterip-gs2qk\naffinity-clusterip-gs2qk"
Jun 10 17:26:01.623: INFO: Received response from host: affinity-clusterip-gs2qk
Jun 10 17:26:01.623: INFO: Received response from host: affinity-clusterip-gs2qk
Jun 10 17:26:01.623: INFO: Received response from host: affinity-clusterip-gs2qk
Jun 10 17:26:01.623: INFO: Received response from host: affinity-clusterip-gs2qk
Jun 10 17:26:01.623: INFO: Received response from host: affinity-clusterip-gs2qk
Jun 10 17:26:01.623: INFO: Received response from host: affinity-clusterip-gs2qk
Jun 10 17:26:01.623: INFO: Received response from host: affinity-clusterip-gs2qk
Jun 10 17:26:01.623: INFO: Received response from host: affinity-clusterip-gs2qk
Jun 10 17:26:01.623: INFO: Received response from host: affinity-clusterip-gs2qk
Jun 10 17:26:01.623: INFO: Received response from host: affinity-clusterip-gs2qk
Jun 10 17:26:01.623: INFO: Received response from host: affinity-clusterip-gs2qk
Jun 10 17:26:01.623: INFO: Received response from host: affinity-clusterip-gs2qk
Jun 10 17:26:01.623: INFO: Received response from host: affinity-clusterip-gs2qk
Jun 10 17:26:01.623: INFO: Received response from host: affinity-clusterip-gs2qk
Jun 10 17:26:01.623: INFO: Received response from host: affinity-clusterip-gs2qk
Jun 10 17:26:01.623: INFO: Received response from host: affinity-clusterip-gs2qk
Jun 10 17:26:01.623: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-4283, will wait for the garbage collector to delete the pods
Jun 10 17:26:01.711: INFO: Deleting ReplicationController affinity-clusterip took: 11.724901ms
Jun 10 17:26:03.211: INFO: Terminating ReplicationController affinity-clusterip pods took: 1.500222332s
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:26:15.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4283" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:21.317 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":299,"skipped":5030,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:26:15.558: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jun 10 17:26:15.676: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jun 10 17:26:20.680: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 10 17:26:20.680: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jun 10 17:26:22.685: INFO: Creating deployment "test-rollover-deployment"
Jun 10 17:26:22.690: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jun 10 17:26:24.699: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jun 10 17:26:24.715: INFO: Ensure that both replica sets have 1 created replica
Jun 10 17:26:24.737: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jun 10 17:26:24.749: INFO: Updating deployment test-rollover-deployment
Jun 10 17:26:24.749: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jun 10 17:26:26.759: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jun 10 17:26:26.763: INFO: Make sure deployment "test-rollover-deployment" is complete
Jun 10 17:26:26.768: INFO: all replica sets need to contain the pod-template-hash label
Jun 10 17:26:26.768: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942782, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942782, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942786, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942782, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 10 17:26:28.787: INFO: all replica sets need to contain the pod-template-hash label
Jun 10 17:26:28.787: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942782, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942782, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942786, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942782, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 10 17:26:30.773: INFO: all replica sets need to contain the pod-template-hash label
Jun 10 17:26:30.773: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942782, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942782, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942786, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942782, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 10 17:26:32.772: INFO: all replica sets need to contain the pod-template-hash label
Jun 10 17:26:32.772: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942782, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942782, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942786, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942782, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 10 17:26:34.774: INFO: all replica sets need to contain the pod-template-hash label
Jun 10 17:26:34.774: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942782, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942782, loc:(*time.Location)(0x770e980)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942786, loc:(*time.Location)(0x770e980)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758942782, loc:(*time.Location)(0x770e980)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 10 17:26:36.801: INFO: 
Jun 10 17:26:36.801: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Jun 10 17:26:36.813: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-1793 /apis/apps/v1/namespaces/deployment-1793/deployments/test-rollover-deployment 0a798477-fc98-49c1-9263-c9e6e266dc23 61152 2 2021-06-10 17:26:22 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-06-10 17:26:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-06-10 17:26:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0058b8d68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-06-10 17:26:22 +0000 UTC,LastTransitionTime:2021-06-10 17:26:22 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-5797c7764" has successfully progressed.,LastUpdateTime:2021-06-10 17:26:36 +0000 UTC,LastTransitionTime:2021-06-10 17:26:22 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 10 17:26:36.817: INFO: New ReplicaSet "test-rollover-deployment-5797c7764" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-5797c7764  deployment-1793 /apis/apps/v1/namespaces/deployment-1793/replicasets/test-rollover-deployment-5797c7764 36686731-7f3a-4559-8247-c32ddd7367d4 61141 2 2021-06-10 17:26:24 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 0a798477-fc98-49c1-9263-c9e6e266dc23 0xc0058b9290 0xc0058b9291}] []  [{kube-controller-manager Update apps/v1 2021-06-10 17:26:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a798477-fc98-49c1-9263-c9e6e266dc23\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5797c7764,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0058b9308 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 10 17:26:36.817: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jun 10 17:26:36.817: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-1793 /apis/apps/v1/namespaces/deployment-1793/replicasets/test-rollover-controller 5a3aff20-de7a-4a80-90c3-cbbe2267117c 61151 2 2021-06-10 17:26:15 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 0a798477-fc98-49c1-9263-c9e6e266dc23 0xc0058b9187 0xc0058b9188}] []  [{e2e.test Update apps/v1 2021-06-10 17:26:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-06-10 17:26:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a798477-fc98-49c1-9263-c9e6e266dc23\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0058b9228 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 10 17:26:36.817: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-1793 /apis/apps/v1/namespaces/deployment-1793/replicasets/test-rollover-deployment-78bc8b888c ab2747e3-3b0a-48f2-9b52-b80208498615 61057 2 2021-06-10 17:26:22 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 0a798477-fc98-49c1-9263-c9e6e266dc23 0xc0058b9377 0xc0058b9378}] []  [{kube-controller-manager Update apps/v1 2021-06-10 17:26:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a798477-fc98-49c1-9263-c9e6e266dc23\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0058b9408 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 10 17:26:36.822: INFO: Pod "test-rollover-deployment-5797c7764-h8xg7" is available:
&Pod{ObjectMeta:{test-rollover-deployment-5797c7764-h8xg7 test-rollover-deployment-5797c7764- deployment-1793 /api/v1/namespaces/deployment-1793/pods/test-rollover-deployment-5797c7764-h8xg7 818cbe42-57ee-4fef-808d-55d4a6b4cc39 61084 0 2021-06-10 17:26:24 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[cni.projectcalico.org/podIP:192.168.24.139/32 cni.projectcalico.org/podIPs:192.168.24.139/32] [{apps/v1 ReplicaSet test-rollover-deployment-5797c7764 36686731-7f3a-4559-8247-c32ddd7367d4 0xc0058b99f0 0xc0058b99f1}] []  [{kube-controller-manager Update v1 2021-06-10 17:26:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"36686731-7f3a-4559-8247-c32ddd7367d4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-10 17:26:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-06-10 17:26:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.24.139\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wx44v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wx44v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wx44v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:target-cluster-md-0-6b59c4f65-5pcx5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:26:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:26:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:26:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 17:26:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.17.0.6,PodIP:192.168.24.139,StartTime:2021-06-10 17:26:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-10 17:26:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:containerd://6984290e8f2440c979e649cc574e239795ecb0d61a4723ffe955d8890e06dd66,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.24.139,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:26:36.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1793" for this suite.

• [SLOW TEST:21.280 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":305,"completed":300,"skipped":5045,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:26:36.838: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:26:36.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2498" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":305,"completed":301,"skipped":5068,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:26:36.939: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's args
Jun 10 17:26:37.005: INFO: Waiting up to 5m0s for pod "var-expansion-7a7d1541-73dd-47b9-964e-c571903724a1" in namespace "var-expansion-8172" to be "Succeeded or Failed"
Jun 10 17:26:37.013: INFO: Pod "var-expansion-7a7d1541-73dd-47b9-964e-c571903724a1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.563661ms
Jun 10 17:26:39.020: INFO: Pod "var-expansion-7a7d1541-73dd-47b9-964e-c571903724a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014802041s
Jun 10 17:26:41.024: INFO: Pod "var-expansion-7a7d1541-73dd-47b9-964e-c571903724a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018737607s
STEP: Saw pod success
Jun 10 17:26:41.024: INFO: Pod "var-expansion-7a7d1541-73dd-47b9-964e-c571903724a1" satisfied condition "Succeeded or Failed"
Jun 10 17:26:41.026: INFO: Trying to get logs from node target-cluster-md-0-6b59c4f65-cqpjz pod var-expansion-7a7d1541-73dd-47b9-964e-c571903724a1 container dapi-container: <nil>
STEP: delete the pod
Jun 10 17:26:41.060: INFO: Waiting for pod var-expansion-7a7d1541-73dd-47b9-964e-c571903724a1 to disappear
Jun 10 17:26:41.068: INFO: Pod var-expansion-7a7d1541-73dd-47b9-964e-c571903724a1 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:26:41.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8172" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":305,"completed":302,"skipped":5078,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:26:41.099: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun 10 17:26:45.774: INFO: Successfully updated pod "pod-update-4dbdfbb4-c6d2-44b8-a050-aee4483f3077"
STEP: verifying the updated pod is in kubernetes
Jun 10 17:26:45.782: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:26:45.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5255" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":305,"completed":303,"skipped":5129,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:26:45.807: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating server pod server in namespace prestop-9103
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-9103
STEP: Deleting pre-stop pod
Jun 10 17:26:54.917: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:26:54.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-9103" for this suite.

• [SLOW TEST:9.166 seconds]
[k8s.io] [sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":305,"completed":304,"skipped":5146,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 17:26:54.978: INFO: >>> kubeConfig: /tmp/kubeconfig-615187751
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0610 17:27:05.074080      26 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jun 10 17:28:07.090: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 17:28:07.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4218" for this suite.

• [SLOW TEST:72.120 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":305,"completed":305,"skipped":5163,"failed":0}
SSSSSSSSSSSSSSSSJun 10 17:28:07.097: INFO: Running AfterSuite actions on all nodes
Jun 10 17:28:07.098: INFO: Running AfterSuite actions on node 1
Jun 10 17:28:07.098: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":305,"completed":305,"skipped":5179,"failed":0}

Ran 305 of 5484 Specs in 6486.754 seconds
SUCCESS! -- 305 Passed | 0 Failed | 0 Pending | 5179 Skipped
PASS

Ginkgo ran 1 suite in 1h48m10.634309658s
Test Suite Passed
