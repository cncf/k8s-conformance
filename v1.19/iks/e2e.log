I1026 16:37:35.611808      27 test_context.go:416] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-940878786
I1026 16:37:35.611838      27 test_context.go:429] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I1026 16:37:35.611966      27 e2e.go:129] Starting e2e run "abecb1f6-f360-40a4-a88c-8a3ae39152ae" on Ginkgo node 1
{"msg":"Test Suite starting","total":305,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1603730253 - Will randomize all specs
Will run 305 of 5233 specs

Oct 26 16:37:35.628: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
E1026 16:37:35.630726      27 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Oct 26 16:37:35.632: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Oct 26 16:37:35.694: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Oct 26 16:37:35.812: INFO: 25 / 25 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Oct 26 16:37:35.812: INFO: expected 16 pod replicas in namespace 'kube-system', 16 are Running and Ready.
Oct 26 16:37:35.812: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Oct 26 16:37:35.842: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Oct 26 16:37:35.843: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
Oct 26 16:37:35.843: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
Oct 26 16:37:35.843: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-driver-installer' (0 seconds elapsed)
Oct 26 16:37:35.843: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-gpu-device-plugin' (0 seconds elapsed)
Oct 26 16:37:35.843: INFO: e2e test version: v1.19.3
Oct 26 16:37:35.849: INFO: kube-apiserver version: v1.19.3+IKS
Oct 26 16:37:35.849: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
Oct 26 16:37:35.867: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:37:35.871: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename crd-publish-openapi
Oct 26 16:37:35.980: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Oct 26 16:37:36.012: INFO: PSP annotation exists on dry run pod: "ibm-privileged-psp"; assuming PodSecurityPolicy is enabled
Oct 26 16:37:36.087: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8038
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 26 16:37:36.249: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Oct 26 16:37:42.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 --namespace=crd-publish-openapi-8038 create -f -'
Oct 26 16:37:43.023: INFO: stderr: ""
Oct 26 16:37:43.023: INFO: stdout: "e2e-test-crd-publish-openapi-94-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Oct 26 16:37:43.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 --namespace=crd-publish-openapi-8038 delete e2e-test-crd-publish-openapi-94-crds test-cr'
Oct 26 16:37:43.203: INFO: stderr: ""
Oct 26 16:37:43.203: INFO: stdout: "e2e-test-crd-publish-openapi-94-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Oct 26 16:37:43.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 --namespace=crd-publish-openapi-8038 apply -f -'
Oct 26 16:37:43.558: INFO: stderr: ""
Oct 26 16:37:43.558: INFO: stdout: "e2e-test-crd-publish-openapi-94-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Oct 26 16:37:43.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 --namespace=crd-publish-openapi-8038 delete e2e-test-crd-publish-openapi-94-crds test-cr'
Oct 26 16:37:43.727: INFO: stderr: ""
Oct 26 16:37:43.727: INFO: stdout: "e2e-test-crd-publish-openapi-94-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Oct 26 16:37:43.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 explain e2e-test-crd-publish-openapi-94-crds'
Oct 26 16:37:43.997: INFO: stderr: ""
Oct 26 16:37:43.997: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-94-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:37:49.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8038" for this suite.

• [SLOW TEST:13.530 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":305,"completed":1,"skipped":67,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:37:49.401: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-4229
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Oct 26 16:37:49.720: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4229 /api/v1/namespaces/watch-4229/configmaps/e2e-watch-test-watch-closed b0ac7b60-b468-4537-9d56-6d104ccc93d5 26788 0 2020-10-26 16:37:49 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-10-26 16:37:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 26 16:37:49.721: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4229 /api/v1/namespaces/watch-4229/configmaps/e2e-watch-test-watch-closed b0ac7b60-b468-4537-9d56-6d104ccc93d5 26789 0 2020-10-26 16:37:49 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-10-26 16:37:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Oct 26 16:37:49.811: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4229 /api/v1/namespaces/watch-4229/configmaps/e2e-watch-test-watch-closed b0ac7b60-b468-4537-9d56-6d104ccc93d5 26790 0 2020-10-26 16:37:49 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-10-26 16:37:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 26 16:37:49.811: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4229 /api/v1/namespaces/watch-4229/configmaps/e2e-watch-test-watch-closed b0ac7b60-b468-4537-9d56-6d104ccc93d5 26791 0 2020-10-26 16:37:49 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-10-26 16:37:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:37:49.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4229" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":305,"completed":2,"skipped":72,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:37:49.873: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-5698
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Oct 26 16:37:50.193: INFO: Waiting up to 1m0s for all nodes to be ready
Oct 26 16:38:50.348: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:38:50.370: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-path-4004
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:487
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Oct 26 16:38:52.783: INFO: found a healthy node: 10.112.67.203
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 26 16:39:13.151: INFO: pods created so far: [1 1 1]
Oct 26 16:39:13.151: INFO: length of pods created so far: 3
Oct 26 16:39:25.191: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:39:32.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-4004" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:461
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:39:32.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5698" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:102.840 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:450
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":305,"completed":3,"skipped":104,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:39:32.724: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3385
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 26 16:39:33.096: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Oct 26 16:39:33.155: INFO: Number of nodes with available pods: 0
Oct 26 16:39:33.155: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 16:39:34.208: INFO: Number of nodes with available pods: 0
Oct 26 16:39:34.208: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 16:39:35.195: INFO: Number of nodes with available pods: 0
Oct 26 16:39:35.195: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 16:39:36.197: INFO: Number of nodes with available pods: 0
Oct 26 16:39:36.197: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 16:39:37.196: INFO: Number of nodes with available pods: 0
Oct 26 16:39:37.196: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 16:39:38.197: INFO: Number of nodes with available pods: 0
Oct 26 16:39:38.197: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 16:39:39.207: INFO: Number of nodes with available pods: 0
Oct 26 16:39:39.207: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 16:39:40.199: INFO: Number of nodes with available pods: 0
Oct 26 16:39:40.199: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 16:39:41.198: INFO: Number of nodes with available pods: 0
Oct 26 16:39:41.199: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 16:39:42.194: INFO: Number of nodes with available pods: 1
Oct 26 16:39:42.194: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 16:39:43.197: INFO: Number of nodes with available pods: 3
Oct 26 16:39:43.197: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Oct 26 16:39:43.326: INFO: Wrong image for pod: daemon-set-jcbrs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:39:43.326: INFO: Wrong image for pod: daemon-set-mthdz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:39:43.326: INFO: Wrong image for pod: daemon-set-x7b76. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:39:44.364: INFO: Wrong image for pod: daemon-set-jcbrs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:39:44.365: INFO: Wrong image for pod: daemon-set-mthdz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:39:44.365: INFO: Wrong image for pod: daemon-set-x7b76. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:39:45.364: INFO: Wrong image for pod: daemon-set-jcbrs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:39:45.364: INFO: Wrong image for pod: daemon-set-mthdz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:39:45.364: INFO: Wrong image for pod: daemon-set-x7b76. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:39:46.365: INFO: Wrong image for pod: daemon-set-jcbrs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:39:46.365: INFO: Wrong image for pod: daemon-set-mthdz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:39:46.365: INFO: Pod daemon-set-mthdz is not available
Oct 26 16:39:46.365: INFO: Wrong image for pod: daemon-set-x7b76. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:39:47.368: INFO: Wrong image for pod: daemon-set-jcbrs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:39:47.368: INFO: Wrong image for pod: daemon-set-mthdz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:39:47.368: INFO: Pod daemon-set-mthdz is not available
Oct 26 16:39:47.368: INFO: Wrong image for pod: daemon-set-x7b76. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:39:48.366: INFO: Pod daemon-set-dvlwf is not available
Oct 26 16:39:48.367: INFO: Wrong image for pod: daemon-set-jcbrs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:39:48.367: INFO: Wrong image for pod: daemon-set-x7b76. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:39:49.372: INFO: Pod daemon-set-dvlwf is not available
Oct 26 16:39:49.372: INFO: Wrong image for pod: daemon-set-jcbrs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:39:49.372: INFO: Wrong image for pod: daemon-set-x7b76. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:39:50.371: INFO: Pod daemon-set-dvlwf is not available
Oct 26 16:39:50.371: INFO: Wrong image for pod: daemon-set-jcbrs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:39:50.371: INFO: Wrong image for pod: daemon-set-x7b76. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:39:51.366: INFO: Pod daemon-set-dvlwf is not available
Oct 26 16:39:51.366: INFO: Wrong image for pod: daemon-set-jcbrs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:39:51.366: INFO: Wrong image for pod: daemon-set-x7b76. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:39:52.365: INFO: Pod daemon-set-dvlwf is not available
Oct 26 16:39:52.365: INFO: Wrong image for pod: daemon-set-jcbrs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:39:52.365: INFO: Wrong image for pod: daemon-set-x7b76. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:39:53.365: INFO: Pod daemon-set-dvlwf is not available
Oct 26 16:39:53.366: INFO: Wrong image for pod: daemon-set-jcbrs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:39:53.366: INFO: Wrong image for pod: daemon-set-x7b76. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:39:54.364: INFO: Wrong image for pod: daemon-set-jcbrs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:39:54.364: INFO: Wrong image for pod: daemon-set-x7b76. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:39:55.365: INFO: Wrong image for pod: daemon-set-jcbrs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:39:55.365: INFO: Pod daemon-set-jcbrs is not available
Oct 26 16:39:55.365: INFO: Wrong image for pod: daemon-set-x7b76. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:39:56.363: INFO: Wrong image for pod: daemon-set-jcbrs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:39:56.363: INFO: Pod daemon-set-jcbrs is not available
Oct 26 16:39:56.363: INFO: Wrong image for pod: daemon-set-x7b76. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:39:57.365: INFO: Wrong image for pod: daemon-set-jcbrs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:39:57.365: INFO: Pod daemon-set-jcbrs is not available
Oct 26 16:39:57.365: INFO: Wrong image for pod: daemon-set-x7b76. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:39:58.366: INFO: Wrong image for pod: daemon-set-jcbrs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:39:58.366: INFO: Pod daemon-set-jcbrs is not available
Oct 26 16:39:58.366: INFO: Wrong image for pod: daemon-set-x7b76. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:39:59.363: INFO: Wrong image for pod: daemon-set-jcbrs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:39:59.363: INFO: Pod daemon-set-jcbrs is not available
Oct 26 16:39:59.363: INFO: Wrong image for pod: daemon-set-x7b76. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:40:00.369: INFO: Pod daemon-set-ttdpx is not available
Oct 26 16:40:00.369: INFO: Wrong image for pod: daemon-set-x7b76. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:40:01.370: INFO: Pod daemon-set-ttdpx is not available
Oct 26 16:40:01.370: INFO: Wrong image for pod: daemon-set-x7b76. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:40:02.368: INFO: Pod daemon-set-ttdpx is not available
Oct 26 16:40:02.369: INFO: Wrong image for pod: daemon-set-x7b76. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:40:03.364: INFO: Pod daemon-set-ttdpx is not available
Oct 26 16:40:03.364: INFO: Wrong image for pod: daemon-set-x7b76. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:40:04.368: INFO: Pod daemon-set-ttdpx is not available
Oct 26 16:40:04.368: INFO: Wrong image for pod: daemon-set-x7b76. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:40:05.368: INFO: Pod daemon-set-ttdpx is not available
Oct 26 16:40:05.368: INFO: Wrong image for pod: daemon-set-x7b76. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:40:06.368: INFO: Pod daemon-set-ttdpx is not available
Oct 26 16:40:06.368: INFO: Wrong image for pod: daemon-set-x7b76. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:40:07.368: INFO: Wrong image for pod: daemon-set-x7b76. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:40:08.364: INFO: Wrong image for pod: daemon-set-x7b76. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:40:09.366: INFO: Wrong image for pod: daemon-set-x7b76. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:40:09.366: INFO: Pod daemon-set-x7b76 is not available
Oct 26 16:40:10.364: INFO: Wrong image for pod: daemon-set-x7b76. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:40:10.364: INFO: Pod daemon-set-x7b76 is not available
Oct 26 16:40:11.365: INFO: Wrong image for pod: daemon-set-x7b76. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Oct 26 16:40:11.365: INFO: Pod daemon-set-x7b76 is not available
Oct 26 16:40:12.367: INFO: Pod daemon-set-9mvwf is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Oct 26 16:40:12.431: INFO: Number of nodes with available pods: 2
Oct 26 16:40:12.431: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 16:40:13.477: INFO: Number of nodes with available pods: 2
Oct 26 16:40:13.477: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 16:40:14.470: INFO: Number of nodes with available pods: 2
Oct 26 16:40:14.470: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 16:40:15.475: INFO: Number of nodes with available pods: 2
Oct 26 16:40:15.475: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 16:40:16.476: INFO: Number of nodes with available pods: 2
Oct 26 16:40:16.476: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 16:40:17.482: INFO: Number of nodes with available pods: 3
Oct 26 16:40:17.482: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3385, will wait for the garbage collector to delete the pods
Oct 26 16:40:17.652: INFO: Deleting DaemonSet.extensions daemon-set took: 29.404725ms
Oct 26 16:40:17.752: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.235593ms
Oct 26 16:40:31.968: INFO: Number of nodes with available pods: 0
Oct 26 16:40:31.968: INFO: Number of running nodes: 0, number of available pods: 0
Oct 26 16:40:31.987: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3385/daemonsets","resourceVersion":"27613"},"items":null}

Oct 26 16:40:32.004: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3385/pods","resourceVersion":"27613"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:40:32.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3385" for this suite.

• [SLOW TEST:59.405 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":305,"completed":4,"skipped":147,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:40:32.129: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5038
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5038.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-5038.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5038.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5038.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5038.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-5038.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5038.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-5038.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5038.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5038.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-5038.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5038.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-5038.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5038.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-5038.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5038.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-5038.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5038.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct 26 16:40:46.532: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5038.svc.cluster.local from pod dns-5038/dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f: the server could not find the requested resource (get pods dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f)
Oct 26 16:40:46.554: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5038.svc.cluster.local from pod dns-5038/dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f: the server could not find the requested resource (get pods dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f)
Oct 26 16:40:46.580: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5038.svc.cluster.local from pod dns-5038/dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f: the server could not find the requested resource (get pods dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f)
Oct 26 16:40:46.604: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5038.svc.cluster.local from pod dns-5038/dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f: the server could not find the requested resource (get pods dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f)
Oct 26 16:40:46.683: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5038.svc.cluster.local from pod dns-5038/dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f: the server could not find the requested resource (get pods dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f)
Oct 26 16:40:46.707: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5038.svc.cluster.local from pod dns-5038/dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f: the server could not find the requested resource (get pods dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f)
Oct 26 16:40:46.730: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5038.svc.cluster.local from pod dns-5038/dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f: the server could not find the requested resource (get pods dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f)
Oct 26 16:40:46.755: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5038.svc.cluster.local from pod dns-5038/dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f: the server could not find the requested resource (get pods dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f)
Oct 26 16:40:46.811: INFO: Lookups using dns-5038/dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5038.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5038.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5038.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5038.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5038.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5038.svc.cluster.local jessie_udp@dns-test-service-2.dns-5038.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5038.svc.cluster.local]

Oct 26 16:40:51.834: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5038.svc.cluster.local from pod dns-5038/dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f: the server could not find the requested resource (get pods dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f)
Oct 26 16:40:51.857: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5038.svc.cluster.local from pod dns-5038/dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f: the server could not find the requested resource (get pods dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f)
Oct 26 16:40:51.880: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5038.svc.cluster.local from pod dns-5038/dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f: the server could not find the requested resource (get pods dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f)
Oct 26 16:40:51.901: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5038.svc.cluster.local from pod dns-5038/dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f: the server could not find the requested resource (get pods dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f)
Oct 26 16:40:51.971: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5038.svc.cluster.local from pod dns-5038/dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f: the server could not find the requested resource (get pods dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f)
Oct 26 16:40:51.993: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5038.svc.cluster.local from pod dns-5038/dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f: the server could not find the requested resource (get pods dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f)
Oct 26 16:40:52.019: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5038.svc.cluster.local from pod dns-5038/dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f: the server could not find the requested resource (get pods dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f)
Oct 26 16:40:52.047: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5038.svc.cluster.local from pod dns-5038/dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f: the server could not find the requested resource (get pods dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f)
Oct 26 16:40:52.092: INFO: Lookups using dns-5038/dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5038.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5038.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5038.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5038.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5038.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5038.svc.cluster.local jessie_udp@dns-test-service-2.dns-5038.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5038.svc.cluster.local]

Oct 26 16:40:56.839: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5038.svc.cluster.local from pod dns-5038/dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f: the server could not find the requested resource (get pods dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f)
Oct 26 16:40:56.873: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5038.svc.cluster.local from pod dns-5038/dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f: the server could not find the requested resource (get pods dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f)
Oct 26 16:40:56.897: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5038.svc.cluster.local from pod dns-5038/dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f: the server could not find the requested resource (get pods dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f)
Oct 26 16:40:56.922: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5038.svc.cluster.local from pod dns-5038/dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f: the server could not find the requested resource (get pods dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f)
Oct 26 16:40:56.993: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5038.svc.cluster.local from pod dns-5038/dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f: the server could not find the requested resource (get pods dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f)
Oct 26 16:40:57.017: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5038.svc.cluster.local from pod dns-5038/dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f: the server could not find the requested resource (get pods dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f)
Oct 26 16:40:57.040: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5038.svc.cluster.local from pod dns-5038/dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f: the server could not find the requested resource (get pods dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f)
Oct 26 16:40:57.062: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5038.svc.cluster.local from pod dns-5038/dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f: the server could not find the requested resource (get pods dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f)
Oct 26 16:40:57.360: INFO: Lookups using dns-5038/dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5038.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5038.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5038.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5038.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5038.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5038.svc.cluster.local jessie_udp@dns-test-service-2.dns-5038.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5038.svc.cluster.local]

Oct 26 16:41:01.838: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5038.svc.cluster.local from pod dns-5038/dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f: the server could not find the requested resource (get pods dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f)
Oct 26 16:41:01.861: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5038.svc.cluster.local from pod dns-5038/dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f: the server could not find the requested resource (get pods dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f)
Oct 26 16:41:01.886: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5038.svc.cluster.local from pod dns-5038/dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f: the server could not find the requested resource (get pods dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f)
Oct 26 16:41:01.912: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5038.svc.cluster.local from pod dns-5038/dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f: the server could not find the requested resource (get pods dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f)
Oct 26 16:41:01.996: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5038.svc.cluster.local from pod dns-5038/dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f: the server could not find the requested resource (get pods dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f)
Oct 26 16:41:02.020: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5038.svc.cluster.local from pod dns-5038/dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f: the server could not find the requested resource (get pods dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f)
Oct 26 16:41:02.049: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5038.svc.cluster.local from pod dns-5038/dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f: the server could not find the requested resource (get pods dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f)
Oct 26 16:41:02.071: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5038.svc.cluster.local from pod dns-5038/dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f: the server could not find the requested resource (get pods dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f)
Oct 26 16:41:02.125: INFO: Lookups using dns-5038/dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5038.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5038.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5038.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5038.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5038.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5038.svc.cluster.local jessie_udp@dns-test-service-2.dns-5038.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5038.svc.cluster.local]

Oct 26 16:41:07.141: INFO: DNS probes using dns-5038/dns-test-f3cc8cf8-fac1-47d7-b99a-2b9af4950c5f succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:41:07.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5038" for this suite.

• [SLOW TEST:35.217 seconds]
[sig-network] DNS
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":305,"completed":5,"skipped":156,"failed":0}
S
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:41:07.346: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3229
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-projected-all-test-volume-b843cd4a-c9e9-4c2b-9df5-c189d9450c11
STEP: Creating secret with name secret-projected-all-test-volume-e659ef64-857f-40c1-a784-f4c513bad89f
STEP: Creating a pod to test Check all projections for projected volume plugin
Oct 26 16:41:07.697: INFO: Waiting up to 5m0s for pod "projected-volume-758faf50-ba56-4ea8-ab92-1509435d65f7" in namespace "projected-3229" to be "Succeeded or Failed"
Oct 26 16:41:07.721: INFO: Pod "projected-volume-758faf50-ba56-4ea8-ab92-1509435d65f7": Phase="Pending", Reason="", readiness=false. Elapsed: 24.218746ms
Oct 26 16:41:09.736: INFO: Pod "projected-volume-758faf50-ba56-4ea8-ab92-1509435d65f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039015294s
Oct 26 16:41:11.752: INFO: Pod "projected-volume-758faf50-ba56-4ea8-ab92-1509435d65f7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05455156s
Oct 26 16:41:13.768: INFO: Pod "projected-volume-758faf50-ba56-4ea8-ab92-1509435d65f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.070343008s
STEP: Saw pod success
Oct 26 16:41:13.768: INFO: Pod "projected-volume-758faf50-ba56-4ea8-ab92-1509435d65f7" satisfied condition "Succeeded or Failed"
Oct 26 16:41:13.783: INFO: Trying to get logs from node 10.112.67.203 pod projected-volume-758faf50-ba56-4ea8-ab92-1509435d65f7 container projected-all-volume-test: <nil>
STEP: delete the pod
Oct 26 16:41:13.933: INFO: Waiting for pod projected-volume-758faf50-ba56-4ea8-ab92-1509435d65f7 to disappear
Oct 26 16:41:13.946: INFO: Pod projected-volume-758faf50-ba56-4ea8-ab92-1509435d65f7 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:41:13.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3229" for this suite.

• [SLOW TEST:6.658 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:32
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":305,"completed":6,"skipped":157,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:41:14.005: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1656
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Oct 26 16:41:14.293: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5273489f-0a25-4a12-9df1-f40e625c999b" in namespace "projected-1656" to be "Succeeded or Failed"
Oct 26 16:41:14.307: INFO: Pod "downwardapi-volume-5273489f-0a25-4a12-9df1-f40e625c999b": Phase="Pending", Reason="", readiness=false. Elapsed: 13.70045ms
Oct 26 16:41:16.325: INFO: Pod "downwardapi-volume-5273489f-0a25-4a12-9df1-f40e625c999b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031368487s
Oct 26 16:41:18.360: INFO: Pod "downwardapi-volume-5273489f-0a25-4a12-9df1-f40e625c999b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066915576s
STEP: Saw pod success
Oct 26 16:41:18.360: INFO: Pod "downwardapi-volume-5273489f-0a25-4a12-9df1-f40e625c999b" satisfied condition "Succeeded or Failed"
Oct 26 16:41:18.380: INFO: Trying to get logs from node 10.112.67.203 pod downwardapi-volume-5273489f-0a25-4a12-9df1-f40e625c999b container client-container: <nil>
STEP: delete the pod
Oct 26 16:41:18.452: INFO: Waiting for pod downwardapi-volume-5273489f-0a25-4a12-9df1-f40e625c999b to disappear
Oct 26 16:41:18.466: INFO: Pod downwardapi-volume-5273489f-0a25-4a12-9df1-f40e625c999b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:41:18.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1656" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":7,"skipped":200,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:41:18.533: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6363
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Oct 26 16:41:18.817: INFO: Waiting up to 5m0s for pod "pod-0caa42b4-0e95-4646-96e4-ffdb25498738" in namespace "emptydir-6363" to be "Succeeded or Failed"
Oct 26 16:41:18.835: INFO: Pod "pod-0caa42b4-0e95-4646-96e4-ffdb25498738": Phase="Pending", Reason="", readiness=false. Elapsed: 17.621643ms
Oct 26 16:41:20.852: INFO: Pod "pod-0caa42b4-0e95-4646-96e4-ffdb25498738": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034640728s
Oct 26 16:41:22.870: INFO: Pod "pod-0caa42b4-0e95-4646-96e4-ffdb25498738": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052230088s
STEP: Saw pod success
Oct 26 16:41:22.870: INFO: Pod "pod-0caa42b4-0e95-4646-96e4-ffdb25498738" satisfied condition "Succeeded or Failed"
Oct 26 16:41:22.885: INFO: Trying to get logs from node 10.112.67.203 pod pod-0caa42b4-0e95-4646-96e4-ffdb25498738 container test-container: <nil>
STEP: delete the pod
Oct 26 16:41:22.969: INFO: Waiting for pod pod-0caa42b4-0e95-4646-96e4-ffdb25498738 to disappear
Oct 26 16:41:22.982: INFO: Pod pod-0caa42b4-0e95-4646-96e4-ffdb25498738 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:41:22.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6363" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":8,"skipped":234,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:41:23.052: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2119
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-a2872908-4fed-404c-8df9-8d285be1166a
STEP: Creating a pod to test consume configMaps
Oct 26 16:41:23.364: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-11db79dc-86bf-4b54-8816-aba207e9dc85" in namespace "projected-2119" to be "Succeeded or Failed"
Oct 26 16:41:23.379: INFO: Pod "pod-projected-configmaps-11db79dc-86bf-4b54-8816-aba207e9dc85": Phase="Pending", Reason="", readiness=false. Elapsed: 14.449269ms
Oct 26 16:41:25.395: INFO: Pod "pod-projected-configmaps-11db79dc-86bf-4b54-8816-aba207e9dc85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030118226s
Oct 26 16:41:27.413: INFO: Pod "pod-projected-configmaps-11db79dc-86bf-4b54-8816-aba207e9dc85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048210617s
STEP: Saw pod success
Oct 26 16:41:27.413: INFO: Pod "pod-projected-configmaps-11db79dc-86bf-4b54-8816-aba207e9dc85" satisfied condition "Succeeded or Failed"
Oct 26 16:41:27.429: INFO: Trying to get logs from node 10.112.67.203 pod pod-projected-configmaps-11db79dc-86bf-4b54-8816-aba207e9dc85 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Oct 26 16:41:27.505: INFO: Waiting for pod pod-projected-configmaps-11db79dc-86bf-4b54-8816-aba207e9dc85 to disappear
Oct 26 16:41:27.526: INFO: Pod pod-projected-configmaps-11db79dc-86bf-4b54-8816-aba207e9dc85 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:41:27.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2119" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":9,"skipped":248,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:41:27.623: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-8485
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:41:29.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8485" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":10,"skipped":258,"failed":0}
S
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:41:30.051: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2156
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should find a service from listing all namespaces [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:41:30.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2156" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":305,"completed":11,"skipped":259,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:41:30.376: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-5311
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Oct 26 16:41:31.257: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Oct 26 16:41:33.322: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739327291, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739327291, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739327291, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739327291, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 26 16:41:36.388: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 26 16:41:36.409: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:41:38.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5311" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:8.094 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":305,"completed":12,"skipped":265,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:41:38.470: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7968
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 26 16:41:40.191: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 26 16:41:42.241: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739327300, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739327300, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739327300, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739327300, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 26 16:41:45.317: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
Oct 26 16:41:55.451: INFO: Waiting for webhook configuration to be ready...
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:41:55.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7968" for this suite.
STEP: Destroying namespace "webhook-7968-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:17.584 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":305,"completed":13,"skipped":278,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:41:56.054: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7679
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Oct 26 16:41:56.362: INFO: Waiting up to 5m0s for pod "downward-api-7ec1e7ab-11e7-43bb-a39c-9a8b49470b50" in namespace "downward-api-7679" to be "Succeeded or Failed"
Oct 26 16:41:56.385: INFO: Pod "downward-api-7ec1e7ab-11e7-43bb-a39c-9a8b49470b50": Phase="Pending", Reason="", readiness=false. Elapsed: 23.321863ms
Oct 26 16:41:58.400: INFO: Pod "downward-api-7ec1e7ab-11e7-43bb-a39c-9a8b49470b50": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038641208s
Oct 26 16:42:00.419: INFO: Pod "downward-api-7ec1e7ab-11e7-43bb-a39c-9a8b49470b50": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05726614s
Oct 26 16:42:02.435: INFO: Pod "downward-api-7ec1e7ab-11e7-43bb-a39c-9a8b49470b50": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.07349514s
STEP: Saw pod success
Oct 26 16:42:02.435: INFO: Pod "downward-api-7ec1e7ab-11e7-43bb-a39c-9a8b49470b50" satisfied condition "Succeeded or Failed"
Oct 26 16:42:02.463: INFO: Trying to get logs from node 10.112.67.207 pod downward-api-7ec1e7ab-11e7-43bb-a39c-9a8b49470b50 container dapi-container: <nil>
STEP: delete the pod
Oct 26 16:42:02.601: INFO: Waiting for pod downward-api-7ec1e7ab-11e7-43bb-a39c-9a8b49470b50 to disappear
Oct 26 16:42:02.615: INFO: Pod downward-api-7ec1e7ab-11e7-43bb-a39c-9a8b49470b50 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:42:02.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7679" for this suite.

• [SLOW TEST:6.614 seconds]
[sig-node] Downward API
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":305,"completed":14,"skipped":292,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:42:02.677: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9245
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-0ed1a777-908a-48d0-8b27-adcab9e4b500
STEP: Creating configMap with name cm-test-opt-upd-c8e9ea4d-34f0-4364-af13-1879fd6ca37a
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-0ed1a777-908a-48d0-8b27-adcab9e4b500
STEP: Updating configmap cm-test-opt-upd-c8e9ea4d-34f0-4364-af13-1879fd6ca37a
STEP: Creating configMap with name cm-test-opt-create-cee779a1-f04f-403e-96c3-14f0b79491c1
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:43:23.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9245" for this suite.

• [SLOW TEST:80.853 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":15,"skipped":332,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:43:23.529: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9032
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name projected-secret-test-f61fec64-cd15-4bd6-8e44-bc880cdf1f6a
STEP: Creating a pod to test consume secrets
Oct 26 16:43:23.826: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7a244577-436f-4417-91ed-87a596b7cc35" in namespace "projected-9032" to be "Succeeded or Failed"
Oct 26 16:43:23.841: INFO: Pod "pod-projected-secrets-7a244577-436f-4417-91ed-87a596b7cc35": Phase="Pending", Reason="", readiness=false. Elapsed: 14.96698ms
Oct 26 16:43:25.857: INFO: Pod "pod-projected-secrets-7a244577-436f-4417-91ed-87a596b7cc35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030723881s
Oct 26 16:43:27.874: INFO: Pod "pod-projected-secrets-7a244577-436f-4417-91ed-87a596b7cc35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047926631s
STEP: Saw pod success
Oct 26 16:43:27.874: INFO: Pod "pod-projected-secrets-7a244577-436f-4417-91ed-87a596b7cc35" satisfied condition "Succeeded or Failed"
Oct 26 16:43:27.890: INFO: Trying to get logs from node 10.112.67.207 pod pod-projected-secrets-7a244577-436f-4417-91ed-87a596b7cc35 container secret-volume-test: <nil>
STEP: delete the pod
Oct 26 16:43:27.984: INFO: Waiting for pod pod-projected-secrets-7a244577-436f-4417-91ed-87a596b7cc35 to disappear
Oct 26 16:43:27.998: INFO: Pod pod-projected-secrets-7a244577-436f-4417-91ed-87a596b7cc35 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:43:27.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9032" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":305,"completed":16,"skipped":348,"failed":0}
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:43:28.064: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7481
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's command
Oct 26 16:43:28.355: INFO: Waiting up to 5m0s for pod "var-expansion-b0b4c5a7-fe0c-4df5-8abb-ddfa1c885818" in namespace "var-expansion-7481" to be "Succeeded or Failed"
Oct 26 16:43:28.370: INFO: Pod "var-expansion-b0b4c5a7-fe0c-4df5-8abb-ddfa1c885818": Phase="Pending", Reason="", readiness=false. Elapsed: 14.806794ms
Oct 26 16:43:30.389: INFO: Pod "var-expansion-b0b4c5a7-fe0c-4df5-8abb-ddfa1c885818": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032955786s
STEP: Saw pod success
Oct 26 16:43:30.389: INFO: Pod "var-expansion-b0b4c5a7-fe0c-4df5-8abb-ddfa1c885818" satisfied condition "Succeeded or Failed"
Oct 26 16:43:30.403: INFO: Trying to get logs from node 10.112.67.207 pod var-expansion-b0b4c5a7-fe0c-4df5-8abb-ddfa1c885818 container dapi-container: <nil>
STEP: delete the pod
Oct 26 16:43:30.486: INFO: Waiting for pod var-expansion-b0b4c5a7-fe0c-4df5-8abb-ddfa1c885818 to disappear
Oct 26 16:43:30.501: INFO: Pod var-expansion-b0b4c5a7-fe0c-4df5-8abb-ddfa1c885818 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:43:30.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7481" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":305,"completed":17,"skipped":352,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:43:30.560: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1007
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Update Demo
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:308
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Oct 26 16:43:30.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 create -f - --namespace=kubectl-1007'
Oct 26 16:43:31.331: INFO: stderr: ""
Oct 26 16:43:31.331: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Oct 26 16:43:31.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1007'
Oct 26 16:43:31.494: INFO: stderr: ""
Oct 26 16:43:31.494: INFO: stdout: "update-demo-nautilus-p542v update-demo-nautilus-pqx66 "
Oct 26 16:43:31.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 get pods update-demo-nautilus-p542v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1007'
Oct 26 16:43:31.628: INFO: stderr: ""
Oct 26 16:43:31.628: INFO: stdout: ""
Oct 26 16:43:31.628: INFO: update-demo-nautilus-p542v is created but not running
Oct 26 16:43:36.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1007'
Oct 26 16:43:36.761: INFO: stderr: ""
Oct 26 16:43:36.761: INFO: stdout: "update-demo-nautilus-p542v update-demo-nautilus-pqx66 "
Oct 26 16:43:36.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 get pods update-demo-nautilus-p542v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1007'
Oct 26 16:43:36.890: INFO: stderr: ""
Oct 26 16:43:36.890: INFO: stdout: "true"
Oct 26 16:43:36.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 get pods update-demo-nautilus-p542v -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1007'
Oct 26 16:43:37.023: INFO: stderr: ""
Oct 26 16:43:37.023: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Oct 26 16:43:37.023: INFO: validating pod update-demo-nautilus-p542v
Oct 26 16:43:37.063: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 26 16:43:37.064: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 26 16:43:37.064: INFO: update-demo-nautilus-p542v is verified up and running
Oct 26 16:43:37.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 get pods update-demo-nautilus-pqx66 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1007'
Oct 26 16:43:37.197: INFO: stderr: ""
Oct 26 16:43:37.197: INFO: stdout: "true"
Oct 26 16:43:37.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 get pods update-demo-nautilus-pqx66 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1007'
Oct 26 16:43:37.314: INFO: stderr: ""
Oct 26 16:43:37.314: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Oct 26 16:43:37.314: INFO: validating pod update-demo-nautilus-pqx66
Oct 26 16:43:37.349: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 26 16:43:37.349: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 26 16:43:37.349: INFO: update-demo-nautilus-pqx66 is verified up and running
STEP: using delete to clean up resources
Oct 26 16:43:37.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 delete --grace-period=0 --force -f - --namespace=kubectl-1007'
Oct 26 16:43:37.525: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 26 16:43:37.525: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Oct 26 16:43:37.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1007'
Oct 26 16:43:37.702: INFO: stderr: "No resources found in kubectl-1007 namespace.\n"
Oct 26 16:43:37.702: INFO: stdout: ""
Oct 26 16:43:37.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 get pods -l name=update-demo --namespace=kubectl-1007 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Oct 26 16:43:37.856: INFO: stderr: ""
Oct 26 16:43:37.856: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:43:37.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1007" for this suite.

• [SLOW TEST:7.355 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:306
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":305,"completed":18,"skipped":361,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:43:37.915: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-8309
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:43:38.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8309" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":305,"completed":19,"skipped":379,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:43:38.517: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5844
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:161
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:43:38.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5844" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":305,"completed":20,"skipped":432,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:43:38.870: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8743
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-0b1af7da-ac98-431a-ba2c-b5459964a72c
STEP: Creating a pod to test consume secrets
Oct 26 16:43:39.332: INFO: Waiting up to 5m0s for pod "pod-secrets-fe5f5099-3a59-4e00-8a4f-db107e80c868" in namespace "secrets-8743" to be "Succeeded or Failed"
Oct 26 16:43:39.346: INFO: Pod "pod-secrets-fe5f5099-3a59-4e00-8a4f-db107e80c868": Phase="Pending", Reason="", readiness=false. Elapsed: 13.852946ms
Oct 26 16:43:41.363: INFO: Pod "pod-secrets-fe5f5099-3a59-4e00-8a4f-db107e80c868": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031200501s
Oct 26 16:43:43.379: INFO: Pod "pod-secrets-fe5f5099-3a59-4e00-8a4f-db107e80c868": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046877476s
STEP: Saw pod success
Oct 26 16:43:43.379: INFO: Pod "pod-secrets-fe5f5099-3a59-4e00-8a4f-db107e80c868" satisfied condition "Succeeded or Failed"
Oct 26 16:43:43.394: INFO: Trying to get logs from node 10.112.67.207 pod pod-secrets-fe5f5099-3a59-4e00-8a4f-db107e80c868 container secret-volume-test: <nil>
STEP: delete the pod
Oct 26 16:43:43.473: INFO: Waiting for pod pod-secrets-fe5f5099-3a59-4e00-8a4f-db107e80c868 to disappear
Oct 26 16:43:43.487: INFO: Pod pod-secrets-fe5f5099-3a59-4e00-8a4f-db107e80c868 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:43:43.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8743" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":305,"completed":21,"skipped":444,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:43:43.557: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8936
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Oct 26 16:43:43.833: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:44:15.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8936" for this suite.

• [SLOW TEST:31.948 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":305,"completed":22,"skipped":464,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:44:15.506: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3823
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Update Demo
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:308
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Oct 26 16:44:15.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 create -f - --namespace=kubectl-3823'
Oct 26 16:44:16.354: INFO: stderr: ""
Oct 26 16:44:16.355: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Oct 26 16:44:16.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3823'
Oct 26 16:44:16.510: INFO: stderr: ""
Oct 26 16:44:16.510: INFO: stdout: "update-demo-nautilus-c9s9m update-demo-nautilus-md64p "
Oct 26 16:44:16.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 get pods update-demo-nautilus-c9s9m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3823'
Oct 26 16:44:16.642: INFO: stderr: ""
Oct 26 16:44:16.642: INFO: stdout: ""
Oct 26 16:44:16.642: INFO: update-demo-nautilus-c9s9m is created but not running
Oct 26 16:44:21.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3823'
Oct 26 16:44:22.130: INFO: stderr: ""
Oct 26 16:44:22.130: INFO: stdout: "update-demo-nautilus-c9s9m update-demo-nautilus-md64p "
Oct 26 16:44:22.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 get pods update-demo-nautilus-c9s9m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3823'
Oct 26 16:44:22.257: INFO: stderr: ""
Oct 26 16:44:22.257: INFO: stdout: "true"
Oct 26 16:44:22.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 get pods update-demo-nautilus-c9s9m -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3823'
Oct 26 16:44:22.376: INFO: stderr: ""
Oct 26 16:44:22.376: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Oct 26 16:44:22.376: INFO: validating pod update-demo-nautilus-c9s9m
Oct 26 16:44:22.411: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 26 16:44:22.411: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 26 16:44:22.411: INFO: update-demo-nautilus-c9s9m is verified up and running
Oct 26 16:44:22.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 get pods update-demo-nautilus-md64p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3823'
Oct 26 16:44:22.540: INFO: stderr: ""
Oct 26 16:44:22.540: INFO: stdout: "true"
Oct 26 16:44:22.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 get pods update-demo-nautilus-md64p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3823'
Oct 26 16:44:22.671: INFO: stderr: ""
Oct 26 16:44:22.671: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Oct 26 16:44:22.671: INFO: validating pod update-demo-nautilus-md64p
Oct 26 16:44:22.706: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 26 16:44:22.706: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 26 16:44:22.706: INFO: update-demo-nautilus-md64p is verified up and running
STEP: scaling down the replication controller
Oct 26 16:44:22.709: INFO: scanned /root for discovery docs: <nil>
Oct 26 16:44:22.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-3823'
Oct 26 16:44:23.883: INFO: stderr: ""
Oct 26 16:44:23.883: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Oct 26 16:44:23.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3823'
Oct 26 16:44:24.027: INFO: stderr: ""
Oct 26 16:44:24.027: INFO: stdout: "update-demo-nautilus-c9s9m update-demo-nautilus-md64p "
STEP: Replicas for name=update-demo: expected=1 actual=2
Oct 26 16:44:29.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3823'
Oct 26 16:44:29.168: INFO: stderr: ""
Oct 26 16:44:29.169: INFO: stdout: "update-demo-nautilus-c9s9m update-demo-nautilus-md64p "
STEP: Replicas for name=update-demo: expected=1 actual=2
Oct 26 16:44:34.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3823'
Oct 26 16:44:34.300: INFO: stderr: ""
Oct 26 16:44:34.300: INFO: stdout: "update-demo-nautilus-md64p "
Oct 26 16:44:34.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 get pods update-demo-nautilus-md64p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3823'
Oct 26 16:44:34.437: INFO: stderr: ""
Oct 26 16:44:34.437: INFO: stdout: "true"
Oct 26 16:44:34.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 get pods update-demo-nautilus-md64p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3823'
Oct 26 16:44:34.571: INFO: stderr: ""
Oct 26 16:44:34.571: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Oct 26 16:44:34.571: INFO: validating pod update-demo-nautilus-md64p
Oct 26 16:44:34.603: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 26 16:44:34.603: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 26 16:44:34.603: INFO: update-demo-nautilus-md64p is verified up and running
STEP: scaling up the replication controller
Oct 26 16:44:34.606: INFO: scanned /root for discovery docs: <nil>
Oct 26 16:44:34.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-3823'
Oct 26 16:44:35.830: INFO: stderr: ""
Oct 26 16:44:35.830: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Oct 26 16:44:35.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3823'
Oct 26 16:44:35.976: INFO: stderr: ""
Oct 26 16:44:35.976: INFO: stdout: "update-demo-nautilus-47rns update-demo-nautilus-md64p "
Oct 26 16:44:35.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 get pods update-demo-nautilus-47rns -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3823'
Oct 26 16:44:36.107: INFO: stderr: ""
Oct 26 16:44:36.107: INFO: stdout: ""
Oct 26 16:44:36.107: INFO: update-demo-nautilus-47rns is created but not running
Oct 26 16:44:41.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3823'
Oct 26 16:44:41.249: INFO: stderr: ""
Oct 26 16:44:41.249: INFO: stdout: "update-demo-nautilus-47rns update-demo-nautilus-md64p "
Oct 26 16:44:41.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 get pods update-demo-nautilus-47rns -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3823'
Oct 26 16:44:41.382: INFO: stderr: ""
Oct 26 16:44:41.382: INFO: stdout: "true"
Oct 26 16:44:41.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 get pods update-demo-nautilus-47rns -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3823'
Oct 26 16:44:41.508: INFO: stderr: ""
Oct 26 16:44:41.508: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Oct 26 16:44:41.508: INFO: validating pod update-demo-nautilus-47rns
Oct 26 16:44:41.540: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 26 16:44:41.540: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 26 16:44:41.540: INFO: update-demo-nautilus-47rns is verified up and running
Oct 26 16:44:41.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 get pods update-demo-nautilus-md64p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3823'
Oct 26 16:44:41.671: INFO: stderr: ""
Oct 26 16:44:41.671: INFO: stdout: "true"
Oct 26 16:44:41.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 get pods update-demo-nautilus-md64p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3823'
Oct 26 16:44:41.812: INFO: stderr: ""
Oct 26 16:44:41.812: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Oct 26 16:44:41.812: INFO: validating pod update-demo-nautilus-md64p
Oct 26 16:44:41.837: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 26 16:44:41.837: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 26 16:44:41.837: INFO: update-demo-nautilus-md64p is verified up and running
STEP: using delete to clean up resources
Oct 26 16:44:41.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 delete --grace-period=0 --force -f - --namespace=kubectl-3823'
Oct 26 16:44:41.992: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 26 16:44:41.992: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Oct 26 16:44:41.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-3823'
Oct 26 16:44:42.153: INFO: stderr: "No resources found in kubectl-3823 namespace.\n"
Oct 26 16:44:42.153: INFO: stdout: ""
Oct 26 16:44:42.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 get pods -l name=update-demo --namespace=kubectl-3823 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Oct 26 16:44:42.295: INFO: stderr: ""
Oct 26 16:44:42.295: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:44:42.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3823" for this suite.

• [SLOW TEST:26.847 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:306
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":305,"completed":23,"skipped":504,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:44:42.353: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1252
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-1252
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Oct 26 16:44:42.662: INFO: Found 0 stateful pods, waiting for 3
Oct 26 16:44:52.680: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Oct 26 16:44:52.680: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Oct 26 16:44:52.680: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Oct 26 16:44:52.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=statefulset-1252 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 26 16:44:53.240: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 26 16:44:53.240: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 26 16:44:53.240: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Oct 26 16:45:03.369: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Oct 26 16:45:13.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=statefulset-1252 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 16:45:13.866: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Oct 26 16:45:13.866: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Oct 26 16:45:13.866: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Oct 26 16:45:23.968: INFO: Waiting for StatefulSet statefulset-1252/ss2 to complete update
Oct 26 16:45:23.968: INFO: Waiting for Pod statefulset-1252/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Oct 26 16:45:23.968: INFO: Waiting for Pod statefulset-1252/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Oct 26 16:45:34.004: INFO: Waiting for StatefulSet statefulset-1252/ss2 to complete update
Oct 26 16:45:34.004: INFO: Waiting for Pod statefulset-1252/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Oct 26 16:45:34.004: INFO: Waiting for Pod statefulset-1252/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Oct 26 16:45:44.008: INFO: Waiting for StatefulSet statefulset-1252/ss2 to complete update
Oct 26 16:45:44.008: INFO: Waiting for Pod statefulset-1252/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Oct 26 16:45:54.013: INFO: Waiting for StatefulSet statefulset-1252/ss2 to complete update
Oct 26 16:45:54.014: INFO: Waiting for Pod statefulset-1252/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Oct 26 16:46:04.000: INFO: Waiting for StatefulSet statefulset-1252/ss2 to complete update
STEP: Rolling back to a previous revision
Oct 26 16:46:14.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=statefulset-1252 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 26 16:46:14.348: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 26 16:46:14.348: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 26 16:46:14.348: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Oct 26 16:46:24.513: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Oct 26 16:46:34.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=statefulset-1252 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 16:46:34.940: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Oct 26 16:46:34.940: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Oct 26 16:46:34.940: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Oct 26 16:46:45.057: INFO: Waiting for StatefulSet statefulset-1252/ss2 to complete update
Oct 26 16:46:45.057: INFO: Waiting for Pod statefulset-1252/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Oct 26 16:46:45.057: INFO: Waiting for Pod statefulset-1252/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Oct 26 16:46:55.169: INFO: Waiting for StatefulSet statefulset-1252/ss2 to complete update
Oct 26 16:46:55.169: INFO: Waiting for Pod statefulset-1252/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Oct 26 16:46:55.169: INFO: Waiting for Pod statefulset-1252/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Oct 26 16:47:05.094: INFO: Waiting for StatefulSet statefulset-1252/ss2 to complete update
Oct 26 16:47:05.094: INFO: Waiting for Pod statefulset-1252/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Oct 26 16:47:15.091: INFO: Deleting all statefulset in ns statefulset-1252
Oct 26 16:47:15.111: INFO: Scaling statefulset ss2 to 0
Oct 26 16:47:35.191: INFO: Waiting for statefulset status.replicas updated to 0
Oct 26 16:47:35.209: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:47:35.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1252" for this suite.

• [SLOW TEST:172.992 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":305,"completed":24,"skipped":528,"failed":0}
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:47:35.345: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6512
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6512 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6512;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6512 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6512;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6512.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6512.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6512.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6512.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6512.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6512.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6512.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6512.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6512.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6512.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6512.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6512.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6512.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 7.235.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.235.7_udp@PTR;check="$$(dig +tcp +noall +answer +search 7.235.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.235.7_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6512 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6512;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6512 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6512;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6512.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6512.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6512.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6512.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6512.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6512.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6512.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6512.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6512.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6512.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6512.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6512.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6512.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 7.235.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.235.7_udp@PTR;check="$$(dig +tcp +noall +answer +search 7.235.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.235.7_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct 26 16:47:39.796: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:39.820: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:39.845: INFO: Unable to read wheezy_udp@dns-test-service.dns-6512 from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:39.867: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6512 from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:39.894: INFO: Unable to read wheezy_udp@dns-test-service.dns-6512.svc from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:39.919: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6512.svc from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:39.943: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6512.svc from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:39.967: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6512.svc from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:40.151: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:40.174: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:40.200: INFO: Unable to read jessie_udp@dns-test-service.dns-6512 from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:40.223: INFO: Unable to read jessie_tcp@dns-test-service.dns-6512 from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:40.251: INFO: Unable to read jessie_udp@dns-test-service.dns-6512.svc from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:40.273: INFO: Unable to read jessie_tcp@dns-test-service.dns-6512.svc from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:40.299: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6512.svc from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:40.476: INFO: Lookups using dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6512 wheezy_tcp@dns-test-service.dns-6512 wheezy_udp@dns-test-service.dns-6512.svc wheezy_tcp@dns-test-service.dns-6512.svc wheezy_udp@_http._tcp.dns-test-service.dns-6512.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6512.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6512 jessie_tcp@dns-test-service.dns-6512 jessie_udp@dns-test-service.dns-6512.svc jessie_tcp@dns-test-service.dns-6512.svc jessie_udp@_http._tcp.dns-test-service.dns-6512.svc]

Oct 26 16:47:45.504: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:45.539: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:45.562: INFO: Unable to read wheezy_udp@dns-test-service.dns-6512 from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:45.585: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6512 from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:45.608: INFO: Unable to read wheezy_udp@dns-test-service.dns-6512.svc from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:45.630: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6512.svc from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:45.854: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:45.883: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:45.906: INFO: Unable to read jessie_udp@dns-test-service.dns-6512 from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:45.929: INFO: Unable to read jessie_tcp@dns-test-service.dns-6512 from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:45.955: INFO: Unable to read jessie_udp@dns-test-service.dns-6512.svc from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:45.995: INFO: Unable to read jessie_tcp@dns-test-service.dns-6512.svc from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:46.229: INFO: Lookups using dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6512 wheezy_tcp@dns-test-service.dns-6512 wheezy_udp@dns-test-service.dns-6512.svc wheezy_tcp@dns-test-service.dns-6512.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6512 jessie_tcp@dns-test-service.dns-6512 jessie_udp@dns-test-service.dns-6512.svc jessie_tcp@dns-test-service.dns-6512.svc]

Oct 26 16:47:50.501: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:50.529: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:50.555: INFO: Unable to read wheezy_udp@dns-test-service.dns-6512 from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:50.577: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6512 from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:50.601: INFO: Unable to read wheezy_udp@dns-test-service.dns-6512.svc from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:50.626: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6512.svc from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:50.868: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:50.892: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:50.918: INFO: Unable to read jessie_udp@dns-test-service.dns-6512 from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:50.943: INFO: Unable to read jessie_tcp@dns-test-service.dns-6512 from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:50.966: INFO: Unable to read jessie_udp@dns-test-service.dns-6512.svc from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:50.989: INFO: Unable to read jessie_tcp@dns-test-service.dns-6512.svc from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:51.245: INFO: Lookups using dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6512 wheezy_tcp@dns-test-service.dns-6512 wheezy_udp@dns-test-service.dns-6512.svc wheezy_tcp@dns-test-service.dns-6512.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6512 jessie_tcp@dns-test-service.dns-6512 jessie_udp@dns-test-service.dns-6512.svc jessie_tcp@dns-test-service.dns-6512.svc]

Oct 26 16:47:55.508: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:55.532: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:55.559: INFO: Unable to read wheezy_udp@dns-test-service.dns-6512 from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:55.585: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6512 from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:55.611: INFO: Unable to read wheezy_udp@dns-test-service.dns-6512.svc from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:55.635: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6512.svc from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:55.879: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:55.905: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:55.927: INFO: Unable to read jessie_udp@dns-test-service.dns-6512 from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:55.949: INFO: Unable to read jessie_tcp@dns-test-service.dns-6512 from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:55.972: INFO: Unable to read jessie_udp@dns-test-service.dns-6512.svc from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:55.999: INFO: Unable to read jessie_tcp@dns-test-service.dns-6512.svc from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:47:56.220: INFO: Lookups using dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6512 wheezy_tcp@dns-test-service.dns-6512 wheezy_udp@dns-test-service.dns-6512.svc wheezy_tcp@dns-test-service.dns-6512.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6512 jessie_tcp@dns-test-service.dns-6512 jessie_udp@dns-test-service.dns-6512.svc jessie_tcp@dns-test-service.dns-6512.svc]

Oct 26 16:48:00.508: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:48:00.533: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:48:00.566: INFO: Unable to read wheezy_udp@dns-test-service.dns-6512 from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:48:00.712: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6512 from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:48:00.738: INFO: Unable to read wheezy_udp@dns-test-service.dns-6512.svc from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:48:00.773: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6512.svc from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:48:00.990: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:48:01.030: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:48:01.059: INFO: Unable to read jessie_udp@dns-test-service.dns-6512 from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:48:01.083: INFO: Unable to read jessie_tcp@dns-test-service.dns-6512 from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:48:01.115: INFO: Unable to read jessie_udp@dns-test-service.dns-6512.svc from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:48:01.139: INFO: Unable to read jessie_tcp@dns-test-service.dns-6512.svc from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:48:01.342: INFO: Lookups using dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6512 wheezy_tcp@dns-test-service.dns-6512 wheezy_udp@dns-test-service.dns-6512.svc wheezy_tcp@dns-test-service.dns-6512.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6512 jessie_tcp@dns-test-service.dns-6512 jessie_udp@dns-test-service.dns-6512.svc jessie_tcp@dns-test-service.dns-6512.svc]

Oct 26 16:48:05.504: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:48:05.534: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:48:05.560: INFO: Unable to read wheezy_udp@dns-test-service.dns-6512 from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:48:05.589: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6512 from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:48:05.616: INFO: Unable to read wheezy_udp@dns-test-service.dns-6512.svc from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:48:05.638: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6512.svc from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:48:05.870: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:48:05.901: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:48:05.924: INFO: Unable to read jessie_udp@dns-test-service.dns-6512 from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:48:05.950: INFO: Unable to read jessie_tcp@dns-test-service.dns-6512 from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:48:05.975: INFO: Unable to read jessie_udp@dns-test-service.dns-6512.svc from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:48:06.004: INFO: Unable to read jessie_tcp@dns-test-service.dns-6512.svc from pod dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d: the server could not find the requested resource (get pods dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d)
Oct 26 16:48:06.234: INFO: Lookups using dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6512 wheezy_tcp@dns-test-service.dns-6512 wheezy_udp@dns-test-service.dns-6512.svc wheezy_tcp@dns-test-service.dns-6512.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6512 jessie_tcp@dns-test-service.dns-6512 jessie_udp@dns-test-service.dns-6512.svc jessie_tcp@dns-test-service.dns-6512.svc]

Oct 26 16:48:11.220: INFO: DNS probes using dns-6512/dns-test-82b5783c-4f91-4e47-8e99-9c02060e040d succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:48:11.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6512" for this suite.

• [SLOW TEST:36.180 seconds]
[sig-network] DNS
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":305,"completed":25,"skipped":528,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:48:11.526: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9357
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:48:25.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9357" for this suite.

• [SLOW TEST:13.592 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":305,"completed":26,"skipped":533,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:48:25.118: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2542
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2542
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-2542
I1026 16:48:25.562617      27 runners.go:190] Created replication controller with name: externalname-service, namespace: services-2542, replica count: 2
I1026 16:48:28.613269      27 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 26 16:48:28.613: INFO: Creating new exec pod
Oct 26 16:48:33.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-2542 execpod675vq -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Oct 26 16:48:34.219: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Oct 26 16:48:34.219: INFO: stdout: ""
Oct 26 16:48:34.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-2542 execpod675vq -- /bin/sh -x -c nc -zv -t -w 2 172.21.66.19 80'
Oct 26 16:48:34.563: INFO: stderr: "+ nc -zv -t -w 2 172.21.66.19 80\nConnection to 172.21.66.19 80 port [tcp/http] succeeded!\n"
Oct 26 16:48:34.563: INFO: stdout: ""
Oct 26 16:48:34.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-2542 execpod675vq -- /bin/sh -x -c nc -zv -t -w 2 10.112.67.207 31936'
Oct 26 16:48:34.951: INFO: stderr: "+ nc -zv -t -w 2 10.112.67.207 31936\nConnection to 10.112.67.207 31936 port [tcp/31936] succeeded!\n"
Oct 26 16:48:34.951: INFO: stdout: ""
Oct 26 16:48:34.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-2542 execpod675vq -- /bin/sh -x -c nc -zv -t -w 2 10.112.67.203 31936'
Oct 26 16:48:35.309: INFO: stderr: "+ nc -zv -t -w 2 10.112.67.203 31936\nConnection to 10.112.67.203 31936 port [tcp/31936] succeeded!\n"
Oct 26 16:48:35.309: INFO: stdout: ""
Oct 26 16:48:35.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-2542 execpod675vq -- /bin/sh -x -c nc -zv -t -w 2 5.10.101.150 31936'
Oct 26 16:48:35.642: INFO: stderr: "+ nc -zv -t -w 2 5.10.101.150 31936\nConnection to 5.10.101.150 31936 port [tcp/31936] succeeded!\n"
Oct 26 16:48:35.642: INFO: stdout: ""
Oct 26 16:48:35.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-2542 execpod675vq -- /bin/sh -x -c nc -zv -t -w 2 5.10.101.149 31936'
Oct 26 16:48:35.994: INFO: stderr: "+ nc -zv -t -w 2 5.10.101.149 31936\nConnection to 5.10.101.149 31936 port [tcp/31936] succeeded!\n"
Oct 26 16:48:35.994: INFO: stdout: ""
Oct 26 16:48:35.994: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:48:36.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2542" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:11.081 seconds]
[sig-network] Services
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":305,"completed":27,"skipped":575,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:48:36.199: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8256
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 26 16:48:37.350: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 26 16:48:39.405: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739327717, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739327717, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739327717, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739327717, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 26 16:48:42.476: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 26 16:48:42.499: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Registering the custom resource webhook via the AdmissionRegistration API
Oct 26 16:48:43.140: INFO: Waiting for webhook configuration to be ready...
Oct 26 16:48:43.314: INFO: Waiting for webhook configuration to be ready...
Oct 26 16:48:43.420: INFO: Waiting for webhook configuration to be ready...
Oct 26 16:48:43.512: INFO: Waiting for webhook configuration to be ready...
Oct 26 16:48:43.616: INFO: Waiting for webhook configuration to be ready...
Oct 26 16:48:43.724: INFO: Waiting for webhook configuration to be ready...
Oct 26 16:48:44.835: INFO: Waiting for webhook configuration to be ready...
Oct 26 16:48:45.924: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:48:48.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8256" for this suite.
STEP: Destroying namespace "webhook-8256-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:12.199 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":305,"completed":28,"skipped":579,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:48:48.400: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-9372
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
Oct 26 16:48:49.311: INFO: created pod pod-service-account-defaultsa
Oct 26 16:48:49.311: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Oct 26 16:48:49.336: INFO: created pod pod-service-account-mountsa
Oct 26 16:48:49.336: INFO: pod pod-service-account-mountsa service account token volume mount: true
Oct 26 16:48:49.358: INFO: created pod pod-service-account-nomountsa
Oct 26 16:48:49.358: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Oct 26 16:48:49.391: INFO: created pod pod-service-account-defaultsa-mountspec
Oct 26 16:48:49.391: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Oct 26 16:48:49.414: INFO: created pod pod-service-account-mountsa-mountspec
Oct 26 16:48:49.414: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Oct 26 16:48:49.449: INFO: created pod pod-service-account-nomountsa-mountspec
Oct 26 16:48:49.449: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Oct 26 16:48:49.470: INFO: created pod pod-service-account-defaultsa-nomountspec
Oct 26 16:48:49.470: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Oct 26 16:48:49.492: INFO: created pod pod-service-account-mountsa-nomountspec
Oct 26 16:48:49.492: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Oct 26 16:48:49.513: INFO: created pod pod-service-account-nomountsa-nomountspec
Oct 26 16:48:49.513: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:48:49.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9372" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":305,"completed":29,"skipped":597,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:48:49.579: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2719
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Oct 26 16:48:53.960: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-2719 PodName:pod-sharedvolume-a86c486e-0286-4186-ade3-ea87e219f774 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 16:48:53.960: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
Oct 26 16:48:54.182: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:48:54.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2719" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":305,"completed":30,"skipped":602,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:48:54.242: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3259
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Oct 26 16:49:04.635: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1026 16:49:04.635917      27 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1026 16:49:04.635958      27 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1026 16:49:04.635967      27 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:49:04.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3259" for this suite.

• [SLOW TEST:10.462 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":305,"completed":31,"skipped":614,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:49:04.705: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4874
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 26 16:49:05.839: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 26 16:49:07.898: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739327745, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739327745, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739327745, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739327745, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 26 16:49:10.957: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 26 16:49:10.978: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1341-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:49:12.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4874" for this suite.
STEP: Destroying namespace "webhook-4874-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.475 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":305,"completed":32,"skipped":658,"failed":0}
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:49:13.180: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-9782
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 26 16:49:13.462: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-4ec64d34-8502-4b2f-a2e7-160f2727dbd2" in namespace "security-context-test-9782" to be "Succeeded or Failed"
Oct 26 16:49:13.482: INFO: Pod "busybox-readonly-false-4ec64d34-8502-4b2f-a2e7-160f2727dbd2": Phase="Pending", Reason="", readiness=false. Elapsed: 19.36899ms
Oct 26 16:49:15.500: INFO: Pod "busybox-readonly-false-4ec64d34-8502-4b2f-a2e7-160f2727dbd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.038105378s
Oct 26 16:49:15.500: INFO: Pod "busybox-readonly-false-4ec64d34-8502-4b2f-a2e7-160f2727dbd2" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:49:15.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9782" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":305,"completed":33,"skipped":658,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:49:15.560: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-932
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating secret secrets-932/secret-test-83f01c42-96e9-4ecd-81c3-7f317bc110ab
STEP: Creating a pod to test consume secrets
Oct 26 16:49:15.881: INFO: Waiting up to 5m0s for pod "pod-configmaps-ea935fa4-f5d9-4d7d-84d0-704269f951f6" in namespace "secrets-932" to be "Succeeded or Failed"
Oct 26 16:49:15.900: INFO: Pod "pod-configmaps-ea935fa4-f5d9-4d7d-84d0-704269f951f6": Phase="Pending", Reason="", readiness=false. Elapsed: 18.195101ms
Oct 26 16:49:17.916: INFO: Pod "pod-configmaps-ea935fa4-f5d9-4d7d-84d0-704269f951f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034595778s
Oct 26 16:49:19.932: INFO: Pod "pod-configmaps-ea935fa4-f5d9-4d7d-84d0-704269f951f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050031395s
STEP: Saw pod success
Oct 26 16:49:19.932: INFO: Pod "pod-configmaps-ea935fa4-f5d9-4d7d-84d0-704269f951f6" satisfied condition "Succeeded or Failed"
Oct 26 16:49:19.946: INFO: Trying to get logs from node 10.112.67.203 pod pod-configmaps-ea935fa4-f5d9-4d7d-84d0-704269f951f6 container env-test: <nil>
STEP: delete the pod
Oct 26 16:49:20.073: INFO: Waiting for pod pod-configmaps-ea935fa4-f5d9-4d7d-84d0-704269f951f6 to disappear
Oct 26 16:49:20.089: INFO: Pod pod-configmaps-ea935fa4-f5d9-4d7d-84d0-704269f951f6 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:49:20.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-932" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":305,"completed":34,"skipped":668,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:49:20.329: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5902
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 26 16:49:21.501: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 26 16:49:23.555: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739327761, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739327761, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739327761, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739327761, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 26 16:49:26.637: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:49:26.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5902" for this suite.
STEP: Destroying namespace "webhook-5902-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.648 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":305,"completed":35,"skipped":702,"failed":0}
SS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:49:26.978: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8214
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 26 16:49:27.230: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:49:31.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8214" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":305,"completed":36,"skipped":704,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:49:31.538: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-7927
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Oct 26 16:49:36.951: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:49:37.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7927" for this suite.

• [SLOW TEST:5.544 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":305,"completed":37,"skipped":725,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:49:37.082: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-7929
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Oct 26 16:49:39.443: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:49:39.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7929" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":38,"skipped":742,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:49:39.560: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8579
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-bfd63bc1-9188-428a-8e10-bb758452f65e
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:49:44.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8579" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":39,"skipped":751,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:49:44.166: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1519
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 26 16:49:45.445: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 26 16:49:47.501: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739327785, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739327785, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739327785, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739327785, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 26 16:49:50.565: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:49:50.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1519" for this suite.
STEP: Destroying namespace "webhook-1519-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.895 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":305,"completed":40,"skipped":768,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:49:51.062: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-2059
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override arguments
Oct 26 16:49:51.350: INFO: Waiting up to 5m0s for pod "client-containers-682576c0-a80f-4251-ade7-792e56efdda1" in namespace "containers-2059" to be "Succeeded or Failed"
Oct 26 16:49:51.365: INFO: Pod "client-containers-682576c0-a80f-4251-ade7-792e56efdda1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.470167ms
Oct 26 16:49:53.384: INFO: Pod "client-containers-682576c0-a80f-4251-ade7-792e56efdda1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.033705125s
STEP: Saw pod success
Oct 26 16:49:53.384: INFO: Pod "client-containers-682576c0-a80f-4251-ade7-792e56efdda1" satisfied condition "Succeeded or Failed"
Oct 26 16:49:53.400: INFO: Trying to get logs from node 10.112.67.201 pod client-containers-682576c0-a80f-4251-ade7-792e56efdda1 container test-container: <nil>
STEP: delete the pod
Oct 26 16:49:53.530: INFO: Waiting for pod client-containers-682576c0-a80f-4251-ade7-792e56efdda1 to disappear
Oct 26 16:49:53.545: INFO: Pod client-containers-682576c0-a80f-4251-ade7-792e56efdda1 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:49:53.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2059" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":305,"completed":41,"skipped":783,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:49:53.608: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4215
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Oct 26 16:49:53.902: INFO: Waiting up to 5m0s for pod "pod-c6cc8438-a776-486c-898b-9c178cf05b36" in namespace "emptydir-4215" to be "Succeeded or Failed"
Oct 26 16:49:53.918: INFO: Pod "pod-c6cc8438-a776-486c-898b-9c178cf05b36": Phase="Pending", Reason="", readiness=false. Elapsed: 15.527846ms
Oct 26 16:49:55.935: INFO: Pod "pod-c6cc8438-a776-486c-898b-9c178cf05b36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033178918s
Oct 26 16:49:57.955: INFO: Pod "pod-c6cc8438-a776-486c-898b-9c178cf05b36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052806234s
STEP: Saw pod success
Oct 26 16:49:57.955: INFO: Pod "pod-c6cc8438-a776-486c-898b-9c178cf05b36" satisfied condition "Succeeded or Failed"
Oct 26 16:49:57.970: INFO: Trying to get logs from node 10.112.67.201 pod pod-c6cc8438-a776-486c-898b-9c178cf05b36 container test-container: <nil>
STEP: delete the pod
Oct 26 16:49:58.061: INFO: Waiting for pod pod-c6cc8438-a776-486c-898b-9c178cf05b36 to disappear
Oct 26 16:49:58.077: INFO: Pod pod-c6cc8438-a776-486c-898b-9c178cf05b36 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:49:58.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4215" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":42,"skipped":805,"failed":0}

------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:49:58.152: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9771
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Oct 26 16:49:58.557: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9771 /api/v1/namespaces/watch-9771/configmaps/e2e-watch-test-resource-version 153b90a0-f5da-4fab-8b8b-e62f4e4d7e55 31606 0 2020-10-26 16:49:58 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2020-10-26 16:49:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 26 16:49:58.557: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9771 /api/v1/namespaces/watch-9771/configmaps/e2e-watch-test-resource-version 153b90a0-f5da-4fab-8b8b-e62f4e4d7e55 31607 0 2020-10-26 16:49:58 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2020-10-26 16:49:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:49:58.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9771" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":305,"completed":43,"skipped":805,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:49:58.610: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7501
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Oct 26 16:49:58.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-7501'
Oct 26 16:49:59.011: INFO: stderr: ""
Oct 26 16:49:59.011: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1550
Oct 26 16:49:59.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 delete pods e2e-test-httpd-pod --namespace=kubectl-7501'
Oct 26 16:50:07.993: INFO: stderr: ""
Oct 26 16:50:07.993: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:50:07.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7501" for this suite.

• [SLOW TEST:9.458 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1541
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":305,"completed":44,"skipped":810,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:50:08.068: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-965
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-965/configmap-test-064686c6-fd8c-4406-8690-b38d727c39cc
STEP: Creating a pod to test consume configMaps
Oct 26 16:50:08.410: INFO: Waiting up to 5m0s for pod "pod-configmaps-30c54e3c-c630-4382-932a-a146c060523c" in namespace "configmap-965" to be "Succeeded or Failed"
Oct 26 16:50:08.427: INFO: Pod "pod-configmaps-30c54e3c-c630-4382-932a-a146c060523c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.571957ms
Oct 26 16:50:10.443: INFO: Pod "pod-configmaps-30c54e3c-c630-4382-932a-a146c060523c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032870876s
STEP: Saw pod success
Oct 26 16:50:10.444: INFO: Pod "pod-configmaps-30c54e3c-c630-4382-932a-a146c060523c" satisfied condition "Succeeded or Failed"
Oct 26 16:50:10.457: INFO: Trying to get logs from node 10.112.67.207 pod pod-configmaps-30c54e3c-c630-4382-932a-a146c060523c container env-test: <nil>
STEP: delete the pod
Oct 26 16:50:10.561: INFO: Waiting for pod pod-configmaps-30c54e3c-c630-4382-932a-a146c060523c to disappear
Oct 26 16:50:10.593: INFO: Pod pod-configmaps-30c54e3c-c630-4382-932a-a146c060523c no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:50:10.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-965" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":305,"completed":45,"skipped":823,"failed":0}
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:50:10.668: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1732
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Oct 26 16:50:10.967: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Oct 26 16:50:11.001: INFO: Waiting for terminating namespaces to be deleted...
Oct 26 16:50:11.046: INFO: 
Logging pods the apiserver thinks is on node 10.112.67.201 before test
Oct 26 16:50:11.086: INFO: ibm-cloud-provider-ip-159-8-176-190-5d7d4b6856-tq96m from ibm-system started at 2020-10-26 15:04:09 +0000 UTC (1 container statuses recorded)
Oct 26 16:50:11.086: INFO: 	Container ibm-cloud-provider-ip-159-8-176-190 ready: true, restart count 0
Oct 26 16:50:11.087: INFO: olm-operator-5cbbb5c89d-9gzjm from ibm-system started at 2020-10-26 15:03:06 +0000 UTC (1 container statuses recorded)
Oct 26 16:50:11.087: INFO: 	Container olm-operator ready: true, restart count 0
Oct 26 16:50:11.087: INFO: calico-kube-controllers-68ddfff8d5-klh68 from kube-system started at 2020-10-26 15:03:05 +0000 UTC (1 container statuses recorded)
Oct 26 16:50:11.087: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Oct 26 16:50:11.087: INFO: calico-node-9zj7g from kube-system started at 2020-10-26 15:02:45 +0000 UTC (1 container statuses recorded)
Oct 26 16:50:11.087: INFO: 	Container calico-node ready: true, restart count 0
Oct 26 16:50:11.087: INFO: calico-typha-d497c4cc8-gzkpz from kube-system started at 2020-10-26 15:03:05 +0000 UTC (1 container statuses recorded)
Oct 26 16:50:11.087: INFO: 	Container calico-typha ready: true, restart count 0
Oct 26 16:50:11.087: INFO: coredns-658bf88df8-4cjw8 from kube-system started at 2020-10-26 15:11:47 +0000 UTC (1 container statuses recorded)
Oct 26 16:50:11.087: INFO: 	Container coredns ready: true, restart count 0
Oct 26 16:50:11.087: INFO: dashboard-metrics-scraper-f99788cf9-h8dpn from kube-system started at 2020-10-26 15:03:06 +0000 UTC (1 container statuses recorded)
Oct 26 16:50:11.087: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Oct 26 16:50:11.087: INFO: ibm-keepalived-watcher-2tk7w from kube-system started at 2020-10-26 15:02:45 +0000 UTC (1 container statuses recorded)
Oct 26 16:50:11.087: INFO: 	Container keepalived-watcher ready: true, restart count 0
Oct 26 16:50:11.087: INFO: ibm-master-proxy-static-10.112.67.201 from kube-system started at 2020-10-26 15:02:31 +0000 UTC (2 container statuses recorded)
Oct 26 16:50:11.087: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Oct 26 16:50:11.087: INFO: 	Container pause ready: true, restart count 0
Oct 26 16:50:11.087: INFO: ibm-storage-watcher-567779df5-xjzzb from kube-system started at 2020-10-26 15:03:06 +0000 UTC (1 container statuses recorded)
Oct 26 16:50:11.088: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Oct 26 16:50:11.088: INFO: public-crbube2gcl0spv5v02m2cg-alb1-69897d7d5-wd5xh from kube-system started at 2020-10-26 15:07:34 +0000 UTC (4 container statuses recorded)
Oct 26 16:50:11.088: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Oct 26 16:50:11.088: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Oct 26 16:50:11.088: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Oct 26 16:50:11.088: INFO: 	Container nginx-ingress ready: true, restart count 0
Oct 26 16:50:11.088: INFO: sonobuoy-e2e-job-bb5d7dc03d4b45a5 from sonobuoy started at 2020-10-26 16:37:14 +0000 UTC (2 container statuses recorded)
Oct 26 16:50:11.088: INFO: 	Container e2e ready: true, restart count 0
Oct 26 16:50:11.088: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Oct 26 16:50:11.088: INFO: sonobuoy-systemd-logs-daemon-set-4f27bf473b6a4e03-6rq9w from sonobuoy started at 2020-10-26 16:37:14 +0000 UTC (2 container statuses recorded)
Oct 26 16:50:11.088: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Oct 26 16:50:11.088: INFO: 	Container systemd-logs ready: true, restart count 0
Oct 26 16:50:11.088: INFO: 
Logging pods the apiserver thinks is on node 10.112.67.203 before test
Oct 26 16:50:11.116: INFO: addon-catalog-source-chwdh from ibm-system started at 2020-10-26 15:07:41 +0000 UTC (1 container statuses recorded)
Oct 26 16:50:11.116: INFO: 	Container configmap-registry-server ready: true, restart count 0
Oct 26 16:50:11.116: INFO: catalog-operator-7654f857d5-jp4gd from ibm-system started at 2020-10-26 15:03:03 +0000 UTC (1 container statuses recorded)
Oct 26 16:50:11.116: INFO: 	Container catalog-operator ready: true, restart count 0
Oct 26 16:50:11.116: INFO: calico-node-dcxtp from kube-system started at 2020-10-26 15:02:43 +0000 UTC (1 container statuses recorded)
Oct 26 16:50:11.116: INFO: 	Container calico-node ready: true, restart count 0
Oct 26 16:50:11.116: INFO: calico-typha-d497c4cc8-6rvls from kube-system started at 2020-10-26 15:03:05 +0000 UTC (1 container statuses recorded)
Oct 26 16:50:11.116: INFO: 	Container calico-typha ready: true, restart count 0
Oct 26 16:50:11.116: INFO: coredns-658bf88df8-qbcdt from kube-system started at 2020-10-26 15:11:47 +0000 UTC (1 container statuses recorded)
Oct 26 16:50:11.116: INFO: 	Container coredns ready: true, restart count 0
Oct 26 16:50:11.116: INFO: coredns-autoscaler-65b4b99bb7-8ztj9 from kube-system started at 2020-10-26 15:03:06 +0000 UTC (1 container statuses recorded)
Oct 26 16:50:11.116: INFO: 	Container autoscaler ready: true, restart count 0
Oct 26 16:50:11.116: INFO: ibm-file-plugin-847d88c686-jdpcc from kube-system started at 2020-10-26 15:03:06 +0000 UTC (1 container statuses recorded)
Oct 26 16:50:11.116: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Oct 26 16:50:11.116: INFO: ibm-keepalived-watcher-wlxd7 from kube-system started at 2020-10-26 15:02:43 +0000 UTC (1 container statuses recorded)
Oct 26 16:50:11.116: INFO: 	Container keepalived-watcher ready: true, restart count 0
Oct 26 16:50:11.116: INFO: ibm-master-proxy-static-10.112.67.203 from kube-system started at 2020-10-26 15:02:39 +0000 UTC (2 container statuses recorded)
Oct 26 16:50:11.116: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Oct 26 16:50:11.116: INFO: 	Container pause ready: true, restart count 0
Oct 26 16:50:11.116: INFO: kubernetes-dashboard-7c8884c686-h5xml from kube-system started at 2020-10-26 15:03:05 +0000 UTC (1 container statuses recorded)
Oct 26 16:50:11.116: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Oct 26 16:50:11.116: INFO: metrics-server-66957c64f5-4k2wb from kube-system started at 2020-10-26 15:03:42 +0000 UTC (2 container statuses recorded)
Oct 26 16:50:11.116: INFO: 	Container metrics-server ready: true, restart count 0
Oct 26 16:50:11.116: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Oct 26 16:50:11.116: INFO: vpn-7d76994fc5-68vrp from kube-system started at 2020-10-26 15:11:26 +0000 UTC (1 container statuses recorded)
Oct 26 16:50:11.116: INFO: 	Container vpn ready: true, restart count 0
Oct 26 16:50:11.116: INFO: pod-exec-websocket-1bb6a737-b8f1-47c5-821e-29a3f9c33a49 from pods-8214 started at 2020-10-26 16:49:27 +0000 UTC (1 container statuses recorded)
Oct 26 16:50:11.116: INFO: 	Container main ready: false, restart count 0
Oct 26 16:50:11.116: INFO: sonobuoy from sonobuoy started at 2020-10-26 16:37:06 +0000 UTC (1 container statuses recorded)
Oct 26 16:50:11.116: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Oct 26 16:50:11.116: INFO: sonobuoy-systemd-logs-daemon-set-4f27bf473b6a4e03-75n8p from sonobuoy started at 2020-10-26 16:37:14 +0000 UTC (2 container statuses recorded)
Oct 26 16:50:11.116: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Oct 26 16:50:11.116: INFO: 	Container systemd-logs ready: true, restart count 0
Oct 26 16:50:11.116: INFO: 
Logging pods the apiserver thinks is on node 10.112.67.207 before test
Oct 26 16:50:11.141: INFO: test-k8s-e2e-pvg-master-verification from default started at 2020-10-26 15:05:30 +0000 UTC (1 container statuses recorded)
Oct 26 16:50:11.141: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Oct 26 16:50:11.141: INFO: ibm-cloud-provider-ip-159-8-176-190-5d7d4b6856-77vh5 from ibm-system started at 2020-10-26 15:04:09 +0000 UTC (1 container statuses recorded)
Oct 26 16:50:11.141: INFO: 	Container ibm-cloud-provider-ip-159-8-176-190 ready: true, restart count 0
Oct 26 16:50:11.141: INFO: calico-node-k5p5w from kube-system started at 2020-10-26 15:02:53 +0000 UTC (1 container statuses recorded)
Oct 26 16:50:11.141: INFO: 	Container calico-node ready: true, restart count 0
Oct 26 16:50:11.141: INFO: calico-typha-d497c4cc8-6p86s from kube-system started at 2020-10-26 15:03:16 +0000 UTC (1 container statuses recorded)
Oct 26 16:50:11.141: INFO: 	Container calico-typha ready: true, restart count 0
Oct 26 16:50:11.141: INFO: coredns-658bf88df8-pn52m from kube-system started at 2020-10-26 15:11:47 +0000 UTC (1 container statuses recorded)
Oct 26 16:50:11.141: INFO: 	Container coredns ready: true, restart count 0
Oct 26 16:50:11.141: INFO: ibm-keepalived-watcher-drkhn from kube-system started at 2020-10-26 15:02:53 +0000 UTC (1 container statuses recorded)
Oct 26 16:50:11.141: INFO: 	Container keepalived-watcher ready: true, restart count 0
Oct 26 16:50:11.141: INFO: ibm-master-proxy-static-10.112.67.207 from kube-system started at 2020-10-26 15:02:38 +0000 UTC (2 container statuses recorded)
Oct 26 16:50:11.141: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Oct 26 16:50:11.141: INFO: 	Container pause ready: true, restart count 0
Oct 26 16:50:11.141: INFO: public-crbube2gcl0spv5v02m2cg-alb1-69897d7d5-rh4l5 from kube-system started at 2020-10-26 15:07:34 +0000 UTC (4 container statuses recorded)
Oct 26 16:50:11.141: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Oct 26 16:50:11.141: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Oct 26 16:50:11.141: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Oct 26 16:50:11.141: INFO: 	Container nginx-ingress ready: true, restart count 0
Oct 26 16:50:11.141: INFO: sonobuoy-systemd-logs-daemon-set-4f27bf473b6a4e03-pnpc9 from sonobuoy started at 2020-10-26 16:37:14 +0000 UTC (2 container statuses recorded)
Oct 26 16:50:11.141: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Oct 26 16:50:11.141: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-5a4596b4-1c46-4dc9-a7e8-98cd8616199f 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-5a4596b4-1c46-4dc9-a7e8-98cd8616199f off the node 10.112.67.207
STEP: verifying the node doesn't have the label kubernetes.io/e2e-5a4596b4-1c46-4dc9-a7e8-98cd8616199f
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:55:19.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1732" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:308.928 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":305,"completed":46,"skipped":827,"failed":0}
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:55:19.596: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-5179
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Oct 26 16:55:20.989: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Oct 26 16:55:23.056: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739328121, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739328121, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739328121, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739328120, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 26 16:55:26.128: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 26 16:55:26.147: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:55:27.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5179" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:8.437 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":305,"completed":47,"skipped":827,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:55:28.033: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-1176
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-x74p
STEP: Creating a pod to test atomic-volume-subpath
Oct 26 16:55:28.374: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-x74p" in namespace "subpath-1176" to be "Succeeded or Failed"
Oct 26 16:55:28.388: INFO: Pod "pod-subpath-test-configmap-x74p": Phase="Pending", Reason="", readiness=false. Elapsed: 13.465761ms
Oct 26 16:55:30.403: INFO: Pod "pod-subpath-test-configmap-x74p": Phase="Running", Reason="", readiness=true. Elapsed: 2.028971258s
Oct 26 16:55:32.428: INFO: Pod "pod-subpath-test-configmap-x74p": Phase="Running", Reason="", readiness=true. Elapsed: 4.053690823s
Oct 26 16:55:34.445: INFO: Pod "pod-subpath-test-configmap-x74p": Phase="Running", Reason="", readiness=true. Elapsed: 6.070812791s
Oct 26 16:55:36.460: INFO: Pod "pod-subpath-test-configmap-x74p": Phase="Running", Reason="", readiness=true. Elapsed: 8.086294806s
Oct 26 16:55:38.479: INFO: Pod "pod-subpath-test-configmap-x74p": Phase="Running", Reason="", readiness=true. Elapsed: 10.104782878s
Oct 26 16:55:40.505: INFO: Pod "pod-subpath-test-configmap-x74p": Phase="Running", Reason="", readiness=true. Elapsed: 12.131255852s
Oct 26 16:55:42.533: INFO: Pod "pod-subpath-test-configmap-x74p": Phase="Running", Reason="", readiness=true. Elapsed: 14.159286007s
Oct 26 16:55:44.551: INFO: Pod "pod-subpath-test-configmap-x74p": Phase="Running", Reason="", readiness=true. Elapsed: 16.177111424s
Oct 26 16:55:46.567: INFO: Pod "pod-subpath-test-configmap-x74p": Phase="Running", Reason="", readiness=true. Elapsed: 18.19251095s
Oct 26 16:55:48.592: INFO: Pod "pod-subpath-test-configmap-x74p": Phase="Running", Reason="", readiness=true. Elapsed: 20.217976864s
Oct 26 16:55:50.609: INFO: Pod "pod-subpath-test-configmap-x74p": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.234423808s
STEP: Saw pod success
Oct 26 16:55:50.609: INFO: Pod "pod-subpath-test-configmap-x74p" satisfied condition "Succeeded or Failed"
Oct 26 16:55:50.624: INFO: Trying to get logs from node 10.112.67.203 pod pod-subpath-test-configmap-x74p container test-container-subpath-configmap-x74p: <nil>
STEP: delete the pod
Oct 26 16:55:50.743: INFO: Waiting for pod pod-subpath-test-configmap-x74p to disappear
Oct 26 16:55:50.756: INFO: Pod pod-subpath-test-configmap-x74p no longer exists
STEP: Deleting pod pod-subpath-test-configmap-x74p
Oct 26 16:55:50.756: INFO: Deleting pod "pod-subpath-test-configmap-x74p" in namespace "subpath-1176"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:55:50.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1176" for this suite.

• [SLOW TEST:22.802 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":305,"completed":48,"skipped":834,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:55:50.846: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9577
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should delete a collection of pods [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pods
Oct 26 16:55:51.147: INFO: created test-pod-1
Oct 26 16:55:51.172: INFO: created test-pod-2
Oct 26 16:55:51.193: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:55:51.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9577" for this suite.
•{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":305,"completed":49,"skipped":912,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:55:51.403: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8432
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Oct 26 16:55:51.725: INFO: Waiting up to 5m0s for pod "pod-118338af-441f-4b2c-8d08-b8c0a1cb7a76" in namespace "emptydir-8432" to be "Succeeded or Failed"
Oct 26 16:55:51.741: INFO: Pod "pod-118338af-441f-4b2c-8d08-b8c0a1cb7a76": Phase="Pending", Reason="", readiness=false. Elapsed: 15.840549ms
Oct 26 16:55:53.757: INFO: Pod "pod-118338af-441f-4b2c-8d08-b8c0a1cb7a76": Phase="Running", Reason="", readiness=true. Elapsed: 2.031700393s
Oct 26 16:55:55.780: INFO: Pod "pod-118338af-441f-4b2c-8d08-b8c0a1cb7a76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054389177s
STEP: Saw pod success
Oct 26 16:55:55.780: INFO: Pod "pod-118338af-441f-4b2c-8d08-b8c0a1cb7a76" satisfied condition "Succeeded or Failed"
Oct 26 16:55:55.793: INFO: Trying to get logs from node 10.112.67.203 pod pod-118338af-441f-4b2c-8d08-b8c0a1cb7a76 container test-container: <nil>
STEP: delete the pod
Oct 26 16:55:55.866: INFO: Waiting for pod pod-118338af-441f-4b2c-8d08-b8c0a1cb7a76 to disappear
Oct 26 16:55:55.880: INFO: Pod pod-118338af-441f-4b2c-8d08-b8c0a1cb7a76 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:55:55.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8432" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":50,"skipped":921,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:55:55.933: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-100
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-9757e289-714c-46b7-a931-bfbbe9f92fa3
STEP: Creating a pod to test consume secrets
Oct 26 16:55:56.252: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0cb01aa0-276c-49d2-acb1-a86040b7d3d2" in namespace "projected-100" to be "Succeeded or Failed"
Oct 26 16:55:56.267: INFO: Pod "pod-projected-secrets-0cb01aa0-276c-49d2-acb1-a86040b7d3d2": Phase="Pending", Reason="", readiness=false. Elapsed: 15.029251ms
Oct 26 16:55:58.284: INFO: Pod "pod-projected-secrets-0cb01aa0-276c-49d2-acb1-a86040b7d3d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032225515s
Oct 26 16:56:00.301: INFO: Pod "pod-projected-secrets-0cb01aa0-276c-49d2-acb1-a86040b7d3d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04944149s
STEP: Saw pod success
Oct 26 16:56:00.301: INFO: Pod "pod-projected-secrets-0cb01aa0-276c-49d2-acb1-a86040b7d3d2" satisfied condition "Succeeded or Failed"
Oct 26 16:56:00.317: INFO: Trying to get logs from node 10.112.67.203 pod pod-projected-secrets-0cb01aa0-276c-49d2-acb1-a86040b7d3d2 container projected-secret-volume-test: <nil>
STEP: delete the pod
Oct 26 16:56:00.396: INFO: Waiting for pod pod-projected-secrets-0cb01aa0-276c-49d2-acb1-a86040b7d3d2 to disappear
Oct 26 16:56:00.414: INFO: Pod pod-projected-secrets-0cb01aa0-276c-49d2-acb1-a86040b7d3d2 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:56:00.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-100" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":51,"skipped":931,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:56:00.472: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6659
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 26 16:56:01.619: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 26 16:56:04.735: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:56:05.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6659" for this suite.
STEP: Destroying namespace "webhook-6659-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":305,"completed":52,"skipped":935,"failed":0}

------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:56:05.410: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7376
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Oct 26 16:56:05.702: INFO: Waiting up to 5m0s for pod "downwardapi-volume-053ecccd-c9b0-4681-b720-2c44b112402d" in namespace "downward-api-7376" to be "Succeeded or Failed"
Oct 26 16:56:05.716: INFO: Pod "downwardapi-volume-053ecccd-c9b0-4681-b720-2c44b112402d": Phase="Pending", Reason="", readiness=false. Elapsed: 14.318196ms
Oct 26 16:56:07.733: INFO: Pod "downwardapi-volume-053ecccd-c9b0-4681-b720-2c44b112402d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.03103249s
STEP: Saw pod success
Oct 26 16:56:07.733: INFO: Pod "downwardapi-volume-053ecccd-c9b0-4681-b720-2c44b112402d" satisfied condition "Succeeded or Failed"
Oct 26 16:56:07.749: INFO: Trying to get logs from node 10.112.67.203 pod downwardapi-volume-053ecccd-c9b0-4681-b720-2c44b112402d container client-container: <nil>
STEP: delete the pod
Oct 26 16:56:07.956: INFO: Waiting for pod downwardapi-volume-053ecccd-c9b0-4681-b720-2c44b112402d to disappear
Oct 26 16:56:07.972: INFO: Pod downwardapi-volume-053ecccd-c9b0-4681-b720-2c44b112402d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 16:56:07.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7376" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":53,"skipped":935,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 16:56:08.034: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-25
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-136dfcb1-e1b0-4c92-aab2-105e829446e1 in namespace container-probe-25
Oct 26 16:56:12.356: INFO: Started pod liveness-136dfcb1-e1b0-4c92-aab2-105e829446e1 in namespace container-probe-25
STEP: checking the pod's current state and verifying that restartCount is present
Oct 26 16:56:12.371: INFO: Initial restart count of pod liveness-136dfcb1-e1b0-4c92-aab2-105e829446e1 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:00:12.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-25" for this suite.

• [SLOW TEST:244.567 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":305,"completed":54,"skipped":953,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:00:12.602: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1961
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-bcafec25-b427-4ecc-a662-1ef8155d28f4
STEP: Creating a pod to test consume secrets
Oct 26 17:00:12.964: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bd8ceab2-e0f5-42b9-af91-02dafdd39ec9" in namespace "projected-1961" to be "Succeeded or Failed"
Oct 26 17:00:12.989: INFO: Pod "pod-projected-secrets-bd8ceab2-e0f5-42b9-af91-02dafdd39ec9": Phase="Pending", Reason="", readiness=false. Elapsed: 24.935307ms
Oct 26 17:00:15.022: INFO: Pod "pod-projected-secrets-bd8ceab2-e0f5-42b9-af91-02dafdd39ec9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057609417s
Oct 26 17:00:17.039: INFO: Pod "pod-projected-secrets-bd8ceab2-e0f5-42b9-af91-02dafdd39ec9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.074775051s
STEP: Saw pod success
Oct 26 17:00:17.039: INFO: Pod "pod-projected-secrets-bd8ceab2-e0f5-42b9-af91-02dafdd39ec9" satisfied condition "Succeeded or Failed"
Oct 26 17:00:17.054: INFO: Trying to get logs from node 10.112.67.203 pod pod-projected-secrets-bd8ceab2-e0f5-42b9-af91-02dafdd39ec9 container projected-secret-volume-test: <nil>
STEP: delete the pod
Oct 26 17:00:17.176: INFO: Waiting for pod pod-projected-secrets-bd8ceab2-e0f5-42b9-af91-02dafdd39ec9 to disappear
Oct 26 17:00:17.192: INFO: Pod pod-projected-secrets-bd8ceab2-e0f5-42b9-af91-02dafdd39ec9 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:00:17.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1961" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":55,"skipped":959,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:00:17.264: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-539
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-c1171246-1d85-4a40-b159-0dc4f34039f0
STEP: Creating a pod to test consume secrets
Oct 26 17:00:17.576: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2599f513-2eaf-4e4c-b1d0-4443d882618c" in namespace "projected-539" to be "Succeeded or Failed"
Oct 26 17:00:17.592: INFO: Pod "pod-projected-secrets-2599f513-2eaf-4e4c-b1d0-4443d882618c": Phase="Pending", Reason="", readiness=false. Elapsed: 15.955951ms
Oct 26 17:00:19.606: INFO: Pod "pod-projected-secrets-2599f513-2eaf-4e4c-b1d0-4443d882618c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03024829s
Oct 26 17:00:21.627: INFO: Pod "pod-projected-secrets-2599f513-2eaf-4e4c-b1d0-4443d882618c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051429798s
STEP: Saw pod success
Oct 26 17:00:21.627: INFO: Pod "pod-projected-secrets-2599f513-2eaf-4e4c-b1d0-4443d882618c" satisfied condition "Succeeded or Failed"
Oct 26 17:00:21.671: INFO: Trying to get logs from node 10.112.67.203 pod pod-projected-secrets-2599f513-2eaf-4e4c-b1d0-4443d882618c container projected-secret-volume-test: <nil>
STEP: delete the pod
Oct 26 17:00:21.783: INFO: Waiting for pod pod-projected-secrets-2599f513-2eaf-4e4c-b1d0-4443d882618c to disappear
Oct 26 17:00:21.803: INFO: Pod pod-projected-secrets-2599f513-2eaf-4e4c-b1d0-4443d882618c no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:00:21.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-539" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":56,"skipped":995,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:00:21.872: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5768
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Oct 26 17:00:22.180: INFO: Waiting up to 5m0s for pod "downwardapi-volume-73997444-1f68-4628-868f-3403faa5d058" in namespace "downward-api-5768" to be "Succeeded or Failed"
Oct 26 17:00:22.194: INFO: Pod "downwardapi-volume-73997444-1f68-4628-868f-3403faa5d058": Phase="Pending", Reason="", readiness=false. Elapsed: 14.34089ms
Oct 26 17:00:24.211: INFO: Pod "downwardapi-volume-73997444-1f68-4628-868f-3403faa5d058": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031272655s
STEP: Saw pod success
Oct 26 17:00:24.211: INFO: Pod "downwardapi-volume-73997444-1f68-4628-868f-3403faa5d058" satisfied condition "Succeeded or Failed"
Oct 26 17:00:24.228: INFO: Trying to get logs from node 10.112.67.203 pod downwardapi-volume-73997444-1f68-4628-868f-3403faa5d058 container client-container: <nil>
STEP: delete the pod
Oct 26 17:00:24.322: INFO: Waiting for pod downwardapi-volume-73997444-1f68-4628-868f-3403faa5d058 to disappear
Oct 26 17:00:24.347: INFO: Pod downwardapi-volume-73997444-1f68-4628-868f-3403faa5d058 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:00:24.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5768" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":305,"completed":57,"skipped":1009,"failed":0}
SSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:00:24.409: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5162
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Oct 26 17:00:24.741: INFO: Waiting up to 5m0s for pod "downward-api-9c538bca-0bd6-4811-bb88-2de578c2facc" in namespace "downward-api-5162" to be "Succeeded or Failed"
Oct 26 17:00:24.757: INFO: Pod "downward-api-9c538bca-0bd6-4811-bb88-2de578c2facc": Phase="Pending", Reason="", readiness=false. Elapsed: 15.849363ms
Oct 26 17:00:26.777: INFO: Pod "downward-api-9c538bca-0bd6-4811-bb88-2de578c2facc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035405087s
Oct 26 17:00:28.794: INFO: Pod "downward-api-9c538bca-0bd6-4811-bb88-2de578c2facc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052920092s
STEP: Saw pod success
Oct 26 17:00:28.794: INFO: Pod "downward-api-9c538bca-0bd6-4811-bb88-2de578c2facc" satisfied condition "Succeeded or Failed"
Oct 26 17:00:28.811: INFO: Trying to get logs from node 10.112.67.203 pod downward-api-9c538bca-0bd6-4811-bb88-2de578c2facc container dapi-container: <nil>
STEP: delete the pod
Oct 26 17:00:28.895: INFO: Waiting for pod downward-api-9c538bca-0bd6-4811-bb88-2de578c2facc to disappear
Oct 26 17:00:28.909: INFO: Pod downward-api-9c538bca-0bd6-4811-bb88-2de578c2facc no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:00:28.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5162" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":305,"completed":58,"skipped":1013,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:00:28.981: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7697
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Oct 26 17:00:29.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 create -f - --namespace=kubectl-7697'
Oct 26 17:00:29.917: INFO: stderr: ""
Oct 26 17:00:29.917: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Oct 26 17:00:30.941: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 26 17:00:30.941: INFO: Found 0 / 1
Oct 26 17:00:31.940: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 26 17:00:31.940: INFO: Found 0 / 1
Oct 26 17:00:32.933: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 26 17:00:32.933: INFO: Found 1 / 1
Oct 26 17:00:32.933: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Oct 26 17:00:32.949: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 26 17:00:32.949: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Oct 26 17:00:32.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 patch pod agnhost-primary-pg5qc --namespace=kubectl-7697 -p {"metadata":{"annotations":{"x":"y"}}}'
Oct 26 17:00:33.136: INFO: stderr: ""
Oct 26 17:00:33.136: INFO: stdout: "pod/agnhost-primary-pg5qc patched\n"
STEP: checking annotations
Oct 26 17:00:33.159: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 26 17:00:33.160: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:00:33.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7697" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":305,"completed":59,"skipped":1028,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:00:33.215: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6803
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-6803
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-6803
STEP: Creating statefulset with conflicting port in namespace statefulset-6803
STEP: Waiting until pod test-pod will start running in namespace statefulset-6803
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6803
Oct 26 17:00:37.623: INFO: Observed stateful pod in namespace: statefulset-6803, name: ss-0, uid: 5d865c9a-1cad-4dcb-99de-cd24b4d8b1e1, status phase: Pending. Waiting for statefulset controller to delete.
Oct 26 17:00:37.658: INFO: Observed stateful pod in namespace: statefulset-6803, name: ss-0, uid: 5d865c9a-1cad-4dcb-99de-cd24b4d8b1e1, status phase: Failed. Waiting for statefulset controller to delete.
Oct 26 17:00:37.682: INFO: Observed stateful pod in namespace: statefulset-6803, name: ss-0, uid: 5d865c9a-1cad-4dcb-99de-cd24b4d8b1e1, status phase: Failed. Waiting for statefulset controller to delete.
Oct 26 17:00:37.703: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6803
STEP: Removing pod with conflicting port in namespace statefulset-6803
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6803 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Oct 26 17:00:41.806: INFO: Deleting all statefulset in ns statefulset-6803
Oct 26 17:00:41.824: INFO: Scaling statefulset ss to 0
Oct 26 17:00:51.890: INFO: Waiting for statefulset status.replicas updated to 0
Oct 26 17:00:51.907: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:00:51.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6803" for this suite.

• [SLOW TEST:18.829 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":305,"completed":60,"skipped":1066,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:00:52.044: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-356
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Oct 26 17:00:52.343: INFO: Waiting up to 5m0s for pod "downwardapi-volume-19509157-3267-48ea-b562-629cd6da22b2" in namespace "projected-356" to be "Succeeded or Failed"
Oct 26 17:00:52.360: INFO: Pod "downwardapi-volume-19509157-3267-48ea-b562-629cd6da22b2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.946467ms
Oct 26 17:00:54.378: INFO: Pod "downwardapi-volume-19509157-3267-48ea-b562-629cd6da22b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.035125249s
STEP: Saw pod success
Oct 26 17:00:54.379: INFO: Pod "downwardapi-volume-19509157-3267-48ea-b562-629cd6da22b2" satisfied condition "Succeeded or Failed"
Oct 26 17:00:54.393: INFO: Trying to get logs from node 10.112.67.203 pod downwardapi-volume-19509157-3267-48ea-b562-629cd6da22b2 container client-container: <nil>
STEP: delete the pod
Oct 26 17:00:54.481: INFO: Waiting for pod downwardapi-volume-19509157-3267-48ea-b562-629cd6da22b2 to disappear
Oct 26 17:00:54.496: INFO: Pod downwardapi-volume-19509157-3267-48ea-b562-629cd6da22b2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:00:54.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-356" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":305,"completed":61,"skipped":1075,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:00:54.550: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1595
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating all guestbook components
Oct 26 17:00:54.797: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Oct 26 17:00:54.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 create -f - --namespace=kubectl-1595'
Oct 26 17:00:55.290: INFO: stderr: ""
Oct 26 17:00:55.290: INFO: stdout: "service/agnhost-replica created\n"
Oct 26 17:00:55.291: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Oct 26 17:00:55.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 create -f - --namespace=kubectl-1595'
Oct 26 17:00:55.896: INFO: stderr: ""
Oct 26 17:00:55.896: INFO: stdout: "service/agnhost-primary created\n"
Oct 26 17:00:55.896: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Oct 26 17:00:55.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 create -f - --namespace=kubectl-1595'
Oct 26 17:00:56.349: INFO: stderr: ""
Oct 26 17:00:56.349: INFO: stdout: "service/frontend created\n"
Oct 26 17:00:56.349: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Oct 26 17:00:56.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 create -f - --namespace=kubectl-1595'
Oct 26 17:00:56.655: INFO: stderr: ""
Oct 26 17:00:56.655: INFO: stdout: "deployment.apps/frontend created\n"
Oct 26 17:00:56.656: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Oct 26 17:00:56.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 create -f - --namespace=kubectl-1595'
Oct 26 17:00:57.164: INFO: stderr: ""
Oct 26 17:00:57.164: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Oct 26 17:00:57.164: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Oct 26 17:00:57.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 create -f - --namespace=kubectl-1595'
Oct 26 17:00:57.480: INFO: stderr: ""
Oct 26 17:00:57.480: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Oct 26 17:00:57.480: INFO: Waiting for all frontend pods to be Running.
Oct 26 17:01:02.531: INFO: Waiting for frontend to serve content.
Oct 26 17:01:02.605: INFO: Trying to add a new entry to the guestbook.
Oct 26 17:01:02.663: INFO: Verifying that added entry can be retrieved.
Oct 26 17:01:02.713: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources
Oct 26 17:01:07.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 delete --grace-period=0 --force -f - --namespace=kubectl-1595'
Oct 26 17:01:07.969: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 26 17:01:07.969: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Oct 26 17:01:07.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 delete --grace-period=0 --force -f - --namespace=kubectl-1595'
Oct 26 17:01:08.213: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 26 17:01:08.213: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Oct 26 17:01:08.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 delete --grace-period=0 --force -f - --namespace=kubectl-1595'
Oct 26 17:01:08.432: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 26 17:01:08.432: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Oct 26 17:01:08.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 delete --grace-period=0 --force -f - --namespace=kubectl-1595'
Oct 26 17:01:08.619: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 26 17:01:08.619: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Oct 26 17:01:08.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 delete --grace-period=0 --force -f - --namespace=kubectl-1595'
Oct 26 17:01:08.809: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 26 17:01:08.809: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Oct 26 17:01:08.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 delete --grace-period=0 --force -f - --namespace=kubectl-1595'
Oct 26 17:01:08.995: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 26 17:01:08.995: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:01:08.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1595" for this suite.

• [SLOW TEST:14.516 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:351
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":305,"completed":62,"skipped":1083,"failed":0}
S
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:01:09.066: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9261
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Oct 26 17:01:09.339: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:01:27.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9261" for this suite.

• [SLOW TEST:19.004 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":305,"completed":63,"skipped":1084,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:01:28.073: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5970
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Oct 26 17:01:28.360: INFO: Waiting up to 5m0s for pod "pod-c997f95d-c04c-47c0-b0f5-962f83290e48" in namespace "emptydir-5970" to be "Succeeded or Failed"
Oct 26 17:01:28.376: INFO: Pod "pod-c997f95d-c04c-47c0-b0f5-962f83290e48": Phase="Pending", Reason="", readiness=false. Elapsed: 15.507343ms
Oct 26 17:01:30.396: INFO: Pod "pod-c997f95d-c04c-47c0-b0f5-962f83290e48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.036278277s
STEP: Saw pod success
Oct 26 17:01:30.397: INFO: Pod "pod-c997f95d-c04c-47c0-b0f5-962f83290e48" satisfied condition "Succeeded or Failed"
Oct 26 17:01:30.417: INFO: Trying to get logs from node 10.112.67.203 pod pod-c997f95d-c04c-47c0-b0f5-962f83290e48 container test-container: <nil>
STEP: delete the pod
Oct 26 17:01:30.501: INFO: Waiting for pod pod-c997f95d-c04c-47c0-b0f5-962f83290e48 to disappear
Oct 26 17:01:30.517: INFO: Pod pod-c997f95d-c04c-47c0-b0f5-962f83290e48 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:01:30.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5970" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":64,"skipped":1110,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:01:30.596: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-3517
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Oct 26 17:01:39.065: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Oct 26 17:01:39.081: INFO: Pod pod-with-prestop-exec-hook still exists
Oct 26 17:01:41.081: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Oct 26 17:01:41.099: INFO: Pod pod-with-prestop-exec-hook still exists
Oct 26 17:01:43.081: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Oct 26 17:01:43.098: INFO: Pod pod-with-prestop-exec-hook still exists
Oct 26 17:01:45.081: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Oct 26 17:01:45.106: INFO: Pod pod-with-prestop-exec-hook still exists
Oct 26 17:01:47.081: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Oct 26 17:01:47.100: INFO: Pod pod-with-prestop-exec-hook still exists
Oct 26 17:01:49.081: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Oct 26 17:01:49.098: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:01:49.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3517" for this suite.

• [SLOW TEST:18.595 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":305,"completed":65,"skipped":1126,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:01:49.192: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9366
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Oct 26 17:01:54.179: INFO: Successfully updated pod "labelsupdate46fa58f6-de0c-46c7-aa36-82053abbb8a4"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:01:56.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9366" for this suite.

• [SLOW TEST:7.133 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":305,"completed":66,"skipped":1134,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:01:56.328: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-8026
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:02:20.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8026" for this suite.

• [SLOW TEST:24.182 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":305,"completed":67,"skipped":1147,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:02:20.510: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2055
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-downwardapi-2mts
STEP: Creating a pod to test atomic-volume-subpath
Oct 26 17:02:20.863: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-2mts" in namespace "subpath-2055" to be "Succeeded or Failed"
Oct 26 17:02:20.890: INFO: Pod "pod-subpath-test-downwardapi-2mts": Phase="Pending", Reason="", readiness=false. Elapsed: 26.856722ms
Oct 26 17:02:22.904: INFO: Pod "pod-subpath-test-downwardapi-2mts": Phase="Running", Reason="", readiness=true. Elapsed: 2.041717393s
Oct 26 17:02:24.923: INFO: Pod "pod-subpath-test-downwardapi-2mts": Phase="Running", Reason="", readiness=true. Elapsed: 4.060250263s
Oct 26 17:02:26.942: INFO: Pod "pod-subpath-test-downwardapi-2mts": Phase="Running", Reason="", readiness=true. Elapsed: 6.079165873s
Oct 26 17:02:28.962: INFO: Pod "pod-subpath-test-downwardapi-2mts": Phase="Running", Reason="", readiness=true. Elapsed: 8.099459484s
Oct 26 17:02:30.981: INFO: Pod "pod-subpath-test-downwardapi-2mts": Phase="Running", Reason="", readiness=true. Elapsed: 10.117803314s
Oct 26 17:02:32.996: INFO: Pod "pod-subpath-test-downwardapi-2mts": Phase="Running", Reason="", readiness=true. Elapsed: 12.133227548s
Oct 26 17:02:35.011: INFO: Pod "pod-subpath-test-downwardapi-2mts": Phase="Running", Reason="", readiness=true. Elapsed: 14.148164284s
Oct 26 17:02:37.034: INFO: Pod "pod-subpath-test-downwardapi-2mts": Phase="Running", Reason="", readiness=true. Elapsed: 16.171195663s
Oct 26 17:02:39.049: INFO: Pod "pod-subpath-test-downwardapi-2mts": Phase="Running", Reason="", readiness=true. Elapsed: 18.186559785s
Oct 26 17:02:41.070: INFO: Pod "pod-subpath-test-downwardapi-2mts": Phase="Running", Reason="", readiness=true. Elapsed: 20.207149567s
Oct 26 17:02:43.088: INFO: Pod "pod-subpath-test-downwardapi-2mts": Phase="Running", Reason="", readiness=true. Elapsed: 22.225620273s
Oct 26 17:02:45.105: INFO: Pod "pod-subpath-test-downwardapi-2mts": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.242539114s
STEP: Saw pod success
Oct 26 17:02:45.106: INFO: Pod "pod-subpath-test-downwardapi-2mts" satisfied condition "Succeeded or Failed"
Oct 26 17:02:45.122: INFO: Trying to get logs from node 10.112.67.203 pod pod-subpath-test-downwardapi-2mts container test-container-subpath-downwardapi-2mts: <nil>
STEP: delete the pod
Oct 26 17:02:45.230: INFO: Waiting for pod pod-subpath-test-downwardapi-2mts to disappear
Oct 26 17:02:45.245: INFO: Pod pod-subpath-test-downwardapi-2mts no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-2mts
Oct 26 17:02:45.245: INFO: Deleting pod "pod-subpath-test-downwardapi-2mts" in namespace "subpath-2055"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:02:45.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2055" for this suite.

• [SLOW TEST:24.811 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":305,"completed":68,"skipped":1172,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:02:45.327: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6792
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-6792
Oct 26 17:02:47.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-6792 kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Oct 26 17:02:48.048: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Oct 26 17:02:48.048: INFO: stdout: "iptables"
Oct 26 17:02:48.048: INFO: proxyMode: iptables
Oct 26 17:02:48.086: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Oct 26 17:02:48.103: INFO: Pod kube-proxy-mode-detector still exists
Oct 26 17:02:50.103: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Oct 26 17:02:50.119: INFO: Pod kube-proxy-mode-detector still exists
Oct 26 17:02:52.103: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Oct 26 17:02:52.120: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-6792
STEP: creating replication controller affinity-clusterip-timeout in namespace services-6792
I1026 17:02:52.194600      27 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-6792, replica count: 3
I1026 17:02:55.245257      27 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 26 17:02:55.281: INFO: Creating new exec pod
Oct 26 17:03:00.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-6792 execpod-affinityxb7zn -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Oct 26 17:03:00.726: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Oct 26 17:03:00.726: INFO: stdout: ""
Oct 26 17:03:00.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-6792 execpod-affinityxb7zn -- /bin/sh -x -c nc -zv -t -w 2 172.21.13.24 80'
Oct 26 17:03:01.038: INFO: stderr: "+ nc -zv -t -w 2 172.21.13.24 80\nConnection to 172.21.13.24 80 port [tcp/http] succeeded!\n"
Oct 26 17:03:01.038: INFO: stdout: ""
Oct 26 17:03:01.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-6792 execpod-affinityxb7zn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.13.24:80/ ; done'
Oct 26 17:03:01.599: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.13.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.13.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.13.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.13.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.13.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.13.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.13.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.13.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.13.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.13.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.13.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.13.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.13.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.13.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.13.24:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.13.24:80/\n"
Oct 26 17:03:01.599: INFO: stdout: "\naffinity-clusterip-timeout-r5h94\naffinity-clusterip-timeout-r5h94\naffinity-clusterip-timeout-r5h94\naffinity-clusterip-timeout-r5h94\naffinity-clusterip-timeout-r5h94\naffinity-clusterip-timeout-r5h94\naffinity-clusterip-timeout-r5h94\naffinity-clusterip-timeout-r5h94\naffinity-clusterip-timeout-r5h94\naffinity-clusterip-timeout-r5h94\naffinity-clusterip-timeout-r5h94\naffinity-clusterip-timeout-r5h94\naffinity-clusterip-timeout-r5h94\naffinity-clusterip-timeout-r5h94\naffinity-clusterip-timeout-r5h94\naffinity-clusterip-timeout-r5h94"
Oct 26 17:03:01.599: INFO: Received response from host: affinity-clusterip-timeout-r5h94
Oct 26 17:03:01.599: INFO: Received response from host: affinity-clusterip-timeout-r5h94
Oct 26 17:03:01.599: INFO: Received response from host: affinity-clusterip-timeout-r5h94
Oct 26 17:03:01.599: INFO: Received response from host: affinity-clusterip-timeout-r5h94
Oct 26 17:03:01.599: INFO: Received response from host: affinity-clusterip-timeout-r5h94
Oct 26 17:03:01.599: INFO: Received response from host: affinity-clusterip-timeout-r5h94
Oct 26 17:03:01.599: INFO: Received response from host: affinity-clusterip-timeout-r5h94
Oct 26 17:03:01.599: INFO: Received response from host: affinity-clusterip-timeout-r5h94
Oct 26 17:03:01.599: INFO: Received response from host: affinity-clusterip-timeout-r5h94
Oct 26 17:03:01.599: INFO: Received response from host: affinity-clusterip-timeout-r5h94
Oct 26 17:03:01.599: INFO: Received response from host: affinity-clusterip-timeout-r5h94
Oct 26 17:03:01.599: INFO: Received response from host: affinity-clusterip-timeout-r5h94
Oct 26 17:03:01.599: INFO: Received response from host: affinity-clusterip-timeout-r5h94
Oct 26 17:03:01.599: INFO: Received response from host: affinity-clusterip-timeout-r5h94
Oct 26 17:03:01.599: INFO: Received response from host: affinity-clusterip-timeout-r5h94
Oct 26 17:03:01.599: INFO: Received response from host: affinity-clusterip-timeout-r5h94
Oct 26 17:03:01.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-6792 execpod-affinityxb7zn -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.13.24:80/'
Oct 26 17:03:01.955: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.13.24:80/\n"
Oct 26 17:03:01.955: INFO: stdout: "affinity-clusterip-timeout-r5h94"
Oct 26 17:03:16.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-6792 execpod-affinityxb7zn -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.13.24:80/'
Oct 26 17:03:17.331: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.13.24:80/\n"
Oct 26 17:03:17.331: INFO: stdout: "affinity-clusterip-timeout-8ht4w"
Oct 26 17:03:17.331: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-6792, will wait for the garbage collector to delete the pods
Oct 26 17:03:17.481: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 31.915104ms
Oct 26 17:03:17.581: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.206618ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:03:31.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6792" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:46.710 seconds]
[sig-network] Services
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":69,"skipped":1189,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:03:32.039: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-1342
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a Namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nspatchtest-d7607509-bcda-4982-99ea-db56bdd1efce-8690
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:03:32.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1342" for this suite.
STEP: Destroying namespace "nspatchtest-d7607509-bcda-4982-99ea-db56bdd1efce-8690" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":305,"completed":70,"skipped":1241,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:03:32.648: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8554
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Oct 26 17:04:13.068: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Oct 26 17:04:13.068: INFO: Deleting pod "simpletest.rc-4mgkm" in namespace "gc-8554"
W1026 17:04:13.068455      27 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1026 17:04:13.068498      27 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1026 17:04:13.068507      27 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Oct 26 17:04:13.111: INFO: Deleting pod "simpletest.rc-8dmdd" in namespace "gc-8554"
Oct 26 17:04:13.166: INFO: Deleting pod "simpletest.rc-bpzhq" in namespace "gc-8554"
Oct 26 17:04:13.209: INFO: Deleting pod "simpletest.rc-dw46v" in namespace "gc-8554"
Oct 26 17:04:13.246: INFO: Deleting pod "simpletest.rc-fr84d" in namespace "gc-8554"
Oct 26 17:04:13.285: INFO: Deleting pod "simpletest.rc-q69w5" in namespace "gc-8554"
Oct 26 17:04:13.333: INFO: Deleting pod "simpletest.rc-qj7c2" in namespace "gc-8554"
Oct 26 17:04:13.373: INFO: Deleting pod "simpletest.rc-rgxpm" in namespace "gc-8554"
Oct 26 17:04:13.415: INFO: Deleting pod "simpletest.rc-sfksl" in namespace "gc-8554"
Oct 26 17:04:13.460: INFO: Deleting pod "simpletest.rc-w6qqs" in namespace "gc-8554"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:04:13.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8554" for this suite.

• [SLOW TEST:40.927 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":305,"completed":71,"skipped":1254,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:04:13.576: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6138
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Oct 26 17:04:13.904: INFO: Waiting up to 5m0s for pod "pod-a48bfdcc-026b-4394-8c49-128a25fe1874" in namespace "emptydir-6138" to be "Succeeded or Failed"
Oct 26 17:04:13.920: INFO: Pod "pod-a48bfdcc-026b-4394-8c49-128a25fe1874": Phase="Pending", Reason="", readiness=false. Elapsed: 15.900901ms
Oct 26 17:04:15.940: INFO: Pod "pod-a48bfdcc-026b-4394-8c49-128a25fe1874": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.035873937s
STEP: Saw pod success
Oct 26 17:04:15.940: INFO: Pod "pod-a48bfdcc-026b-4394-8c49-128a25fe1874" satisfied condition "Succeeded or Failed"
Oct 26 17:04:15.953: INFO: Trying to get logs from node 10.112.67.203 pod pod-a48bfdcc-026b-4394-8c49-128a25fe1874 container test-container: <nil>
STEP: delete the pod
Oct 26 17:04:16.079: INFO: Waiting for pod pod-a48bfdcc-026b-4394-8c49-128a25fe1874 to disappear
Oct 26 17:04:16.094: INFO: Pod pod-a48bfdcc-026b-4394-8c49-128a25fe1874 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:04:16.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6138" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":72,"skipped":1299,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:04:16.164: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5961
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl replace
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1581
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Oct 26 17:04:16.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-5961'
Oct 26 17:04:16.581: INFO: stderr: ""
Oct 26 17:04:16.581: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Oct 26 17:04:21.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 get pod e2e-test-httpd-pod --namespace=kubectl-5961 -o json'
Oct 26 17:04:21.761: INFO: stderr: ""
Oct 26 17:04:21.761: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"172.30.31.250/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.30.31.250/32\",\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2020-10-26T17:04:16Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-10-26T17:04:16Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:annotations\": {\n                            \"f:cni.projectcalico.org/podIP\": {},\n                            \"f:cni.projectcalico.org/podIPs\": {}\n                        }\n                    }\n                },\n                \"manager\": \"calico\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-10-26T17:04:17Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"172.30.31.250\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-10-26T17:04:18Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-5961\",\n        \"resourceVersion\": \"35900\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-5961/pods/e2e-test-httpd-pod\",\n        \"uid\": \"1136ecec-2fa4-4bd2-8e67-97d63d30fd43\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-9vq88\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"10.112.67.203\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 600\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 600\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-9vq88\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-9vq88\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-10-26T17:04:16Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-10-26T17:04:18Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-10-26T17:04:18Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-10-26T17:04:16Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://b1a1aa860f736f5728b30094ce18350e79a62c91ff7315fc2080d2e24b38cf5d\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-10-26T17:04:18Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.112.67.203\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.31.250\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.30.31.250\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-10-26T17:04:16Z\"\n    }\n}\n"
STEP: replace the image in the pod
Oct 26 17:04:21.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 replace -f - --namespace=kubectl-5961'
Oct 26 17:04:22.209: INFO: stderr: ""
Oct 26 17:04:22.209: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1586
Oct 26 17:04:22.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 delete pods e2e-test-httpd-pod --namespace=kubectl-5961'
Oct 26 17:04:24.024: INFO: stderr: ""
Oct 26 17:04:24.024: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:04:24.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5961" for this suite.

• [SLOW TEST:7.942 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1577
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":305,"completed":73,"skipped":1299,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:04:24.106: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-4975
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 26 17:04:24.354: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:04:25.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4975" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":305,"completed":74,"skipped":1334,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:04:25.531: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3399
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 26 17:04:25.871: INFO: The status of Pod test-webserver-caf76cc7-6cd1-4b40-a801-0df89efc624b is Pending, waiting for it to be Running (with Ready = true)
Oct 26 17:04:27.891: INFO: The status of Pod test-webserver-caf76cc7-6cd1-4b40-a801-0df89efc624b is Pending, waiting for it to be Running (with Ready = true)
Oct 26 17:04:29.889: INFO: The status of Pod test-webserver-caf76cc7-6cd1-4b40-a801-0df89efc624b is Running (Ready = false)
Oct 26 17:04:31.888: INFO: The status of Pod test-webserver-caf76cc7-6cd1-4b40-a801-0df89efc624b is Running (Ready = false)
Oct 26 17:04:33.889: INFO: The status of Pod test-webserver-caf76cc7-6cd1-4b40-a801-0df89efc624b is Running (Ready = false)
Oct 26 17:04:35.887: INFO: The status of Pod test-webserver-caf76cc7-6cd1-4b40-a801-0df89efc624b is Running (Ready = false)
Oct 26 17:04:37.888: INFO: The status of Pod test-webserver-caf76cc7-6cd1-4b40-a801-0df89efc624b is Running (Ready = false)
Oct 26 17:04:39.894: INFO: The status of Pod test-webserver-caf76cc7-6cd1-4b40-a801-0df89efc624b is Running (Ready = false)
Oct 26 17:04:41.888: INFO: The status of Pod test-webserver-caf76cc7-6cd1-4b40-a801-0df89efc624b is Running (Ready = false)
Oct 26 17:04:43.892: INFO: The status of Pod test-webserver-caf76cc7-6cd1-4b40-a801-0df89efc624b is Running (Ready = true)
Oct 26 17:04:43.907: INFO: Container started at 2020-10-26 17:04:27 +0000 UTC, pod became ready at 2020-10-26 17:04:42 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:04:43.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3399" for this suite.

• [SLOW TEST:18.434 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":305,"completed":75,"skipped":1442,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:04:43.965: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7826
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:04:55.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7826" for this suite.

• [SLOW TEST:11.495 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":305,"completed":76,"skipped":1462,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:04:55.460: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4308
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 26 17:04:55.840: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Oct 26 17:05:01.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 --namespace=crd-publish-openapi-4308 create -f -'
Oct 26 17:05:02.661: INFO: stderr: ""
Oct 26 17:05:02.661: INFO: stdout: "e2e-test-crd-publish-openapi-421-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Oct 26 17:05:02.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 --namespace=crd-publish-openapi-4308 delete e2e-test-crd-publish-openapi-421-crds test-cr'
Oct 26 17:05:02.810: INFO: stderr: ""
Oct 26 17:05:02.811: INFO: stdout: "e2e-test-crd-publish-openapi-421-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Oct 26 17:05:02.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 --namespace=crd-publish-openapi-4308 apply -f -'
Oct 26 17:05:03.469: INFO: stderr: ""
Oct 26 17:05:03.469: INFO: stdout: "e2e-test-crd-publish-openapi-421-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Oct 26 17:05:03.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 --namespace=crd-publish-openapi-4308 delete e2e-test-crd-publish-openapi-421-crds test-cr'
Oct 26 17:05:03.647: INFO: stderr: ""
Oct 26 17:05:03.647: INFO: stdout: "e2e-test-crd-publish-openapi-421-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Oct 26 17:05:03.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 explain e2e-test-crd-publish-openapi-421-crds'
Oct 26 17:05:03.935: INFO: stderr: ""
Oct 26 17:05:03.935: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-421-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:05:10.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4308" for this suite.

• [SLOW TEST:15.551 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":305,"completed":77,"skipped":1465,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:05:11.011: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9524
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Oct 26 17:05:11.313: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e899cc8a-34e5-4824-9e2d-63550fb36915" in namespace "projected-9524" to be "Succeeded or Failed"
Oct 26 17:05:11.330: INFO: Pod "downwardapi-volume-e899cc8a-34e5-4824-9e2d-63550fb36915": Phase="Pending", Reason="", readiness=false. Elapsed: 16.895043ms
Oct 26 17:05:13.349: INFO: Pod "downwardapi-volume-e899cc8a-34e5-4824-9e2d-63550fb36915": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.035367572s
STEP: Saw pod success
Oct 26 17:05:13.349: INFO: Pod "downwardapi-volume-e899cc8a-34e5-4824-9e2d-63550fb36915" satisfied condition "Succeeded or Failed"
Oct 26 17:05:13.363: INFO: Trying to get logs from node 10.112.67.203 pod downwardapi-volume-e899cc8a-34e5-4824-9e2d-63550fb36915 container client-container: <nil>
STEP: delete the pod
Oct 26 17:05:13.437: INFO: Waiting for pod downwardapi-volume-e899cc8a-34e5-4824-9e2d-63550fb36915 to disappear
Oct 26 17:05:13.452: INFO: Pod downwardapi-volume-e899cc8a-34e5-4824-9e2d-63550fb36915 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:05:13.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9524" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":305,"completed":78,"skipped":1470,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:05:13.506: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4859
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-2bdecf6e-b71b-4f76-8942-9ef0897aee61
STEP: Creating a pod to test consume configMaps
Oct 26 17:05:13.819: INFO: Waiting up to 5m0s for pod "pod-configmaps-8fd9feeb-889a-4131-b7c1-30b135a3d9a9" in namespace "configmap-4859" to be "Succeeded or Failed"
Oct 26 17:05:13.847: INFO: Pod "pod-configmaps-8fd9feeb-889a-4131-b7c1-30b135a3d9a9": Phase="Pending", Reason="", readiness=false. Elapsed: 28.30776ms
Oct 26 17:05:15.867: INFO: Pod "pod-configmaps-8fd9feeb-889a-4131-b7c1-30b135a3d9a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047689156s
Oct 26 17:05:17.884: INFO: Pod "pod-configmaps-8fd9feeb-889a-4131-b7c1-30b135a3d9a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.064413233s
STEP: Saw pod success
Oct 26 17:05:17.884: INFO: Pod "pod-configmaps-8fd9feeb-889a-4131-b7c1-30b135a3d9a9" satisfied condition "Succeeded or Failed"
Oct 26 17:05:17.898: INFO: Trying to get logs from node 10.112.67.203 pod pod-configmaps-8fd9feeb-889a-4131-b7c1-30b135a3d9a9 container configmap-volume-test: <nil>
STEP: delete the pod
Oct 26 17:05:17.974: INFO: Waiting for pod pod-configmaps-8fd9feeb-889a-4131-b7c1-30b135a3d9a9 to disappear
Oct 26 17:05:17.989: INFO: Pod pod-configmaps-8fd9feeb-889a-4131-b7c1-30b135a3d9a9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:05:17.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4859" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":305,"completed":79,"skipped":1472,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:05:18.042: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4059
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-4059
STEP: creating service affinity-clusterip in namespace services-4059
STEP: creating replication controller affinity-clusterip in namespace services-4059
I1026 17:05:18.371405      27 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-4059, replica count: 3
I1026 17:05:21.421952      27 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1026 17:05:24.422203      27 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 26 17:05:24.468: INFO: Creating new exec pod
Oct 26 17:05:27.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-4059 execpod-affinitylxv48 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Oct 26 17:05:27.876: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Oct 26 17:05:27.876: INFO: stdout: ""
Oct 26 17:05:27.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-4059 execpod-affinitylxv48 -- /bin/sh -x -c nc -zv -t -w 2 172.21.194.90 80'
Oct 26 17:05:28.236: INFO: stderr: "+ nc -zv -t -w 2 172.21.194.90 80\nConnection to 172.21.194.90 80 port [tcp/http] succeeded!\n"
Oct 26 17:05:28.236: INFO: stdout: ""
Oct 26 17:05:28.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-4059 execpod-affinitylxv48 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.194.90:80/ ; done'
Oct 26 17:05:28.694: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.194.90:80/\n"
Oct 26 17:05:28.694: INFO: stdout: "\naffinity-clusterip-4xcjs\naffinity-clusterip-4xcjs\naffinity-clusterip-4xcjs\naffinity-clusterip-4xcjs\naffinity-clusterip-4xcjs\naffinity-clusterip-4xcjs\naffinity-clusterip-4xcjs\naffinity-clusterip-4xcjs\naffinity-clusterip-4xcjs\naffinity-clusterip-4xcjs\naffinity-clusterip-4xcjs\naffinity-clusterip-4xcjs\naffinity-clusterip-4xcjs\naffinity-clusterip-4xcjs\naffinity-clusterip-4xcjs\naffinity-clusterip-4xcjs"
Oct 26 17:05:28.694: INFO: Received response from host: affinity-clusterip-4xcjs
Oct 26 17:05:28.694: INFO: Received response from host: affinity-clusterip-4xcjs
Oct 26 17:05:28.694: INFO: Received response from host: affinity-clusterip-4xcjs
Oct 26 17:05:28.694: INFO: Received response from host: affinity-clusterip-4xcjs
Oct 26 17:05:28.694: INFO: Received response from host: affinity-clusterip-4xcjs
Oct 26 17:05:28.694: INFO: Received response from host: affinity-clusterip-4xcjs
Oct 26 17:05:28.694: INFO: Received response from host: affinity-clusterip-4xcjs
Oct 26 17:05:28.694: INFO: Received response from host: affinity-clusterip-4xcjs
Oct 26 17:05:28.694: INFO: Received response from host: affinity-clusterip-4xcjs
Oct 26 17:05:28.694: INFO: Received response from host: affinity-clusterip-4xcjs
Oct 26 17:05:28.694: INFO: Received response from host: affinity-clusterip-4xcjs
Oct 26 17:05:28.694: INFO: Received response from host: affinity-clusterip-4xcjs
Oct 26 17:05:28.694: INFO: Received response from host: affinity-clusterip-4xcjs
Oct 26 17:05:28.694: INFO: Received response from host: affinity-clusterip-4xcjs
Oct 26 17:05:28.694: INFO: Received response from host: affinity-clusterip-4xcjs
Oct 26 17:05:28.694: INFO: Received response from host: affinity-clusterip-4xcjs
Oct 26 17:05:28.694: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-4059, will wait for the garbage collector to delete the pods
Oct 26 17:05:28.848: INFO: Deleting ReplicationController affinity-clusterip took: 32.070524ms
Oct 26 17:05:28.948: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.497668ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:05:42.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4059" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:24.070 seconds]
[sig-network] Services
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":80,"skipped":1501,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:05:42.113: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8578
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1026 17:05:52.890904      27 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1026 17:05:52.891241      27 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1026 17:05:52.891399      27 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Oct 26 17:05:52.891: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Oct 26 17:05:52.891: INFO: Deleting pod "simpletest-rc-to-be-deleted-2fdcn" in namespace "gc-8578"
Oct 26 17:05:52.948: INFO: Deleting pod "simpletest-rc-to-be-deleted-5z9xr" in namespace "gc-8578"
Oct 26 17:05:52.993: INFO: Deleting pod "simpletest-rc-to-be-deleted-8r4d6" in namespace "gc-8578"
Oct 26 17:05:53.038: INFO: Deleting pod "simpletest-rc-to-be-deleted-bv2hx" in namespace "gc-8578"
Oct 26 17:05:53.084: INFO: Deleting pod "simpletest-rc-to-be-deleted-fflw8" in namespace "gc-8578"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:05:53.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8578" for this suite.

• [SLOW TEST:11.068 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":305,"completed":81,"skipped":1523,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:05:53.181: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-7451
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:05:53.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7451" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":305,"completed":82,"skipped":1542,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:05:53.630: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-498
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-7de0c1f9-140a-468d-8520-dbdc68eab150
STEP: Creating secret with name s-test-opt-upd-69500034-4cce-4c5e-9fbf-2d77e107edf9
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-7de0c1f9-140a-468d-8520-dbdc68eab150
STEP: Updating secret s-test-opt-upd-69500034-4cce-4c5e-9fbf-2d77e107edf9
STEP: Creating secret with name s-test-opt-create-d2afed90-56f1-40e7-babb-02bd3f12e569
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:06:00.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-498" for this suite.

• [SLOW TEST:6.838 seconds]
[sig-storage] Secrets
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":83,"skipped":1554,"failed":0}
SSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:06:00.469: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-4779
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:06:08.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4779" for this suite.

• [SLOW TEST:8.352 seconds]
[sig-apps] Job
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":305,"completed":84,"skipped":1557,"failed":0}
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:06:08.821: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-5890
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-5890
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Oct 26 17:06:09.076: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Oct 26 17:06:09.364: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Oct 26 17:06:11.382: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Oct 26 17:06:13.381: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 26 17:06:15.385: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 26 17:06:17.383: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 26 17:06:19.388: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 26 17:06:21.381: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 26 17:06:23.379: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 26 17:06:25.380: INFO: The status of Pod netserver-0 is Running (Ready = true)
Oct 26 17:06:25.412: INFO: The status of Pod netserver-1 is Running (Ready = true)
Oct 26 17:06:25.443: INFO: The status of Pod netserver-2 is Running (Ready = false)
Oct 26 17:06:27.460: INFO: The status of Pod netserver-2 is Running (Ready = false)
Oct 26 17:06:29.458: INFO: The status of Pod netserver-2 is Running (Ready = false)
Oct 26 17:06:31.458: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Oct 26 17:06:33.556: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.31.193:8080/dial?request=hostname&protocol=udp&host=172.30.41.221&port=8081&tries=1'] Namespace:pod-network-test-5890 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 17:06:33.556: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
Oct 26 17:06:33.765: INFO: Waiting for responses: map[]
Oct 26 17:06:33.781: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.31.193:8080/dial?request=hostname&protocol=udp&host=172.30.31.205&port=8081&tries=1'] Namespace:pod-network-test-5890 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 17:06:33.781: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
Oct 26 17:06:33.966: INFO: Waiting for responses: map[]
Oct 26 17:06:33.981: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.31.193:8080/dial?request=hostname&protocol=udp&host=172.30.67.235&port=8081&tries=1'] Namespace:pod-network-test-5890 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 17:06:33.981: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
Oct 26 17:06:34.224: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:06:34.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5890" for this suite.

• [SLOW TEST:25.488 seconds]
[sig-network] Networking
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":305,"completed":85,"skipped":1560,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:06:34.311: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9316
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-secret-7mmc
STEP: Creating a pod to test atomic-volume-subpath
Oct 26 17:06:34.642: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-7mmc" in namespace "subpath-9316" to be "Succeeded or Failed"
Oct 26 17:06:34.662: INFO: Pod "pod-subpath-test-secret-7mmc": Phase="Pending", Reason="", readiness=false. Elapsed: 19.58234ms
Oct 26 17:06:36.681: INFO: Pod "pod-subpath-test-secret-7mmc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03876148s
Oct 26 17:06:38.698: INFO: Pod "pod-subpath-test-secret-7mmc": Phase="Running", Reason="", readiness=true. Elapsed: 4.055557348s
Oct 26 17:06:40.716: INFO: Pod "pod-subpath-test-secret-7mmc": Phase="Running", Reason="", readiness=true. Elapsed: 6.073581084s
Oct 26 17:06:42.731: INFO: Pod "pod-subpath-test-secret-7mmc": Phase="Running", Reason="", readiness=true. Elapsed: 8.088902986s
Oct 26 17:06:44.748: INFO: Pod "pod-subpath-test-secret-7mmc": Phase="Running", Reason="", readiness=true. Elapsed: 10.10552383s
Oct 26 17:06:46.766: INFO: Pod "pod-subpath-test-secret-7mmc": Phase="Running", Reason="", readiness=true. Elapsed: 12.124096377s
Oct 26 17:06:48.783: INFO: Pod "pod-subpath-test-secret-7mmc": Phase="Running", Reason="", readiness=true. Elapsed: 14.140566515s
Oct 26 17:06:50.809: INFO: Pod "pod-subpath-test-secret-7mmc": Phase="Running", Reason="", readiness=true. Elapsed: 16.1667573s
Oct 26 17:06:52.824: INFO: Pod "pod-subpath-test-secret-7mmc": Phase="Running", Reason="", readiness=true. Elapsed: 18.181706693s
Oct 26 17:06:54.840: INFO: Pod "pod-subpath-test-secret-7mmc": Phase="Running", Reason="", readiness=true. Elapsed: 20.197558377s
Oct 26 17:06:56.858: INFO: Pod "pod-subpath-test-secret-7mmc": Phase="Running", Reason="", readiness=true. Elapsed: 22.215596878s
Oct 26 17:06:58.875: INFO: Pod "pod-subpath-test-secret-7mmc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.232456957s
STEP: Saw pod success
Oct 26 17:06:58.875: INFO: Pod "pod-subpath-test-secret-7mmc" satisfied condition "Succeeded or Failed"
Oct 26 17:06:58.890: INFO: Trying to get logs from node 10.112.67.207 pod pod-subpath-test-secret-7mmc container test-container-subpath-secret-7mmc: <nil>
STEP: delete the pod
Oct 26 17:06:59.040: INFO: Waiting for pod pod-subpath-test-secret-7mmc to disappear
Oct 26 17:06:59.060: INFO: Pod pod-subpath-test-secret-7mmc no longer exists
STEP: Deleting pod pod-subpath-test-secret-7mmc
Oct 26 17:06:59.061: INFO: Deleting pod "pod-subpath-test-secret-7mmc" in namespace "subpath-9316"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:06:59.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9316" for this suite.

• [SLOW TEST:24.822 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":305,"completed":86,"skipped":1573,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:06:59.133: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7746
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-7746/configmap-test-8c0ec043-8929-4b8f-9542-673913946f56
STEP: Creating a pod to test consume configMaps
Oct 26 17:06:59.437: INFO: Waiting up to 5m0s for pod "pod-configmaps-c08fe724-a3d4-4201-802d-381c6bd12006" in namespace "configmap-7746" to be "Succeeded or Failed"
Oct 26 17:06:59.455: INFO: Pod "pod-configmaps-c08fe724-a3d4-4201-802d-381c6bd12006": Phase="Pending", Reason="", readiness=false. Elapsed: 17.970431ms
Oct 26 17:07:01.471: INFO: Pod "pod-configmaps-c08fe724-a3d4-4201-802d-381c6bd12006": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033632417s
Oct 26 17:07:03.488: INFO: Pod "pod-configmaps-c08fe724-a3d4-4201-802d-381c6bd12006": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050290758s
STEP: Saw pod success
Oct 26 17:07:03.488: INFO: Pod "pod-configmaps-c08fe724-a3d4-4201-802d-381c6bd12006" satisfied condition "Succeeded or Failed"
Oct 26 17:07:03.504: INFO: Trying to get logs from node 10.112.67.203 pod pod-configmaps-c08fe724-a3d4-4201-802d-381c6bd12006 container env-test: <nil>
STEP: delete the pod
Oct 26 17:07:03.611: INFO: Waiting for pod pod-configmaps-c08fe724-a3d4-4201-802d-381c6bd12006 to disappear
Oct 26 17:07:03.624: INFO: Pod pod-configmaps-c08fe724-a3d4-4201-802d-381c6bd12006 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:07:03.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7746" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":305,"completed":87,"skipped":1590,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:07:03.671: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1019
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Oct 26 17:07:08.532: INFO: Successfully updated pod "labelsupdate67c3e08a-289d-4fb3-9b94-b31d44f530a7"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:07:10.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1019" for this suite.

• [SLOW TEST:7.001 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":305,"completed":88,"skipped":1592,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:07:10.677: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3675
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:07:10.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3675" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":305,"completed":89,"skipped":1609,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:07:10.991: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5068
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 26 17:07:11.209: INFO: Creating deployment "webserver-deployment"
Oct 26 17:07:11.230: INFO: Waiting for observed generation 1
Oct 26 17:07:13.266: INFO: Waiting for all required pods to come up
Oct 26 17:07:13.291: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Oct 26 17:07:15.343: INFO: Waiting for deployment "webserver-deployment" to complete
Oct 26 17:07:15.367: INFO: Updating deployment "webserver-deployment" with a non-existent image
Oct 26 17:07:15.397: INFO: Updating deployment webserver-deployment
Oct 26 17:07:15.397: INFO: Waiting for observed generation 2
Oct 26 17:07:17.424: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Oct 26 17:07:17.445: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Oct 26 17:07:17.468: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Oct 26 17:07:17.516: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Oct 26 17:07:17.516: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Oct 26 17:07:17.539: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Oct 26 17:07:17.569: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Oct 26 17:07:17.569: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Oct 26 17:07:17.610: INFO: Updating deployment webserver-deployment
Oct 26 17:07:17.610: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Oct 26 17:07:17.643: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Oct 26 17:07:17.661: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Oct 26 17:07:17.715: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-5068 /apis/apps/v1/namespaces/deployment-5068/deployments/webserver-deployment b046845f-84e7-490e-a9cf-d0538842155b 37760 3 2020-10-26 17:07:11 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-10-26 17:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-10-26 17:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000c680b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2020-10-26 17:07:15 +0000 UTC,LastTransitionTime:2020-10-26 17:07:11 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-10-26 17:07:17 +0000 UTC,LastTransitionTime:2020-10-26 17:07:17 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Oct 26 17:07:17.741: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-5068 /apis/apps/v1/namespaces/deployment-5068/replicasets/webserver-deployment-795d758f88 fc7eebf6-c101-4fdd-9c4d-6516f7a1f29e 37753 3 2020-10-26 17:07:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment b046845f-84e7-490e-a9cf-d0538842155b 0xc0043e2907 0xc0043e2908}] []  [{kube-controller-manager Update apps/v1 2020-10-26 17:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b046845f-84e7-490e-a9cf-d0538842155b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0043e2988 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Oct 26 17:07:17.741: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Oct 26 17:07:17.741: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-dd94f59b7  deployment-5068 /apis/apps/v1/namespaces/deployment-5068/replicasets/webserver-deployment-dd94f59b7 dcd635e1-7e0a-4736-bc77-2256631772b1 37791 3 2020-10-26 17:07:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment b046845f-84e7-490e-a9cf-d0538842155b 0xc0043e29e7 0xc0043e29e8}] []  [{kube-controller-manager Update apps/v1 2020-10-26 17:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b046845f-84e7-490e-a9cf-d0538842155b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: dd94f59b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0043e2a58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Oct 26 17:07:17.788: INFO: Pod "webserver-deployment-795d758f88-2brfp" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-2brfp webserver-deployment-795d758f88- deployment-5068 /api/v1/namespaces/deployment-5068/pods/webserver-deployment-795d758f88-2brfp e88a545e-d693-4497-b3db-91e7220a1c40 37796 0 2020-10-26 17:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 fc7eebf6-c101-4fdd-9c4d-6516f7a1f29e 0xc002913b37 0xc002913b38}] []  [{kube-controller-manager Update v1 2020-10-26 17:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fc7eebf6-c101-4fdd-9c4d-6516f7a1f29e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q87nm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q87nm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q87nm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.112.67.201,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 26 17:07:17.788: INFO: Pod "webserver-deployment-795d758f88-7zc64" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-7zc64 webserver-deployment-795d758f88- deployment-5068 /api/v1/namespaces/deployment-5068/pods/webserver-deployment-795d758f88-7zc64 9032bca7-b8ce-48de-bd3b-c79d3da93dcb 37731 0 2020-10-26 17:07:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.30.31.214/32 cni.projectcalico.org/podIPs:172.30.31.214/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 fc7eebf6-c101-4fdd-9c4d-6516f7a1f29e 0xc002913ca0 0xc002913ca1}] []  [{kube-controller-manager Update v1 2020-10-26 17:07:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fc7eebf6-c101-4fdd-9c4d-6516f7a1f29e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-10-26 17:07:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2020-10-26 17:07:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q87nm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q87nm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q87nm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.112.67.203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.112.67.203,PodIP:,StartTime:2020-10-26 17:07:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 26 17:07:17.789: INFO: Pod "webserver-deployment-795d758f88-cf2mh" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-cf2mh webserver-deployment-795d758f88- deployment-5068 /api/v1/namespaces/deployment-5068/pods/webserver-deployment-795d758f88-cf2mh e33a1fd7-48c3-4c66-9b33-1b00fc51d057 37725 0 2020-10-26 17:07:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.30.67.242/32 cni.projectcalico.org/podIPs:172.30.67.242/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 fc7eebf6-c101-4fdd-9c4d-6516f7a1f29e 0xc002913e97 0xc002913e98}] []  [{kube-controller-manager Update v1 2020-10-26 17:07:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fc7eebf6-c101-4fdd-9c4d-6516f7a1f29e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-10-26 17:07:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2020-10-26 17:07:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q87nm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q87nm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q87nm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.112.67.207,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.112.67.207,PodIP:,StartTime:2020-10-26 17:07:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 26 17:07:17.789: INFO: Pod "webserver-deployment-795d758f88-ktrm4" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-ktrm4 webserver-deployment-795d758f88- deployment-5068 /api/v1/namespaces/deployment-5068/pods/webserver-deployment-795d758f88-ktrm4 a24926e5-6068-45ca-b217-ce86fd0f7190 37794 0 2020-10-26 17:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 fc7eebf6-c101-4fdd-9c4d-6516f7a1f29e 0xc007020057 0xc007020058}] []  [{kube-controller-manager Update v1 2020-10-26 17:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fc7eebf6-c101-4fdd-9c4d-6516f7a1f29e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-10-26 17:07:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q87nm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q87nm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q87nm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.112.67.201,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.112.67.201,PodIP:,StartTime:2020-10-26 17:07:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 26 17:07:17.790: INFO: Pod "webserver-deployment-795d758f88-kwjdp" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-kwjdp webserver-deployment-795d758f88- deployment-5068 /api/v1/namespaces/deployment-5068/pods/webserver-deployment-795d758f88-kwjdp 23b6e1d6-4435-4176-ae8b-b8c80504a47e 37800 0 2020-10-26 17:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 fc7eebf6-c101-4fdd-9c4d-6516f7a1f29e 0xc007020207 0xc007020208}] []  [{kube-controller-manager Update v1 2020-10-26 17:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fc7eebf6-c101-4fdd-9c4d-6516f7a1f29e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q87nm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q87nm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q87nm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.112.67.207,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 26 17:07:17.790: INFO: Pod "webserver-deployment-795d758f88-mtrk5" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-mtrk5 webserver-deployment-795d758f88- deployment-5068 /api/v1/namespaces/deployment-5068/pods/webserver-deployment-795d758f88-mtrk5 24e11925-f20f-462c-9737-1088cf6d9afa 37808 0 2020-10-26 17:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 fc7eebf6-c101-4fdd-9c4d-6516f7a1f29e 0xc007020340 0xc007020341}] []  [{kube-controller-manager Update v1 2020-10-26 17:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fc7eebf6-c101-4fdd-9c4d-6516f7a1f29e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q87nm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q87nm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q87nm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.112.67.203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 26 17:07:17.790: INFO: Pod "webserver-deployment-795d758f88-rnwvs" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-rnwvs webserver-deployment-795d758f88- deployment-5068 /api/v1/namespaces/deployment-5068/pods/webserver-deployment-795d758f88-rnwvs 572d119c-65c4-4ddc-8798-cbfbbb127f7d 37730 0 2020-10-26 17:07:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.30.67.243/32 cni.projectcalico.org/podIPs:172.30.67.243/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 fc7eebf6-c101-4fdd-9c4d-6516f7a1f29e 0xc0070204a0 0xc0070204a1}] []  [{kube-controller-manager Update v1 2020-10-26 17:07:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fc7eebf6-c101-4fdd-9c4d-6516f7a1f29e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-10-26 17:07:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2020-10-26 17:07:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q87nm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q87nm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q87nm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.112.67.207,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.112.67.207,PodIP:,StartTime:2020-10-26 17:07:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 26 17:07:17.790: INFO: Pod "webserver-deployment-795d758f88-sp5mr" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-sp5mr webserver-deployment-795d758f88- deployment-5068 /api/v1/namespaces/deployment-5068/pods/webserver-deployment-795d758f88-sp5mr 60290f09-fa62-4b2a-83a6-b43fb4e93a3c 37785 0 2020-10-26 17:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 fc7eebf6-c101-4fdd-9c4d-6516f7a1f29e 0xc007020667 0xc007020668}] []  [{kube-controller-manager Update v1 2020-10-26 17:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fc7eebf6-c101-4fdd-9c4d-6516f7a1f29e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q87nm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q87nm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q87nm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.112.67.207,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 26 17:07:17.791: INFO: Pod "webserver-deployment-795d758f88-ss4bg" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-ss4bg webserver-deployment-795d758f88- deployment-5068 /api/v1/namespaces/deployment-5068/pods/webserver-deployment-795d758f88-ss4bg 04a8759c-497f-467d-99fd-bf6dbcb60547 37807 0 2020-10-26 17:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 fc7eebf6-c101-4fdd-9c4d-6516f7a1f29e 0xc0070207a0 0xc0070207a1}] []  [{kube-controller-manager Update v1 2020-10-26 17:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fc7eebf6-c101-4fdd-9c4d-6516f7a1f29e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q87nm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q87nm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q87nm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.112.67.201,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 26 17:07:17.791: INFO: Pod "webserver-deployment-795d758f88-vgflw" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-vgflw webserver-deployment-795d758f88- deployment-5068 /api/v1/namespaces/deployment-5068/pods/webserver-deployment-795d758f88-vgflw cdb6b9b1-c27d-4994-952c-8ca28fd9a03c 37723 0 2020-10-26 17:07:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.30.41.226/32 cni.projectcalico.org/podIPs:172.30.41.226/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 fc7eebf6-c101-4fdd-9c4d-6516f7a1f29e 0xc007020920 0xc007020921}] []  [{kube-controller-manager Update v1 2020-10-26 17:07:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fc7eebf6-c101-4fdd-9c4d-6516f7a1f29e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-10-26 17:07:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2020-10-26 17:07:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q87nm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q87nm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q87nm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.112.67.201,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.112.67.201,PodIP:,StartTime:2020-10-26 17:07:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 26 17:07:17.792: INFO: Pod "webserver-deployment-795d758f88-xh8j2" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-xh8j2 webserver-deployment-795d758f88- deployment-5068 /api/v1/namespaces/deployment-5068/pods/webserver-deployment-795d758f88-xh8j2 04219973-7d54-4957-afdd-532da21364f9 37718 0 2020-10-26 17:07:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.30.31.213/32 cni.projectcalico.org/podIPs:172.30.31.213/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 fc7eebf6-c101-4fdd-9c4d-6516f7a1f29e 0xc007020b17 0xc007020b18}] []  [{kube-controller-manager Update v1 2020-10-26 17:07:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fc7eebf6-c101-4fdd-9c4d-6516f7a1f29e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-10-26 17:07:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2020-10-26 17:07:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q87nm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q87nm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q87nm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.112.67.203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.112.67.203,PodIP:,StartTime:2020-10-26 17:07:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 26 17:07:17.793: INFO: Pod "webserver-deployment-795d758f88-xkx2k" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-xkx2k webserver-deployment-795d758f88- deployment-5068 /api/v1/namespaces/deployment-5068/pods/webserver-deployment-795d758f88-xkx2k 4576a6a6-8e10-4c22-ac1f-3ed3ca147a77 37803 0 2020-10-26 17:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 fc7eebf6-c101-4fdd-9c4d-6516f7a1f29e 0xc007020cd7 0xc007020cd8}] []  [{kube-controller-manager Update v1 2020-10-26 17:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fc7eebf6-c101-4fdd-9c4d-6516f7a1f29e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q87nm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q87nm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q87nm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.112.67.203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 26 17:07:17.793: INFO: Pod "webserver-deployment-795d758f88-zw8zn" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-zw8zn webserver-deployment-795d758f88- deployment-5068 /api/v1/namespaces/deployment-5068/pods/webserver-deployment-795d758f88-zw8zn e0f1c67f-dfb7-4f81-a99d-e563626f6feb 37790 0 2020-10-26 17:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 fc7eebf6-c101-4fdd-9c4d-6516f7a1f29e 0xc007020e10 0xc007020e11}] []  [{kube-controller-manager Update v1 2020-10-26 17:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fc7eebf6-c101-4fdd-9c4d-6516f7a1f29e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q87nm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q87nm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q87nm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.112.67.203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 26 17:07:17.793: INFO: Pod "webserver-deployment-dd94f59b7-2xsmt" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-2xsmt webserver-deployment-dd94f59b7- deployment-5068 /api/v1/namespaces/deployment-5068/pods/webserver-deployment-dd94f59b7-2xsmt b0c8c494-9f7d-4d79-bcfa-11350596c54b 37798 0 2020-10-26 17:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 dcd635e1-7e0a-4736-bc77-2256631772b1 0xc007020f50 0xc007020f51}] []  [{kube-controller-manager Update v1 2020-10-26 17:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dcd635e1-7e0a-4736-bc77-2256631772b1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q87nm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q87nm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q87nm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.112.67.203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 26 17:07:17.793: INFO: Pod "webserver-deployment-dd94f59b7-42t7p" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-42t7p webserver-deployment-dd94f59b7- deployment-5068 /api/v1/namespaces/deployment-5068/pods/webserver-deployment-dd94f59b7-42t7p c3fd463f-37a4-4f7d-9c2c-07e39cdd4f7a 37797 0 2020-10-26 17:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 dcd635e1-7e0a-4736-bc77-2256631772b1 0xc007021080 0xc007021081}] []  [{kube-controller-manager Update v1 2020-10-26 17:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dcd635e1-7e0a-4736-bc77-2256631772b1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q87nm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q87nm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q87nm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.112.67.201,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 26 17:07:17.794: INFO: Pod "webserver-deployment-dd94f59b7-67drv" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-67drv webserver-deployment-dd94f59b7- deployment-5068 /api/v1/namespaces/deployment-5068/pods/webserver-deployment-dd94f59b7-67drv f340fde3-c54c-4e9f-860a-685987978f7a 37793 0 2020-10-26 17:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 dcd635e1-7e0a-4736-bc77-2256631772b1 0xc0070211b0 0xc0070211b1}] []  [{kube-controller-manager Update v1 2020-10-26 17:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dcd635e1-7e0a-4736-bc77-2256631772b1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q87nm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q87nm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q87nm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.112.67.207,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 26 17:07:17.794: INFO: Pod "webserver-deployment-dd94f59b7-67s8b" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-67s8b webserver-deployment-dd94f59b7- deployment-5068 /api/v1/namespaces/deployment-5068/pods/webserver-deployment-dd94f59b7-67s8b 6c7655e7-f332-4131-8ae7-2c921b3cc737 37626 0 2020-10-26 17:07:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.30.67.238/32 cni.projectcalico.org/podIPs:172.30.67.238/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 dcd635e1-7e0a-4736-bc77-2256631772b1 0xc007021300 0xc007021301}] []  [{kube-controller-manager Update v1 2020-10-26 17:07:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dcd635e1-7e0a-4736-bc77-2256631772b1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-10-26 17:07:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-10-26 17:07:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.67.238\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q87nm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q87nm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q87nm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.112.67.207,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.112.67.207,PodIP:172.30.67.238,StartTime:2020-10-26 17:07:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-10-26 17:07:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://cfeebea101f8f60fc6acc9cbefccf92113a4ed3b38a94b0b2d6d977d954f4073,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.67.238,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 26 17:07:17.794: INFO: Pod "webserver-deployment-dd94f59b7-6m554" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-6m554 webserver-deployment-dd94f59b7- deployment-5068 /api/v1/namespaces/deployment-5068/pods/webserver-deployment-dd94f59b7-6m554 debe4060-bb80-4184-9c40-4ca72b8d0433 37623 0 2020-10-26 17:07:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.30.41.223/32 cni.projectcalico.org/podIPs:172.30.41.223/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 dcd635e1-7e0a-4736-bc77-2256631772b1 0xc0070214f7 0xc0070214f8}] []  [{kube-controller-manager Update v1 2020-10-26 17:07:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dcd635e1-7e0a-4736-bc77-2256631772b1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-10-26 17:07:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-10-26 17:07:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.41.223\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q87nm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q87nm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q87nm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.112.67.201,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.112.67.201,PodIP:172.30.41.223,StartTime:2020-10-26 17:07:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-10-26 17:07:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://6aac106e40f7b0019d161d579bd43d11fcc9b0e754c14ec83fa7a4f0f191e5f7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.41.223,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 26 17:07:17.795: INFO: Pod "webserver-deployment-dd94f59b7-96f7g" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-96f7g webserver-deployment-dd94f59b7- deployment-5068 /api/v1/namespaces/deployment-5068/pods/webserver-deployment-dd94f59b7-96f7g 8a001dc0-7c29-4fec-b3b0-168b9699dbe0 37772 0 2020-10-26 17:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 dcd635e1-7e0a-4736-bc77-2256631772b1 0xc0070216b7 0xc0070216b8}] []  [{kube-controller-manager Update v1 2020-10-26 17:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dcd635e1-7e0a-4736-bc77-2256631772b1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q87nm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q87nm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q87nm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.112.67.201,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 26 17:07:17.795: INFO: Pod "webserver-deployment-dd94f59b7-9rhj2" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-9rhj2 webserver-deployment-dd94f59b7- deployment-5068 /api/v1/namespaces/deployment-5068/pods/webserver-deployment-dd94f59b7-9rhj2 63dc0677-5441-4a47-8941-59c4954a7e8e 37641 0 2020-10-26 17:07:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.30.31.215/32 cni.projectcalico.org/podIPs:172.30.31.215/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 dcd635e1-7e0a-4736-bc77-2256631772b1 0xc007021810 0xc007021811}] []  [{kube-controller-manager Update v1 2020-10-26 17:07:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dcd635e1-7e0a-4736-bc77-2256631772b1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-10-26 17:07:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-10-26 17:07:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.31.215\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q87nm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q87nm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q87nm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.112.67.203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.112.67.203,PodIP:172.30.31.215,StartTime:2020-10-26 17:07:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-10-26 17:07:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://6874a00ffb1cd73139d0434c14e87f12deae8d68e624d3a2474966cfaebfd2d7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.31.215,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 26 17:07:17.797: INFO: Pod "webserver-deployment-dd94f59b7-fdj6z" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-fdj6z webserver-deployment-dd94f59b7- deployment-5068 /api/v1/namespaces/deployment-5068/pods/webserver-deployment-dd94f59b7-fdj6z 0cc31ea7-84f5-401a-ac1c-978795f91f6b 37608 0 2020-10-26 17:07:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.30.31.212/32 cni.projectcalico.org/podIPs:172.30.31.212/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 dcd635e1-7e0a-4736-bc77-2256631772b1 0xc0070219f7 0xc0070219f8}] []  [{kube-controller-manager Update v1 2020-10-26 17:07:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dcd635e1-7e0a-4736-bc77-2256631772b1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-10-26 17:07:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-10-26 17:07:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.31.212\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q87nm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q87nm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q87nm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.112.67.203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.112.67.203,PodIP:172.30.31.212,StartTime:2020-10-26 17:07:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-10-26 17:07:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://882f549df0e669f3fe2c9d2c4b7d422c3020fc80f8cd9108c579a4116256d9b5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.31.212,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 26 17:07:17.797: INFO: Pod "webserver-deployment-dd94f59b7-fml44" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-fml44 webserver-deployment-dd94f59b7- deployment-5068 /api/v1/namespaces/deployment-5068/pods/webserver-deployment-dd94f59b7-fml44 f4569ad2-60e2-468f-b805-2da3e101b87b 37768 0 2020-10-26 17:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 dcd635e1-7e0a-4736-bc77-2256631772b1 0xc007021bb7 0xc007021bb8}] []  [{kube-controller-manager Update v1 2020-10-26 17:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dcd635e1-7e0a-4736-bc77-2256631772b1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q87nm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q87nm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q87nm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.112.67.207,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 26 17:07:17.798: INFO: Pod "webserver-deployment-dd94f59b7-gllwx" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-gllwx webserver-deployment-dd94f59b7- deployment-5068 /api/v1/namespaces/deployment-5068/pods/webserver-deployment-dd94f59b7-gllwx a48a093f-a026-4b4d-a11a-ac667bd865a7 37789 0 2020-10-26 17:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 dcd635e1-7e0a-4736-bc77-2256631772b1 0xc007021ce0 0xc007021ce1}] []  [{kube-controller-manager Update v1 2020-10-26 17:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dcd635e1-7e0a-4736-bc77-2256631772b1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q87nm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q87nm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q87nm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.112.67.203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 26 17:07:17.798: INFO: Pod "webserver-deployment-dd94f59b7-gnkcw" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-gnkcw webserver-deployment-dd94f59b7- deployment-5068 /api/v1/namespaces/deployment-5068/pods/webserver-deployment-dd94f59b7-gnkcw 7e2cdc70-1906-4fb9-ace2-bbc8b5c5e7ca 37618 0 2020-10-26 17:07:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.30.41.225/32 cni.projectcalico.org/podIPs:172.30.41.225/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 dcd635e1-7e0a-4736-bc77-2256631772b1 0xc007021e30 0xc007021e31}] []  [{kube-controller-manager Update v1 2020-10-26 17:07:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dcd635e1-7e0a-4736-bc77-2256631772b1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-10-26 17:07:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-10-26 17:07:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.41.225\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q87nm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q87nm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q87nm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.112.67.201,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.112.67.201,PodIP:172.30.41.225,StartTime:2020-10-26 17:07:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-10-26 17:07:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://9027c12695b3334c25c639643fe892a8419aff36506816544a458d4ae632664f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.41.225,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 26 17:07:17.800: INFO: Pod "webserver-deployment-dd94f59b7-gw9sn" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-gw9sn webserver-deployment-dd94f59b7- deployment-5068 /api/v1/namespaces/deployment-5068/pods/webserver-deployment-dd94f59b7-gw9sn 89c56006-5a69-49b5-8bc0-24fbe936d361 37614 0 2020-10-26 17:07:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.30.41.224/32 cni.projectcalico.org/podIPs:172.30.41.224/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 dcd635e1-7e0a-4736-bc77-2256631772b1 0xc00374c077 0xc00374c078}] []  [{kube-controller-manager Update v1 2020-10-26 17:07:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dcd635e1-7e0a-4736-bc77-2256631772b1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-10-26 17:07:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-10-26 17:07:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.41.224\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q87nm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q87nm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q87nm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.112.67.201,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.112.67.201,PodIP:172.30.41.224,StartTime:2020-10-26 17:07:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-10-26 17:07:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://5ea23c0a46b1113886bd7638f497990fb8ddea5369182bdb74b29c34377e45b2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.41.224,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 26 17:07:17.800: INFO: Pod "webserver-deployment-dd94f59b7-j8mgg" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-j8mgg webserver-deployment-dd94f59b7- deployment-5068 /api/v1/namespaces/deployment-5068/pods/webserver-deployment-dd94f59b7-j8mgg 110871ec-7d3a-4117-a6d2-9f8d70fe7226 37799 0 2020-10-26 17:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 dcd635e1-7e0a-4736-bc77-2256631772b1 0xc00374c267 0xc00374c268}] []  [{kube-controller-manager Update v1 2020-10-26 17:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dcd635e1-7e0a-4736-bc77-2256631772b1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-10-26 17:07:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q87nm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q87nm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q87nm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.112.67.203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.112.67.203,PodIP:,StartTime:2020-10-26 17:07:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 26 17:07:17.801: INFO: Pod "webserver-deployment-dd94f59b7-khfk4" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-khfk4 webserver-deployment-dd94f59b7- deployment-5068 /api/v1/namespaces/deployment-5068/pods/webserver-deployment-dd94f59b7-khfk4 4eeaac7f-6ea0-4c4d-9bf4-e36c804a0809 37773 0 2020-10-26 17:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 dcd635e1-7e0a-4736-bc77-2256631772b1 0xc00374c3f7 0xc00374c3f8}] []  [{kube-controller-manager Update v1 2020-10-26 17:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dcd635e1-7e0a-4736-bc77-2256631772b1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q87nm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q87nm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q87nm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.112.67.203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 26 17:07:17.801: INFO: Pod "webserver-deployment-dd94f59b7-l6m6c" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-l6m6c webserver-deployment-dd94f59b7- deployment-5068 /api/v1/namespaces/deployment-5068/pods/webserver-deployment-dd94f59b7-l6m6c 678f3767-f6fa-4ad5-87da-da8c3357d784 37806 0 2020-10-26 17:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 dcd635e1-7e0a-4736-bc77-2256631772b1 0xc00374c520 0xc00374c521}] []  [{kube-controller-manager Update v1 2020-10-26 17:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dcd635e1-7e0a-4736-bc77-2256631772b1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q87nm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q87nm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q87nm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.112.67.207,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 26 17:07:17.802: INFO: Pod "webserver-deployment-dd94f59b7-mzmkv" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-mzmkv webserver-deployment-dd94f59b7- deployment-5068 /api/v1/namespaces/deployment-5068/pods/webserver-deployment-dd94f59b7-mzmkv 025ad442-88aa-4baf-8c5b-a09c9700956b 37633 0 2020-10-26 17:07:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.30.67.239/32 cni.projectcalico.org/podIPs:172.30.67.239/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 dcd635e1-7e0a-4736-bc77-2256631772b1 0xc00374c680 0xc00374c681}] []  [{kube-controller-manager Update v1 2020-10-26 17:07:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dcd635e1-7e0a-4736-bc77-2256631772b1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-10-26 17:07:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-10-26 17:07:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.67.239\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q87nm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q87nm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q87nm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.112.67.207,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.112.67.207,PodIP:172.30.67.239,StartTime:2020-10-26 17:07:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-10-26 17:07:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://430172aaa9252aaaa24a9dd09c68a14e0a2e1de0253cfaf14573a1293bd2c865,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.67.239,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 26 17:07:17.802: INFO: Pod "webserver-deployment-dd94f59b7-p2n62" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-p2n62 webserver-deployment-dd94f59b7- deployment-5068 /api/v1/namespaces/deployment-5068/pods/webserver-deployment-dd94f59b7-p2n62 6bc4d303-4a2f-4758-8d71-4f48a4e3bf37 37782 0 2020-10-26 17:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 dcd635e1-7e0a-4736-bc77-2256631772b1 0xc00374c857 0xc00374c858}] []  [{kube-controller-manager Update v1 2020-10-26 17:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dcd635e1-7e0a-4736-bc77-2256631772b1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q87nm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q87nm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q87nm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.112.67.201,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 26 17:07:17.802: INFO: Pod "webserver-deployment-dd94f59b7-sdvd4" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-sdvd4 webserver-deployment-dd94f59b7- deployment-5068 /api/v1/namespaces/deployment-5068/pods/webserver-deployment-dd94f59b7-sdvd4 e7449526-fa2d-4803-a66a-a61185f39e6e 37611 0 2020-10-26 17:07:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.30.31.210/32 cni.projectcalico.org/podIPs:172.30.31.210/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 dcd635e1-7e0a-4736-bc77-2256631772b1 0xc00374c9a0 0xc00374c9a1}] []  [{kube-controller-manager Update v1 2020-10-26 17:07:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dcd635e1-7e0a-4736-bc77-2256631772b1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-10-26 17:07:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-10-26 17:07:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.31.210\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q87nm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q87nm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q87nm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.112.67.203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.112.67.203,PodIP:172.30.31.210,StartTime:2020-10-26 17:07:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-10-26 17:07:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://456b9b74310a4f0e0eda0ac7be39a3b2a4201aa49869a61bfbf0a4646487998c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.31.210,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 26 17:07:17.803: INFO: Pod "webserver-deployment-dd94f59b7-vscdq" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-vscdq webserver-deployment-dd94f59b7- deployment-5068 /api/v1/namespaces/deployment-5068/pods/webserver-deployment-dd94f59b7-vscdq 7e98b067-e697-4fbf-b693-53ff9e7c9721 37788 0 2020-10-26 17:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 dcd635e1-7e0a-4736-bc77-2256631772b1 0xc00374cb87 0xc00374cb88}] []  [{kube-controller-manager Update v1 2020-10-26 17:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dcd635e1-7e0a-4736-bc77-2256631772b1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-10-26 17:07:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q87nm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q87nm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q87nm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.112.67.207,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.112.67.207,PodIP:,StartTime:2020-10-26 17:07:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 26 17:07:17.803: INFO: Pod "webserver-deployment-dd94f59b7-x22tg" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-x22tg webserver-deployment-dd94f59b7- deployment-5068 /api/v1/namespaces/deployment-5068/pods/webserver-deployment-dd94f59b7-x22tg dd692f9e-628e-44d1-a9e5-f8f5f3070302 37777 0 2020-10-26 17:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 dcd635e1-7e0a-4736-bc77-2256631772b1 0xc00374cd17 0xc00374cd18}] []  [{kube-controller-manager Update v1 2020-10-26 17:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dcd635e1-7e0a-4736-bc77-2256631772b1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q87nm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q87nm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q87nm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.112.67.207,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:07:17.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5068" for this suite.

• [SLOW TEST:6.889 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":305,"completed":90,"skipped":1622,"failed":0}
SSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:07:17.881: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2199
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Oct 26 17:07:18.128: INFO: Waiting up to 5m0s for pod "downward-api-927a0496-b3ac-4657-b861-0c2f65b12725" in namespace "downward-api-2199" to be "Succeeded or Failed"
Oct 26 17:07:18.142: INFO: Pod "downward-api-927a0496-b3ac-4657-b861-0c2f65b12725": Phase="Pending", Reason="", readiness=false. Elapsed: 13.958559ms
Oct 26 17:07:20.160: INFO: Pod "downward-api-927a0496-b3ac-4657-b861-0c2f65b12725": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031542961s
Oct 26 17:07:22.174: INFO: Pod "downward-api-927a0496-b3ac-4657-b861-0c2f65b12725": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045138887s
STEP: Saw pod success
Oct 26 17:07:22.174: INFO: Pod "downward-api-927a0496-b3ac-4657-b861-0c2f65b12725" satisfied condition "Succeeded or Failed"
Oct 26 17:07:22.190: INFO: Trying to get logs from node 10.112.67.207 pod downward-api-927a0496-b3ac-4657-b861-0c2f65b12725 container dapi-container: <nil>
STEP: delete the pod
Oct 26 17:07:22.350: INFO: Waiting for pod downward-api-927a0496-b3ac-4657-b861-0c2f65b12725 to disappear
Oct 26 17:07:22.367: INFO: Pod downward-api-927a0496-b3ac-4657-b861-0c2f65b12725 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:07:22.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2199" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":305,"completed":91,"skipped":1625,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:07:22.428: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-9856
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Oct 26 17:07:26.754: INFO: &Pod{ObjectMeta:{send-events-efc88459-da1e-4469-8316-c746341f8c87  events-9856 /api/v1/namespaces/events-9856/pods/send-events-efc88459-da1e-4469-8316-c746341f8c87 d3a0c6f9-8801-4d6c-824f-7df44af94fff 38190 0 2020-10-26 17:07:22 +0000 UTC <nil> <nil> map[name:foo time:640947648] map[cni.projectcalico.org/podIP:172.30.67.251/32 cni.projectcalico.org/podIPs:172.30.67.251/32 kubernetes.io/psp:e2e-test-privileged-psp] [] []  [{e2e.test Update v1 2020-10-26 17:07:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-10-26 17:07:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-10-26 17:07:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.67.251\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-l67rs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-l67rs,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-l67rs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.112.67.207,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:07:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.112.67.207,PodIP:172.30.67.251,StartTime:2020-10-26 17:07:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-10-26 17:07:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:containerd://3a717a5661955ce70713f8eef2759532ddd17276b9634dcd8a3618000d314d9d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.67.251,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Oct 26 17:07:28.778: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Oct 26 17:07:30.795: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:07:30.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9856" for this suite.

• [SLOW TEST:8.444 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":305,"completed":92,"skipped":1646,"failed":0}
SS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:07:30.874: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-2549
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 26 17:09:31.193: INFO: Deleting pod "var-expansion-04f5619e-d00a-4aae-8bb5-3a54e10a0f39" in namespace "var-expansion-2549"
Oct 26 17:09:31.243: INFO: Wait up to 5m0s for pod "var-expansion-04f5619e-d00a-4aae-8bb5-3a54e10a0f39" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:09:33.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2549" for this suite.

• [SLOW TEST:122.454 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":305,"completed":93,"skipped":1648,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:09:33.328: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2679
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Oct 26 17:09:33.571: INFO: Waiting up to 5m0s for pod "pod-b7a13517-d721-4053-8d01-5d15d8dad115" in namespace "emptydir-2679" to be "Succeeded or Failed"
Oct 26 17:09:33.587: INFO: Pod "pod-b7a13517-d721-4053-8d01-5d15d8dad115": Phase="Pending", Reason="", readiness=false. Elapsed: 16.701774ms
Oct 26 17:09:35.601: INFO: Pod "pod-b7a13517-d721-4053-8d01-5d15d8dad115": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030261472s
Oct 26 17:09:37.616: INFO: Pod "pod-b7a13517-d721-4053-8d01-5d15d8dad115": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045745872s
STEP: Saw pod success
Oct 26 17:09:37.616: INFO: Pod "pod-b7a13517-d721-4053-8d01-5d15d8dad115" satisfied condition "Succeeded or Failed"
Oct 26 17:09:37.630: INFO: Trying to get logs from node 10.112.67.203 pod pod-b7a13517-d721-4053-8d01-5d15d8dad115 container test-container: <nil>
STEP: delete the pod
Oct 26 17:09:37.739: INFO: Waiting for pod pod-b7a13517-d721-4053-8d01-5d15d8dad115 to disappear
Oct 26 17:09:37.760: INFO: Pod pod-b7a13517-d721-4053-8d01-5d15d8dad115 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:09:37.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2679" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":94,"skipped":1658,"failed":0}
S
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:09:37.818: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9798
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-9754
STEP: Creating secret with name secret-test-19327449-0784-4e7d-8ceb-ab1b60982fdc
STEP: Creating a pod to test consume secrets
Oct 26 17:09:38.285: INFO: Waiting up to 5m0s for pod "pod-secrets-0c9112ae-536e-472d-9969-d0a86fccb3ba" in namespace "secrets-9798" to be "Succeeded or Failed"
Oct 26 17:09:38.304: INFO: Pod "pod-secrets-0c9112ae-536e-472d-9969-d0a86fccb3ba": Phase="Pending", Reason="", readiness=false. Elapsed: 18.810637ms
Oct 26 17:09:40.318: INFO: Pod "pod-secrets-0c9112ae-536e-472d-9969-d0a86fccb3ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032724109s
STEP: Saw pod success
Oct 26 17:09:40.318: INFO: Pod "pod-secrets-0c9112ae-536e-472d-9969-d0a86fccb3ba" satisfied condition "Succeeded or Failed"
Oct 26 17:09:40.331: INFO: Trying to get logs from node 10.112.67.203 pod pod-secrets-0c9112ae-536e-472d-9969-d0a86fccb3ba container secret-volume-test: <nil>
STEP: delete the pod
Oct 26 17:09:40.408: INFO: Waiting for pod pod-secrets-0c9112ae-536e-472d-9969-d0a86fccb3ba to disappear
Oct 26 17:09:40.433: INFO: Pod pod-secrets-0c9112ae-536e-472d-9969-d0a86fccb3ba no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:09:40.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9798" for this suite.
STEP: Destroying namespace "secret-namespace-9754" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":305,"completed":95,"skipped":1659,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:09:40.528: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3874
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-3025a8ed-ed8f-46a7-a675-31efd1f77733
STEP: Creating a pod to test consume secrets
Oct 26 17:09:40.792: INFO: Waiting up to 5m0s for pod "pod-secrets-51928ec9-b10a-4d54-ab53-93f009cebe29" in namespace "secrets-3874" to be "Succeeded or Failed"
Oct 26 17:09:40.813: INFO: Pod "pod-secrets-51928ec9-b10a-4d54-ab53-93f009cebe29": Phase="Pending", Reason="", readiness=false. Elapsed: 20.718444ms
Oct 26 17:09:42.828: INFO: Pod "pod-secrets-51928ec9-b10a-4d54-ab53-93f009cebe29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035574689s
Oct 26 17:09:44.842: INFO: Pod "pod-secrets-51928ec9-b10a-4d54-ab53-93f009cebe29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049866336s
STEP: Saw pod success
Oct 26 17:09:44.842: INFO: Pod "pod-secrets-51928ec9-b10a-4d54-ab53-93f009cebe29" satisfied condition "Succeeded or Failed"
Oct 26 17:09:44.854: INFO: Trying to get logs from node 10.112.67.203 pod pod-secrets-51928ec9-b10a-4d54-ab53-93f009cebe29 container secret-volume-test: <nil>
STEP: delete the pod
Oct 26 17:09:44.931: INFO: Waiting for pod pod-secrets-51928ec9-b10a-4d54-ab53-93f009cebe29 to disappear
Oct 26 17:09:44.943: INFO: Pod pod-secrets-51928ec9-b10a-4d54-ab53-93f009cebe29 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:09:44.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3874" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":96,"skipped":1688,"failed":0}
SS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:09:44.993: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-2280
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:09:45.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-2280" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":305,"completed":97,"skipped":1690,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:09:45.422: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1672
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 26 17:09:45.680: INFO: Pod name rollover-pod: Found 0 pods out of 1
Oct 26 17:09:50.693: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Oct 26 17:09:50.693: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Oct 26 17:09:52.708: INFO: Creating deployment "test-rollover-deployment"
Oct 26 17:09:52.743: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Oct 26 17:09:54.771: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Oct 26 17:09:54.798: INFO: Ensure that both replica sets have 1 created replica
Oct 26 17:09:54.830: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Oct 26 17:09:54.864: INFO: Updating deployment test-rollover-deployment
Oct 26 17:09:54.864: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Oct 26 17:09:56.893: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Oct 26 17:09:56.926: INFO: Make sure deployment "test-rollover-deployment" is complete
Oct 26 17:09:56.955: INFO: all replica sets need to contain the pod-template-hash label
Oct 26 17:09:56.955: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739328992, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739328992, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739328996, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739328992, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 26 17:09:58.988: INFO: all replica sets need to contain the pod-template-hash label
Oct 26 17:09:58.989: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739328992, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739328992, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739328996, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739328992, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 26 17:10:00.989: INFO: all replica sets need to contain the pod-template-hash label
Oct 26 17:10:00.989: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739328992, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739328992, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739328996, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739328992, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 26 17:10:02.988: INFO: all replica sets need to contain the pod-template-hash label
Oct 26 17:10:02.988: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739328992, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739328992, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739328996, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739328992, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 26 17:10:04.981: INFO: all replica sets need to contain the pod-template-hash label
Oct 26 17:10:04.981: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739328992, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739328992, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739328996, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739328992, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 26 17:10:07.003: INFO: 
Oct 26 17:10:07.003: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Oct 26 17:10:07.061: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-1672 /apis/apps/v1/namespaces/deployment-1672/deployments/test-rollover-deployment 90f9e109-3ebc-4f54-8781-a15443534c44 39071 2 2020-10-26 17:09:52 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-10-26 17:09:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-10-26 17:10:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0044ac218 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-10-26 17:09:52 +0000 UTC,LastTransitionTime:2020-10-26 17:09:52 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-5797c7764" has successfully progressed.,LastUpdateTime:2020-10-26 17:10:06 +0000 UTC,LastTransitionTime:2020-10-26 17:09:52 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Oct 26 17:10:07.080: INFO: New ReplicaSet "test-rollover-deployment-5797c7764" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-5797c7764  deployment-1672 /apis/apps/v1/namespaces/deployment-1672/replicasets/test-rollover-deployment-5797c7764 9f139f87-da2b-46a2-b321-196e66aa491d 39060 2 2020-10-26 17:09:54 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 90f9e109-3ebc-4f54-8781-a15443534c44 0xc0044ac750 0xc0044ac751}] []  [{kube-controller-manager Update apps/v1 2020-10-26 17:10:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"90f9e109-3ebc-4f54-8781-a15443534c44\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5797c7764,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0044ac7c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Oct 26 17:10:07.080: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Oct 26 17:10:07.080: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-1672 /apis/apps/v1/namespaces/deployment-1672/replicasets/test-rollover-controller 78f9b2e2-f3cc-4ce8-94d5-bcec8a3f9c48 39070 2 2020-10-26 17:09:45 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 90f9e109-3ebc-4f54-8781-a15443534c44 0xc0044ac647 0xc0044ac648}] []  [{e2e.test Update apps/v1 2020-10-26 17:09:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-10-26 17:10:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"90f9e109-3ebc-4f54-8781-a15443534c44\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0044ac6e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Oct 26 17:10:07.080: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-1672 /apis/apps/v1/namespaces/deployment-1672/replicasets/test-rollover-deployment-78bc8b888c 53bb34d8-bc64-4a38-a2da-708d70f0bc52 39011 2 2020-10-26 17:09:52 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 90f9e109-3ebc-4f54-8781-a15443534c44 0xc0044ac837 0xc0044ac838}] []  [{kube-controller-manager Update apps/v1 2020-10-26 17:09:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"90f9e109-3ebc-4f54-8781-a15443534c44\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0044ac8c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Oct 26 17:10:07.102: INFO: Pod "test-rollover-deployment-5797c7764-zjrmh" is available:
&Pod{ObjectMeta:{test-rollover-deployment-5797c7764-zjrmh test-rollover-deployment-5797c7764- deployment-1672 /api/v1/namespaces/deployment-1672/pods/test-rollover-deployment-5797c7764-zjrmh db6f58cb-a497-4d86-bcce-4c68e697676b 39033 0 2020-10-26 17:09:54 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[cni.projectcalico.org/podIP:172.30.67.253/32 cni.projectcalico.org/podIPs:172.30.67.253/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rollover-deployment-5797c7764 9f139f87-da2b-46a2-b321-196e66aa491d 0xc0044aceb0 0xc0044aceb1}] []  [{kube-controller-manager Update v1 2020-10-26 17:09:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9f139f87-da2b-46a2-b321-196e66aa491d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-10-26 17:09:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-10-26 17:09:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.67.253\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-99tc8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-99tc8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-99tc8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.112.67.207,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:09:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:09:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:09:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 17:09:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.112.67.207,PodIP:172.30.67.253,StartTime:2020-10-26 17:09:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-10-26 17:09:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:containerd://7efbbecd7be8d13e0a1e1753be3d2a7ca4fbe37a9710a0b7caea5bcb5f482787,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.67.253,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:10:07.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1672" for this suite.

• [SLOW TEST:21.739 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":305,"completed":98,"skipped":1733,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:10:07.166: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6150
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl label
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1333
STEP: creating the pod
Oct 26 17:10:07.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 create -f - --namespace=kubectl-6150'
Oct 26 17:10:07.946: INFO: stderr: ""
Oct 26 17:10:07.946: INFO: stdout: "pod/pause created\n"
Oct 26 17:10:07.946: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Oct 26 17:10:07.946: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6150" to be "running and ready"
Oct 26 17:10:07.971: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 24.980998ms
Oct 26 17:10:09.985: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039303025s
Oct 26 17:10:11.999: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.05321657s
Oct 26 17:10:11.999: INFO: Pod "pause" satisfied condition "running and ready"
Oct 26 17:10:11.999: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: adding the label testing-label with value testing-label-value to a pod
Oct 26 17:10:12.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 label pods pause testing-label=testing-label-value --namespace=kubectl-6150'
Oct 26 17:10:12.168: INFO: stderr: ""
Oct 26 17:10:12.168: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Oct 26 17:10:12.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 get pod pause -L testing-label --namespace=kubectl-6150'
Oct 26 17:10:12.299: INFO: stderr: ""
Oct 26 17:10:12.299: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Oct 26 17:10:12.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 label pods pause testing-label- --namespace=kubectl-6150'
Oct 26 17:10:12.446: INFO: stderr: ""
Oct 26 17:10:12.446: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Oct 26 17:10:12.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 get pod pause -L testing-label --namespace=kubectl-6150'
Oct 26 17:10:12.575: INFO: stderr: ""
Oct 26 17:10:12.575: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1340
STEP: using delete to clean up resources
Oct 26 17:10:12.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 delete --grace-period=0 --force -f - --namespace=kubectl-6150'
Oct 26 17:10:12.747: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 26 17:10:12.747: INFO: stdout: "pod \"pause\" force deleted\n"
Oct 26 17:10:12.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 get rc,svc -l name=pause --no-headers --namespace=kubectl-6150'
Oct 26 17:10:12.894: INFO: stderr: "No resources found in kubectl-6150 namespace.\n"
Oct 26 17:10:12.895: INFO: stdout: ""
Oct 26 17:10:12.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 get pods -l name=pause --namespace=kubectl-6150 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Oct 26 17:10:13.032: INFO: stderr: ""
Oct 26 17:10:13.032: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:10:13.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6150" for this suite.

• [SLOW TEST:5.923 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1330
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":305,"completed":99,"skipped":1743,"failed":0}
S
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:10:13.089: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8635
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Oct 26 17:10:13.337: INFO: Created pod &Pod{ObjectMeta:{dns-8635  dns-8635 /api/v1/namespaces/dns-8635/pods/dns-8635 11f78e60-b21d-43a4-b637-0e9439b49a38 39144 0 2020-10-26 17:10:13 +0000 UTC <nil> <nil> map[] map[kubernetes.io/psp:e2e-test-privileged-psp] [] []  [{e2e.test Update v1 2020-10-26 17:10:13 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dz5s8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dz5s8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dz5s8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 26 17:10:13.353: INFO: The status of Pod dns-8635 is Pending, waiting for it to be Running (with Ready = true)
Oct 26 17:10:15.375: INFO: The status of Pod dns-8635 is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Oct 26 17:10:15.376: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-8635 PodName:dns-8635 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 17:10:15.376: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Verifying customized DNS server is configured on pod...
Oct 26 17:10:15.608: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-8635 PodName:dns-8635 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 17:10:15.609: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
Oct 26 17:10:15.824: INFO: Deleting pod dns-8635...
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:10:15.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8635" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":305,"completed":100,"skipped":1744,"failed":0}
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:10:15.929: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1526
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Oct 26 17:10:16.174: INFO: Waiting up to 5m0s for pod "downward-api-407c1c1b-de60-4eeb-97e5-0a51212b4925" in namespace "downward-api-1526" to be "Succeeded or Failed"
Oct 26 17:10:16.198: INFO: Pod "downward-api-407c1c1b-de60-4eeb-97e5-0a51212b4925": Phase="Pending", Reason="", readiness=false. Elapsed: 24.456408ms
Oct 26 17:10:18.217: INFO: Pod "downward-api-407c1c1b-de60-4eeb-97e5-0a51212b4925": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042533689s
Oct 26 17:10:20.231: INFO: Pod "downward-api-407c1c1b-de60-4eeb-97e5-0a51212b4925": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.057434682s
STEP: Saw pod success
Oct 26 17:10:20.231: INFO: Pod "downward-api-407c1c1b-de60-4eeb-97e5-0a51212b4925" satisfied condition "Succeeded or Failed"
Oct 26 17:10:20.245: INFO: Trying to get logs from node 10.112.67.203 pod downward-api-407c1c1b-de60-4eeb-97e5-0a51212b4925 container dapi-container: <nil>
STEP: delete the pod
Oct 26 17:10:20.337: INFO: Waiting for pod downward-api-407c1c1b-de60-4eeb-97e5-0a51212b4925 to disappear
Oct 26 17:10:20.350: INFO: Pod downward-api-407c1c1b-de60-4eeb-97e5-0a51212b4925 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:10:20.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1526" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":305,"completed":101,"skipped":1751,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:10:20.407: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-2981
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Oct 26 17:10:20.622: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the sample API server.
Oct 26 17:10:21.660: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Oct 26 17:10:23.828: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739329021, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739329021, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739329021, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739329021, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 26 17:10:25.842: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739329021, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739329021, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739329021, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739329021, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 26 17:10:27.843: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739329021, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739329021, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739329021, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739329021, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 26 17:10:29.842: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739329021, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739329021, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739329021, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739329021, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 26 17:10:31.845: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739329021, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739329021, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739329021, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739329021, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 26 17:10:33.843: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739329021, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739329021, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739329021, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739329021, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 26 17:10:35.843: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739329021, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739329021, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739329021, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739329021, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 26 17:10:39.244: INFO: Waited 1.381717948s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:10:39.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-2981" for this suite.

• [SLOW TEST:19.704 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":305,"completed":102,"skipped":1756,"failed":0}
SSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:10:40.111: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5952
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod test-webserver-b358b406-d2e4-42a0-83c4-65b85638915e in namespace container-probe-5952
Oct 26 17:10:42.426: INFO: Started pod test-webserver-b358b406-d2e4-42a0-83c4-65b85638915e in namespace container-probe-5952
STEP: checking the pod's current state and verifying that restartCount is present
Oct 26 17:10:42.444: INFO: Initial restart count of pod test-webserver-b358b406-d2e4-42a0-83c4-65b85638915e is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:14:42.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5952" for this suite.

• [SLOW TEST:242.511 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":305,"completed":103,"skipped":1761,"failed":0}
S
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:14:42.625: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-8293
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Oct 26 17:14:42.973: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8293 /api/v1/namespaces/watch-8293/configmaps/e2e-watch-test-label-changed 18e35952-cb62-4c6e-9eb3-48de44a01f87 40052 0 2020-10-26 17:14:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-10-26 17:14:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 26 17:14:42.973: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8293 /api/v1/namespaces/watch-8293/configmaps/e2e-watch-test-label-changed 18e35952-cb62-4c6e-9eb3-48de44a01f87 40053 0 2020-10-26 17:14:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-10-26 17:14:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 26 17:14:42.974: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8293 /api/v1/namespaces/watch-8293/configmaps/e2e-watch-test-label-changed 18e35952-cb62-4c6e-9eb3-48de44a01f87 40054 0 2020-10-26 17:14:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-10-26 17:14:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Oct 26 17:14:53.140: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8293 /api/v1/namespaces/watch-8293/configmaps/e2e-watch-test-label-changed 18e35952-cb62-4c6e-9eb3-48de44a01f87 40098 0 2020-10-26 17:14:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-10-26 17:14:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 26 17:14:53.140: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8293 /api/v1/namespaces/watch-8293/configmaps/e2e-watch-test-label-changed 18e35952-cb62-4c6e-9eb3-48de44a01f87 40099 0 2020-10-26 17:14:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-10-26 17:14:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 26 17:14:53.141: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8293 /api/v1/namespaces/watch-8293/configmaps/e2e-watch-test-label-changed 18e35952-cb62-4c6e-9eb3-48de44a01f87 40100 0 2020-10-26 17:14:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-10-26 17:14:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:14:53.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8293" for this suite.

• [SLOW TEST:10.564 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":305,"completed":104,"skipped":1762,"failed":0}
S
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:14:53.192: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-530
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-530
STEP: creating service affinity-clusterip-transition in namespace services-530
STEP: creating replication controller affinity-clusterip-transition in namespace services-530
I1026 17:14:53.501646      27 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-530, replica count: 3
I1026 17:14:56.552127      27 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 26 17:14:56.580: INFO: Creating new exec pod
Oct 26 17:15:01.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-530 execpod-affinity6sjfr -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
Oct 26 17:15:02.067: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Oct 26 17:15:02.067: INFO: stdout: ""
Oct 26 17:15:02.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-530 execpod-affinity6sjfr -- /bin/sh -x -c nc -zv -t -w 2 172.21.138.34 80'
Oct 26 17:15:02.454: INFO: stderr: "+ nc -zv -t -w 2 172.21.138.34 80\nConnection to 172.21.138.34 80 port [tcp/http] succeeded!\n"
Oct 26 17:15:02.454: INFO: stdout: ""
Oct 26 17:15:02.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-530 execpod-affinity6sjfr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.138.34:80/ ; done'
Oct 26 17:15:03.211: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n"
Oct 26 17:15:03.211: INFO: stdout: "\naffinity-clusterip-transition-c8msk\naffinity-clusterip-transition-c8msk\naffinity-clusterip-transition-c8msk\naffinity-clusterip-transition-c8msk\naffinity-clusterip-transition-c8msk\naffinity-clusterip-transition-c8msk\naffinity-clusterip-transition-c8msk\naffinity-clusterip-transition-c8msk\naffinity-clusterip-transition-c8msk\naffinity-clusterip-transition-c8msk\naffinity-clusterip-transition-c8msk\naffinity-clusterip-transition-c8msk\naffinity-clusterip-transition-c8msk\naffinity-clusterip-transition-c8msk\naffinity-clusterip-transition-c8msk\naffinity-clusterip-transition-c8msk"
Oct 26 17:15:03.211: INFO: Received response from host: affinity-clusterip-transition-c8msk
Oct 26 17:15:03.211: INFO: Received response from host: affinity-clusterip-transition-c8msk
Oct 26 17:15:03.211: INFO: Received response from host: affinity-clusterip-transition-c8msk
Oct 26 17:15:03.211: INFO: Received response from host: affinity-clusterip-transition-c8msk
Oct 26 17:15:03.211: INFO: Received response from host: affinity-clusterip-transition-c8msk
Oct 26 17:15:03.211: INFO: Received response from host: affinity-clusterip-transition-c8msk
Oct 26 17:15:03.211: INFO: Received response from host: affinity-clusterip-transition-c8msk
Oct 26 17:15:03.211: INFO: Received response from host: affinity-clusterip-transition-c8msk
Oct 26 17:15:03.211: INFO: Received response from host: affinity-clusterip-transition-c8msk
Oct 26 17:15:03.211: INFO: Received response from host: affinity-clusterip-transition-c8msk
Oct 26 17:15:03.211: INFO: Received response from host: affinity-clusterip-transition-c8msk
Oct 26 17:15:03.211: INFO: Received response from host: affinity-clusterip-transition-c8msk
Oct 26 17:15:03.211: INFO: Received response from host: affinity-clusterip-transition-c8msk
Oct 26 17:15:03.211: INFO: Received response from host: affinity-clusterip-transition-c8msk
Oct 26 17:15:03.211: INFO: Received response from host: affinity-clusterip-transition-c8msk
Oct 26 17:15:03.211: INFO: Received response from host: affinity-clusterip-transition-c8msk
Oct 26 17:15:33.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-530 execpod-affinity6sjfr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.138.34:80/ ; done'
Oct 26 17:15:34.039: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n"
Oct 26 17:15:34.040: INFO: stdout: "\naffinity-clusterip-transition-hndfd\naffinity-clusterip-transition-hndfd\naffinity-clusterip-transition-nnk2f\naffinity-clusterip-transition-c8msk\naffinity-clusterip-transition-c8msk\naffinity-clusterip-transition-c8msk\naffinity-clusterip-transition-hndfd\naffinity-clusterip-transition-c8msk\naffinity-clusterip-transition-c8msk\naffinity-clusterip-transition-nnk2f\naffinity-clusterip-transition-hndfd\naffinity-clusterip-transition-nnk2f\naffinity-clusterip-transition-hndfd\naffinity-clusterip-transition-c8msk\naffinity-clusterip-transition-nnk2f\naffinity-clusterip-transition-nnk2f"
Oct 26 17:15:34.040: INFO: Received response from host: affinity-clusterip-transition-hndfd
Oct 26 17:15:34.040: INFO: Received response from host: affinity-clusterip-transition-hndfd
Oct 26 17:15:34.040: INFO: Received response from host: affinity-clusterip-transition-nnk2f
Oct 26 17:15:34.040: INFO: Received response from host: affinity-clusterip-transition-c8msk
Oct 26 17:15:34.040: INFO: Received response from host: affinity-clusterip-transition-c8msk
Oct 26 17:15:34.040: INFO: Received response from host: affinity-clusterip-transition-c8msk
Oct 26 17:15:34.040: INFO: Received response from host: affinity-clusterip-transition-hndfd
Oct 26 17:15:34.040: INFO: Received response from host: affinity-clusterip-transition-c8msk
Oct 26 17:15:34.040: INFO: Received response from host: affinity-clusterip-transition-c8msk
Oct 26 17:15:34.040: INFO: Received response from host: affinity-clusterip-transition-nnk2f
Oct 26 17:15:34.040: INFO: Received response from host: affinity-clusterip-transition-hndfd
Oct 26 17:15:34.040: INFO: Received response from host: affinity-clusterip-transition-nnk2f
Oct 26 17:15:34.040: INFO: Received response from host: affinity-clusterip-transition-hndfd
Oct 26 17:15:34.040: INFO: Received response from host: affinity-clusterip-transition-c8msk
Oct 26 17:15:34.040: INFO: Received response from host: affinity-clusterip-transition-nnk2f
Oct 26 17:15:34.040: INFO: Received response from host: affinity-clusterip-transition-nnk2f
Oct 26 17:15:34.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-530 execpod-affinity6sjfr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.138.34:80/ ; done'
Oct 26 17:15:34.629: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.34:80/\n"
Oct 26 17:15:34.629: INFO: stdout: "\naffinity-clusterip-transition-c8msk\naffinity-clusterip-transition-c8msk\naffinity-clusterip-transition-c8msk\naffinity-clusterip-transition-c8msk\naffinity-clusterip-transition-c8msk\naffinity-clusterip-transition-c8msk\naffinity-clusterip-transition-c8msk\naffinity-clusterip-transition-c8msk\naffinity-clusterip-transition-c8msk\naffinity-clusterip-transition-c8msk\naffinity-clusterip-transition-c8msk\naffinity-clusterip-transition-c8msk\naffinity-clusterip-transition-c8msk\naffinity-clusterip-transition-c8msk\naffinity-clusterip-transition-c8msk\naffinity-clusterip-transition-c8msk"
Oct 26 17:15:34.629: INFO: Received response from host: affinity-clusterip-transition-c8msk
Oct 26 17:15:34.629: INFO: Received response from host: affinity-clusterip-transition-c8msk
Oct 26 17:15:34.629: INFO: Received response from host: affinity-clusterip-transition-c8msk
Oct 26 17:15:34.629: INFO: Received response from host: affinity-clusterip-transition-c8msk
Oct 26 17:15:34.629: INFO: Received response from host: affinity-clusterip-transition-c8msk
Oct 26 17:15:34.629: INFO: Received response from host: affinity-clusterip-transition-c8msk
Oct 26 17:15:34.629: INFO: Received response from host: affinity-clusterip-transition-c8msk
Oct 26 17:15:34.629: INFO: Received response from host: affinity-clusterip-transition-c8msk
Oct 26 17:15:34.629: INFO: Received response from host: affinity-clusterip-transition-c8msk
Oct 26 17:15:34.629: INFO: Received response from host: affinity-clusterip-transition-c8msk
Oct 26 17:15:34.629: INFO: Received response from host: affinity-clusterip-transition-c8msk
Oct 26 17:15:34.629: INFO: Received response from host: affinity-clusterip-transition-c8msk
Oct 26 17:15:34.629: INFO: Received response from host: affinity-clusterip-transition-c8msk
Oct 26 17:15:34.629: INFO: Received response from host: affinity-clusterip-transition-c8msk
Oct 26 17:15:34.629: INFO: Received response from host: affinity-clusterip-transition-c8msk
Oct 26 17:15:34.629: INFO: Received response from host: affinity-clusterip-transition-c8msk
Oct 26 17:15:34.629: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-530, will wait for the garbage collector to delete the pods
Oct 26 17:15:34.782: INFO: Deleting ReplicationController affinity-clusterip-transition took: 36.187652ms
Oct 26 17:15:34.883: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.359007ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:15:49.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-530" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:56.851 seconds]
[sig-network] Services
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":105,"skipped":1763,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:15:50.043: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-645
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Oct 26 17:15:50.306: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7920f73c-5a6a-45da-b382-c6c64c79faff" in namespace "downward-api-645" to be "Succeeded or Failed"
Oct 26 17:15:50.324: INFO: Pod "downwardapi-volume-7920f73c-5a6a-45da-b382-c6c64c79faff": Phase="Pending", Reason="", readiness=false. Elapsed: 17.987541ms
Oct 26 17:15:52.338: INFO: Pod "downwardapi-volume-7920f73c-5a6a-45da-b382-c6c64c79faff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032444304s
STEP: Saw pod success
Oct 26 17:15:52.338: INFO: Pod "downwardapi-volume-7920f73c-5a6a-45da-b382-c6c64c79faff" satisfied condition "Succeeded or Failed"
Oct 26 17:15:52.350: INFO: Trying to get logs from node 10.112.67.203 pod downwardapi-volume-7920f73c-5a6a-45da-b382-c6c64c79faff container client-container: <nil>
STEP: delete the pod
Oct 26 17:15:52.458: INFO: Waiting for pod downwardapi-volume-7920f73c-5a6a-45da-b382-c6c64c79faff to disappear
Oct 26 17:15:52.469: INFO: Pod downwardapi-volume-7920f73c-5a6a-45da-b382-c6c64c79faff no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:15:52.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-645" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":305,"completed":106,"skipped":1764,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:15:52.520: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8604
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-8604
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-8604
STEP: creating replication controller externalsvc in namespace services-8604
I1026 17:15:52.850223      27 runners.go:190] Created replication controller with name: externalsvc, namespace: services-8604, replica count: 2
I1026 17:15:55.900781      27 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Oct 26 17:15:55.983: INFO: Creating new exec pod
Oct 26 17:16:00.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-8604 execpodx6scr -- /bin/sh -x -c nslookup clusterip-service.services-8604.svc.cluster.local'
Oct 26 17:16:00.464: INFO: stderr: "+ nslookup clusterip-service.services-8604.svc.cluster.local\n"
Oct 26 17:16:00.465: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-8604.svc.cluster.local\tcanonical name = externalsvc.services-8604.svc.cluster.local.\nName:\texternalsvc.services-8604.svc.cluster.local\nAddress: 172.21.197.205\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8604, will wait for the garbage collector to delete the pods
Oct 26 17:16:00.573: INFO: Deleting ReplicationController externalsvc took: 43.784547ms
Oct 26 17:16:00.673: INFO: Terminating ReplicationController externalsvc pods took: 100.266893ms
Oct 26 17:16:10.391: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:16:10.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8604" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:17.999 seconds]
[sig-network] Services
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":305,"completed":107,"skipped":1783,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:16:10.521: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7962
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-d9c519ee-1a0c-47c1-b624-39baeecd9a2d
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-d9c519ee-1a0c-47c1-b624-39baeecd9a2d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:16:15.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7962" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":108,"skipped":1792,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:16:15.072: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-8113
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override command
Oct 26 17:16:15.338: INFO: Waiting up to 5m0s for pod "client-containers-5b7d0ad1-2f53-408e-8ab0-3623b98b6e61" in namespace "containers-8113" to be "Succeeded or Failed"
Oct 26 17:16:15.357: INFO: Pod "client-containers-5b7d0ad1-2f53-408e-8ab0-3623b98b6e61": Phase="Pending", Reason="", readiness=false. Elapsed: 18.486834ms
Oct 26 17:16:17.370: INFO: Pod "client-containers-5b7d0ad1-2f53-408e-8ab0-3623b98b6e61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031738595s
STEP: Saw pod success
Oct 26 17:16:17.370: INFO: Pod "client-containers-5b7d0ad1-2f53-408e-8ab0-3623b98b6e61" satisfied condition "Succeeded or Failed"
Oct 26 17:16:17.390: INFO: Trying to get logs from node 10.112.67.203 pod client-containers-5b7d0ad1-2f53-408e-8ab0-3623b98b6e61 container test-container: <nil>
STEP: delete the pod
Oct 26 17:16:17.472: INFO: Waiting for pod client-containers-5b7d0ad1-2f53-408e-8ab0-3623b98b6e61 to disappear
Oct 26 17:16:17.488: INFO: Pod client-containers-5b7d0ad1-2f53-408e-8ab0-3623b98b6e61 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:16:17.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8113" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":305,"completed":109,"skipped":1851,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:16:17.540: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6439
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Oct 26 17:16:17.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-6439'
Oct 26 17:16:17.913: INFO: stderr: ""
Oct 26 17:16:17.913: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Oct 26 17:16:17.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 get pod e2e-test-httpd-pod -o json --namespace=kubectl-6439'
Oct 26 17:16:18.038: INFO: stderr: ""
Oct 26 17:16:18.038: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2020-10-26T17:16:17Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-10-26T17:16:17Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:message\": {},\n                                \"f:reason\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:message\": {},\n                                \"f:reason\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-10-26T17:16:17Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6439\",\n        \"resourceVersion\": \"40688\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-6439/pods/e2e-test-httpd-pod\",\n        \"uid\": \"b2da38fe-c3ab-4d6f-8ab7-6495e1e7d9a7\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-crrzx\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"10.112.67.203\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 600\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 600\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-crrzx\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-crrzx\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-10-26T17:16:17Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-10-26T17:16:17Z\",\n                \"message\": \"containers with unready status: [e2e-test-httpd-pod]\",\n                \"reason\": \"ContainersNotReady\",\n                \"status\": \"False\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-10-26T17:16:17Z\",\n                \"message\": \"containers with unready status: [e2e-test-httpd-pod]\",\n                \"reason\": \"ContainersNotReady\",\n                \"status\": \"False\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-10-26T17:16:17Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": false,\n                \"restartCount\": 0,\n                \"started\": false,\n                \"state\": {\n                    \"waiting\": {\n                        \"reason\": \"ContainerCreating\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.112.67.203\",\n        \"phase\": \"Pending\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-10-26T17:16:17Z\"\n    }\n}\n"
Oct 26 17:16:18.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 replace -f - --dry-run server --namespace=kubectl-6439'
Oct 26 17:16:18.448: INFO: stderr: "W1026 17:16:18.120120     746 helpers.go:553] --dry-run is deprecated and can be replaced with --dry-run=client.\n"
Oct 26 17:16:18.448: INFO: stdout: "pod/e2e-test-httpd-pod replaced (dry run)\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
Oct 26 17:16:18.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 delete pods e2e-test-httpd-pod --namespace=kubectl-6439'
Oct 26 17:16:29.844: INFO: stderr: ""
Oct 26 17:16:29.844: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:16:29.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6439" for this suite.

• [SLOW TEST:12.360 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:919
    should check if kubectl can dry-run update Pods [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":305,"completed":110,"skipped":1874,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:16:29.902: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6597
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6597.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-6597.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6597.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6597.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-6597.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6597.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct 26 17:16:34.419: INFO: DNS probes using dns-6597/dns-test-55f8d2e6-932f-464e-b39a-27d1d349ae83 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:16:34.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6597" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":305,"completed":111,"skipped":1894,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:16:34.578: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9890
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Oct 26 17:16:34.788: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
Oct 26 17:16:39.573: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:16:59.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9890" for this suite.

• [SLOW TEST:24.477 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":305,"completed":112,"skipped":1896,"failed":0}
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:16:59.055: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2246
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Oct 26 17:16:59.307: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ba2fe910-e5eb-4f37-bfc8-39e4f3259116" in namespace "projected-2246" to be "Succeeded or Failed"
Oct 26 17:16:59.324: INFO: Pod "downwardapi-volume-ba2fe910-e5eb-4f37-bfc8-39e4f3259116": Phase="Pending", Reason="", readiness=false. Elapsed: 17.459928ms
Oct 26 17:17:01.339: INFO: Pod "downwardapi-volume-ba2fe910-e5eb-4f37-bfc8-39e4f3259116": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032548061s
Oct 26 17:17:03.356: INFO: Pod "downwardapi-volume-ba2fe910-e5eb-4f37-bfc8-39e4f3259116": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049214404s
STEP: Saw pod success
Oct 26 17:17:03.356: INFO: Pod "downwardapi-volume-ba2fe910-e5eb-4f37-bfc8-39e4f3259116" satisfied condition "Succeeded or Failed"
Oct 26 17:17:03.371: INFO: Trying to get logs from node 10.112.67.203 pod downwardapi-volume-ba2fe910-e5eb-4f37-bfc8-39e4f3259116 container client-container: <nil>
STEP: delete the pod
Oct 26 17:17:03.479: INFO: Waiting for pod downwardapi-volume-ba2fe910-e5eb-4f37-bfc8-39e4f3259116 to disappear
Oct 26 17:17:03.503: INFO: Pod downwardapi-volume-ba2fe910-e5eb-4f37-bfc8-39e4f3259116 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:17:03.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2246" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":113,"skipped":1896,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:17:03.552: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-2699
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-5334
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-5467
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:17:37.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2699" for this suite.
STEP: Destroying namespace "nsdeletetest-5334" for this suite.
Oct 26 17:17:37.375: INFO: Namespace nsdeletetest-5334 was already deleted
STEP: Destroying namespace "nsdeletetest-5467" for this suite.

• [SLOW TEST:33.852 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":305,"completed":114,"skipped":1901,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:17:37.404: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-9317
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Oct 26 17:17:37.623: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Oct 26 17:17:37.665: INFO: Waiting for terminating namespaces to be deleted...
Oct 26 17:17:37.681: INFO: 
Logging pods the apiserver thinks is on node 10.112.67.201 before test
Oct 26 17:17:37.712: INFO: ibm-cloud-provider-ip-159-8-176-190-5d7d4b6856-tq96m from ibm-system started at 2020-10-26 15:04:09 +0000 UTC (1 container statuses recorded)
Oct 26 17:17:37.712: INFO: 	Container ibm-cloud-provider-ip-159-8-176-190 ready: true, restart count 0
Oct 26 17:17:37.712: INFO: olm-operator-5cbbb5c89d-9gzjm from ibm-system started at 2020-10-26 15:03:06 +0000 UTC (1 container statuses recorded)
Oct 26 17:17:37.712: INFO: 	Container olm-operator ready: true, restart count 0
Oct 26 17:17:37.712: INFO: calico-kube-controllers-68ddfff8d5-klh68 from kube-system started at 2020-10-26 15:03:05 +0000 UTC (1 container statuses recorded)
Oct 26 17:17:37.712: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Oct 26 17:17:37.712: INFO: calico-node-9zj7g from kube-system started at 2020-10-26 15:02:45 +0000 UTC (1 container statuses recorded)
Oct 26 17:17:37.713: INFO: 	Container calico-node ready: true, restart count 0
Oct 26 17:17:37.713: INFO: calico-typha-d497c4cc8-gzkpz from kube-system started at 2020-10-26 15:03:05 +0000 UTC (1 container statuses recorded)
Oct 26 17:17:37.713: INFO: 	Container calico-typha ready: true, restart count 0
Oct 26 17:17:37.713: INFO: coredns-658bf88df8-4cjw8 from kube-system started at 2020-10-26 15:11:47 +0000 UTC (1 container statuses recorded)
Oct 26 17:17:37.713: INFO: 	Container coredns ready: true, restart count 0
Oct 26 17:17:37.713: INFO: dashboard-metrics-scraper-f99788cf9-h8dpn from kube-system started at 2020-10-26 15:03:06 +0000 UTC (1 container statuses recorded)
Oct 26 17:17:37.713: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Oct 26 17:17:37.713: INFO: ibm-keepalived-watcher-2tk7w from kube-system started at 2020-10-26 15:02:45 +0000 UTC (1 container statuses recorded)
Oct 26 17:17:37.713: INFO: 	Container keepalived-watcher ready: true, restart count 0
Oct 26 17:17:37.713: INFO: ibm-master-proxy-static-10.112.67.201 from kube-system started at 2020-10-26 15:02:31 +0000 UTC (2 container statuses recorded)
Oct 26 17:17:37.713: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Oct 26 17:17:37.713: INFO: 	Container pause ready: true, restart count 0
Oct 26 17:17:37.713: INFO: ibm-storage-watcher-567779df5-xjzzb from kube-system started at 2020-10-26 15:03:06 +0000 UTC (1 container statuses recorded)
Oct 26 17:17:37.714: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Oct 26 17:17:37.714: INFO: public-crbube2gcl0spv5v02m2cg-alb1-69897d7d5-wd5xh from kube-system started at 2020-10-26 15:07:34 +0000 UTC (4 container statuses recorded)
Oct 26 17:17:37.714: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Oct 26 17:17:37.714: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Oct 26 17:17:37.714: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Oct 26 17:17:37.714: INFO: 	Container nginx-ingress ready: true, restart count 0
Oct 26 17:17:37.714: INFO: sonobuoy-e2e-job-bb5d7dc03d4b45a5 from sonobuoy started at 2020-10-26 16:37:14 +0000 UTC (2 container statuses recorded)
Oct 26 17:17:37.714: INFO: 	Container e2e ready: true, restart count 0
Oct 26 17:17:37.715: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Oct 26 17:17:37.715: INFO: sonobuoy-systemd-logs-daemon-set-4f27bf473b6a4e03-6rq9w from sonobuoy started at 2020-10-26 16:37:14 +0000 UTC (2 container statuses recorded)
Oct 26 17:17:37.715: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Oct 26 17:17:37.715: INFO: 	Container systemd-logs ready: true, restart count 0
Oct 26 17:17:37.715: INFO: 
Logging pods the apiserver thinks is on node 10.112.67.203 before test
Oct 26 17:17:37.742: INFO: addon-catalog-source-chwdh from ibm-system started at 2020-10-26 15:07:41 +0000 UTC (1 container statuses recorded)
Oct 26 17:17:37.742: INFO: 	Container configmap-registry-server ready: true, restart count 0
Oct 26 17:17:37.743: INFO: catalog-operator-7654f857d5-jp4gd from ibm-system started at 2020-10-26 15:03:03 +0000 UTC (1 container statuses recorded)
Oct 26 17:17:37.743: INFO: 	Container catalog-operator ready: true, restart count 0
Oct 26 17:17:37.743: INFO: calico-node-dcxtp from kube-system started at 2020-10-26 15:02:43 +0000 UTC (1 container statuses recorded)
Oct 26 17:17:37.743: INFO: 	Container calico-node ready: true, restart count 0
Oct 26 17:17:37.743: INFO: calico-typha-d497c4cc8-6rvls from kube-system started at 2020-10-26 15:03:05 +0000 UTC (1 container statuses recorded)
Oct 26 17:17:37.743: INFO: 	Container calico-typha ready: true, restart count 0
Oct 26 17:17:37.743: INFO: coredns-658bf88df8-qbcdt from kube-system started at 2020-10-26 15:11:47 +0000 UTC (1 container statuses recorded)
Oct 26 17:17:37.743: INFO: 	Container coredns ready: true, restart count 0
Oct 26 17:17:37.743: INFO: coredns-autoscaler-65b4b99bb7-8ztj9 from kube-system started at 2020-10-26 15:03:06 +0000 UTC (1 container statuses recorded)
Oct 26 17:17:37.743: INFO: 	Container autoscaler ready: true, restart count 0
Oct 26 17:17:37.743: INFO: ibm-file-plugin-847d88c686-jdpcc from kube-system started at 2020-10-26 15:03:06 +0000 UTC (1 container statuses recorded)
Oct 26 17:17:37.743: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Oct 26 17:17:37.743: INFO: ibm-keepalived-watcher-wlxd7 from kube-system started at 2020-10-26 15:02:43 +0000 UTC (1 container statuses recorded)
Oct 26 17:17:37.743: INFO: 	Container keepalived-watcher ready: true, restart count 0
Oct 26 17:17:37.743: INFO: ibm-master-proxy-static-10.112.67.203 from kube-system started at 2020-10-26 15:02:39 +0000 UTC (2 container statuses recorded)
Oct 26 17:17:37.743: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Oct 26 17:17:37.743: INFO: 	Container pause ready: true, restart count 0
Oct 26 17:17:37.743: INFO: kubernetes-dashboard-7c8884c686-h5xml from kube-system started at 2020-10-26 15:03:05 +0000 UTC (1 container statuses recorded)
Oct 26 17:17:37.743: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Oct 26 17:17:37.743: INFO: metrics-server-66957c64f5-4k2wb from kube-system started at 2020-10-26 15:03:42 +0000 UTC (2 container statuses recorded)
Oct 26 17:17:37.743: INFO: 	Container metrics-server ready: true, restart count 0
Oct 26 17:17:37.743: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Oct 26 17:17:37.743: INFO: vpn-7d76994fc5-68vrp from kube-system started at 2020-10-26 15:11:26 +0000 UTC (1 container statuses recorded)
Oct 26 17:17:37.743: INFO: 	Container vpn ready: true, restart count 0
Oct 26 17:17:37.744: INFO: sonobuoy from sonobuoy started at 2020-10-26 16:37:06 +0000 UTC (1 container statuses recorded)
Oct 26 17:17:37.744: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Oct 26 17:17:37.744: INFO: sonobuoy-systemd-logs-daemon-set-4f27bf473b6a4e03-75n8p from sonobuoy started at 2020-10-26 16:37:14 +0000 UTC (2 container statuses recorded)
Oct 26 17:17:37.744: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Oct 26 17:17:37.744: INFO: 	Container systemd-logs ready: true, restart count 0
Oct 26 17:17:37.744: INFO: 
Logging pods the apiserver thinks is on node 10.112.67.207 before test
Oct 26 17:17:37.782: INFO: test-k8s-e2e-pvg-master-verification from default started at 2020-10-26 15:05:30 +0000 UTC (1 container statuses recorded)
Oct 26 17:17:37.782: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Oct 26 17:17:37.782: INFO: ibm-cloud-provider-ip-159-8-176-190-5d7d4b6856-77vh5 from ibm-system started at 2020-10-26 15:04:09 +0000 UTC (1 container statuses recorded)
Oct 26 17:17:37.782: INFO: 	Container ibm-cloud-provider-ip-159-8-176-190 ready: true, restart count 0
Oct 26 17:17:37.782: INFO: calico-node-k5p5w from kube-system started at 2020-10-26 15:02:53 +0000 UTC (1 container statuses recorded)
Oct 26 17:17:37.782: INFO: 	Container calico-node ready: true, restart count 0
Oct 26 17:17:37.782: INFO: calico-typha-d497c4cc8-6p86s from kube-system started at 2020-10-26 15:03:16 +0000 UTC (1 container statuses recorded)
Oct 26 17:17:37.782: INFO: 	Container calico-typha ready: true, restart count 0
Oct 26 17:17:37.783: INFO: coredns-658bf88df8-pn52m from kube-system started at 2020-10-26 15:11:47 +0000 UTC (1 container statuses recorded)
Oct 26 17:17:37.783: INFO: 	Container coredns ready: true, restart count 0
Oct 26 17:17:37.783: INFO: ibm-keepalived-watcher-drkhn from kube-system started at 2020-10-26 15:02:53 +0000 UTC (1 container statuses recorded)
Oct 26 17:17:37.783: INFO: 	Container keepalived-watcher ready: true, restart count 0
Oct 26 17:17:37.783: INFO: ibm-master-proxy-static-10.112.67.207 from kube-system started at 2020-10-26 15:02:38 +0000 UTC (2 container statuses recorded)
Oct 26 17:17:37.783: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Oct 26 17:17:37.783: INFO: 	Container pause ready: true, restart count 0
Oct 26 17:17:37.783: INFO: public-crbube2gcl0spv5v02m2cg-alb1-69897d7d5-rh4l5 from kube-system started at 2020-10-26 15:07:34 +0000 UTC (4 container statuses recorded)
Oct 26 17:17:37.783: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Oct 26 17:17:37.783: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Oct 26 17:17:37.783: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Oct 26 17:17:37.783: INFO: 	Container nginx-ingress ready: true, restart count 0
Oct 26 17:17:37.783: INFO: sonobuoy-systemd-logs-daemon-set-4f27bf473b6a4e03-pnpc9 from sonobuoy started at 2020-10-26 16:37:14 +0000 UTC (2 container statuses recorded)
Oct 26 17:17:37.783: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Oct 26 17:17:37.783: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: verifying the node has the label node 10.112.67.201
STEP: verifying the node has the label node 10.112.67.203
STEP: verifying the node has the label node 10.112.67.207
Oct 26 17:17:37.980: INFO: Pod test-k8s-e2e-pvg-master-verification requesting resource cpu=0m on Node 10.112.67.207
Oct 26 17:17:37.980: INFO: Pod addon-catalog-source-chwdh requesting resource cpu=10m on Node 10.112.67.203
Oct 26 17:17:37.980: INFO: Pod catalog-operator-7654f857d5-jp4gd requesting resource cpu=10m on Node 10.112.67.203
Oct 26 17:17:37.980: INFO: Pod ibm-cloud-provider-ip-159-8-176-190-5d7d4b6856-77vh5 requesting resource cpu=5m on Node 10.112.67.207
Oct 26 17:17:37.980: INFO: Pod ibm-cloud-provider-ip-159-8-176-190-5d7d4b6856-tq96m requesting resource cpu=5m on Node 10.112.67.201
Oct 26 17:17:37.980: INFO: Pod olm-operator-5cbbb5c89d-9gzjm requesting resource cpu=10m on Node 10.112.67.201
Oct 26 17:17:37.980: INFO: Pod calico-kube-controllers-68ddfff8d5-klh68 requesting resource cpu=10m on Node 10.112.67.201
Oct 26 17:17:37.980: INFO: Pod calico-node-9zj7g requesting resource cpu=250m on Node 10.112.67.201
Oct 26 17:17:37.980: INFO: Pod calico-node-dcxtp requesting resource cpu=250m on Node 10.112.67.203
Oct 26 17:17:37.980: INFO: Pod calico-node-k5p5w requesting resource cpu=250m on Node 10.112.67.207
Oct 26 17:17:37.980: INFO: Pod calico-typha-d497c4cc8-6p86s requesting resource cpu=250m on Node 10.112.67.207
Oct 26 17:17:37.981: INFO: Pod calico-typha-d497c4cc8-6rvls requesting resource cpu=250m on Node 10.112.67.203
Oct 26 17:17:37.981: INFO: Pod calico-typha-d497c4cc8-gzkpz requesting resource cpu=250m on Node 10.112.67.201
Oct 26 17:17:37.981: INFO: Pod coredns-658bf88df8-4cjw8 requesting resource cpu=100m on Node 10.112.67.201
Oct 26 17:17:37.981: INFO: Pod coredns-658bf88df8-pn52m requesting resource cpu=100m on Node 10.112.67.207
Oct 26 17:17:37.981: INFO: Pod coredns-658bf88df8-qbcdt requesting resource cpu=100m on Node 10.112.67.203
Oct 26 17:17:37.981: INFO: Pod coredns-autoscaler-65b4b99bb7-8ztj9 requesting resource cpu=20m on Node 10.112.67.203
Oct 26 17:17:37.981: INFO: Pod dashboard-metrics-scraper-f99788cf9-h8dpn requesting resource cpu=1m on Node 10.112.67.201
Oct 26 17:17:37.981: INFO: Pod ibm-file-plugin-847d88c686-jdpcc requesting resource cpu=50m on Node 10.112.67.203
Oct 26 17:17:37.981: INFO: Pod ibm-keepalived-watcher-2tk7w requesting resource cpu=5m on Node 10.112.67.201
Oct 26 17:17:37.981: INFO: Pod ibm-keepalived-watcher-drkhn requesting resource cpu=5m on Node 10.112.67.207
Oct 26 17:17:37.981: INFO: Pod ibm-keepalived-watcher-wlxd7 requesting resource cpu=5m on Node 10.112.67.203
Oct 26 17:17:37.981: INFO: Pod ibm-master-proxy-static-10.112.67.201 requesting resource cpu=25m on Node 10.112.67.201
Oct 26 17:17:37.981: INFO: Pod ibm-master-proxy-static-10.112.67.203 requesting resource cpu=25m on Node 10.112.67.203
Oct 26 17:17:37.981: INFO: Pod ibm-master-proxy-static-10.112.67.207 requesting resource cpu=25m on Node 10.112.67.207
Oct 26 17:17:37.981: INFO: Pod ibm-storage-watcher-567779df5-xjzzb requesting resource cpu=50m on Node 10.112.67.201
Oct 26 17:17:37.981: INFO: Pod kubernetes-dashboard-7c8884c686-h5xml requesting resource cpu=50m on Node 10.112.67.203
Oct 26 17:17:37.981: INFO: Pod metrics-server-66957c64f5-4k2wb requesting resource cpu=121m on Node 10.112.67.203
Oct 26 17:17:37.981: INFO: Pod public-crbube2gcl0spv5v02m2cg-alb1-69897d7d5-rh4l5 requesting resource cpu=10m on Node 10.112.67.207
Oct 26 17:17:37.981: INFO: Pod public-crbube2gcl0spv5v02m2cg-alb1-69897d7d5-wd5xh requesting resource cpu=10m on Node 10.112.67.201
Oct 26 17:17:37.982: INFO: Pod vpn-7d76994fc5-68vrp requesting resource cpu=5m on Node 10.112.67.203
Oct 26 17:17:37.982: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.112.67.203
Oct 26 17:17:37.982: INFO: Pod sonobuoy-e2e-job-bb5d7dc03d4b45a5 requesting resource cpu=0m on Node 10.112.67.201
Oct 26 17:17:37.982: INFO: Pod sonobuoy-systemd-logs-daemon-set-4f27bf473b6a4e03-6rq9w requesting resource cpu=0m on Node 10.112.67.201
Oct 26 17:17:37.982: INFO: Pod sonobuoy-systemd-logs-daemon-set-4f27bf473b6a4e03-75n8p requesting resource cpu=0m on Node 10.112.67.203
Oct 26 17:17:37.982: INFO: Pod sonobuoy-systemd-logs-daemon-set-4f27bf473b6a4e03-pnpc9 requesting resource cpu=0m on Node 10.112.67.207
STEP: Starting Pods to consume most of the cluster CPU.
Oct 26 17:17:37.982: INFO: Creating a pod which consumes cpu=2285m on Node 10.112.67.207
Oct 26 17:17:38.010: INFO: Creating a pod which consumes cpu=2235m on Node 10.112.67.201
Oct 26 17:17:38.027: INFO: Creating a pod which consumes cpu=2109m on Node 10.112.67.203
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-620db08a-a632-49c4-b035-e1844dad2f59.16419a5b4e92728d], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c45ef25d-f9ea-4bb8-b92d-18e9f8ee2dbe.16419a5b4ed90290], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c45ef25d-f9ea-4bb8-b92d-18e9f8ee2dbe.16419a5b542a149a], Reason = [Created], Message = [Created container filler-pod-c45ef25d-f9ea-4bb8-b92d-18e9f8ee2dbe]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a7ec59d2-b3a1-4891-b700-397cca3fd1f1.16419a5b06403875], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9317/filler-pod-a7ec59d2-b3a1-4891-b700-397cca3fd1f1 to 10.112.67.207]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c45ef25d-f9ea-4bb8-b92d-18e9f8ee2dbe.16419a5b05acee16], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9317/filler-pod-c45ef25d-f9ea-4bb8-b92d-18e9f8ee2dbe to 10.112.67.201]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a7ec59d2-b3a1-4891-b700-397cca3fd1f1.16419a5b52700b94], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a7ec59d2-b3a1-4891-b700-397cca3fd1f1.16419a5b62ef5350], Reason = [Started], Message = [Started container filler-pod-a7ec59d2-b3a1-4891-b700-397cca3fd1f1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-620db08a-a632-49c4-b035-e1844dad2f59.16419a5b5ed4b419], Reason = [Started], Message = [Started container filler-pod-620db08a-a632-49c4-b035-e1844dad2f59]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-620db08a-a632-49c4-b035-e1844dad2f59.16419a5b52a42684], Reason = [Created], Message = [Created container filler-pod-620db08a-a632-49c4-b035-e1844dad2f59]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-620db08a-a632-49c4-b035-e1844dad2f59.16419a5b06b4e55c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9317/filler-pod-620db08a-a632-49c4-b035-e1844dad2f59 to 10.112.67.203]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c45ef25d-f9ea-4bb8-b92d-18e9f8ee2dbe.16419a5b609db0d1], Reason = [Started], Message = [Started container filler-pod-c45ef25d-f9ea-4bb8-b92d-18e9f8ee2dbe]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a7ec59d2-b3a1-4891-b700-397cca3fd1f1.16419a5b56fb681d], Reason = [Created], Message = [Created container filler-pod-a7ec59d2-b3a1-4891-b700-397cca3fd1f1]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16419a5bfd7dd81b], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node 10.112.67.201
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.112.67.203
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.112.67.207
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:17:43.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9317" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:6.055 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":305,"completed":115,"skipped":1921,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:17:43.461: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-846
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 26 17:17:43.770: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"f8994922-7201-468b-a24d-3406ce38f2a3", Controller:(*bool)(0xc005725a56), BlockOwnerDeletion:(*bool)(0xc005725a57)}}
Oct 26 17:17:43.789: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"a44e6871-95ef-4fd9-94bc-f5d4cb629d75", Controller:(*bool)(0xc00574a236), BlockOwnerDeletion:(*bool)(0xc00574a237)}}
Oct 26 17:17:43.807: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"a706cc5d-fec3-41b0-8e45-4e892b959373", Controller:(*bool)(0xc00574a616), BlockOwnerDeletion:(*bool)(0xc00574a617)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:17:48.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-846" for this suite.

• [SLOW TEST:5.459 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":305,"completed":116,"skipped":1932,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:17:48.920: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-5331
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Oct 26 17:17:57.375: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Oct 26 17:17:57.402: INFO: Pod pod-with-prestop-http-hook still exists
Oct 26 17:17:59.402: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Oct 26 17:17:59.420: INFO: Pod pod-with-prestop-http-hook still exists
Oct 26 17:18:01.402: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Oct 26 17:18:01.472: INFO: Pod pod-with-prestop-http-hook still exists
Oct 26 17:18:03.402: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Oct 26 17:18:03.418: INFO: Pod pod-with-prestop-http-hook still exists
Oct 26 17:18:05.402: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Oct 26 17:18:05.417: INFO: Pod pod-with-prestop-http-hook still exists
Oct 26 17:18:07.402: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Oct 26 17:18:07.418: INFO: Pod pod-with-prestop-http-hook still exists
Oct 26 17:18:09.402: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Oct 26 17:18:09.417: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:18:09.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5331" for this suite.

• [SLOW TEST:20.580 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":305,"completed":117,"skipped":1941,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:18:09.500: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2510
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-680e5c1b-0089-4241-81e3-ac400b10f628
STEP: Creating a pod to test consume configMaps
Oct 26 17:18:09.780: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f33f959c-55b5-4cbd-a917-d61efe24d768" in namespace "projected-2510" to be "Succeeded or Failed"
Oct 26 17:18:09.804: INFO: Pod "pod-projected-configmaps-f33f959c-55b5-4cbd-a917-d61efe24d768": Phase="Pending", Reason="", readiness=false. Elapsed: 24.146649ms
Oct 26 17:18:11.818: INFO: Pod "pod-projected-configmaps-f33f959c-55b5-4cbd-a917-d61efe24d768": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038055795s
Oct 26 17:18:13.834: INFO: Pod "pod-projected-configmaps-f33f959c-55b5-4cbd-a917-d61efe24d768": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053683622s
STEP: Saw pod success
Oct 26 17:18:13.834: INFO: Pod "pod-projected-configmaps-f33f959c-55b5-4cbd-a917-d61efe24d768" satisfied condition "Succeeded or Failed"
Oct 26 17:18:13.870: INFO: Trying to get logs from node 10.112.67.207 pod pod-projected-configmaps-f33f959c-55b5-4cbd-a917-d61efe24d768 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Oct 26 17:18:13.990: INFO: Waiting for pod pod-projected-configmaps-f33f959c-55b5-4cbd-a917-d61efe24d768 to disappear
Oct 26 17:18:14.008: INFO: Pod pod-projected-configmaps-f33f959c-55b5-4cbd-a917-d61efe24d768 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:18:14.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2510" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":305,"completed":118,"skipped":1942,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:18:14.060: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8645
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8645.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8645.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8645.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8645.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct 26 17:18:28.449: INFO: DNS probes using dns-test-038c4d4a-2786-4892-80cf-796ba70ebf47 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8645.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8645.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8645.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8645.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct 26 17:18:32.654: INFO: File wheezy_udp@dns-test-service-3.dns-8645.svc.cluster.local from pod  dns-8645/dns-test-76a3ef59-3c94-4ac4-ae02-36154e3cf9da contains 'foo.example.com.
' instead of 'bar.example.com.'
Oct 26 17:18:32.680: INFO: File jessie_udp@dns-test-service-3.dns-8645.svc.cluster.local from pod  dns-8645/dns-test-76a3ef59-3c94-4ac4-ae02-36154e3cf9da contains 'foo.example.com.
' instead of 'bar.example.com.'
Oct 26 17:18:32.680: INFO: Lookups using dns-8645/dns-test-76a3ef59-3c94-4ac4-ae02-36154e3cf9da failed for: [wheezy_udp@dns-test-service-3.dns-8645.svc.cluster.local jessie_udp@dns-test-service-3.dns-8645.svc.cluster.local]

Oct 26 17:18:37.703: INFO: File wheezy_udp@dns-test-service-3.dns-8645.svc.cluster.local from pod  dns-8645/dns-test-76a3ef59-3c94-4ac4-ae02-36154e3cf9da contains 'foo.example.com.
' instead of 'bar.example.com.'
Oct 26 17:18:37.723: INFO: File jessie_udp@dns-test-service-3.dns-8645.svc.cluster.local from pod  dns-8645/dns-test-76a3ef59-3c94-4ac4-ae02-36154e3cf9da contains 'foo.example.com.
' instead of 'bar.example.com.'
Oct 26 17:18:37.723: INFO: Lookups using dns-8645/dns-test-76a3ef59-3c94-4ac4-ae02-36154e3cf9da failed for: [wheezy_udp@dns-test-service-3.dns-8645.svc.cluster.local jessie_udp@dns-test-service-3.dns-8645.svc.cluster.local]

Oct 26 17:18:42.707: INFO: File wheezy_udp@dns-test-service-3.dns-8645.svc.cluster.local from pod  dns-8645/dns-test-76a3ef59-3c94-4ac4-ae02-36154e3cf9da contains 'foo.example.com.
' instead of 'bar.example.com.'
Oct 26 17:18:42.725: INFO: File jessie_udp@dns-test-service-3.dns-8645.svc.cluster.local from pod  dns-8645/dns-test-76a3ef59-3c94-4ac4-ae02-36154e3cf9da contains 'foo.example.com.
' instead of 'bar.example.com.'
Oct 26 17:18:42.725: INFO: Lookups using dns-8645/dns-test-76a3ef59-3c94-4ac4-ae02-36154e3cf9da failed for: [wheezy_udp@dns-test-service-3.dns-8645.svc.cluster.local jessie_udp@dns-test-service-3.dns-8645.svc.cluster.local]

Oct 26 17:18:47.733: INFO: File jessie_udp@dns-test-service-3.dns-8645.svc.cluster.local from pod  dns-8645/dns-test-76a3ef59-3c94-4ac4-ae02-36154e3cf9da contains 'foo.example.com.
' instead of 'bar.example.com.'
Oct 26 17:18:47.733: INFO: Lookups using dns-8645/dns-test-76a3ef59-3c94-4ac4-ae02-36154e3cf9da failed for: [jessie_udp@dns-test-service-3.dns-8645.svc.cluster.local]

Oct 26 17:18:52.719: INFO: DNS probes using dns-test-76a3ef59-3c94-4ac4-ae02-36154e3cf9da succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8645.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-8645.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8645.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-8645.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct 26 17:18:56.984: INFO: DNS probes using dns-test-c2c7bf38-74d6-45a2-ad5f-0e654d665f6c succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:18:57.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8645" for this suite.

• [SLOW TEST:43.101 seconds]
[sig-network] DNS
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":305,"completed":119,"skipped":1961,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:18:57.164: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4676
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Oct 26 17:18:57.423: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4d2335df-5c48-4a95-8025-7e0d336149de" in namespace "projected-4676" to be "Succeeded or Failed"
Oct 26 17:18:57.437: INFO: Pod "downwardapi-volume-4d2335df-5c48-4a95-8025-7e0d336149de": Phase="Pending", Reason="", readiness=false. Elapsed: 14.631405ms
Oct 26 17:18:59.451: INFO: Pod "downwardapi-volume-4d2335df-5c48-4a95-8025-7e0d336149de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028039864s
Oct 26 17:19:01.465: INFO: Pod "downwardapi-volume-4d2335df-5c48-4a95-8025-7e0d336149de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042377998s
STEP: Saw pod success
Oct 26 17:19:01.465: INFO: Pod "downwardapi-volume-4d2335df-5c48-4a95-8025-7e0d336149de" satisfied condition "Succeeded or Failed"
Oct 26 17:19:01.480: INFO: Trying to get logs from node 10.112.67.203 pod downwardapi-volume-4d2335df-5c48-4a95-8025-7e0d336149de container client-container: <nil>
STEP: delete the pod
Oct 26 17:19:01.568: INFO: Waiting for pod downwardapi-volume-4d2335df-5c48-4a95-8025-7e0d336149de to disappear
Oct 26 17:19:01.581: INFO: Pod downwardapi-volume-4d2335df-5c48-4a95-8025-7e0d336149de no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:19:01.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4676" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":305,"completed":120,"skipped":1985,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:19:01.642: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6811
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name secret-emptykey-test-a58ae462-ae19-429b-b75e-8b5ed9e46f88
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:19:01.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6811" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":305,"completed":121,"skipped":1994,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:19:01.915: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename ingressclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingressclass-2443
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Oct 26 17:19:02.252: INFO: starting watch
STEP: patching
STEP: updating
Oct 26 17:19:02.295: INFO: waiting for watch events with expected annotations
Oct 26 17:19:02.295: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:19:02.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-2443" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":305,"completed":122,"skipped":2025,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:19:02.501: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-8121
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-7206
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6427
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:19:10.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8121" for this suite.
STEP: Destroying namespace "nsdeletetest-7206" for this suite.
Oct 26 17:19:10.306: INFO: Namespace nsdeletetest-7206 was already deleted
STEP: Destroying namespace "nsdeletetest-6427" for this suite.

• [SLOW TEST:7.832 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":305,"completed":123,"skipped":2042,"failed":0}
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:19:10.333: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4210
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-c555169c-e97c-4203-ae28-53c2b8022403
STEP: Creating a pod to test consume configMaps
Oct 26 17:19:10.606: INFO: Waiting up to 5m0s for pod "pod-configmaps-997a931f-5136-4aac-80c6-8286f711ef37" in namespace "configmap-4210" to be "Succeeded or Failed"
Oct 26 17:19:10.620: INFO: Pod "pod-configmaps-997a931f-5136-4aac-80c6-8286f711ef37": Phase="Pending", Reason="", readiness=false. Elapsed: 14.31792ms
Oct 26 17:19:12.634: INFO: Pod "pod-configmaps-997a931f-5136-4aac-80c6-8286f711ef37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027742438s
Oct 26 17:19:14.649: INFO: Pod "pod-configmaps-997a931f-5136-4aac-80c6-8286f711ef37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042620839s
STEP: Saw pod success
Oct 26 17:19:14.649: INFO: Pod "pod-configmaps-997a931f-5136-4aac-80c6-8286f711ef37" satisfied condition "Succeeded or Failed"
Oct 26 17:19:14.664: INFO: Trying to get logs from node 10.112.67.203 pod pod-configmaps-997a931f-5136-4aac-80c6-8286f711ef37 container configmap-volume-test: <nil>
STEP: delete the pod
Oct 26 17:19:14.758: INFO: Waiting for pod pod-configmaps-997a931f-5136-4aac-80c6-8286f711ef37 to disappear
Oct 26 17:19:14.772: INFO: Pod pod-configmaps-997a931f-5136-4aac-80c6-8286f711ef37 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:19:14.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4210" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":305,"completed":124,"skipped":2042,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:19:14.823: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7875
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-0586e93e-529f-44bc-8ba3-e504220bc5a4 in namespace container-probe-7875
Oct 26 17:19:17.095: INFO: Started pod liveness-0586e93e-529f-44bc-8ba3-e504220bc5a4 in namespace container-probe-7875
STEP: checking the pod's current state and verifying that restartCount is present
Oct 26 17:19:17.110: INFO: Initial restart count of pod liveness-0586e93e-529f-44bc-8ba3-e504220bc5a4 is 0
Oct 26 17:19:31.231: INFO: Restart count of pod container-probe-7875/liveness-0586e93e-529f-44bc-8ba3-e504220bc5a4 is now 1 (14.121031401s elapsed)
Oct 26 17:19:51.383: INFO: Restart count of pod container-probe-7875/liveness-0586e93e-529f-44bc-8ba3-e504220bc5a4 is now 2 (34.273390429s elapsed)
Oct 26 17:20:11.535: INFO: Restart count of pod container-probe-7875/liveness-0586e93e-529f-44bc-8ba3-e504220bc5a4 is now 3 (54.42501239s elapsed)
Oct 26 17:20:31.860: INFO: Restart count of pod container-probe-7875/liveness-0586e93e-529f-44bc-8ba3-e504220bc5a4 is now 4 (1m14.750033614s elapsed)
Oct 26 17:21:32.498: INFO: Restart count of pod container-probe-7875/liveness-0586e93e-529f-44bc-8ba3-e504220bc5a4 is now 5 (2m15.388455152s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:21:32.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7875" for this suite.

• [SLOW TEST:137.791 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":305,"completed":125,"skipped":2087,"failed":0}
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:21:32.615: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2217
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 26 17:21:32.943: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Oct 26 17:21:32.982: INFO: Number of nodes with available pods: 0
Oct 26 17:21:32.982: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Oct 26 17:21:33.068: INFO: Number of nodes with available pods: 0
Oct 26 17:21:33.068: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 17:21:34.082: INFO: Number of nodes with available pods: 0
Oct 26 17:21:34.082: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 17:21:35.084: INFO: Number of nodes with available pods: 0
Oct 26 17:21:35.084: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 17:21:36.084: INFO: Number of nodes with available pods: 1
Oct 26 17:21:36.084: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Oct 26 17:21:36.213: INFO: Number of nodes with available pods: 1
Oct 26 17:21:36.213: INFO: Number of running nodes: 0, number of available pods: 1
Oct 26 17:21:37.231: INFO: Number of nodes with available pods: 0
Oct 26 17:21:37.231: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Oct 26 17:21:37.268: INFO: Number of nodes with available pods: 0
Oct 26 17:21:37.268: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 17:21:38.283: INFO: Number of nodes with available pods: 0
Oct 26 17:21:38.283: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 17:21:39.283: INFO: Number of nodes with available pods: 0
Oct 26 17:21:39.283: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 17:21:40.284: INFO: Number of nodes with available pods: 0
Oct 26 17:21:40.284: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 17:21:41.287: INFO: Number of nodes with available pods: 0
Oct 26 17:21:41.287: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 17:21:42.285: INFO: Number of nodes with available pods: 0
Oct 26 17:21:42.285: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 17:21:43.282: INFO: Number of nodes with available pods: 0
Oct 26 17:21:43.282: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 17:21:44.283: INFO: Number of nodes with available pods: 1
Oct 26 17:21:44.283: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2217, will wait for the garbage collector to delete the pods
Oct 26 17:21:44.413: INFO: Deleting DaemonSet.extensions daemon-set took: 37.724256ms
Oct 26 17:21:44.614: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.26322ms
Oct 26 17:21:51.929: INFO: Number of nodes with available pods: 0
Oct 26 17:21:51.929: INFO: Number of running nodes: 0, number of available pods: 0
Oct 26 17:21:51.944: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2217/daemonsets","resourceVersion":"42429"},"items":null}

Oct 26 17:21:51.962: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2217/pods","resourceVersion":"42429"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:21:52.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2217" for this suite.

• [SLOW TEST:19.489 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":305,"completed":126,"skipped":2090,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:21:52.105: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-879
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Oct 26 17:21:52.317: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
Oct 26 17:21:57.144: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:22:16.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-879" for this suite.

• [SLOW TEST:24.088 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":305,"completed":127,"skipped":2096,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:22:16.194: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1699
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1699.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1699.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1699.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1699.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1699.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1699.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1699.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1699.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1699.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1699.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1699.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1699.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1699.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 186.125.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.125.186_udp@PTR;check="$$(dig +tcp +noall +answer +search 186.125.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.125.186_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1699.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1699.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1699.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1699.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1699.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1699.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1699.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1699.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1699.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1699.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1699.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1699.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1699.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 186.125.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.125.186_udp@PTR;check="$$(dig +tcp +noall +answer +search 186.125.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.125.186_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct 26 17:22:20.778: INFO: Unable to read wheezy_udp@dns-test-service.dns-1699.svc.cluster.local from pod dns-1699/dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee: the server could not find the requested resource (get pods dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee)
Oct 26 17:22:20.797: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1699.svc.cluster.local from pod dns-1699/dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee: the server could not find the requested resource (get pods dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee)
Oct 26 17:22:20.818: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1699.svc.cluster.local from pod dns-1699/dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee: the server could not find the requested resource (get pods dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee)
Oct 26 17:22:20.969: INFO: Unable to read jessie_udp@dns-test-service.dns-1699.svc.cluster.local from pod dns-1699/dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee: the server could not find the requested resource (get pods dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee)
Oct 26 17:22:20.988: INFO: Unable to read jessie_tcp@dns-test-service.dns-1699.svc.cluster.local from pod dns-1699/dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee: the server could not find the requested resource (get pods dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee)
Oct 26 17:22:21.010: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1699.svc.cluster.local from pod dns-1699/dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee: the server could not find the requested resource (get pods dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee)
Oct 26 17:22:21.034: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1699.svc.cluster.local from pod dns-1699/dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee: the server could not find the requested resource (get pods dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee)
Oct 26 17:22:21.152: INFO: Lookups using dns-1699/dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee failed for: [wheezy_udp@dns-test-service.dns-1699.svc.cluster.local wheezy_tcp@dns-test-service.dns-1699.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1699.svc.cluster.local jessie_udp@dns-test-service.dns-1699.svc.cluster.local jessie_tcp@dns-test-service.dns-1699.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1699.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1699.svc.cluster.local]

Oct 26 17:22:26.184: INFO: Unable to read wheezy_udp@dns-test-service.dns-1699.svc.cluster.local from pod dns-1699/dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee: the server could not find the requested resource (get pods dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee)
Oct 26 17:22:26.206: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1699.svc.cluster.local from pod dns-1699/dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee: the server could not find the requested resource (get pods dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee)
Oct 26 17:22:26.399: INFO: Unable to read jessie_udp@dns-test-service.dns-1699.svc.cluster.local from pod dns-1699/dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee: the server could not find the requested resource (get pods dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee)
Oct 26 17:22:26.431: INFO: Unable to read jessie_tcp@dns-test-service.dns-1699.svc.cluster.local from pod dns-1699/dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee: the server could not find the requested resource (get pods dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee)
Oct 26 17:22:26.470: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1699.svc.cluster.local from pod dns-1699/dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee: the server could not find the requested resource (get pods dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee)
Oct 26 17:22:26.597: INFO: Lookups using dns-1699/dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee failed for: [wheezy_udp@dns-test-service.dns-1699.svc.cluster.local wheezy_tcp@dns-test-service.dns-1699.svc.cluster.local jessie_udp@dns-test-service.dns-1699.svc.cluster.local jessie_tcp@dns-test-service.dns-1699.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1699.svc.cluster.local]

Oct 26 17:22:31.177: INFO: Unable to read wheezy_udp@dns-test-service.dns-1699.svc.cluster.local from pod dns-1699/dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee: the server could not find the requested resource (get pods dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee)
Oct 26 17:22:31.199: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1699.svc.cluster.local from pod dns-1699/dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee: the server could not find the requested resource (get pods dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee)
Oct 26 17:22:31.580: INFO: Unable to read jessie_udp@dns-test-service.dns-1699.svc.cluster.local from pod dns-1699/dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee: the server could not find the requested resource (get pods dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee)
Oct 26 17:22:31.601: INFO: Unable to read jessie_tcp@dns-test-service.dns-1699.svc.cluster.local from pod dns-1699/dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee: the server could not find the requested resource (get pods dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee)
Oct 26 17:22:31.644: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1699.svc.cluster.local from pod dns-1699/dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee: the server could not find the requested resource (get pods dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee)
Oct 26 17:22:31.796: INFO: Lookups using dns-1699/dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee failed for: [wheezy_udp@dns-test-service.dns-1699.svc.cluster.local wheezy_tcp@dns-test-service.dns-1699.svc.cluster.local jessie_udp@dns-test-service.dns-1699.svc.cluster.local jessie_tcp@dns-test-service.dns-1699.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1699.svc.cluster.local]

Oct 26 17:22:36.174: INFO: Unable to read wheezy_udp@dns-test-service.dns-1699.svc.cluster.local from pod dns-1699/dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee: the server could not find the requested resource (get pods dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee)
Oct 26 17:22:36.196: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1699.svc.cluster.local from pod dns-1699/dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee: the server could not find the requested resource (get pods dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee)
Oct 26 17:22:36.384: INFO: Unable to read jessie_udp@dns-test-service.dns-1699.svc.cluster.local from pod dns-1699/dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee: the server could not find the requested resource (get pods dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee)
Oct 26 17:22:36.407: INFO: Unable to read jessie_tcp@dns-test-service.dns-1699.svc.cluster.local from pod dns-1699/dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee: the server could not find the requested resource (get pods dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee)
Oct 26 17:22:36.567: INFO: Lookups using dns-1699/dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee failed for: [wheezy_udp@dns-test-service.dns-1699.svc.cluster.local wheezy_tcp@dns-test-service.dns-1699.svc.cluster.local jessie_udp@dns-test-service.dns-1699.svc.cluster.local jessie_tcp@dns-test-service.dns-1699.svc.cluster.local]

Oct 26 17:22:41.171: INFO: Unable to read wheezy_udp@dns-test-service.dns-1699.svc.cluster.local from pod dns-1699/dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee: the server could not find the requested resource (get pods dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee)
Oct 26 17:22:41.191: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1699.svc.cluster.local from pod dns-1699/dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee: the server could not find the requested resource (get pods dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee)
Oct 26 17:22:41.378: INFO: Unable to read jessie_udp@dns-test-service.dns-1699.svc.cluster.local from pod dns-1699/dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee: the server could not find the requested resource (get pods dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee)
Oct 26 17:22:41.396: INFO: Unable to read jessie_tcp@dns-test-service.dns-1699.svc.cluster.local from pod dns-1699/dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee: the server could not find the requested resource (get pods dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee)
Oct 26 17:22:41.577: INFO: Lookups using dns-1699/dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee failed for: [wheezy_udp@dns-test-service.dns-1699.svc.cluster.local wheezy_tcp@dns-test-service.dns-1699.svc.cluster.local jessie_udp@dns-test-service.dns-1699.svc.cluster.local jessie_tcp@dns-test-service.dns-1699.svc.cluster.local]

Oct 26 17:22:46.172: INFO: Unable to read wheezy_udp@dns-test-service.dns-1699.svc.cluster.local from pod dns-1699/dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee: the server could not find the requested resource (get pods dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee)
Oct 26 17:22:46.190: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1699.svc.cluster.local from pod dns-1699/dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee: the server could not find the requested resource (get pods dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee)
Oct 26 17:22:46.380: INFO: Unable to read jessie_udp@dns-test-service.dns-1699.svc.cluster.local from pod dns-1699/dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee: the server could not find the requested resource (get pods dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee)
Oct 26 17:22:46.412: INFO: Unable to read jessie_tcp@dns-test-service.dns-1699.svc.cluster.local from pod dns-1699/dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee: the server could not find the requested resource (get pods dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee)
Oct 26 17:22:46.593: INFO: Lookups using dns-1699/dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee failed for: [wheezy_udp@dns-test-service.dns-1699.svc.cluster.local wheezy_tcp@dns-test-service.dns-1699.svc.cluster.local jessie_udp@dns-test-service.dns-1699.svc.cluster.local jessie_tcp@dns-test-service.dns-1699.svc.cluster.local]

Oct 26 17:22:51.589: INFO: DNS probes using dns-1699/dns-test-ff249b02-c968-4e1c-afa0-b3d9cb43aaee succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:22:51.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1699" for this suite.

• [SLOW TEST:35.674 seconds]
[sig-network] DNS
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":305,"completed":128,"skipped":2150,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:22:51.870: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8498
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Oct 26 17:22:52.126: INFO: Waiting up to 5m0s for pod "downwardapi-volume-238d135e-139b-4f3b-ac0c-4b32d5b10316" in namespace "downward-api-8498" to be "Succeeded or Failed"
Oct 26 17:22:52.146: INFO: Pod "downwardapi-volume-238d135e-139b-4f3b-ac0c-4b32d5b10316": Phase="Pending", Reason="", readiness=false. Elapsed: 19.278541ms
Oct 26 17:22:54.160: INFO: Pod "downwardapi-volume-238d135e-139b-4f3b-ac0c-4b32d5b10316": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033343262s
Oct 26 17:22:56.173: INFO: Pod "downwardapi-volume-238d135e-139b-4f3b-ac0c-4b32d5b10316": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046887757s
STEP: Saw pod success
Oct 26 17:22:56.173: INFO: Pod "downwardapi-volume-238d135e-139b-4f3b-ac0c-4b32d5b10316" satisfied condition "Succeeded or Failed"
Oct 26 17:22:56.189: INFO: Trying to get logs from node 10.112.67.203 pod downwardapi-volume-238d135e-139b-4f3b-ac0c-4b32d5b10316 container client-container: <nil>
STEP: delete the pod
Oct 26 17:22:56.303: INFO: Waiting for pod downwardapi-volume-238d135e-139b-4f3b-ac0c-4b32d5b10316 to disappear
Oct 26 17:22:56.315: INFO: Pod downwardapi-volume-238d135e-139b-4f3b-ac0c-4b32d5b10316 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:22:56.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8498" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":305,"completed":129,"skipped":2185,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:22:56.364: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2731
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 26 17:22:57.171: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 26 17:22:59.213: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739329777, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739329777, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739329777, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739329777, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 26 17:23:02.266: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the crd webhook via the AdmissionRegistration API
Oct 26 17:23:02.362: INFO: Waiting for webhook configuration to be ready...
Oct 26 17:23:02.557: INFO: Waiting for webhook configuration to be ready...
Oct 26 17:23:02.623: INFO: Waiting for webhook configuration to be ready...
Oct 26 17:23:02.728: INFO: Waiting for webhook configuration to be ready...
Oct 26 17:23:02.840: INFO: Waiting for webhook configuration to be ready...
Oct 26 17:23:02.925: INFO: Waiting for webhook configuration to be ready...
Oct 26 17:23:04.049: INFO: Waiting for webhook configuration to be ready...
Oct 26 17:23:05.202: INFO: Waiting for webhook configuration to be ready...
Oct 26 17:23:06.356: INFO: Waiting for webhook configuration to be ready...
Oct 26 17:23:06.423: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource definition that should be denied by the webhook
Oct 26 17:23:06.499: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:23:06.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2731" for this suite.
STEP: Destroying namespace "webhook-2731-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:10.451 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":305,"completed":130,"skipped":2202,"failed":0}
SSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:23:06.815: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-6060
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:171
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating server pod server in namespace prestop-6060
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-6060
STEP: Deleting pre-stop pod
Oct 26 17:23:20.230: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:23:20.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-6060" for this suite.

• [SLOW TEST:13.507 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":305,"completed":131,"skipped":2206,"failed":0}
SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:23:20.323: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-8972
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:23:20.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8972" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":305,"completed":132,"skipped":2208,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:23:20.705: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-9732
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Oct 26 17:23:27.100: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9732 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 17:23:27.100: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
Oct 26 17:23:27.300: INFO: Exec stderr: ""
Oct 26 17:23:27.300: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9732 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 17:23:27.300: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
Oct 26 17:23:27.681: INFO: Exec stderr: ""
Oct 26 17:23:27.681: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9732 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 17:23:27.681: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
Oct 26 17:23:27.903: INFO: Exec stderr: ""
Oct 26 17:23:27.903: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9732 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 17:23:27.903: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
Oct 26 17:23:28.115: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Oct 26 17:23:28.115: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9732 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 17:23:28.115: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
Oct 26 17:23:28.326: INFO: Exec stderr: ""
Oct 26 17:23:28.326: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9732 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 17:23:28.326: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
Oct 26 17:23:28.533: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Oct 26 17:23:28.533: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9732 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 17:23:28.533: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
Oct 26 17:23:28.746: INFO: Exec stderr: ""
Oct 26 17:23:28.746: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9732 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 17:23:28.746: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
Oct 26 17:23:28.964: INFO: Exec stderr: ""
Oct 26 17:23:28.964: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9732 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 17:23:28.964: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
Oct 26 17:23:29.172: INFO: Exec stderr: ""
Oct 26 17:23:29.172: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9732 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 17:23:29.172: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
Oct 26 17:23:29.373: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:23:29.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-9732" for this suite.

• [SLOW TEST:8.728 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":133,"skipped":2233,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:23:29.433: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5940
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-5940
STEP: creating service affinity-nodeport in namespace services-5940
STEP: creating replication controller affinity-nodeport in namespace services-5940
I1026 17:23:29.743562      27 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-5940, replica count: 3
I1026 17:23:32.794036      27 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 26 17:23:32.850: INFO: Creating new exec pod
Oct 26 17:23:35.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-5940 execpod-affinityk92xf -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
Oct 26 17:23:36.333: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Oct 26 17:23:36.333: INFO: stdout: ""
Oct 26 17:23:36.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-5940 execpod-affinityk92xf -- /bin/sh -x -c nc -zv -t -w 2 172.21.15.119 80'
Oct 26 17:23:36.675: INFO: stderr: "+ nc -zv -t -w 2 172.21.15.119 80\nConnection to 172.21.15.119 80 port [tcp/http] succeeded!\n"
Oct 26 17:23:36.675: INFO: stdout: ""
Oct 26 17:23:36.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-5940 execpod-affinityk92xf -- /bin/sh -x -c nc -zv -t -w 2 10.112.67.203 31068'
Oct 26 17:23:37.018: INFO: stderr: "+ nc -zv -t -w 2 10.112.67.203 31068\nConnection to 10.112.67.203 31068 port [tcp/31068] succeeded!\n"
Oct 26 17:23:37.018: INFO: stdout: ""
Oct 26 17:23:37.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-5940 execpod-affinityk92xf -- /bin/sh -x -c nc -zv -t -w 2 10.112.67.207 31068'
Oct 26 17:23:37.351: INFO: stderr: "+ nc -zv -t -w 2 10.112.67.207 31068\nConnection to 10.112.67.207 31068 port [tcp/31068] succeeded!\n"
Oct 26 17:23:37.351: INFO: stdout: ""
Oct 26 17:23:37.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-5940 execpod-affinityk92xf -- /bin/sh -x -c nc -zv -t -w 2 5.10.101.149 31068'
Oct 26 17:23:37.705: INFO: stderr: "+ nc -zv -t -w 2 5.10.101.149 31068\nConnection to 5.10.101.149 31068 port [tcp/31068] succeeded!\n"
Oct 26 17:23:37.705: INFO: stdout: ""
Oct 26 17:23:37.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-5940 execpod-affinityk92xf -- /bin/sh -x -c nc -zv -t -w 2 5.10.101.150 31068'
Oct 26 17:23:38.066: INFO: stderr: "+ nc -zv -t -w 2 5.10.101.150 31068\nConnection to 5.10.101.150 31068 port [tcp/31068] succeeded!\n"
Oct 26 17:23:38.066: INFO: stdout: ""
Oct 26 17:23:38.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-5940 execpod-affinityk92xf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.112.67.201:31068/ ; done'
Oct 26 17:23:38.615: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:31068/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:31068/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:31068/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:31068/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:31068/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:31068/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:31068/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:31068/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:31068/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:31068/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:31068/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:31068/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:31068/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:31068/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:31068/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:31068/\n"
Oct 26 17:23:38.615: INFO: stdout: "\naffinity-nodeport-bwdj6\naffinity-nodeport-bwdj6\naffinity-nodeport-bwdj6\naffinity-nodeport-bwdj6\naffinity-nodeport-bwdj6\naffinity-nodeport-bwdj6\naffinity-nodeport-bwdj6\naffinity-nodeport-bwdj6\naffinity-nodeport-bwdj6\naffinity-nodeport-bwdj6\naffinity-nodeport-bwdj6\naffinity-nodeport-bwdj6\naffinity-nodeport-bwdj6\naffinity-nodeport-bwdj6\naffinity-nodeport-bwdj6\naffinity-nodeport-bwdj6"
Oct 26 17:23:38.616: INFO: Received response from host: affinity-nodeport-bwdj6
Oct 26 17:23:38.616: INFO: Received response from host: affinity-nodeport-bwdj6
Oct 26 17:23:38.616: INFO: Received response from host: affinity-nodeport-bwdj6
Oct 26 17:23:38.616: INFO: Received response from host: affinity-nodeport-bwdj6
Oct 26 17:23:38.616: INFO: Received response from host: affinity-nodeport-bwdj6
Oct 26 17:23:38.616: INFO: Received response from host: affinity-nodeport-bwdj6
Oct 26 17:23:38.616: INFO: Received response from host: affinity-nodeport-bwdj6
Oct 26 17:23:38.616: INFO: Received response from host: affinity-nodeport-bwdj6
Oct 26 17:23:38.616: INFO: Received response from host: affinity-nodeport-bwdj6
Oct 26 17:23:38.616: INFO: Received response from host: affinity-nodeport-bwdj6
Oct 26 17:23:38.616: INFO: Received response from host: affinity-nodeport-bwdj6
Oct 26 17:23:38.616: INFO: Received response from host: affinity-nodeport-bwdj6
Oct 26 17:23:38.616: INFO: Received response from host: affinity-nodeport-bwdj6
Oct 26 17:23:38.616: INFO: Received response from host: affinity-nodeport-bwdj6
Oct 26 17:23:38.616: INFO: Received response from host: affinity-nodeport-bwdj6
Oct 26 17:23:38.616: INFO: Received response from host: affinity-nodeport-bwdj6
Oct 26 17:23:38.616: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-5940, will wait for the garbage collector to delete the pods
Oct 26 17:23:38.785: INFO: Deleting ReplicationController affinity-nodeport took: 38.101584ms
Oct 26 17:23:38.885: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.2417ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:23:51.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5940" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:22.621 seconds]
[sig-network] Services
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":134,"skipped":2238,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:23:52.055: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8677
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl logs
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1415
STEP: creating an pod
Oct 26 17:23:52.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.20 --namespace=kubectl-8677 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Oct 26 17:23:52.437: INFO: stderr: ""
Oct 26 17:23:52.437: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Waiting for log generator to start.
Oct 26 17:23:52.437: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Oct 26 17:23:52.437: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-8677" to be "running and ready, or succeeded"
Oct 26 17:23:52.450: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 13.449512ms
Oct 26 17:23:54.468: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030598659s
Oct 26 17:23:56.498: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.061260429s
Oct 26 17:23:56.498: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Oct 26 17:23:56.498: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Oct 26 17:23:56.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 logs logs-generator logs-generator --namespace=kubectl-8677'
Oct 26 17:23:56.677: INFO: stderr: ""
Oct 26 17:23:56.677: INFO: stdout: "I1026 17:23:53.969213       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/vw5 483\nI1026 17:23:54.169433       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/wzn 369\nI1026 17:23:54.369402       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/8bxj 510\nI1026 17:23:54.569448       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/vcx9 454\nI1026 17:23:54.769394       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/l2hn 261\nI1026 17:23:54.969254       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/rjk 564\nI1026 17:23:55.169343       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/8jwq 279\nI1026 17:23:55.369342       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/5p5 323\nI1026 17:23:55.569442       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/w5ht 343\nI1026 17:23:55.769360       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/gtc4 358\nI1026 17:23:55.969367       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/b6g7 408\nI1026 17:23:56.169404       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/87m 566\nI1026 17:23:56.369325       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/tj2 501\nI1026 17:23:56.569444       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/4qz 508\n"
STEP: limiting log lines
Oct 26 17:23:56.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 logs logs-generator logs-generator --namespace=kubectl-8677 --tail=1'
Oct 26 17:23:56.876: INFO: stderr: ""
Oct 26 17:23:56.877: INFO: stdout: "I1026 17:23:56.769444       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/nk4z 272\n"
Oct 26 17:23:56.877: INFO: got output "I1026 17:23:56.769444       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/nk4z 272\n"
STEP: limiting log bytes
Oct 26 17:23:56.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 logs logs-generator logs-generator --namespace=kubectl-8677 --limit-bytes=1'
Oct 26 17:23:57.079: INFO: stderr: ""
Oct 26 17:23:57.079: INFO: stdout: "I"
Oct 26 17:23:57.079: INFO: got output "I"
STEP: exposing timestamps
Oct 26 17:23:57.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 logs logs-generator logs-generator --namespace=kubectl-8677 --tail=1 --timestamps'
Oct 26 17:23:57.248: INFO: stderr: ""
Oct 26 17:23:57.248: INFO: stdout: "2020-10-26T17:23:57.169738040Z I1026 17:23:57.169380       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/bpl5 395\n"
Oct 26 17:23:57.248: INFO: got output "2020-10-26T17:23:57.169738040Z I1026 17:23:57.169380       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/bpl5 395\n"
STEP: restricting to a time range
Oct 26 17:23:59.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 logs logs-generator logs-generator --namespace=kubectl-8677 --since=1s'
Oct 26 17:23:59.937: INFO: stderr: ""
Oct 26 17:23:59.937: INFO: stdout: "I1026 17:23:58.969319       1 logs_generator.go:76] 25 POST /api/v1/namespaces/default/pods/vx5x 462\nI1026 17:23:59.169316       1 logs_generator.go:76] 26 POST /api/v1/namespaces/kube-system/pods/kxzk 307\nI1026 17:23:59.369449       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/default/pods/lg4m 541\nI1026 17:23:59.569388       1 logs_generator.go:76] 28 GET /api/v1/namespaces/kube-system/pods/47d 486\nI1026 17:23:59.769361       1 logs_generator.go:76] 29 POST /api/v1/namespaces/ns/pods/pmz2 502\n"
Oct 26 17:23:59.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 logs logs-generator logs-generator --namespace=kubectl-8677 --since=24h'
Oct 26 17:24:00.135: INFO: stderr: ""
Oct 26 17:24:00.135: INFO: stdout: "I1026 17:23:53.969213       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/vw5 483\nI1026 17:23:54.169433       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/wzn 369\nI1026 17:23:54.369402       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/8bxj 510\nI1026 17:23:54.569448       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/vcx9 454\nI1026 17:23:54.769394       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/l2hn 261\nI1026 17:23:54.969254       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/rjk 564\nI1026 17:23:55.169343       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/8jwq 279\nI1026 17:23:55.369342       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/5p5 323\nI1026 17:23:55.569442       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/w5ht 343\nI1026 17:23:55.769360       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/gtc4 358\nI1026 17:23:55.969367       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/b6g7 408\nI1026 17:23:56.169404       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/87m 566\nI1026 17:23:56.369325       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/tj2 501\nI1026 17:23:56.569444       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/4qz 508\nI1026 17:23:56.769444       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/nk4z 272\nI1026 17:23:56.969375       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/qhq 473\nI1026 17:23:57.169380       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/bpl5 395\nI1026 17:23:57.369368       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/2lhc 512\nI1026 17:23:57.569417       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/jfn 504\nI1026 17:23:57.769347       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/mp6h 548\nI1026 17:23:57.969360       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/7tf 312\nI1026 17:23:58.169348       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/2cw 464\nI1026 17:23:58.369327       1 logs_generator.go:76] 22 POST /api/v1/namespaces/ns/pods/x595 505\nI1026 17:23:58.569362       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/ns/pods/b8q 584\nI1026 17:23:58.769446       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/kube-system/pods/sbxr 300\nI1026 17:23:58.969319       1 logs_generator.go:76] 25 POST /api/v1/namespaces/default/pods/vx5x 462\nI1026 17:23:59.169316       1 logs_generator.go:76] 26 POST /api/v1/namespaces/kube-system/pods/kxzk 307\nI1026 17:23:59.369449       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/default/pods/lg4m 541\nI1026 17:23:59.569388       1 logs_generator.go:76] 28 GET /api/v1/namespaces/kube-system/pods/47d 486\nI1026 17:23:59.769361       1 logs_generator.go:76] 29 POST /api/v1/namespaces/ns/pods/pmz2 502\nI1026 17:23:59.969322       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/ns/pods/n248 266\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
Oct 26 17:24:00.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 delete pod logs-generator --namespace=kubectl-8677'
Oct 26 17:24:09.876: INFO: stderr: ""
Oct 26 17:24:09.876: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:24:09.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8677" for this suite.

• [SLOW TEST:17.877 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1411
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":305,"completed":135,"skipped":2268,"failed":0}
SSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:24:09.933: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8078
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 26 17:24:10.162: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:24:14.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8078" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":305,"completed":136,"skipped":2271,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:24:14.415: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9442
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Oct 26 17:24:14.697: INFO: Waiting up to 5m0s for pod "pod-454828bc-8087-4f61-b088-1683ef477571" in namespace "emptydir-9442" to be "Succeeded or Failed"
Oct 26 17:24:14.717: INFO: Pod "pod-454828bc-8087-4f61-b088-1683ef477571": Phase="Pending", Reason="", readiness=false. Elapsed: 20.288568ms
Oct 26 17:24:16.734: INFO: Pod "pod-454828bc-8087-4f61-b088-1683ef477571": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036622829s
Oct 26 17:24:18.752: INFO: Pod "pod-454828bc-8087-4f61-b088-1683ef477571": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054761309s
STEP: Saw pod success
Oct 26 17:24:18.752: INFO: Pod "pod-454828bc-8087-4f61-b088-1683ef477571" satisfied condition "Succeeded or Failed"
Oct 26 17:24:18.765: INFO: Trying to get logs from node 10.112.67.203 pod pod-454828bc-8087-4f61-b088-1683ef477571 container test-container: <nil>
STEP: delete the pod
Oct 26 17:24:18.848: INFO: Waiting for pod pod-454828bc-8087-4f61-b088-1683ef477571 to disappear
Oct 26 17:24:18.864: INFO: Pod pod-454828bc-8087-4f61-b088-1683ef477571 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:24:18.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9442" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":137,"skipped":2310,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:24:18.923: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1765
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 26 17:24:21.261: INFO: Waiting up to 5m0s for pod "client-envvars-7507fb17-79b1-469b-bb82-fee0d4e1d9db" in namespace "pods-1765" to be "Succeeded or Failed"
Oct 26 17:24:21.284: INFO: Pod "client-envvars-7507fb17-79b1-469b-bb82-fee0d4e1d9db": Phase="Pending", Reason="", readiness=false. Elapsed: 23.048131ms
Oct 26 17:24:23.297: INFO: Pod "client-envvars-7507fb17-79b1-469b-bb82-fee0d4e1d9db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036488419s
Oct 26 17:24:25.312: INFO: Pod "client-envvars-7507fb17-79b1-469b-bb82-fee0d4e1d9db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051145696s
STEP: Saw pod success
Oct 26 17:24:25.312: INFO: Pod "client-envvars-7507fb17-79b1-469b-bb82-fee0d4e1d9db" satisfied condition "Succeeded or Failed"
Oct 26 17:24:25.325: INFO: Trying to get logs from node 10.112.67.203 pod client-envvars-7507fb17-79b1-469b-bb82-fee0d4e1d9db container env3cont: <nil>
STEP: delete the pod
Oct 26 17:24:25.401: INFO: Waiting for pod client-envvars-7507fb17-79b1-469b-bb82-fee0d4e1d9db to disappear
Oct 26 17:24:25.416: INFO: Pod client-envvars-7507fb17-79b1-469b-bb82-fee0d4e1d9db no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:24:25.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1765" for this suite.

• [SLOW TEST:6.546 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":305,"completed":138,"skipped":2318,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:24:25.471: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8983
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating api versions
Oct 26 17:24:25.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 api-versions'
Oct 26 17:24:25.799: INFO: stderr: ""
Oct 26 17:24:25.799: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nibm.com/v1alpha1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:24:25.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8983" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":305,"completed":139,"skipped":2331,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:24:25.853: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3798
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-07f01be4-9849-4a5b-a6ab-adf9b65233cc in namespace container-probe-3798
Oct 26 17:24:28.120: INFO: Started pod busybox-07f01be4-9849-4a5b-a6ab-adf9b65233cc in namespace container-probe-3798
STEP: checking the pod's current state and verifying that restartCount is present
Oct 26 17:24:28.133: INFO: Initial restart count of pod busybox-07f01be4-9849-4a5b-a6ab-adf9b65233cc is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:28:28.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3798" for this suite.

• [SLOW TEST:242.626 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":305,"completed":140,"skipped":2354,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:28:28.480: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8450
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Oct 26 17:28:28.769: INFO: Waiting up to 5m0s for pod "downwardapi-volume-28fee6d5-3918-489d-85ed-7eb5e6b02c36" in namespace "downward-api-8450" to be "Succeeded or Failed"
Oct 26 17:28:28.819: INFO: Pod "downwardapi-volume-28fee6d5-3918-489d-85ed-7eb5e6b02c36": Phase="Pending", Reason="", readiness=false. Elapsed: 49.910588ms
Oct 26 17:28:30.832: INFO: Pod "downwardapi-volume-28fee6d5-3918-489d-85ed-7eb5e6b02c36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.062985478s
STEP: Saw pod success
Oct 26 17:28:30.832: INFO: Pod "downwardapi-volume-28fee6d5-3918-489d-85ed-7eb5e6b02c36" satisfied condition "Succeeded or Failed"
Oct 26 17:28:30.846: INFO: Trying to get logs from node 10.112.67.203 pod downwardapi-volume-28fee6d5-3918-489d-85ed-7eb5e6b02c36 container client-container: <nil>
STEP: delete the pod
Oct 26 17:28:30.967: INFO: Waiting for pod downwardapi-volume-28fee6d5-3918-489d-85ed-7eb5e6b02c36 to disappear
Oct 26 17:28:30.985: INFO: Pod downwardapi-volume-28fee6d5-3918-489d-85ed-7eb5e6b02c36 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:28:30.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8450" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":141,"skipped":2365,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:28:31.035: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-7953
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Oct 26 17:28:31.265: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:28:36.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7953" for this suite.

• [SLOW TEST:5.573 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":305,"completed":142,"skipped":2388,"failed":0}
SSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:28:36.609: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9825
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:28:36.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9825" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":305,"completed":143,"skipped":2395,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:28:37.045: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8730
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-8730
STEP: creating service affinity-nodeport-transition in namespace services-8730
STEP: creating replication controller affinity-nodeport-transition in namespace services-8730
I1026 17:28:37.374913      27 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-8730, replica count: 3
I1026 17:28:40.425762      27 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 26 17:28:40.466: INFO: Creating new exec pod
Oct 26 17:28:43.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-8730 execpod-affinityg67qd -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
Oct 26 17:28:44.031: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Oct 26 17:28:44.031: INFO: stdout: ""
Oct 26 17:28:44.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-8730 execpod-affinityg67qd -- /bin/sh -x -c nc -zv -t -w 2 172.21.39.143 80'
Oct 26 17:28:44.383: INFO: stderr: "+ nc -zv -t -w 2 172.21.39.143 80\nConnection to 172.21.39.143 80 port [tcp/http] succeeded!\n"
Oct 26 17:28:44.383: INFO: stdout: ""
Oct 26 17:28:44.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-8730 execpod-affinityg67qd -- /bin/sh -x -c nc -zv -t -w 2 10.112.67.203 30073'
Oct 26 17:28:44.820: INFO: stderr: "+ nc -zv -t -w 2 10.112.67.203 30073\nConnection to 10.112.67.203 30073 port [tcp/30073] succeeded!\n"
Oct 26 17:28:44.820: INFO: stdout: ""
Oct 26 17:28:44.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-8730 execpod-affinityg67qd -- /bin/sh -x -c nc -zv -t -w 2 10.112.67.207 30073'
Oct 26 17:28:45.204: INFO: stderr: "+ nc -zv -t -w 2 10.112.67.207 30073\nConnection to 10.112.67.207 30073 port [tcp/30073] succeeded!\n"
Oct 26 17:28:45.204: INFO: stdout: ""
Oct 26 17:28:45.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-8730 execpod-affinityg67qd -- /bin/sh -x -c nc -zv -t -w 2 5.10.101.149 30073'
Oct 26 17:28:45.544: INFO: stderr: "+ nc -zv -t -w 2 5.10.101.149 30073\nConnection to 5.10.101.149 30073 port [tcp/30073] succeeded!\n"
Oct 26 17:28:45.544: INFO: stdout: ""
Oct 26 17:28:45.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-8730 execpod-affinityg67qd -- /bin/sh -x -c nc -zv -t -w 2 5.10.101.150 30073'
Oct 26 17:28:45.883: INFO: stderr: "+ nc -zv -t -w 2 5.10.101.150 30073\nConnection to 5.10.101.150 30073 port [tcp/30073] succeeded!\n"
Oct 26 17:28:45.883: INFO: stdout: ""
Oct 26 17:28:45.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-8730 execpod-affinityg67qd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.112.67.201:30073/ ; done'
Oct 26 17:28:46.480: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n"
Oct 26 17:28:46.481: INFO: stdout: "\naffinity-nodeport-transition-zdkm4\naffinity-nodeport-transition-zdkm4\naffinity-nodeport-transition-zdkm4\naffinity-nodeport-transition-zdkm4\naffinity-nodeport-transition-zdkm4\naffinity-nodeport-transition-zdkm4\naffinity-nodeport-transition-zdkm4\naffinity-nodeport-transition-zdkm4\naffinity-nodeport-transition-zdkm4\naffinity-nodeport-transition-zdkm4\naffinity-nodeport-transition-zdkm4\naffinity-nodeport-transition-zdkm4\naffinity-nodeport-transition-zdkm4\naffinity-nodeport-transition-zdkm4\naffinity-nodeport-transition-zdkm4\naffinity-nodeport-transition-zdkm4"
Oct 26 17:28:46.481: INFO: Received response from host: affinity-nodeport-transition-zdkm4
Oct 26 17:28:46.481: INFO: Received response from host: affinity-nodeport-transition-zdkm4
Oct 26 17:28:46.481: INFO: Received response from host: affinity-nodeport-transition-zdkm4
Oct 26 17:28:46.481: INFO: Received response from host: affinity-nodeport-transition-zdkm4
Oct 26 17:28:46.481: INFO: Received response from host: affinity-nodeport-transition-zdkm4
Oct 26 17:28:46.481: INFO: Received response from host: affinity-nodeport-transition-zdkm4
Oct 26 17:28:46.481: INFO: Received response from host: affinity-nodeport-transition-zdkm4
Oct 26 17:28:46.481: INFO: Received response from host: affinity-nodeport-transition-zdkm4
Oct 26 17:28:46.481: INFO: Received response from host: affinity-nodeport-transition-zdkm4
Oct 26 17:28:46.481: INFO: Received response from host: affinity-nodeport-transition-zdkm4
Oct 26 17:28:46.481: INFO: Received response from host: affinity-nodeport-transition-zdkm4
Oct 26 17:28:46.481: INFO: Received response from host: affinity-nodeport-transition-zdkm4
Oct 26 17:28:46.481: INFO: Received response from host: affinity-nodeport-transition-zdkm4
Oct 26 17:28:46.481: INFO: Received response from host: affinity-nodeport-transition-zdkm4
Oct 26 17:28:46.481: INFO: Received response from host: affinity-nodeport-transition-zdkm4
Oct 26 17:28:46.481: INFO: Received response from host: affinity-nodeport-transition-zdkm4
Oct 26 17:29:16.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-8730 execpod-affinityg67qd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.112.67.201:30073/ ; done'
Oct 26 17:29:17.008: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n"
Oct 26 17:29:17.008: INFO: stdout: "\naffinity-nodeport-transition-fp8gh\naffinity-nodeport-transition-zdkm4\naffinity-nodeport-transition-zdkm4\naffinity-nodeport-transition-fp8gh\naffinity-nodeport-transition-fp8gh\naffinity-nodeport-transition-fp8gh\naffinity-nodeport-transition-5nk57\naffinity-nodeport-transition-5nk57\naffinity-nodeport-transition-zdkm4\naffinity-nodeport-transition-5nk57\naffinity-nodeport-transition-5nk57\naffinity-nodeport-transition-5nk57\naffinity-nodeport-transition-zdkm4\naffinity-nodeport-transition-5nk57\naffinity-nodeport-transition-5nk57\naffinity-nodeport-transition-fp8gh"
Oct 26 17:29:17.008: INFO: Received response from host: affinity-nodeport-transition-fp8gh
Oct 26 17:29:17.008: INFO: Received response from host: affinity-nodeport-transition-zdkm4
Oct 26 17:29:17.009: INFO: Received response from host: affinity-nodeport-transition-zdkm4
Oct 26 17:29:17.009: INFO: Received response from host: affinity-nodeport-transition-fp8gh
Oct 26 17:29:17.009: INFO: Received response from host: affinity-nodeport-transition-fp8gh
Oct 26 17:29:17.009: INFO: Received response from host: affinity-nodeport-transition-fp8gh
Oct 26 17:29:17.009: INFO: Received response from host: affinity-nodeport-transition-5nk57
Oct 26 17:29:17.009: INFO: Received response from host: affinity-nodeport-transition-5nk57
Oct 26 17:29:17.009: INFO: Received response from host: affinity-nodeport-transition-zdkm4
Oct 26 17:29:17.009: INFO: Received response from host: affinity-nodeport-transition-5nk57
Oct 26 17:29:17.009: INFO: Received response from host: affinity-nodeport-transition-5nk57
Oct 26 17:29:17.009: INFO: Received response from host: affinity-nodeport-transition-5nk57
Oct 26 17:29:17.009: INFO: Received response from host: affinity-nodeport-transition-zdkm4
Oct 26 17:29:17.009: INFO: Received response from host: affinity-nodeport-transition-5nk57
Oct 26 17:29:17.009: INFO: Received response from host: affinity-nodeport-transition-5nk57
Oct 26 17:29:17.009: INFO: Received response from host: affinity-nodeport-transition-fp8gh
Oct 26 17:29:17.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-8730 execpod-affinityg67qd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.112.67.201:30073/ ; done'
Oct 26 17:29:17.582: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30073/\n"
Oct 26 17:29:17.582: INFO: stdout: "\naffinity-nodeport-transition-fp8gh\naffinity-nodeport-transition-fp8gh\naffinity-nodeport-transition-fp8gh\naffinity-nodeport-transition-fp8gh\naffinity-nodeport-transition-fp8gh\naffinity-nodeport-transition-fp8gh\naffinity-nodeport-transition-fp8gh\naffinity-nodeport-transition-fp8gh\naffinity-nodeport-transition-fp8gh\naffinity-nodeport-transition-fp8gh\naffinity-nodeport-transition-fp8gh\naffinity-nodeport-transition-fp8gh\naffinity-nodeport-transition-fp8gh\naffinity-nodeport-transition-fp8gh\naffinity-nodeport-transition-fp8gh\naffinity-nodeport-transition-fp8gh"
Oct 26 17:29:17.582: INFO: Received response from host: affinity-nodeport-transition-fp8gh
Oct 26 17:29:17.582: INFO: Received response from host: affinity-nodeport-transition-fp8gh
Oct 26 17:29:17.582: INFO: Received response from host: affinity-nodeport-transition-fp8gh
Oct 26 17:29:17.582: INFO: Received response from host: affinity-nodeport-transition-fp8gh
Oct 26 17:29:17.582: INFO: Received response from host: affinity-nodeport-transition-fp8gh
Oct 26 17:29:17.582: INFO: Received response from host: affinity-nodeport-transition-fp8gh
Oct 26 17:29:17.582: INFO: Received response from host: affinity-nodeport-transition-fp8gh
Oct 26 17:29:17.582: INFO: Received response from host: affinity-nodeport-transition-fp8gh
Oct 26 17:29:17.582: INFO: Received response from host: affinity-nodeport-transition-fp8gh
Oct 26 17:29:17.582: INFO: Received response from host: affinity-nodeport-transition-fp8gh
Oct 26 17:29:17.582: INFO: Received response from host: affinity-nodeport-transition-fp8gh
Oct 26 17:29:17.582: INFO: Received response from host: affinity-nodeport-transition-fp8gh
Oct 26 17:29:17.582: INFO: Received response from host: affinity-nodeport-transition-fp8gh
Oct 26 17:29:17.582: INFO: Received response from host: affinity-nodeport-transition-fp8gh
Oct 26 17:29:17.582: INFO: Received response from host: affinity-nodeport-transition-fp8gh
Oct 26 17:29:17.582: INFO: Received response from host: affinity-nodeport-transition-fp8gh
Oct 26 17:29:17.582: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-8730, will wait for the garbage collector to delete the pods
Oct 26 17:29:17.742: INFO: Deleting ReplicationController affinity-nodeport-transition took: 42.364232ms
Oct 26 17:29:17.842: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.28033ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:29:30.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8730" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:53.067 seconds]
[sig-network] Services
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":144,"skipped":2429,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:29:30.114: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3887
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with configMap that has name projected-configmap-test-upd-1d4db4a5-4403-40e5-819b-3404f61ce511
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-1d4db4a5-4403-40e5-819b-3404f61ce511
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:30:39.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3887" for this suite.

• [SLOW TEST:69.832 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":145,"skipped":2444,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:30:39.946: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4089
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-b0cc1179-8369-4a92-a022-be91c3c6fde7
STEP: Creating a pod to test consume secrets
Oct 26 17:30:40.222: INFO: Waiting up to 5m0s for pod "pod-secrets-cc6ea7cf-3b0c-4bb0-83da-119b95eef6c4" in namespace "secrets-4089" to be "Succeeded or Failed"
Oct 26 17:30:40.233: INFO: Pod "pod-secrets-cc6ea7cf-3b0c-4bb0-83da-119b95eef6c4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.397673ms
Oct 26 17:30:42.245: INFO: Pod "pod-secrets-cc6ea7cf-3b0c-4bb0-83da-119b95eef6c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023006274s
Oct 26 17:30:44.257: INFO: Pod "pod-secrets-cc6ea7cf-3b0c-4bb0-83da-119b95eef6c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035003689s
STEP: Saw pod success
Oct 26 17:30:44.257: INFO: Pod "pod-secrets-cc6ea7cf-3b0c-4bb0-83da-119b95eef6c4" satisfied condition "Succeeded or Failed"
Oct 26 17:30:44.267: INFO: Trying to get logs from node 10.112.67.207 pod pod-secrets-cc6ea7cf-3b0c-4bb0-83da-119b95eef6c4 container secret-volume-test: <nil>
STEP: delete the pod
Oct 26 17:30:44.399: INFO: Waiting for pod pod-secrets-cc6ea7cf-3b0c-4bb0-83da-119b95eef6c4 to disappear
Oct 26 17:30:44.412: INFO: Pod pod-secrets-cc6ea7cf-3b0c-4bb0-83da-119b95eef6c4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:30:44.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4089" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":146,"skipped":2450,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:30:44.469: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-6868
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Oct 26 17:30:44.804: INFO: PodSpec: initContainers in spec.initContainers
Oct 26 17:31:27.986: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-e886419f-e09a-4d7a-bd80-b2e3ac1f2f47", GenerateName:"", Namespace:"init-container-6868", SelfLink:"/api/v1/namespaces/init-container-6868/pods/pod-init-e886419f-e09a-4d7a-bd80-b2e3ac1f2f47", UID:"8827fa29-1939-44db-a3ee-f685d058d59f", ResourceVersion:"45049", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63739330244, loc:(*time.Location)(0x770e880)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"804313202"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"172.30.67.209/32", "cni.projectcalico.org/podIPs":"172.30.67.209/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc004a337a0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004a337c0)}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc004a337e0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004a33800)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc004a33820), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004a33840)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-gslwp", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc00202f4c0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-gslwp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-gslwp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-gslwp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002a9a7e8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.112.67.207", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002cfb5e0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002a9a870)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002a9a890)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002a9a898), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002a9a89c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc002ebdcd0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739330244, loc:(*time.Location)(0x770e880)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739330244, loc:(*time.Location)(0x770e880)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739330244, loc:(*time.Location)(0x770e880)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739330244, loc:(*time.Location)(0x770e880)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.112.67.207", PodIP:"172.30.67.209", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.67.209"}}, StartTime:(*v1.Time)(0xc004a33860), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002cfb6c0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002cfb730)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"containerd://9dc4eb8c40ffc97c541126fc5541c2b28853df9f206dad5a68b4592c6f04efa2", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004a338a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004a33880), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc002a9a934)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:31:27.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6868" for this suite.

• [SLOW TEST:43.566 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":305,"completed":147,"skipped":2528,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:31:28.036: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6381
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Oct 26 17:31:28.289: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f21eb7b1-7b87-4a04-9b13-9d02b4eae23a" in namespace "projected-6381" to be "Succeeded or Failed"
Oct 26 17:31:28.299: INFO: Pod "downwardapi-volume-f21eb7b1-7b87-4a04-9b13-9d02b4eae23a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.830252ms
Oct 26 17:31:30.324: INFO: Pod "downwardapi-volume-f21eb7b1-7b87-4a04-9b13-9d02b4eae23a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.035329539s
STEP: Saw pod success
Oct 26 17:31:30.325: INFO: Pod "downwardapi-volume-f21eb7b1-7b87-4a04-9b13-9d02b4eae23a" satisfied condition "Succeeded or Failed"
Oct 26 17:31:30.339: INFO: Trying to get logs from node 10.112.67.203 pod downwardapi-volume-f21eb7b1-7b87-4a04-9b13-9d02b4eae23a container client-container: <nil>
STEP: delete the pod
Oct 26 17:31:30.410: INFO: Waiting for pod downwardapi-volume-f21eb7b1-7b87-4a04-9b13-9d02b4eae23a to disappear
Oct 26 17:31:30.423: INFO: Pod downwardapi-volume-f21eb7b1-7b87-4a04-9b13-9d02b4eae23a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:31:30.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6381" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":305,"completed":148,"skipped":2532,"failed":0}
S
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:31:30.472: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2760
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 26 17:31:30.728: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Oct 26 17:31:35.739: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Oct 26 17:31:35.739: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Oct 26 17:31:35.824: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-2760 /apis/apps/v1/namespaces/deployment-2760/deployments/test-cleanup-deployment b201ea8e-d177-428f-8465-b1dfa6a37486 45128 1 2020-10-26 17:31:35 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2020-10-26 17:31:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002d41ac8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Oct 26 17:31:35.840: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:31:35.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2760" for this suite.

• [SLOW TEST:5.441 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":305,"completed":149,"skipped":2533,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:31:35.913: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-7335
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Oct 26 17:31:36.224: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:31:36.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7335" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":305,"completed":150,"skipped":2546,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:31:36.355: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-3481
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 26 17:31:36.602: INFO: Waiting up to 5m0s for pod "busybox-user-65534-1838754c-7c6b-4a04-93d8-e5e5b3d0091e" in namespace "security-context-test-3481" to be "Succeeded or Failed"
Oct 26 17:31:36.613: INFO: Pod "busybox-user-65534-1838754c-7c6b-4a04-93d8-e5e5b3d0091e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.981096ms
Oct 26 17:31:38.628: INFO: Pod "busybox-user-65534-1838754c-7c6b-4a04-93d8-e5e5b3d0091e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026387766s
Oct 26 17:31:40.640: INFO: Pod "busybox-user-65534-1838754c-7c6b-4a04-93d8-e5e5b3d0091e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038506739s
Oct 26 17:31:40.640: INFO: Pod "busybox-user-65534-1838754c-7c6b-4a04-93d8-e5e5b3d0091e" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:31:40.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3481" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":151,"skipped":2560,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:31:40.688: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-4175
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 26 17:31:40.914: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Creating first CR 
Oct 26 17:31:42.242: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-10-26T17:31:42Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-10-26T17:31:42Z]] name:name1 resourceVersion:45260 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:663515ff-596a-4c9e-b2d9-2948906ecf9c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Oct 26 17:31:52.290: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-10-26T17:31:52Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-10-26T17:31:52Z]] name:name2 resourceVersion:45303 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:dd8b29d4-d379-494b-8eba-13a4049333ad] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Oct 26 17:32:02.315: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-10-26T17:31:42Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-10-26T17:32:02Z]] name:name1 resourceVersion:45328 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:663515ff-596a-4c9e-b2d9-2948906ecf9c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Oct 26 17:32:12.338: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-10-26T17:31:52Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-10-26T17:32:12Z]] name:name2 resourceVersion:45357 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:dd8b29d4-d379-494b-8eba-13a4049333ad] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Oct 26 17:32:22.379: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-10-26T17:31:42Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-10-26T17:32:02Z]] name:name1 resourceVersion:45385 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:663515ff-596a-4c9e-b2d9-2948906ecf9c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Oct 26 17:32:32.417: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-10-26T17:31:52Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-10-26T17:32:12Z]] name:name2 resourceVersion:45410 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:dd8b29d4-d379-494b-8eba-13a4049333ad] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:32:42.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-4175" for this suite.

• [SLOW TEST:62.320 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":305,"completed":152,"skipped":2570,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Ingress API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:32:43.008: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename ingress
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingress-4398
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Oct 26 17:32:43.338: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Oct 26 17:32:43.353: INFO: starting watch
STEP: patching
STEP: updating
Oct 26 17:32:43.396: INFO: waiting for watch events with expected annotations
Oct 26 17:32:43.396: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:32:43.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-4398" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":305,"completed":153,"skipped":2607,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:32:43.618: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-5992
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:32:44.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5992" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":305,"completed":154,"skipped":2628,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:32:44.086: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7370
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-21353902-7770-42f8-bba6-da86898e7ee0
STEP: Creating secret with name s-test-opt-upd-34df966e-ea07-462b-8d38-3490e3b74026
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-21353902-7770-42f8-bba6-da86898e7ee0
STEP: Updating secret s-test-opt-upd-34df966e-ea07-462b-8d38-3490e3b74026
STEP: Creating secret with name s-test-opt-create-14946a8b-0dd7-4787-b79d-d28513312919
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:34:12.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7370" for this suite.

• [SLOW TEST:88.809 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":155,"skipped":2642,"failed":0}
SS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] version v1
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:34:12.896: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-9265
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-cq6tg in namespace proxy-9265
I1026 17:34:13.205820      27 runners.go:190] Created replication controller with name: proxy-service-cq6tg, namespace: proxy-9265, replica count: 1
I1026 17:34:14.256332      27 runners.go:190] proxy-service-cq6tg Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1026 17:34:15.256625      27 runners.go:190] proxy-service-cq6tg Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1026 17:34:16.256909      27 runners.go:190] proxy-service-cq6tg Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1026 17:34:17.257283      27 runners.go:190] proxy-service-cq6tg Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1026 17:34:18.257541      27 runners.go:190] proxy-service-cq6tg Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1026 17:34:19.257674      27 runners.go:190] proxy-service-cq6tg Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1026 17:34:20.257904      27 runners.go:190] proxy-service-cq6tg Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1026 17:34:21.258268      27 runners.go:190] proxy-service-cq6tg Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1026 17:34:22.258597      27 runners.go:190] proxy-service-cq6tg Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1026 17:34:23.259049      27 runners.go:190] proxy-service-cq6tg Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1026 17:34:24.259599      27 runners.go:190] proxy-service-cq6tg Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1026 17:34:25.260007      27 runners.go:190] proxy-service-cq6tg Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 26 17:34:25.273: INFO: setup took 12.133399173s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Oct 26 17:34:25.310: INFO: (0) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:1080/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:1080/proxy/rewriteme">test<... (200; 36.380994ms)
Oct 26 17:34:25.310: INFO: (0) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:160/proxy/: foo (200; 36.758681ms)
Oct 26 17:34:25.310: INFO: (0) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:162/proxy/: bar (200; 36.671742ms)
Oct 26 17:34:25.324: INFO: (0) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:160/proxy/: foo (200; 51.17762ms)
Oct 26 17:34:25.326: INFO: (0) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:162/proxy/: bar (200; 52.150177ms)
Oct 26 17:34:25.326: INFO: (0) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:1080/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:1080/proxy/rewriteme">... (200; 52.802512ms)
Oct 26 17:34:25.327: INFO: (0) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds/proxy/rewriteme">test</a> (200; 53.710576ms)
Oct 26 17:34:25.327: INFO: (0) /api/v1/namespaces/proxy-9265/services/proxy-service-cq6tg:portname2/proxy/: bar (200; 53.982079ms)
Oct 26 17:34:25.330: INFO: (0) /api/v1/namespaces/proxy-9265/services/http:proxy-service-cq6tg:portname2/proxy/: bar (200; 56.122732ms)
Oct 26 17:34:25.330: INFO: (0) /api/v1/namespaces/proxy-9265/services/http:proxy-service-cq6tg:portname1/proxy/: foo (200; 56.822368ms)
Oct 26 17:34:25.331: INFO: (0) /api/v1/namespaces/proxy-9265/services/proxy-service-cq6tg:portname1/proxy/: foo (200; 57.024446ms)
Oct 26 17:34:25.337: INFO: (0) /api/v1/namespaces/proxy-9265/services/https:proxy-service-cq6tg:tlsportname2/proxy/: tls qux (200; 64.194261ms)
Oct 26 17:34:25.338: INFO: (0) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:462/proxy/: tls qux (200; 65.184802ms)
Oct 26 17:34:25.338: INFO: (0) /api/v1/namespaces/proxy-9265/services/https:proxy-service-cq6tg:tlsportname1/proxy/: tls baz (200; 64.307214ms)
Oct 26 17:34:25.338: INFO: (0) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:460/proxy/: tls baz (200; 64.316434ms)
Oct 26 17:34:25.341: INFO: (0) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:443/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:443/proxy/tlsrewritem... (200; 68.016161ms)
Oct 26 17:34:25.363: INFO: (1) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:460/proxy/: tls baz (200; 21.187211ms)
Oct 26 17:34:25.364: INFO: (1) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:1080/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:1080/proxy/rewriteme">test<... (200; 22.78569ms)
Oct 26 17:34:25.365: INFO: (1) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:1080/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:1080/proxy/rewriteme">... (200; 23.154281ms)
Oct 26 17:34:25.366: INFO: (1) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds/proxy/rewriteme">test</a> (200; 24.488411ms)
Oct 26 17:34:25.367: INFO: (1) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:462/proxy/: tls qux (200; 25.292976ms)
Oct 26 17:34:25.367: INFO: (1) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:162/proxy/: bar (200; 25.422816ms)
Oct 26 17:34:25.367: INFO: (1) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:160/proxy/: foo (200; 25.720699ms)
Oct 26 17:34:25.367: INFO: (1) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:160/proxy/: foo (200; 25.734974ms)
Oct 26 17:34:25.368: INFO: (1) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:443/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:443/proxy/tlsrewritem... (200; 26.019561ms)
Oct 26 17:34:25.368: INFO: (1) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:162/proxy/: bar (200; 26.78552ms)
Oct 26 17:34:25.376: INFO: (1) /api/v1/namespaces/proxy-9265/services/proxy-service-cq6tg:portname1/proxy/: foo (200; 35.190782ms)
Oct 26 17:34:25.379: INFO: (1) /api/v1/namespaces/proxy-9265/services/http:proxy-service-cq6tg:portname2/proxy/: bar (200; 37.580167ms)
Oct 26 17:34:25.385: INFO: (1) /api/v1/namespaces/proxy-9265/services/http:proxy-service-cq6tg:portname1/proxy/: foo (200; 44.117141ms)
Oct 26 17:34:25.386: INFO: (1) /api/v1/namespaces/proxy-9265/services/https:proxy-service-cq6tg:tlsportname1/proxy/: tls baz (200; 44.035059ms)
Oct 26 17:34:25.386: INFO: (1) /api/v1/namespaces/proxy-9265/services/https:proxy-service-cq6tg:tlsportname2/proxy/: tls qux (200; 44.251128ms)
Oct 26 17:34:25.386: INFO: (1) /api/v1/namespaces/proxy-9265/services/proxy-service-cq6tg:portname2/proxy/: bar (200; 44.627477ms)
Oct 26 17:34:25.408: INFO: (2) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:160/proxy/: foo (200; 21.084679ms)
Oct 26 17:34:25.412: INFO: (2) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:443/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:443/proxy/tlsrewritem... (200; 25.317536ms)
Oct 26 17:34:25.412: INFO: (2) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:162/proxy/: bar (200; 24.24594ms)
Oct 26 17:34:25.413: INFO: (2) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds/proxy/rewriteme">test</a> (200; 24.957938ms)
Oct 26 17:34:25.428: INFO: (2) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:1080/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:1080/proxy/rewriteme">test<... (200; 39.608408ms)
Oct 26 17:34:25.428: INFO: (2) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:462/proxy/: tls qux (200; 41.288622ms)
Oct 26 17:34:25.428: INFO: (2) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:162/proxy/: bar (200; 39.60978ms)
Oct 26 17:34:25.428: INFO: (2) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:1080/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:1080/proxy/rewriteme">... (200; 40.555328ms)
Oct 26 17:34:25.428: INFO: (2) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:460/proxy/: tls baz (200; 39.630897ms)
Oct 26 17:34:25.428: INFO: (2) /api/v1/namespaces/proxy-9265/services/http:proxy-service-cq6tg:portname1/proxy/: foo (200; 40.441636ms)
Oct 26 17:34:25.428: INFO: (2) /api/v1/namespaces/proxy-9265/services/proxy-service-cq6tg:portname1/proxy/: foo (200; 41.165128ms)
Oct 26 17:34:25.428: INFO: (2) /api/v1/namespaces/proxy-9265/services/http:proxy-service-cq6tg:portname2/proxy/: bar (200; 40.992044ms)
Oct 26 17:34:25.428: INFO: (2) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:160/proxy/: foo (200; 40.083882ms)
Oct 26 17:34:25.429: INFO: (2) /api/v1/namespaces/proxy-9265/services/https:proxy-service-cq6tg:tlsportname1/proxy/: tls baz (200; 41.919243ms)
Oct 26 17:34:25.429: INFO: (2) /api/v1/namespaces/proxy-9265/services/proxy-service-cq6tg:portname2/proxy/: bar (200; 41.449492ms)
Oct 26 17:34:25.429: INFO: (2) /api/v1/namespaces/proxy-9265/services/https:proxy-service-cq6tg:tlsportname2/proxy/: tls qux (200; 41.350912ms)
Oct 26 17:34:25.457: INFO: (3) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:160/proxy/: foo (200; 27.701626ms)
Oct 26 17:34:25.457: INFO: (3) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:160/proxy/: foo (200; 27.701908ms)
Oct 26 17:34:25.457: INFO: (3) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:1080/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:1080/proxy/rewriteme">... (200; 28.167561ms)
Oct 26 17:34:25.458: INFO: (3) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:443/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:443/proxy/tlsrewritem... (200; 28.756708ms)
Oct 26 17:34:25.458: INFO: (3) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:460/proxy/: tls baz (200; 28.582129ms)
Oct 26 17:34:25.465: INFO: (3) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:462/proxy/: tls qux (200; 35.771044ms)
Oct 26 17:34:25.466: INFO: (3) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:162/proxy/: bar (200; 36.86365ms)
Oct 26 17:34:25.466: INFO: (3) /api/v1/namespaces/proxy-9265/services/http:proxy-service-cq6tg:portname1/proxy/: foo (200; 36.879721ms)
Oct 26 17:34:25.467: INFO: (3) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:162/proxy/: bar (200; 37.585008ms)
Oct 26 17:34:25.467: INFO: (3) /api/v1/namespaces/proxy-9265/services/proxy-service-cq6tg:portname1/proxy/: foo (200; 37.816978ms)
Oct 26 17:34:25.468: INFO: (3) /api/v1/namespaces/proxy-9265/services/https:proxy-service-cq6tg:tlsportname2/proxy/: tls qux (200; 38.903264ms)
Oct 26 17:34:25.468: INFO: (3) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds/proxy/rewriteme">test</a> (200; 39.211531ms)
Oct 26 17:34:25.469: INFO: (3) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:1080/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:1080/proxy/rewriteme">test<... (200; 39.286259ms)
Oct 26 17:34:25.476: INFO: (3) /api/v1/namespaces/proxy-9265/services/https:proxy-service-cq6tg:tlsportname1/proxy/: tls baz (200; 45.954836ms)
Oct 26 17:34:25.476: INFO: (3) /api/v1/namespaces/proxy-9265/services/http:proxy-service-cq6tg:portname2/proxy/: bar (200; 46.306822ms)
Oct 26 17:34:25.476: INFO: (3) /api/v1/namespaces/proxy-9265/services/proxy-service-cq6tg:portname2/proxy/: bar (200; 46.720315ms)
Oct 26 17:34:25.496: INFO: (4) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:162/proxy/: bar (200; 19.363868ms)
Oct 26 17:34:25.688: INFO: (4) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:160/proxy/: foo (200; 211.279156ms)
Oct 26 17:34:25.688: INFO: (4) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds/proxy/rewriteme">test</a> (200; 211.352035ms)
Oct 26 17:34:25.688: INFO: (4) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:443/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:443/proxy/tlsrewritem... (200; 211.385572ms)
Oct 26 17:34:25.688: INFO: (4) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:462/proxy/: tls qux (200; 211.074314ms)
Oct 26 17:34:25.688: INFO: (4) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:162/proxy/: bar (200; 211.207142ms)
Oct 26 17:34:25.688: INFO: (4) /api/v1/namespaces/proxy-9265/services/https:proxy-service-cq6tg:tlsportname1/proxy/: tls baz (200; 211.811109ms)
Oct 26 17:34:25.688: INFO: (4) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:160/proxy/: foo (200; 211.607305ms)
Oct 26 17:34:25.688: INFO: (4) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:1080/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:1080/proxy/rewriteme">... (200; 211.709762ms)
Oct 26 17:34:25.688: INFO: (4) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:460/proxy/: tls baz (200; 211.400358ms)
Oct 26 17:34:25.689: INFO: (4) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:1080/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:1080/proxy/rewriteme">test<... (200; 211.643944ms)
Oct 26 17:34:25.689: INFO: (4) /api/v1/namespaces/proxy-9265/services/https:proxy-service-cq6tg:tlsportname2/proxy/: tls qux (200; 211.988677ms)
Oct 26 17:34:25.689: INFO: (4) /api/v1/namespaces/proxy-9265/services/proxy-service-cq6tg:portname2/proxy/: bar (200; 212.168931ms)
Oct 26 17:34:25.697: INFO: (4) /api/v1/namespaces/proxy-9265/services/http:proxy-service-cq6tg:portname2/proxy/: bar (200; 219.886745ms)
Oct 26 17:34:25.700: INFO: (4) /api/v1/namespaces/proxy-9265/services/proxy-service-cq6tg:portname1/proxy/: foo (200; 222.915888ms)
Oct 26 17:34:25.704: INFO: (4) /api/v1/namespaces/proxy-9265/services/http:proxy-service-cq6tg:portname1/proxy/: foo (200; 226.821493ms)
Oct 26 17:34:25.722: INFO: (5) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:460/proxy/: tls baz (200; 18.306872ms)
Oct 26 17:34:25.727: INFO: (5) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds/proxy/rewriteme">test</a> (200; 23.316515ms)
Oct 26 17:34:25.727: INFO: (5) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:162/proxy/: bar (200; 23.199318ms)
Oct 26 17:34:25.727: INFO: (5) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:443/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:443/proxy/tlsrewritem... (200; 23.614401ms)
Oct 26 17:34:25.727: INFO: (5) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:1080/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:1080/proxy/rewriteme">test<... (200; 23.049498ms)
Oct 26 17:34:25.728: INFO: (5) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:462/proxy/: tls qux (200; 23.793329ms)
Oct 26 17:34:25.728: INFO: (5) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:162/proxy/: bar (200; 23.189231ms)
Oct 26 17:34:25.728: INFO: (5) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:160/proxy/: foo (200; 23.706604ms)
Oct 26 17:34:25.728: INFO: (5) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:1080/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:1080/proxy/rewriteme">... (200; 23.492063ms)
Oct 26 17:34:25.728: INFO: (5) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:160/proxy/: foo (200; 23.727849ms)
Oct 26 17:34:25.838: INFO: (5) /api/v1/namespaces/proxy-9265/services/https:proxy-service-cq6tg:tlsportname1/proxy/: tls baz (200; 134.578962ms)
Oct 26 17:34:25.838: INFO: (5) /api/v1/namespaces/proxy-9265/services/http:proxy-service-cq6tg:portname1/proxy/: foo (200; 134.129486ms)
Oct 26 17:34:25.838: INFO: (5) /api/v1/namespaces/proxy-9265/services/proxy-service-cq6tg:portname1/proxy/: foo (200; 134.291278ms)
Oct 26 17:34:25.839: INFO: (5) /api/v1/namespaces/proxy-9265/services/http:proxy-service-cq6tg:portname2/proxy/: bar (200; 134.370039ms)
Oct 26 17:34:25.838: INFO: (5) /api/v1/namespaces/proxy-9265/services/proxy-service-cq6tg:portname2/proxy/: bar (200; 133.784789ms)
Oct 26 17:34:25.839: INFO: (5) /api/v1/namespaces/proxy-9265/services/https:proxy-service-cq6tg:tlsportname2/proxy/: tls qux (200; 134.386403ms)
Oct 26 17:34:25.858: INFO: (6) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:160/proxy/: foo (200; 19.02549ms)
Oct 26 17:34:25.859: INFO: (6) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds/proxy/rewriteme">test</a> (200; 19.860973ms)
Oct 26 17:34:25.860: INFO: (6) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:1080/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:1080/proxy/rewriteme">test<... (200; 20.801108ms)
Oct 26 17:34:25.861: INFO: (6) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:443/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:443/proxy/tlsrewritem... (200; 21.586711ms)
Oct 26 17:34:25.861: INFO: (6) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:162/proxy/: bar (200; 21.988499ms)
Oct 26 17:34:25.862: INFO: (6) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:460/proxy/: tls baz (200; 23.032496ms)
Oct 26 17:34:25.862: INFO: (6) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:462/proxy/: tls qux (200; 22.991903ms)
Oct 26 17:34:25.862: INFO: (6) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:160/proxy/: foo (200; 22.605966ms)
Oct 26 17:34:25.862: INFO: (6) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:162/proxy/: bar (200; 22.768847ms)
Oct 26 17:34:25.865: INFO: (6) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:1080/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:1080/proxy/rewriteme">... (200; 25.347652ms)
Oct 26 17:34:25.866: INFO: (6) /api/v1/namespaces/proxy-9265/services/https:proxy-service-cq6tg:tlsportname2/proxy/: tls qux (200; 26.891447ms)
Oct 26 17:34:25.878: INFO: (6) /api/v1/namespaces/proxy-9265/services/proxy-service-cq6tg:portname2/proxy/: bar (200; 38.48103ms)
Oct 26 17:34:25.880: INFO: (6) /api/v1/namespaces/proxy-9265/services/http:proxy-service-cq6tg:portname1/proxy/: foo (200; 41.035246ms)
Oct 26 17:34:25.882: INFO: (6) /api/v1/namespaces/proxy-9265/services/proxy-service-cq6tg:portname1/proxy/: foo (200; 42.721793ms)
Oct 26 17:34:25.889: INFO: (6) /api/v1/namespaces/proxy-9265/services/http:proxy-service-cq6tg:portname2/proxy/: bar (200; 49.892488ms)
Oct 26 17:34:25.889: INFO: (6) /api/v1/namespaces/proxy-9265/services/https:proxy-service-cq6tg:tlsportname1/proxy/: tls baz (200; 49.679152ms)
Oct 26 17:34:25.907: INFO: (7) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:1080/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:1080/proxy/rewriteme">... (200; 17.065708ms)
Oct 26 17:34:25.916: INFO: (7) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:1080/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:1080/proxy/rewriteme">test<... (200; 26.277876ms)
Oct 26 17:34:25.917: INFO: (7) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:162/proxy/: bar (200; 27.126976ms)
Oct 26 17:34:25.917: INFO: (7) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:160/proxy/: foo (200; 27.501742ms)
Oct 26 17:34:25.917: INFO: (7) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds/proxy/rewriteme">test</a> (200; 27.433717ms)
Oct 26 17:34:25.917: INFO: (7) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:443/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:443/proxy/tlsrewritem... (200; 27.816906ms)
Oct 26 17:34:25.917: INFO: (7) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:460/proxy/: tls baz (200; 27.012131ms)
Oct 26 17:34:25.917: INFO: (7) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:462/proxy/: tls qux (200; 27.610801ms)
Oct 26 17:34:25.917: INFO: (7) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:162/proxy/: bar (200; 27.317602ms)
Oct 26 17:34:25.917: INFO: (7) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:160/proxy/: foo (200; 27.737054ms)
Oct 26 17:34:25.922: INFO: (7) /api/v1/namespaces/proxy-9265/services/https:proxy-service-cq6tg:tlsportname2/proxy/: tls qux (200; 32.121452ms)
Oct 26 17:34:25.925: INFO: (7) /api/v1/namespaces/proxy-9265/services/http:proxy-service-cq6tg:portname1/proxy/: foo (200; 35.657761ms)
Oct 26 17:34:25.925: INFO: (7) /api/v1/namespaces/proxy-9265/services/proxy-service-cq6tg:portname2/proxy/: bar (200; 35.620915ms)
Oct 26 17:34:25.927: INFO: (7) /api/v1/namespaces/proxy-9265/services/proxy-service-cq6tg:portname1/proxy/: foo (200; 36.73828ms)
Oct 26 17:34:25.927: INFO: (7) /api/v1/namespaces/proxy-9265/services/http:proxy-service-cq6tg:portname2/proxy/: bar (200; 36.749173ms)
Oct 26 17:34:25.927: INFO: (7) /api/v1/namespaces/proxy-9265/services/https:proxy-service-cq6tg:tlsportname1/proxy/: tls baz (200; 36.635562ms)
Oct 26 17:34:25.950: INFO: (8) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:1080/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:1080/proxy/rewriteme">test<... (200; 21.949024ms)
Oct 26 17:34:25.952: INFO: (8) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:443/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:443/proxy/tlsrewritem... (200; 24.830564ms)
Oct 26 17:34:25.952: INFO: (8) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:160/proxy/: foo (200; 24.930697ms)
Oct 26 17:34:25.953: INFO: (8) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:1080/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:1080/proxy/rewriteme">... (200; 25.260276ms)
Oct 26 17:34:25.953: INFO: (8) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:160/proxy/: foo (200; 25.658331ms)
Oct 26 17:34:25.954: INFO: (8) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds/proxy/rewriteme">test</a> (200; 26.171965ms)
Oct 26 17:34:25.963: INFO: (8) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:162/proxy/: bar (200; 35.617859ms)
Oct 26 17:34:25.963: INFO: (8) /api/v1/namespaces/proxy-9265/services/http:proxy-service-cq6tg:portname2/proxy/: bar (200; 35.881074ms)
Oct 26 17:34:25.963: INFO: (8) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:162/proxy/: bar (200; 35.643838ms)
Oct 26 17:34:25.963: INFO: (8) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:460/proxy/: tls baz (200; 35.621747ms)
Oct 26 17:34:25.963: INFO: (8) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:462/proxy/: tls qux (200; 35.459246ms)
Oct 26 17:34:25.966: INFO: (8) /api/v1/namespaces/proxy-9265/services/https:proxy-service-cq6tg:tlsportname2/proxy/: tls qux (200; 38.225778ms)
Oct 26 17:34:25.969: INFO: (8) /api/v1/namespaces/proxy-9265/services/proxy-service-cq6tg:portname1/proxy/: foo (200; 41.571568ms)
Oct 26 17:34:25.969: INFO: (8) /api/v1/namespaces/proxy-9265/services/https:proxy-service-cq6tg:tlsportname1/proxy/: tls baz (200; 41.657733ms)
Oct 26 17:34:25.973: INFO: (8) /api/v1/namespaces/proxy-9265/services/http:proxy-service-cq6tg:portname1/proxy/: foo (200; 46.063403ms)
Oct 26 17:34:25.973: INFO: (8) /api/v1/namespaces/proxy-9265/services/proxy-service-cq6tg:portname2/proxy/: bar (200; 45.891386ms)
Oct 26 17:34:25.992: INFO: (9) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds/proxy/rewriteme">test</a> (200; 18.220048ms)
Oct 26 17:34:25.997: INFO: (9) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:1080/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:1080/proxy/rewriteme">test<... (200; 23.298012ms)
Oct 26 17:34:25.997: INFO: (9) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:162/proxy/: bar (200; 23.148953ms)
Oct 26 17:34:25.997: INFO: (9) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:443/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:443/proxy/tlsrewritem... (200; 23.349497ms)
Oct 26 17:34:26.000: INFO: (9) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:1080/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:1080/proxy/rewriteme">... (200; 25.862588ms)
Oct 26 17:34:26.000: INFO: (9) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:460/proxy/: tls baz (200; 25.798486ms)
Oct 26 17:34:26.000: INFO: (9) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:160/proxy/: foo (200; 25.573751ms)
Oct 26 17:34:26.000: INFO: (9) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:462/proxy/: tls qux (200; 25.79302ms)
Oct 26 17:34:26.000: INFO: (9) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:160/proxy/: foo (200; 26.01966ms)
Oct 26 17:34:26.000: INFO: (9) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:162/proxy/: bar (200; 25.855804ms)
Oct 26 17:34:26.006: INFO: (9) /api/v1/namespaces/proxy-9265/services/https:proxy-service-cq6tg:tlsportname2/proxy/: tls qux (200; 31.456588ms)
Oct 26 17:34:26.006: INFO: (9) /api/v1/namespaces/proxy-9265/services/https:proxy-service-cq6tg:tlsportname1/proxy/: tls baz (200; 32.191883ms)
Oct 26 17:34:26.008: INFO: (9) /api/v1/namespaces/proxy-9265/services/http:proxy-service-cq6tg:portname1/proxy/: foo (200; 33.994214ms)
Oct 26 17:34:26.010: INFO: (9) /api/v1/namespaces/proxy-9265/services/proxy-service-cq6tg:portname1/proxy/: foo (200; 35.841417ms)
Oct 26 17:34:26.010: INFO: (9) /api/v1/namespaces/proxy-9265/services/http:proxy-service-cq6tg:portname2/proxy/: bar (200; 36.173744ms)
Oct 26 17:34:26.019: INFO: (9) /api/v1/namespaces/proxy-9265/services/proxy-service-cq6tg:portname2/proxy/: bar (200; 45.451007ms)
Oct 26 17:34:26.047: INFO: (10) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:1080/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:1080/proxy/rewriteme">... (200; 27.63797ms)
Oct 26 17:34:26.048: INFO: (10) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:443/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:443/proxy/tlsrewritem... (200; 28.856578ms)
Oct 26 17:34:26.048: INFO: (10) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:462/proxy/: tls qux (200; 28.358304ms)
Oct 26 17:34:26.048: INFO: (10) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:160/proxy/: foo (200; 28.75204ms)
Oct 26 17:34:26.048: INFO: (10) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:162/proxy/: bar (200; 28.90555ms)
Oct 26 17:34:26.048: INFO: (10) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:162/proxy/: bar (200; 28.551224ms)
Oct 26 17:34:26.048: INFO: (10) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds/proxy/rewriteme">test</a> (200; 28.850266ms)
Oct 26 17:34:26.048: INFO: (10) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:160/proxy/: foo (200; 28.778023ms)
Oct 26 17:34:26.059: INFO: (10) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:1080/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:1080/proxy/rewriteme">test<... (200; 39.590392ms)
Oct 26 17:34:26.059: INFO: (10) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:460/proxy/: tls baz (200; 39.985859ms)
Oct 26 17:34:26.067: INFO: (10) /api/v1/namespaces/proxy-9265/services/https:proxy-service-cq6tg:tlsportname2/proxy/: tls qux (200; 46.845448ms)
Oct 26 17:34:26.067: INFO: (10) /api/v1/namespaces/proxy-9265/services/proxy-service-cq6tg:portname2/proxy/: bar (200; 47.382839ms)
Oct 26 17:34:26.067: INFO: (10) /api/v1/namespaces/proxy-9265/services/http:proxy-service-cq6tg:portname2/proxy/: bar (200; 47.525598ms)
Oct 26 17:34:26.067: INFO: (10) /api/v1/namespaces/proxy-9265/services/proxy-service-cq6tg:portname1/proxy/: foo (200; 47.626209ms)
Oct 26 17:34:26.068: INFO: (10) /api/v1/namespaces/proxy-9265/services/http:proxy-service-cq6tg:portname1/proxy/: foo (200; 48.075055ms)
Oct 26 17:34:26.068: INFO: (10) /api/v1/namespaces/proxy-9265/services/https:proxy-service-cq6tg:tlsportname1/proxy/: tls baz (200; 48.693051ms)
Oct 26 17:34:26.089: INFO: (11) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:1080/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:1080/proxy/rewriteme">test<... (200; 20.664293ms)
Oct 26 17:34:26.094: INFO: (11) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:443/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:443/proxy/tlsrewritem... (200; 25.40249ms)
Oct 26 17:34:26.095: INFO: (11) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:462/proxy/: tls qux (200; 26.283754ms)
Oct 26 17:34:26.097: INFO: (11) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:1080/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:1080/proxy/rewriteme">... (200; 27.944998ms)
Oct 26 17:34:26.097: INFO: (11) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:162/proxy/: bar (200; 28.440999ms)
Oct 26 17:34:26.097: INFO: (11) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:162/proxy/: bar (200; 29.024399ms)
Oct 26 17:34:26.097: INFO: (11) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:160/proxy/: foo (200; 28.668408ms)
Oct 26 17:34:26.098: INFO: (11) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:460/proxy/: tls baz (200; 29.644076ms)
Oct 26 17:34:26.102: INFO: (11) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:160/proxy/: foo (200; 33.303081ms)
Oct 26 17:34:26.102: INFO: (11) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds/proxy/rewriteme">test</a> (200; 33.058778ms)
Oct 26 17:34:26.125: INFO: (11) /api/v1/namespaces/proxy-9265/services/http:proxy-service-cq6tg:portname1/proxy/: foo (200; 56.63706ms)
Oct 26 17:34:26.125: INFO: (11) /api/v1/namespaces/proxy-9265/services/proxy-service-cq6tg:portname2/proxy/: bar (200; 56.617802ms)
Oct 26 17:34:26.125: INFO: (11) /api/v1/namespaces/proxy-9265/services/https:proxy-service-cq6tg:tlsportname1/proxy/: tls baz (200; 56.607579ms)
Oct 26 17:34:26.125: INFO: (11) /api/v1/namespaces/proxy-9265/services/https:proxy-service-cq6tg:tlsportname2/proxy/: tls qux (200; 56.814492ms)
Oct 26 17:34:26.125: INFO: (11) /api/v1/namespaces/proxy-9265/services/proxy-service-cq6tg:portname1/proxy/: foo (200; 56.715255ms)
Oct 26 17:34:26.125: INFO: (11) /api/v1/namespaces/proxy-9265/services/http:proxy-service-cq6tg:portname2/proxy/: bar (200; 56.691851ms)
Oct 26 17:34:26.151: INFO: (12) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:1080/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:1080/proxy/rewriteme">... (200; 24.749565ms)
Oct 26 17:34:26.157: INFO: (12) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:460/proxy/: tls baz (200; 30.405996ms)
Oct 26 17:34:26.157: INFO: (12) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:443/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:443/proxy/tlsrewritem... (200; 30.665414ms)
Oct 26 17:34:26.157: INFO: (12) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:162/proxy/: bar (200; 30.545014ms)
Oct 26 17:34:26.157: INFO: (12) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds/proxy/rewriteme">test</a> (200; 30.635645ms)
Oct 26 17:34:26.157: INFO: (12) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:160/proxy/: foo (200; 30.672064ms)
Oct 26 17:34:26.157: INFO: (12) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:462/proxy/: tls qux (200; 30.65722ms)
Oct 26 17:34:26.157: INFO: (12) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:1080/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:1080/proxy/rewriteme">test<... (200; 31.063519ms)
Oct 26 17:34:26.157: INFO: (12) /api/v1/namespaces/proxy-9265/services/https:proxy-service-cq6tg:tlsportname2/proxy/: tls qux (200; 31.253649ms)
Oct 26 17:34:26.157: INFO: (12) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:160/proxy/: foo (200; 30.945123ms)
Oct 26 17:34:26.157: INFO: (12) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:162/proxy/: bar (200; 30.907306ms)
Oct 26 17:34:26.167: INFO: (12) /api/v1/namespaces/proxy-9265/services/https:proxy-service-cq6tg:tlsportname1/proxy/: tls baz (200; 41.13793ms)
Oct 26 17:34:26.169: INFO: (12) /api/v1/namespaces/proxy-9265/services/proxy-service-cq6tg:portname2/proxy/: bar (200; 42.853422ms)
Oct 26 17:34:26.169: INFO: (12) /api/v1/namespaces/proxy-9265/services/http:proxy-service-cq6tg:portname1/proxy/: foo (200; 43.167321ms)
Oct 26 17:34:26.169: INFO: (12) /api/v1/namespaces/proxy-9265/services/http:proxy-service-cq6tg:portname2/proxy/: bar (200; 43.099564ms)
Oct 26 17:34:26.169: INFO: (12) /api/v1/namespaces/proxy-9265/services/proxy-service-cq6tg:portname1/proxy/: foo (200; 42.948815ms)
Oct 26 17:34:26.205: INFO: (13) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:443/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:443/proxy/tlsrewritem... (200; 35.76953ms)
Oct 26 17:34:26.207: INFO: (13) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:1080/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:1080/proxy/rewriteme">test<... (200; 37.21027ms)
Oct 26 17:34:26.207: INFO: (13) /api/v1/namespaces/proxy-9265/services/https:proxy-service-cq6tg:tlsportname1/proxy/: tls baz (200; 37.012556ms)
Oct 26 17:34:26.207: INFO: (13) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:460/proxy/: tls baz (200; 37.546079ms)
Oct 26 17:34:26.207: INFO: (13) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:162/proxy/: bar (200; 37.496654ms)
Oct 26 17:34:26.207: INFO: (13) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:160/proxy/: foo (200; 37.499367ms)
Oct 26 17:34:26.212: INFO: (13) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:1080/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:1080/proxy/rewriteme">... (200; 41.95095ms)
Oct 26 17:34:26.212: INFO: (13) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:160/proxy/: foo (200; 42.454325ms)
Oct 26 17:34:26.214: INFO: (13) /api/v1/namespaces/proxy-9265/services/https:proxy-service-cq6tg:tlsportname2/proxy/: tls qux (200; 45.01087ms)
Oct 26 17:34:26.214: INFO: (13) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:162/proxy/: bar (200; 45.292845ms)
Oct 26 17:34:26.215: INFO: (13) /api/v1/namespaces/proxy-9265/services/proxy-service-cq6tg:portname1/proxy/: foo (200; 44.949195ms)
Oct 26 17:34:26.215: INFO: (13) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds/proxy/rewriteme">test</a> (200; 45.039188ms)
Oct 26 17:34:26.215: INFO: (13) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:462/proxy/: tls qux (200; 45.27241ms)
Oct 26 17:34:26.217: INFO: (13) /api/v1/namespaces/proxy-9265/services/proxy-service-cq6tg:portname2/proxy/: bar (200; 47.022815ms)
Oct 26 17:34:26.217: INFO: (13) /api/v1/namespaces/proxy-9265/services/http:proxy-service-cq6tg:portname2/proxy/: bar (200; 47.172319ms)
Oct 26 17:34:26.219: INFO: (13) /api/v1/namespaces/proxy-9265/services/http:proxy-service-cq6tg:portname1/proxy/: foo (200; 49.456348ms)
Oct 26 17:34:26.238: INFO: (14) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:160/proxy/: foo (200; 18.398667ms)
Oct 26 17:34:26.241: INFO: (14) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:1080/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:1080/proxy/rewriteme">test<... (200; 21.913159ms)
Oct 26 17:34:26.243: INFO: (14) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:443/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:443/proxy/tlsrewritem... (200; 23.098281ms)
Oct 26 17:34:26.247: INFO: (14) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:162/proxy/: bar (200; 28.107607ms)
Oct 26 17:34:26.248: INFO: (14) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:460/proxy/: tls baz (200; 28.088232ms)
Oct 26 17:34:26.248: INFO: (14) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:160/proxy/: foo (200; 28.037904ms)
Oct 26 17:34:26.248: INFO: (14) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:162/proxy/: bar (200; 28.136036ms)
Oct 26 17:34:26.248: INFO: (14) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds/proxy/rewriteme">test</a> (200; 28.120913ms)
Oct 26 17:34:26.249: INFO: (14) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:462/proxy/: tls qux (200; 29.699272ms)
Oct 26 17:34:26.250: INFO: (14) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:1080/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:1080/proxy/rewriteme">... (200; 30.290497ms)
Oct 26 17:34:26.253: INFO: (14) /api/v1/namespaces/proxy-9265/services/proxy-service-cq6tg:portname1/proxy/: foo (200; 33.711393ms)
Oct 26 17:34:26.258: INFO: (14) /api/v1/namespaces/proxy-9265/services/https:proxy-service-cq6tg:tlsportname1/proxy/: tls baz (200; 39.209958ms)
Oct 26 17:34:26.259: INFO: (14) /api/v1/namespaces/proxy-9265/services/http:proxy-service-cq6tg:portname2/proxy/: bar (200; 40.205038ms)
Oct 26 17:34:26.260: INFO: (14) /api/v1/namespaces/proxy-9265/services/proxy-service-cq6tg:portname2/proxy/: bar (200; 40.342297ms)
Oct 26 17:34:26.267: INFO: (14) /api/v1/namespaces/proxy-9265/services/http:proxy-service-cq6tg:portname1/proxy/: foo (200; 47.020977ms)
Oct 26 17:34:26.267: INFO: (14) /api/v1/namespaces/proxy-9265/services/https:proxy-service-cq6tg:tlsportname2/proxy/: tls qux (200; 47.450054ms)
Oct 26 17:34:26.286: INFO: (15) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:443/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:443/proxy/tlsrewritem... (200; 19.126259ms)
Oct 26 17:34:26.287: INFO: (15) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:460/proxy/: tls baz (200; 19.407635ms)
Oct 26 17:34:26.287: INFO: (15) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:162/proxy/: bar (200; 19.700372ms)
Oct 26 17:34:26.288: INFO: (15) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:162/proxy/: bar (200; 20.381931ms)
Oct 26 17:34:26.288: INFO: (15) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:1080/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:1080/proxy/rewriteme">... (200; 20.751277ms)
Oct 26 17:34:26.288: INFO: (15) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:160/proxy/: foo (200; 21.254066ms)
Oct 26 17:34:26.289: INFO: (15) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:160/proxy/: foo (200; 21.242433ms)
Oct 26 17:34:26.289: INFO: (15) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:462/proxy/: tls qux (200; 21.421936ms)
Oct 26 17:34:26.291: INFO: (15) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds/proxy/rewriteme">test</a> (200; 23.23223ms)
Oct 26 17:34:26.291: INFO: (15) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:1080/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:1080/proxy/rewriteme">test<... (200; 23.950542ms)
Oct 26 17:34:26.298: INFO: (15) /api/v1/namespaces/proxy-9265/services/https:proxy-service-cq6tg:tlsportname1/proxy/: tls baz (200; 30.093989ms)
Oct 26 17:34:26.298: INFO: (15) /api/v1/namespaces/proxy-9265/services/https:proxy-service-cq6tg:tlsportname2/proxy/: tls qux (200; 30.203211ms)
Oct 26 17:34:26.301: INFO: (15) /api/v1/namespaces/proxy-9265/services/proxy-service-cq6tg:portname2/proxy/: bar (200; 34.024956ms)
Oct 26 17:34:26.302: INFO: (15) /api/v1/namespaces/proxy-9265/services/proxy-service-cq6tg:portname1/proxy/: foo (200; 34.059168ms)
Oct 26 17:34:26.302: INFO: (15) /api/v1/namespaces/proxy-9265/services/http:proxy-service-cq6tg:portname2/proxy/: bar (200; 34.570506ms)
Oct 26 17:34:26.303: INFO: (15) /api/v1/namespaces/proxy-9265/services/http:proxy-service-cq6tg:portname1/proxy/: foo (200; 35.466588ms)
Oct 26 17:34:26.324: INFO: (16) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:1080/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:1080/proxy/rewriteme">test<... (200; 20.785466ms)
Oct 26 17:34:26.324: INFO: (16) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:462/proxy/: tls qux (200; 21.084896ms)
Oct 26 17:34:26.326: INFO: (16) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:443/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:443/proxy/tlsrewritem... (200; 22.720568ms)
Oct 26 17:34:26.330: INFO: (16) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:162/proxy/: bar (200; 27.127057ms)
Oct 26 17:34:26.330: INFO: (16) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:160/proxy/: foo (200; 27.316258ms)
Oct 26 17:34:26.331: INFO: (16) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:162/proxy/: bar (200; 27.798339ms)
Oct 26 17:34:26.331: INFO: (16) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:460/proxy/: tls baz (200; 28.106466ms)
Oct 26 17:34:26.331: INFO: (16) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds/proxy/rewriteme">test</a> (200; 27.99277ms)
Oct 26 17:34:26.331: INFO: (16) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:1080/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:1080/proxy/rewriteme">... (200; 27.569808ms)
Oct 26 17:34:26.331: INFO: (16) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:160/proxy/: foo (200; 28.190107ms)
Oct 26 17:34:26.334: INFO: (16) /api/v1/namespaces/proxy-9265/services/proxy-service-cq6tg:portname1/proxy/: foo (200; 31.014945ms)
Oct 26 17:34:26.334: INFO: (16) /api/v1/namespaces/proxy-9265/services/proxy-service-cq6tg:portname2/proxy/: bar (200; 30.884877ms)
Oct 26 17:34:26.335: INFO: (16) /api/v1/namespaces/proxy-9265/services/http:proxy-service-cq6tg:portname1/proxy/: foo (200; 31.867981ms)
Oct 26 17:34:26.335: INFO: (16) /api/v1/namespaces/proxy-9265/services/https:proxy-service-cq6tg:tlsportname2/proxy/: tls qux (200; 32.078763ms)
Oct 26 17:34:26.335: INFO: (16) /api/v1/namespaces/proxy-9265/services/https:proxy-service-cq6tg:tlsportname1/proxy/: tls baz (200; 32.033449ms)
Oct 26 17:34:26.337: INFO: (16) /api/v1/namespaces/proxy-9265/services/http:proxy-service-cq6tg:portname2/proxy/: bar (200; 33.839307ms)
Oct 26 17:34:26.355: INFO: (17) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:160/proxy/: foo (200; 17.666541ms)
Oct 26 17:34:26.359: INFO: (17) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:162/proxy/: bar (200; 22.085292ms)
Oct 26 17:34:26.360: INFO: (17) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:1080/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:1080/proxy/rewriteme">test<... (200; 21.798243ms)
Oct 26 17:34:26.360: INFO: (17) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:443/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:443/proxy/tlsrewritem... (200; 22.855252ms)
Oct 26 17:34:26.361: INFO: (17) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:162/proxy/: bar (200; 23.39395ms)
Oct 26 17:34:26.363: INFO: (17) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:462/proxy/: tls qux (200; 25.530263ms)
Oct 26 17:34:26.364: INFO: (17) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:460/proxy/: tls baz (200; 25.925111ms)
Oct 26 17:34:26.365: INFO: (17) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:1080/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:1080/proxy/rewriteme">... (200; 27.490227ms)
Oct 26 17:34:26.367: INFO: (17) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds/proxy/rewriteme">test</a> (200; 29.02686ms)
Oct 26 17:34:26.367: INFO: (17) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:160/proxy/: foo (200; 29.123169ms)
Oct 26 17:34:26.369: INFO: (17) /api/v1/namespaces/proxy-9265/services/http:proxy-service-cq6tg:portname1/proxy/: foo (200; 32.165791ms)
Oct 26 17:34:26.370: INFO: (17) /api/v1/namespaces/proxy-9265/services/https:proxy-service-cq6tg:tlsportname2/proxy/: tls qux (200; 32.214795ms)
Oct 26 17:34:26.371: INFO: (17) /api/v1/namespaces/proxy-9265/services/http:proxy-service-cq6tg:portname2/proxy/: bar (200; 33.920285ms)
Oct 26 17:34:26.372: INFO: (17) /api/v1/namespaces/proxy-9265/services/proxy-service-cq6tg:portname2/proxy/: bar (200; 34.246268ms)
Oct 26 17:34:26.374: INFO: (17) /api/v1/namespaces/proxy-9265/services/proxy-service-cq6tg:portname1/proxy/: foo (200; 36.285297ms)
Oct 26 17:34:26.377: INFO: (17) /api/v1/namespaces/proxy-9265/services/https:proxy-service-cq6tg:tlsportname1/proxy/: tls baz (200; 39.274449ms)
Oct 26 17:34:26.398: INFO: (18) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:1080/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:1080/proxy/rewriteme">... (200; 20.844796ms)
Oct 26 17:34:26.417: INFO: (18) /api/v1/namespaces/proxy-9265/services/https:proxy-service-cq6tg:tlsportname1/proxy/: tls baz (200; 39.459657ms)
Oct 26 17:34:26.417: INFO: (18) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:162/proxy/: bar (200; 40.087893ms)
Oct 26 17:34:26.418: INFO: (18) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:1080/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:1080/proxy/rewriteme">test<... (200; 40.762126ms)
Oct 26 17:34:26.418: INFO: (18) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:160/proxy/: foo (200; 40.800387ms)
Oct 26 17:34:26.418: INFO: (18) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:460/proxy/: tls baz (200; 40.758919ms)
Oct 26 17:34:26.418: INFO: (18) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:443/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:443/proxy/tlsrewritem... (200; 41.290461ms)
Oct 26 17:34:26.418: INFO: (18) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:160/proxy/: foo (200; 40.746055ms)
Oct 26 17:34:26.418: INFO: (18) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:162/proxy/: bar (200; 41.05982ms)
Oct 26 17:34:26.420: INFO: (18) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:462/proxy/: tls qux (200; 42.722503ms)
Oct 26 17:34:26.420: INFO: (18) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds/proxy/rewriteme">test</a> (200; 42.661558ms)
Oct 26 17:34:26.420: INFO: (18) /api/v1/namespaces/proxy-9265/services/https:proxy-service-cq6tg:tlsportname2/proxy/: tls qux (200; 42.78569ms)
Oct 26 17:34:26.427: INFO: (18) /api/v1/namespaces/proxy-9265/services/http:proxy-service-cq6tg:portname2/proxy/: bar (200; 49.276104ms)
Oct 26 17:34:26.427: INFO: (18) /api/v1/namespaces/proxy-9265/services/proxy-service-cq6tg:portname2/proxy/: bar (200; 49.356354ms)
Oct 26 17:34:26.427: INFO: (18) /api/v1/namespaces/proxy-9265/services/http:proxy-service-cq6tg:portname1/proxy/: foo (200; 49.564783ms)
Oct 26 17:34:26.427: INFO: (18) /api/v1/namespaces/proxy-9265/services/proxy-service-cq6tg:portname1/proxy/: foo (200; 49.71096ms)
Oct 26 17:34:26.456: INFO: (19) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds/proxy/rewriteme">test</a> (200; 28.128228ms)
Oct 26 17:34:26.458: INFO: (19) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:460/proxy/: tls baz (200; 29.909457ms)
Oct 26 17:34:26.459: INFO: (19) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:160/proxy/: foo (200; 31.492202ms)
Oct 26 17:34:26.459: INFO: (19) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:160/proxy/: foo (200; 31.058853ms)
Oct 26 17:34:26.459: INFO: (19) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:162/proxy/: bar (200; 30.660297ms)
Oct 26 17:34:26.459: INFO: (19) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:1080/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:1080/proxy/rewriteme">test<... (200; 30.271107ms)
Oct 26 17:34:26.459: INFO: (19) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:462/proxy/: tls qux (200; 30.456483ms)
Oct 26 17:34:26.459: INFO: (19) /api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:1080/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/http:proxy-service-cq6tg-mv7ds:1080/proxy/rewriteme">... (200; 31.544825ms)
Oct 26 17:34:26.462: INFO: (19) /api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:443/proxy/: <a href="/api/v1/namespaces/proxy-9265/pods/https:proxy-service-cq6tg-mv7ds:443/proxy/tlsrewritem... (200; 34.80486ms)
Oct 26 17:34:26.462: INFO: (19) /api/v1/namespaces/proxy-9265/pods/proxy-service-cq6tg-mv7ds:162/proxy/: bar (200; 33.760025ms)
Oct 26 17:34:26.465: INFO: (19) /api/v1/namespaces/proxy-9265/services/https:proxy-service-cq6tg:tlsportname2/proxy/: tls qux (200; 36.80007ms)
Oct 26 17:34:26.465: INFO: (19) /api/v1/namespaces/proxy-9265/services/https:proxy-service-cq6tg:tlsportname1/proxy/: tls baz (200; 37.905805ms)
Oct 26 17:34:26.466: INFO: (19) /api/v1/namespaces/proxy-9265/services/http:proxy-service-cq6tg:portname1/proxy/: foo (200; 38.758329ms)
Oct 26 17:34:26.466: INFO: (19) /api/v1/namespaces/proxy-9265/services/http:proxy-service-cq6tg:portname2/proxy/: bar (200; 38.916524ms)
Oct 26 17:34:26.466: INFO: (19) /api/v1/namespaces/proxy-9265/services/proxy-service-cq6tg:portname1/proxy/: foo (200; 39.020676ms)
Oct 26 17:34:26.469: INFO: (19) /api/v1/namespaces/proxy-9265/services/proxy-service-cq6tg:portname2/proxy/: bar (200; 41.514254ms)
STEP: deleting ReplicationController proxy-service-cq6tg in namespace proxy-9265, will wait for the garbage collector to delete the pods
Oct 26 17:34:26.577: INFO: Deleting ReplicationController proxy-service-cq6tg took: 44.489512ms
Oct 26 17:34:26.677: INFO: Terminating ReplicationController proxy-service-cq6tg pods took: 100.281933ms
[AfterEach] version v1
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:34:38.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9265" for this suite.

• [SLOW TEST:25.429 seconds]
[sig-network] Proxy
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":305,"completed":156,"skipped":2644,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:34:38.325: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-491
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-10069483-91d8-4728-bc56-a65d7b4e9bc6
STEP: Creating a pod to test consume configMaps
Oct 26 17:34:38.590: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7347431d-a51b-413b-8942-3fa9b56c5cb4" in namespace "projected-491" to be "Succeeded or Failed"
Oct 26 17:34:38.601: INFO: Pod "pod-projected-configmaps-7347431d-a51b-413b-8942-3fa9b56c5cb4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.321055ms
Oct 26 17:34:40.613: INFO: Pod "pod-projected-configmaps-7347431d-a51b-413b-8942-3fa9b56c5cb4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02233373s
STEP: Saw pod success
Oct 26 17:34:40.613: INFO: Pod "pod-projected-configmaps-7347431d-a51b-413b-8942-3fa9b56c5cb4" satisfied condition "Succeeded or Failed"
Oct 26 17:34:40.623: INFO: Trying to get logs from node 10.112.67.203 pod pod-projected-configmaps-7347431d-a51b-413b-8942-3fa9b56c5cb4 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Oct 26 17:34:40.706: INFO: Waiting for pod pod-projected-configmaps-7347431d-a51b-413b-8942-3fa9b56c5cb4 to disappear
Oct 26 17:34:40.716: INFO: Pod pod-projected-configmaps-7347431d-a51b-413b-8942-3fa9b56c5cb4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:34:40.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-491" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":305,"completed":157,"skipped":2657,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:34:40.759: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4033
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 26 17:34:41.942: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 26 17:34:44.020: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739330481, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739330481, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739330481, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739330481, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 26 17:34:47.097: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
Oct 26 17:34:57.199: INFO: Waiting for webhook configuration to be ready...
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Oct 26 17:34:59.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 attach --namespace=webhook-4033 to-be-attached-pod -i -c=container1'
Oct 26 17:34:59.607: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:34:59.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4033" for this suite.
STEP: Destroying namespace "webhook-4033-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:19.164 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":305,"completed":158,"skipped":2672,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:34:59.924: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-170
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-d6f5a188-ccaf-4c44-9c89-590d1d8f4320
STEP: Creating a pod to test consume configMaps
Oct 26 17:35:00.201: INFO: Waiting up to 5m0s for pod "pod-configmaps-54fe3027-8b2f-44a2-b61a-e4a4fb757cb7" in namespace "configmap-170" to be "Succeeded or Failed"
Oct 26 17:35:00.211: INFO: Pod "pod-configmaps-54fe3027-8b2f-44a2-b61a-e4a4fb757cb7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.32677ms
Oct 26 17:35:02.231: INFO: Pod "pod-configmaps-54fe3027-8b2f-44a2-b61a-e4a4fb757cb7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029841647s
Oct 26 17:35:04.245: INFO: Pod "pod-configmaps-54fe3027-8b2f-44a2-b61a-e4a4fb757cb7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044265256s
STEP: Saw pod success
Oct 26 17:35:04.245: INFO: Pod "pod-configmaps-54fe3027-8b2f-44a2-b61a-e4a4fb757cb7" satisfied condition "Succeeded or Failed"
Oct 26 17:35:04.261: INFO: Trying to get logs from node 10.112.67.203 pod pod-configmaps-54fe3027-8b2f-44a2-b61a-e4a4fb757cb7 container configmap-volume-test: <nil>
STEP: delete the pod
Oct 26 17:35:04.332: INFO: Waiting for pod pod-configmaps-54fe3027-8b2f-44a2-b61a-e4a4fb757cb7 to disappear
Oct 26 17:35:04.343: INFO: Pod pod-configmaps-54fe3027-8b2f-44a2-b61a-e4a4fb757cb7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:35:04.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-170" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":305,"completed":159,"skipped":2701,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:35:04.391: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1129
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-1129
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Oct 26 17:35:04.664: INFO: Found 0 stateful pods, waiting for 3
Oct 26 17:35:14.680: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Oct 26 17:35:14.680: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Oct 26 17:35:14.680: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Oct 26 17:35:14.751: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Oct 26 17:35:24.865: INFO: Updating stateful set ss2
Oct 26 17:35:24.893: INFO: Waiting for Pod statefulset-1129/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Oct 26 17:35:35.087: INFO: Found 2 stateful pods, waiting for 3
Oct 26 17:35:45.101: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Oct 26 17:35:45.101: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Oct 26 17:35:45.101: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Oct 26 17:35:45.177: INFO: Updating stateful set ss2
Oct 26 17:35:45.203: INFO: Waiting for Pod statefulset-1129/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Oct 26 17:35:55.227: INFO: Waiting for Pod statefulset-1129/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Oct 26 17:36:05.283: INFO: Updating stateful set ss2
Oct 26 17:36:05.321: INFO: Waiting for StatefulSet statefulset-1129/ss2 to complete update
Oct 26 17:36:05.321: INFO: Waiting for Pod statefulset-1129/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Oct 26 17:36:15.348: INFO: Deleting all statefulset in ns statefulset-1129
Oct 26 17:36:15.360: INFO: Scaling statefulset ss2 to 0
Oct 26 17:36:45.419: INFO: Waiting for statefulset status.replicas updated to 0
Oct 26 17:36:45.433: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:36:45.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1129" for this suite.

• [SLOW TEST:101.136 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":305,"completed":160,"skipped":2713,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:36:45.534: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7806
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 26 17:38:45.838: INFO: Deleting pod "var-expansion-513aeaf4-2642-4175-babe-411eb28a3bb2" in namespace "var-expansion-7806"
Oct 26 17:38:45.859: INFO: Wait up to 5m0s for pod "var-expansion-513aeaf4-2642-4175-babe-411eb28a3bb2" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:38:49.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7806" for this suite.

• [SLOW TEST:124.389 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":305,"completed":161,"skipped":2736,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] server version
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:38:49.926: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename server-version
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in server-version-2711
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Request ServerVersion
STEP: Confirm major version
Oct 26 17:38:50.169: INFO: Major version: 1
STEP: Confirm minor version
Oct 26 17:38:50.170: INFO: cleanMinorVersion: 19
Oct 26 17:38:50.170: INFO: Minor version: 19
[AfterEach] [sig-api-machinery] server version
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:38:50.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-2711" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":305,"completed":162,"skipped":2766,"failed":0}

------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:38:50.219: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-7148
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pod templates
Oct 26 17:38:50.463: INFO: created test-podtemplate-1
Oct 26 17:38:50.480: INFO: created test-podtemplate-2
Oct 26 17:38:50.496: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Oct 26 17:38:50.510: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Oct 26 17:38:50.606: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:38:50.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-7148" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":305,"completed":163,"skipped":2766,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:38:50.674: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-4515
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 26 17:38:50.887: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:38:57.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4515" for this suite.

• [SLOW TEST:6.891 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":305,"completed":164,"skipped":2768,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:38:57.565: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6520
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on node default medium
Oct 26 17:38:57.842: INFO: Waiting up to 5m0s for pod "pod-32707523-977a-45c8-85af-bb8c7a3e4b0c" in namespace "emptydir-6520" to be "Succeeded or Failed"
Oct 26 17:38:57.852: INFO: Pod "pod-32707523-977a-45c8-85af-bb8c7a3e4b0c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.389988ms
Oct 26 17:38:59.863: INFO: Pod "pod-32707523-977a-45c8-85af-bb8c7a3e4b0c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02082466s
Oct 26 17:39:01.881: INFO: Pod "pod-32707523-977a-45c8-85af-bb8c7a3e4b0c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038714062s
STEP: Saw pod success
Oct 26 17:39:01.881: INFO: Pod "pod-32707523-977a-45c8-85af-bb8c7a3e4b0c" satisfied condition "Succeeded or Failed"
Oct 26 17:39:01.892: INFO: Trying to get logs from node 10.112.67.203 pod pod-32707523-977a-45c8-85af-bb8c7a3e4b0c container test-container: <nil>
STEP: delete the pod
Oct 26 17:39:02.027: INFO: Waiting for pod pod-32707523-977a-45c8-85af-bb8c7a3e4b0c to disappear
Oct 26 17:39:02.039: INFO: Pod pod-32707523-977a-45c8-85af-bb8c7a3e4b0c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:39:02.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6520" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":165,"skipped":2769,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:39:02.104: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8845
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 26 17:39:02.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 version'
Oct 26 17:39:02.459: INFO: stderr: ""
Oct 26 17:39:02.459: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.3\", GitCommit:\"1e11e4a2108024935ecfcb2912226cedeafd99df\", GitTreeState:\"clean\", BuildDate:\"2020-10-14T12:50:19Z\", GoVersion:\"go1.15.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.3+IKS\", GitCommit:\"b946265e89b4e0fa3b6996e3f8e3f71915c1d36b\", GitTreeState:\"clean\", BuildDate:\"2020-10-15T11:38:47Z\", GoVersion:\"go1.15.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:39:02.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8845" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":305,"completed":166,"skipped":2772,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:39:02.501: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2339
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Oct 26 17:39:02.756: INFO: Waiting up to 5m0s for pod "pod-b45e10a4-ffb0-409e-a2ca-fe5dc3dbee3e" in namespace "emptydir-2339" to be "Succeeded or Failed"
Oct 26 17:39:02.766: INFO: Pod "pod-b45e10a4-ffb0-409e-a2ca-fe5dc3dbee3e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.964536ms
Oct 26 17:39:04.777: INFO: Pod "pod-b45e10a4-ffb0-409e-a2ca-fe5dc3dbee3e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021133115s
Oct 26 17:39:06.795: INFO: Pod "pod-b45e10a4-ffb0-409e-a2ca-fe5dc3dbee3e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039204386s
STEP: Saw pod success
Oct 26 17:39:06.795: INFO: Pod "pod-b45e10a4-ffb0-409e-a2ca-fe5dc3dbee3e" satisfied condition "Succeeded or Failed"
Oct 26 17:39:06.807: INFO: Trying to get logs from node 10.112.67.203 pod pod-b45e10a4-ffb0-409e-a2ca-fe5dc3dbee3e container test-container: <nil>
STEP: delete the pod
Oct 26 17:39:06.964: INFO: Waiting for pod pod-b45e10a4-ffb0-409e-a2ca-fe5dc3dbee3e to disappear
Oct 26 17:39:06.983: INFO: Pod pod-b45e10a4-ffb0-409e-a2ca-fe5dc3dbee3e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:39:06.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2339" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":167,"skipped":2780,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:39:07.044: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6770
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-da3ba16e-a782-45aa-85c2-c2abc5a1b3e8
STEP: Creating a pod to test consume secrets
Oct 26 17:39:07.312: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6c658996-bdce-4e74-93db-8cf8112a8158" in namespace "projected-6770" to be "Succeeded or Failed"
Oct 26 17:39:07.322: INFO: Pod "pod-projected-secrets-6c658996-bdce-4e74-93db-8cf8112a8158": Phase="Pending", Reason="", readiness=false. Elapsed: 9.458304ms
Oct 26 17:39:09.339: INFO: Pod "pod-projected-secrets-6c658996-bdce-4e74-93db-8cf8112a8158": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027125742s
STEP: Saw pod success
Oct 26 17:39:09.339: INFO: Pod "pod-projected-secrets-6c658996-bdce-4e74-93db-8cf8112a8158" satisfied condition "Succeeded or Failed"
Oct 26 17:39:09.349: INFO: Trying to get logs from node 10.112.67.203 pod pod-projected-secrets-6c658996-bdce-4e74-93db-8cf8112a8158 container projected-secret-volume-test: <nil>
STEP: delete the pod
Oct 26 17:39:09.414: INFO: Waiting for pod pod-projected-secrets-6c658996-bdce-4e74-93db-8cf8112a8158 to disappear
Oct 26 17:39:09.428: INFO: Pod pod-projected-secrets-6c658996-bdce-4e74-93db-8cf8112a8158 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:39:09.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6770" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":168,"skipped":2791,"failed":0}
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:39:09.482: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-1567
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Oct 26 17:39:09.775: INFO: Waiting up to 1m0s for all nodes to be ready
Oct 26 17:40:09.903: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
Oct 26 17:40:09.986: INFO: Created pod: pod0-sched-preemption-low-priority
Oct 26 17:40:10.035: INFO: Created pod: pod1-sched-preemption-medium-priority
Oct 26 17:40:10.086: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:40:44.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1567" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:95.138 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":305,"completed":169,"skipped":2799,"failed":0}
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:40:44.619: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4191
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Oct 26 17:40:44.875: INFO: Waiting up to 5m0s for pod "pod-f66ad1ff-735d-46c7-a5bb-8eab114765be" in namespace "emptydir-4191" to be "Succeeded or Failed"
Oct 26 17:40:44.891: INFO: Pod "pod-f66ad1ff-735d-46c7-a5bb-8eab114765be": Phase="Pending", Reason="", readiness=false. Elapsed: 15.956212ms
Oct 26 17:40:46.903: INFO: Pod "pod-f66ad1ff-735d-46c7-a5bb-8eab114765be": Phase="Running", Reason="", readiness=true. Elapsed: 2.028079953s
Oct 26 17:40:48.916: INFO: Pod "pod-f66ad1ff-735d-46c7-a5bb-8eab114765be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040986581s
STEP: Saw pod success
Oct 26 17:40:48.916: INFO: Pod "pod-f66ad1ff-735d-46c7-a5bb-8eab114765be" satisfied condition "Succeeded or Failed"
Oct 26 17:40:48.928: INFO: Trying to get logs from node 10.112.67.203 pod pod-f66ad1ff-735d-46c7-a5bb-8eab114765be container test-container: <nil>
STEP: delete the pod
Oct 26 17:40:49.019: INFO: Waiting for pod pod-f66ad1ff-735d-46c7-a5bb-8eab114765be to disappear
Oct 26 17:40:49.031: INFO: Pod pod-f66ad1ff-735d-46c7-a5bb-8eab114765be no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:40:49.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4191" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":170,"skipped":2799,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:40:49.097: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4969
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Oct 26 17:40:49.496: INFO: Number of nodes with available pods: 0
Oct 26 17:40:49.496: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 17:40:50.544: INFO: Number of nodes with available pods: 0
Oct 26 17:40:50.544: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 17:40:51.525: INFO: Number of nodes with available pods: 0
Oct 26 17:40:51.525: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 17:40:52.527: INFO: Number of nodes with available pods: 3
Oct 26 17:40:52.527: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Oct 26 17:40:52.608: INFO: Number of nodes with available pods: 2
Oct 26 17:40:52.608: INFO: Node 10.112.67.203 is running more than one daemon pod
Oct 26 17:40:53.647: INFO: Number of nodes with available pods: 2
Oct 26 17:40:53.647: INFO: Node 10.112.67.203 is running more than one daemon pod
Oct 26 17:40:54.640: INFO: Number of nodes with available pods: 3
Oct 26 17:40:54.640: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4969, will wait for the garbage collector to delete the pods
Oct 26 17:40:54.744: INFO: Deleting DaemonSet.extensions daemon-set took: 24.570044ms
Oct 26 17:40:54.844: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.519005ms
Oct 26 17:41:08.064: INFO: Number of nodes with available pods: 0
Oct 26 17:41:08.064: INFO: Number of running nodes: 0, number of available pods: 0
Oct 26 17:41:08.075: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4969/daemonsets","resourceVersion":"47993"},"items":null}

Oct 26 17:41:08.090: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4969/pods","resourceVersion":"47993"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:41:08.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4969" for this suite.

• [SLOW TEST:19.099 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":305,"completed":171,"skipped":2830,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:41:08.199: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-938
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 26 17:41:08.447: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-2c7bb0c1-e372-4b2b-abba-e5702897a454" in namespace "security-context-test-938" to be "Succeeded or Failed"
Oct 26 17:41:08.459: INFO: Pod "alpine-nnp-false-2c7bb0c1-e372-4b2b-abba-e5702897a454": Phase="Pending", Reason="", readiness=false. Elapsed: 11.942124ms
Oct 26 17:41:10.474: INFO: Pod "alpine-nnp-false-2c7bb0c1-e372-4b2b-abba-e5702897a454": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026767056s
Oct 26 17:41:12.485: INFO: Pod "alpine-nnp-false-2c7bb0c1-e372-4b2b-abba-e5702897a454": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03806124s
Oct 26 17:41:14.499: INFO: Pod "alpine-nnp-false-2c7bb0c1-e372-4b2b-abba-e5702897a454": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.052411835s
Oct 26 17:41:14.500: INFO: Pod "alpine-nnp-false-2c7bb0c1-e372-4b2b-abba-e5702897a454" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:41:14.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-938" for this suite.

• [SLOW TEST:6.377 seconds]
[k8s.io] Security Context
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:291
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":172,"skipped":2879,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:41:14.577: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2230
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-projected-7k5j
STEP: Creating a pod to test atomic-volume-subpath
Oct 26 17:41:14.850: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-7k5j" in namespace "subpath-2230" to be "Succeeded or Failed"
Oct 26 17:41:14.859: INFO: Pod "pod-subpath-test-projected-7k5j": Phase="Pending", Reason="", readiness=false. Elapsed: 8.793171ms
Oct 26 17:41:16.871: INFO: Pod "pod-subpath-test-projected-7k5j": Phase="Running", Reason="", readiness=true. Elapsed: 2.020016137s
Oct 26 17:41:18.882: INFO: Pod "pod-subpath-test-projected-7k5j": Phase="Running", Reason="", readiness=true. Elapsed: 4.031796221s
Oct 26 17:41:20.893: INFO: Pod "pod-subpath-test-projected-7k5j": Phase="Running", Reason="", readiness=true. Elapsed: 6.042519742s
Oct 26 17:41:22.906: INFO: Pod "pod-subpath-test-projected-7k5j": Phase="Running", Reason="", readiness=true. Elapsed: 8.055496216s
Oct 26 17:41:24.918: INFO: Pod "pod-subpath-test-projected-7k5j": Phase="Running", Reason="", readiness=true. Elapsed: 10.067087983s
Oct 26 17:41:26.929: INFO: Pod "pod-subpath-test-projected-7k5j": Phase="Running", Reason="", readiness=true. Elapsed: 12.078132332s
Oct 26 17:41:28.942: INFO: Pod "pod-subpath-test-projected-7k5j": Phase="Running", Reason="", readiness=true. Elapsed: 14.091260707s
Oct 26 17:41:30.951: INFO: Pod "pod-subpath-test-projected-7k5j": Phase="Running", Reason="", readiness=true. Elapsed: 16.100651666s
Oct 26 17:41:32.963: INFO: Pod "pod-subpath-test-projected-7k5j": Phase="Running", Reason="", readiness=true. Elapsed: 18.112160811s
Oct 26 17:41:34.974: INFO: Pod "pod-subpath-test-projected-7k5j": Phase="Running", Reason="", readiness=true. Elapsed: 20.123482896s
Oct 26 17:41:36.988: INFO: Pod "pod-subpath-test-projected-7k5j": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.137211217s
STEP: Saw pod success
Oct 26 17:41:36.988: INFO: Pod "pod-subpath-test-projected-7k5j" satisfied condition "Succeeded or Failed"
Oct 26 17:41:36.999: INFO: Trying to get logs from node 10.112.67.203 pod pod-subpath-test-projected-7k5j container test-container-subpath-projected-7k5j: <nil>
STEP: delete the pod
Oct 26 17:41:37.094: INFO: Waiting for pod pod-subpath-test-projected-7k5j to disappear
Oct 26 17:41:37.107: INFO: Pod pod-subpath-test-projected-7k5j no longer exists
STEP: Deleting pod pod-subpath-test-projected-7k5j
Oct 26 17:41:37.107: INFO: Deleting pod "pod-subpath-test-projected-7k5j" in namespace "subpath-2230"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:41:37.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2230" for this suite.

• [SLOW TEST:22.602 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":305,"completed":173,"skipped":2889,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:41:37.180: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5632
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 26 17:41:38.174: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 26 17:41:40.218: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739330898, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739330898, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739330898, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739330898, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 26 17:41:43.277: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 26 17:41:43.292: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6376-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:41:44.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5632" for this suite.
STEP: Destroying namespace "webhook-5632-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.835 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":305,"completed":174,"skipped":2909,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:41:45.017: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7689
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test env composition
Oct 26 17:41:45.277: INFO: Waiting up to 5m0s for pod "var-expansion-e97dea7a-dc24-4775-929a-ec1da1789ee6" in namespace "var-expansion-7689" to be "Succeeded or Failed"
Oct 26 17:41:45.289: INFO: Pod "var-expansion-e97dea7a-dc24-4775-929a-ec1da1789ee6": Phase="Pending", Reason="", readiness=false. Elapsed: 12.195044ms
Oct 26 17:41:47.300: INFO: Pod "var-expansion-e97dea7a-dc24-4775-929a-ec1da1789ee6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022955101s
Oct 26 17:41:49.314: INFO: Pod "var-expansion-e97dea7a-dc24-4775-929a-ec1da1789ee6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036421034s
STEP: Saw pod success
Oct 26 17:41:49.314: INFO: Pod "var-expansion-e97dea7a-dc24-4775-929a-ec1da1789ee6" satisfied condition "Succeeded or Failed"
Oct 26 17:41:49.324: INFO: Trying to get logs from node 10.112.67.203 pod var-expansion-e97dea7a-dc24-4775-929a-ec1da1789ee6 container dapi-container: <nil>
STEP: delete the pod
Oct 26 17:41:49.393: INFO: Waiting for pod var-expansion-e97dea7a-dc24-4775-929a-ec1da1789ee6 to disappear
Oct 26 17:41:49.419: INFO: Pod var-expansion-e97dea7a-dc24-4775-929a-ec1da1789ee6 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:41:49.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7689" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":305,"completed":175,"skipped":2943,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:41:49.460: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7029
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service endpoint-test2 in namespace services-7029
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7029 to expose endpoints map[]
Oct 26 17:41:49.748: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Oct 26 17:41:50.798: INFO: successfully validated that service endpoint-test2 in namespace services-7029 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7029
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7029 to expose endpoints map[pod1:[80]]
Oct 26 17:41:52.901: INFO: successfully validated that service endpoint-test2 in namespace services-7029 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-7029
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7029 to expose endpoints map[pod1:[80] pod2:[80]]
Oct 26 17:41:55.981: INFO: successfully validated that service endpoint-test2 in namespace services-7029 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-7029
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7029 to expose endpoints map[pod2:[80]]
Oct 26 17:41:56.049: INFO: successfully validated that service endpoint-test2 in namespace services-7029 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-7029
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7029 to expose endpoints map[]
Oct 26 17:41:56.111: INFO: successfully validated that service endpoint-test2 in namespace services-7029 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:41:56.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7029" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:6.805 seconds]
[sig-network] Services
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":305,"completed":176,"skipped":2961,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:41:56.266: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6320
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Oct 26 17:41:56.527: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c536cda1-c31c-4cdf-b111-d3281e2fcb6a" in namespace "downward-api-6320" to be "Succeeded or Failed"
Oct 26 17:41:56.539: INFO: Pod "downwardapi-volume-c536cda1-c31c-4cdf-b111-d3281e2fcb6a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.454558ms
Oct 26 17:41:58.550: INFO: Pod "downwardapi-volume-c536cda1-c31c-4cdf-b111-d3281e2fcb6a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023579424s
Oct 26 17:42:00.564: INFO: Pod "downwardapi-volume-c536cda1-c31c-4cdf-b111-d3281e2fcb6a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037053433s
STEP: Saw pod success
Oct 26 17:42:00.564: INFO: Pod "downwardapi-volume-c536cda1-c31c-4cdf-b111-d3281e2fcb6a" satisfied condition "Succeeded or Failed"
Oct 26 17:42:00.575: INFO: Trying to get logs from node 10.112.67.203 pod downwardapi-volume-c536cda1-c31c-4cdf-b111-d3281e2fcb6a container client-container: <nil>
STEP: delete the pod
Oct 26 17:42:00.649: INFO: Waiting for pod downwardapi-volume-c536cda1-c31c-4cdf-b111-d3281e2fcb6a to disappear
Oct 26 17:42:00.659: INFO: Pod downwardapi-volume-c536cda1-c31c-4cdf-b111-d3281e2fcb6a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:42:00.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6320" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":305,"completed":177,"skipped":2971,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:42:00.706: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-9263
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 26 17:42:01.120: INFO: Creating ReplicaSet my-hostname-basic-8ce17d51-9ef3-4492-a639-45d7913de718
Oct 26 17:42:01.150: INFO: Pod name my-hostname-basic-8ce17d51-9ef3-4492-a639-45d7913de718: Found 0 pods out of 1
Oct 26 17:42:06.164: INFO: Pod name my-hostname-basic-8ce17d51-9ef3-4492-a639-45d7913de718: Found 1 pods out of 1
Oct 26 17:42:06.164: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-8ce17d51-9ef3-4492-a639-45d7913de718" is running
Oct 26 17:42:06.174: INFO: Pod "my-hostname-basic-8ce17d51-9ef3-4492-a639-45d7913de718-52jlb" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-10-26 17:42:01 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-10-26 17:42:02 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-10-26 17:42:02 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-10-26 17:42:01 +0000 UTC Reason: Message:}])
Oct 26 17:42:06.175: INFO: Trying to dial the pod
Oct 26 17:42:11.241: INFO: Controller my-hostname-basic-8ce17d51-9ef3-4492-a639-45d7913de718: Got expected result from replica 1 [my-hostname-basic-8ce17d51-9ef3-4492-a639-45d7913de718-52jlb]: "my-hostname-basic-8ce17d51-9ef3-4492-a639-45d7913de718-52jlb", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:42:11.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9263" for this suite.

• [SLOW TEST:10.588 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":305,"completed":178,"skipped":2980,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:42:11.298: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6126
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-ad36529f-e3a4-4544-8f98-d31aab76fe3c
STEP: Creating a pod to test consume configMaps
Oct 26 17:42:11.573: INFO: Waiting up to 5m0s for pod "pod-configmaps-4c9d6b6b-9fe5-45c1-8485-74b2e532b3d4" in namespace "configmap-6126" to be "Succeeded or Failed"
Oct 26 17:42:11.583: INFO: Pod "pod-configmaps-4c9d6b6b-9fe5-45c1-8485-74b2e532b3d4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.484396ms
Oct 26 17:42:13.602: INFO: Pod "pod-configmaps-4c9d6b6b-9fe5-45c1-8485-74b2e532b3d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02814554s
STEP: Saw pod success
Oct 26 17:42:13.602: INFO: Pod "pod-configmaps-4c9d6b6b-9fe5-45c1-8485-74b2e532b3d4" satisfied condition "Succeeded or Failed"
Oct 26 17:42:13.613: INFO: Trying to get logs from node 10.112.67.207 pod pod-configmaps-4c9d6b6b-9fe5-45c1-8485-74b2e532b3d4 container configmap-volume-test: <nil>
STEP: delete the pod
Oct 26 17:42:13.739: INFO: Waiting for pod pod-configmaps-4c9d6b6b-9fe5-45c1-8485-74b2e532b3d4 to disappear
Oct 26 17:42:13.755: INFO: Pod pod-configmaps-4c9d6b6b-9fe5-45c1-8485-74b2e532b3d4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:42:13.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6126" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":179,"skipped":2991,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:42:13.819: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3106
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-efc55b3b-0e1f-41f0-838f-be009b9eacf8
STEP: Creating a pod to test consume secrets
Oct 26 17:42:14.079: INFO: Waiting up to 5m0s for pod "pod-secrets-be472ed6-7732-4c35-bc8f-ded89dbf148d" in namespace "secrets-3106" to be "Succeeded or Failed"
Oct 26 17:42:14.090: INFO: Pod "pod-secrets-be472ed6-7732-4c35-bc8f-ded89dbf148d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.887647ms
Oct 26 17:42:16.107: INFO: Pod "pod-secrets-be472ed6-7732-4c35-bc8f-ded89dbf148d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028352297s
Oct 26 17:42:18.122: INFO: Pod "pod-secrets-be472ed6-7732-4c35-bc8f-ded89dbf148d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04355567s
STEP: Saw pod success
Oct 26 17:42:18.122: INFO: Pod "pod-secrets-be472ed6-7732-4c35-bc8f-ded89dbf148d" satisfied condition "Succeeded or Failed"
Oct 26 17:42:18.140: INFO: Trying to get logs from node 10.112.67.207 pod pod-secrets-be472ed6-7732-4c35-bc8f-ded89dbf148d container secret-volume-test: <nil>
STEP: delete the pod
Oct 26 17:42:18.206: INFO: Waiting for pod pod-secrets-be472ed6-7732-4c35-bc8f-ded89dbf148d to disappear
Oct 26 17:42:18.217: INFO: Pod pod-secrets-be472ed6-7732-4c35-bc8f-ded89dbf148d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:42:18.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3106" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":180,"skipped":2997,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:42:18.286: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-37
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Oct 26 17:42:18.560: INFO: Waiting up to 5m0s for pod "downward-api-2c206c38-8d66-4740-bd07-3a6c5df716dd" in namespace "downward-api-37" to be "Succeeded or Failed"
Oct 26 17:42:18.572: INFO: Pod "downward-api-2c206c38-8d66-4740-bd07-3a6c5df716dd": Phase="Pending", Reason="", readiness=false. Elapsed: 11.866214ms
Oct 26 17:42:20.582: INFO: Pod "downward-api-2c206c38-8d66-4740-bd07-3a6c5df716dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022497048s
Oct 26 17:42:22.594: INFO: Pod "downward-api-2c206c38-8d66-4740-bd07-3a6c5df716dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034437666s
STEP: Saw pod success
Oct 26 17:42:22.594: INFO: Pod "downward-api-2c206c38-8d66-4740-bd07-3a6c5df716dd" satisfied condition "Succeeded or Failed"
Oct 26 17:42:22.606: INFO: Trying to get logs from node 10.112.67.203 pod downward-api-2c206c38-8d66-4740-bd07-3a6c5df716dd container dapi-container: <nil>
STEP: delete the pod
Oct 26 17:42:22.676: INFO: Waiting for pod downward-api-2c206c38-8d66-4740-bd07-3a6c5df716dd to disappear
Oct 26 17:42:22.688: INFO: Pod downward-api-2c206c38-8d66-4740-bd07-3a6c5df716dd no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:42:22.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-37" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":305,"completed":181,"skipped":3053,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:42:22.739: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3922
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 26 17:42:24.103: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 26 17:42:27.192: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:42:37.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3922" for this suite.
STEP: Destroying namespace "webhook-3922-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:15.184 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":305,"completed":182,"skipped":3062,"failed":0}
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:42:37.923: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5897
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-00bc0957-b761-4232-8506-403fb5ce0e1f
STEP: Creating a pod to test consume configMaps
Oct 26 17:42:38.200: INFO: Waiting up to 5m0s for pod "pod-configmaps-c2ec659c-3708-459d-bea3-ddd29d0ae91f" in namespace "configmap-5897" to be "Succeeded or Failed"
Oct 26 17:42:38.211: INFO: Pod "pod-configmaps-c2ec659c-3708-459d-bea3-ddd29d0ae91f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.802511ms
Oct 26 17:42:40.223: INFO: Pod "pod-configmaps-c2ec659c-3708-459d-bea3-ddd29d0ae91f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023285016s
STEP: Saw pod success
Oct 26 17:42:40.223: INFO: Pod "pod-configmaps-c2ec659c-3708-459d-bea3-ddd29d0ae91f" satisfied condition "Succeeded or Failed"
Oct 26 17:42:40.239: INFO: Trying to get logs from node 10.112.67.203 pod pod-configmaps-c2ec659c-3708-459d-bea3-ddd29d0ae91f container configmap-volume-test: <nil>
STEP: delete the pod
Oct 26 17:42:40.306: INFO: Waiting for pod pod-configmaps-c2ec659c-3708-459d-bea3-ddd29d0ae91f to disappear
Oct 26 17:42:40.330: INFO: Pod pod-configmaps-c2ec659c-3708-459d-bea3-ddd29d0ae91f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:42:40.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5897" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":183,"skipped":3062,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:42:40.373: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2011
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-9bd989d1-2c3c-4748-8cd9-37f0d650dbae in namespace container-probe-2011
Oct 26 17:42:44.671: INFO: Started pod liveness-9bd989d1-2c3c-4748-8cd9-37f0d650dbae in namespace container-probe-2011
STEP: checking the pod's current state and verifying that restartCount is present
Oct 26 17:42:44.682: INFO: Initial restart count of pod liveness-9bd989d1-2c3c-4748-8cd9-37f0d650dbae is 0
Oct 26 17:43:04.813: INFO: Restart count of pod container-probe-2011/liveness-9bd989d1-2c3c-4748-8cd9-37f0d650dbae is now 1 (20.131103575s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:43:04.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2011" for this suite.

• [SLOW TEST:24.546 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":305,"completed":184,"skipped":3088,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:43:04.920: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-9390
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:43:05.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9390" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":305,"completed":185,"skipped":3096,"failed":0}
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:43:05.219: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-9850
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Oct 26 17:43:05.507: INFO: Waiting up to 1m0s for all nodes to be ready
Oct 26 17:44:05.615: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
Oct 26 17:44:05.701: INFO: Created pod: pod0-sched-preemption-low-priority
Oct 26 17:44:05.753: INFO: Created pod: pod1-sched-preemption-medium-priority
Oct 26 17:44:05.810: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:44:23.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9850" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:79.063 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":305,"completed":186,"skipped":3104,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:44:24.294: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-5813
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:44:28.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5813" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":305,"completed":187,"skipped":3128,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:44:28.820: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-4680
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Oct 26 17:44:29.038: INFO: Waiting up to 1m0s for all nodes to be ready
Oct 26 17:45:29.157: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 26 17:45:29.178: INFO: Starting informer...
STEP: Starting pods...
Oct 26 17:45:29.333: INFO: Pod1 is running on 10.112.67.203. Tainting Node
Oct 26 17:45:31.615: INFO: Pod2 is running on 10.112.67.203. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Oct 26 17:45:49.834: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Oct 26 17:45:58.587: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:45:58.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-4680" for this suite.

• [SLOW TEST:89.879 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":305,"completed":188,"skipped":3198,"failed":0}
SSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:45:58.701: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9789
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:46:03.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9789" for this suite.

• [SLOW TEST:5.334 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":305,"completed":189,"skipped":3201,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:46:04.034: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-7349
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-7349
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Oct 26 17:46:04.465: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Oct 26 17:46:04.753: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Oct 26 17:46:06.779: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 26 17:46:08.768: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 26 17:46:10.768: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 26 17:46:12.791: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 26 17:46:14.771: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 26 17:46:16.784: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 26 17:46:18.772: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 26 17:46:20.770: INFO: The status of Pod netserver-0 is Running (Ready = true)
Oct 26 17:46:20.799: INFO: The status of Pod netserver-1 is Running (Ready = false)
Oct 26 17:46:22.814: INFO: The status of Pod netserver-1 is Running (Ready = true)
Oct 26 17:46:22.846: INFO: The status of Pod netserver-2 is Running (Ready = false)
Oct 26 17:46:24.863: INFO: The status of Pod netserver-2 is Running (Ready = false)
Oct 26 17:46:26.860: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Oct 26 17:46:29.055: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.41.252 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7349 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 17:46:29.055: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
Oct 26 17:46:30.297: INFO: Found all expected endpoints: [netserver-0]
Oct 26 17:46:30.311: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.31.242 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7349 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 17:46:30.311: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
Oct 26 17:46:31.554: INFO: Found all expected endpoints: [netserver-1]
Oct 26 17:46:31.581: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.67.223 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7349 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 17:46:31.581: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
Oct 26 17:46:32.829: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:46:32.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7349" for this suite.

• [SLOW TEST:28.853 seconds]
[sig-network] Networking
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":190,"skipped":3219,"failed":0}
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:46:32.891: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4575
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 26 17:46:33.246: INFO: Create a RollingUpdate DaemonSet
Oct 26 17:46:33.266: INFO: Check that daemon pods launch on every node of the cluster
Oct 26 17:46:33.309: INFO: Number of nodes with available pods: 0
Oct 26 17:46:33.309: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 17:46:34.356: INFO: Number of nodes with available pods: 0
Oct 26 17:46:34.356: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 17:46:35.347: INFO: Number of nodes with available pods: 1
Oct 26 17:46:35.347: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 17:46:36.349: INFO: Number of nodes with available pods: 3
Oct 26 17:46:36.349: INFO: Number of running nodes: 3, number of available pods: 3
Oct 26 17:46:36.349: INFO: Update the DaemonSet to trigger a rollout
Oct 26 17:46:36.379: INFO: Updating DaemonSet daemon-set
Oct 26 17:46:50.464: INFO: Roll back the DaemonSet before rollout is complete
Oct 26 17:46:50.496: INFO: Updating DaemonSet daemon-set
Oct 26 17:46:50.496: INFO: Make sure DaemonSet rollback is complete
Oct 26 17:46:50.511: INFO: Wrong image for pod: daemon-set-xs7kn. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Oct 26 17:46:50.511: INFO: Pod daemon-set-xs7kn is not available
Oct 26 17:46:51.549: INFO: Wrong image for pod: daemon-set-xs7kn. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Oct 26 17:46:51.549: INFO: Pod daemon-set-xs7kn is not available
Oct 26 17:46:52.554: INFO: Wrong image for pod: daemon-set-xs7kn. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Oct 26 17:46:52.554: INFO: Pod daemon-set-xs7kn is not available
Oct 26 17:46:53.548: INFO: Pod daemon-set-gt24j is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4575, will wait for the garbage collector to delete the pods
Oct 26 17:46:53.700: INFO: Deleting DaemonSet.extensions daemon-set took: 39.014399ms
Oct 26 17:46:53.900: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.316603ms
Oct 26 17:47:01.924: INFO: Number of nodes with available pods: 0
Oct 26 17:47:01.924: INFO: Number of running nodes: 0, number of available pods: 0
Oct 26 17:47:01.940: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4575/daemonsets","resourceVersion":"50509"},"items":null}

Oct 26 17:47:01.960: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4575/pods","resourceVersion":"50509"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:47:02.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4575" for this suite.

• [SLOW TEST:29.235 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":305,"completed":191,"skipped":3225,"failed":0}
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:47:02.126: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2089
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Oct 26 17:47:08.515: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1026 17:47:08.515474      27 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1026 17:47:08.515515      27 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1026 17:47:08.515527      27 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:47:08.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2089" for this suite.

• [SLOW TEST:6.471 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":305,"completed":192,"skipped":3227,"failed":0}
SS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:47:08.598: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8520
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:48:08.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8520" for this suite.

• [SLOW TEST:60.325 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":305,"completed":193,"skipped":3229,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:48:08.923: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3990
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Oct 26 17:48:11.788: INFO: Successfully updated pod "pod-update-05e28c24-e19c-491c-bfc2-8a3757a3fe54"
STEP: verifying the updated pod is in kubernetes
Oct 26 17:48:11.823: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:48:11.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3990" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":305,"completed":194,"skipped":3237,"failed":0}
S
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:48:11.883: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2657
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:48:14.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2657" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":305,"completed":195,"skipped":3238,"failed":0}

------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:48:14.323: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-9955
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Oct 26 17:48:14.602: INFO: Pod name pod-release: Found 0 pods out of 1
Oct 26 17:48:19.617: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:48:20.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9955" for this suite.

• [SLOW TEST:6.431 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":305,"completed":196,"skipped":3238,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:48:20.754: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3453
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Oct 26 17:48:21.011: INFO: Waiting up to 5m0s for pod "downwardapi-volume-907dec2b-1e5e-443b-ae4d-0090937fb4c8" in namespace "projected-3453" to be "Succeeded or Failed"
Oct 26 17:48:21.039: INFO: Pod "downwardapi-volume-907dec2b-1e5e-443b-ae4d-0090937fb4c8": Phase="Pending", Reason="", readiness=false. Elapsed: 27.975583ms
Oct 26 17:48:23.059: INFO: Pod "downwardapi-volume-907dec2b-1e5e-443b-ae4d-0090937fb4c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.047990232s
STEP: Saw pod success
Oct 26 17:48:23.059: INFO: Pod "downwardapi-volume-907dec2b-1e5e-443b-ae4d-0090937fb4c8" satisfied condition "Succeeded or Failed"
Oct 26 17:48:23.075: INFO: Trying to get logs from node 10.112.67.203 pod downwardapi-volume-907dec2b-1e5e-443b-ae4d-0090937fb4c8 container client-container: <nil>
STEP: delete the pod
Oct 26 17:48:23.167: INFO: Waiting for pod downwardapi-volume-907dec2b-1e5e-443b-ae4d-0090937fb4c8 to disappear
Oct 26 17:48:23.186: INFO: Pod downwardapi-volume-907dec2b-1e5e-443b-ae4d-0090937fb4c8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:48:23.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3453" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":197,"skipped":3242,"failed":0}
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:48:23.249: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-8259
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Oct 26 17:48:26.597: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:48:26.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8259" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":305,"completed":198,"skipped":3246,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Lease
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:48:26.731: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-9170
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Lease
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:48:27.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-9170" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":305,"completed":199,"skipped":3265,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:48:27.266: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7670
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Oct 26 17:48:32.181: INFO: Successfully updated pod "annotationupdatee898ed9c-3878-434d-868b-ce2e5e5f4f8c"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:48:34.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7670" for this suite.

• [SLOW TEST:7.090 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":305,"completed":200,"skipped":3274,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:48:34.358: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2601
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 26 17:48:35.506: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 26 17:48:37.557: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739331315, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739331315, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739331315, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739331315, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 26 17:48:40.615: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:48:53.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2601" for this suite.
STEP: Destroying namespace "webhook-2601-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:19.165 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":305,"completed":201,"skipped":3277,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:48:53.524: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename limitrange
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in limitrange-4411
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Oct 26 17:48:53.771: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Oct 26 17:48:53.810: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Oct 26 17:48:53.810: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Oct 26 17:48:53.843: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Oct 26 17:48:53.843: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Oct 26 17:48:53.872: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Oct 26 17:48:53.872: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Oct 26 17:49:01.014: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:49:01.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-4411" for this suite.

• [SLOW TEST:7.611 seconds]
[sig-scheduling] LimitRange
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":305,"completed":202,"skipped":3334,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:49:01.135: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5343
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Oct 26 17:49:01.401: INFO: Waiting up to 5m0s for pod "pod-02d4f18a-492f-4c8e-802b-13815078eacb" in namespace "emptydir-5343" to be "Succeeded or Failed"
Oct 26 17:49:01.412: INFO: Pod "pod-02d4f18a-492f-4c8e-802b-13815078eacb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.749904ms
Oct 26 17:49:03.428: INFO: Pod "pod-02d4f18a-492f-4c8e-802b-13815078eacb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026189637s
STEP: Saw pod success
Oct 26 17:49:03.428: INFO: Pod "pod-02d4f18a-492f-4c8e-802b-13815078eacb" satisfied condition "Succeeded or Failed"
Oct 26 17:49:03.444: INFO: Trying to get logs from node 10.112.67.203 pod pod-02d4f18a-492f-4c8e-802b-13815078eacb container test-container: <nil>
STEP: delete the pod
Oct 26 17:49:03.540: INFO: Waiting for pod pod-02d4f18a-492f-4c8e-802b-13815078eacb to disappear
Oct 26 17:49:03.564: INFO: Pod pod-02d4f18a-492f-4c8e-802b-13815078eacb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:49:03.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5343" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":203,"skipped":3344,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:49:03.629: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-4456
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Oct 26 17:49:08.437: INFO: Successfully updated pod "adopt-release-djkp7"
STEP: Checking that the Job readopts the Pod
Oct 26 17:49:08.437: INFO: Waiting up to 15m0s for pod "adopt-release-djkp7" in namespace "job-4456" to be "adopted"
Oct 26 17:49:08.449: INFO: Pod "adopt-release-djkp7": Phase="Running", Reason="", readiness=true. Elapsed: 12.033837ms
Oct 26 17:49:10.462: INFO: Pod "adopt-release-djkp7": Phase="Running", Reason="", readiness=true. Elapsed: 2.025105597s
Oct 26 17:49:10.462: INFO: Pod "adopt-release-djkp7" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Oct 26 17:49:10.997: INFO: Successfully updated pod "adopt-release-djkp7"
STEP: Checking that the Job releases the Pod
Oct 26 17:49:10.997: INFO: Waiting up to 15m0s for pod "adopt-release-djkp7" in namespace "job-4456" to be "released"
Oct 26 17:49:11.014: INFO: Pod "adopt-release-djkp7": Phase="Running", Reason="", readiness=true. Elapsed: 16.492935ms
Oct 26 17:49:13.037: INFO: Pod "adopt-release-djkp7": Phase="Running", Reason="", readiness=true. Elapsed: 2.039519872s
Oct 26 17:49:13.037: INFO: Pod "adopt-release-djkp7" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:49:13.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4456" for this suite.

• [SLOW TEST:9.467 seconds]
[sig-apps] Job
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":305,"completed":204,"skipped":3369,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:49:13.097: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-6485
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Oct 26 17:49:13.333: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:49:18.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6485" for this suite.

• [SLOW TEST:5.193 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":305,"completed":205,"skipped":3383,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:49:18.290: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7121
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 26 17:49:19.184: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 26 17:49:21.226: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739331359, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739331359, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739331359, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739331359, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 26 17:49:24.290: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:49:25.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7121" for this suite.
STEP: Destroying namespace "webhook-7121-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.150 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":305,"completed":206,"skipped":3384,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:49:25.441: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8346
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 26 17:49:26.331: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 26 17:49:28.389: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739331366, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739331366, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739331366, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739331366, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 26 17:49:31.457: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a validating webhook configuration
Oct 26 17:49:41.551: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:49:41.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8346" for this suite.
STEP: Destroying namespace "webhook-8346-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:16.808 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":305,"completed":207,"skipped":3384,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:49:42.250: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename discovery
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in discovery-709
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 26 17:49:43.013: INFO: Checking APIGroup: apiregistration.k8s.io
Oct 26 17:49:43.017: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Oct 26 17:49:43.017: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Oct 26 17:49:43.017: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Oct 26 17:49:43.017: INFO: Checking APIGroup: extensions
Oct 26 17:49:43.021: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Oct 26 17:49:43.022: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Oct 26 17:49:43.022: INFO: extensions/v1beta1 matches extensions/v1beta1
Oct 26 17:49:43.022: INFO: Checking APIGroup: apps
Oct 26 17:49:43.026: INFO: PreferredVersion.GroupVersion: apps/v1
Oct 26 17:49:43.026: INFO: Versions found [{apps/v1 v1}]
Oct 26 17:49:43.026: INFO: apps/v1 matches apps/v1
Oct 26 17:49:43.026: INFO: Checking APIGroup: events.k8s.io
Oct 26 17:49:43.042: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Oct 26 17:49:43.042: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Oct 26 17:49:43.042: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Oct 26 17:49:43.042: INFO: Checking APIGroup: authentication.k8s.io
Oct 26 17:49:43.046: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Oct 26 17:49:43.046: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Oct 26 17:49:43.046: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Oct 26 17:49:43.046: INFO: Checking APIGroup: authorization.k8s.io
Oct 26 17:49:43.050: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Oct 26 17:49:43.050: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Oct 26 17:49:43.050: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Oct 26 17:49:43.050: INFO: Checking APIGroup: autoscaling
Oct 26 17:49:43.054: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Oct 26 17:49:43.054: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Oct 26 17:49:43.054: INFO: autoscaling/v1 matches autoscaling/v1
Oct 26 17:49:43.054: INFO: Checking APIGroup: batch
Oct 26 17:49:43.057: INFO: PreferredVersion.GroupVersion: batch/v1
Oct 26 17:49:43.058: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Oct 26 17:49:43.058: INFO: batch/v1 matches batch/v1
Oct 26 17:49:43.058: INFO: Checking APIGroup: certificates.k8s.io
Oct 26 17:49:43.061: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Oct 26 17:49:43.061: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Oct 26 17:49:43.061: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Oct 26 17:49:43.061: INFO: Checking APIGroup: networking.k8s.io
Oct 26 17:49:43.074: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Oct 26 17:49:43.074: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Oct 26 17:49:43.074: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Oct 26 17:49:43.074: INFO: Checking APIGroup: policy
Oct 26 17:49:43.079: INFO: PreferredVersion.GroupVersion: policy/v1beta1
Oct 26 17:49:43.079: INFO: Versions found [{policy/v1beta1 v1beta1}]
Oct 26 17:49:43.079: INFO: policy/v1beta1 matches policy/v1beta1
Oct 26 17:49:43.079: INFO: Checking APIGroup: rbac.authorization.k8s.io
Oct 26 17:49:43.085: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Oct 26 17:49:43.085: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Oct 26 17:49:43.085: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Oct 26 17:49:43.085: INFO: Checking APIGroup: storage.k8s.io
Oct 26 17:49:43.093: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Oct 26 17:49:43.094: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Oct 26 17:49:43.094: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Oct 26 17:49:43.094: INFO: Checking APIGroup: admissionregistration.k8s.io
Oct 26 17:49:43.098: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Oct 26 17:49:43.098: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Oct 26 17:49:43.098: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Oct 26 17:49:43.098: INFO: Checking APIGroup: apiextensions.k8s.io
Oct 26 17:49:43.103: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Oct 26 17:49:43.103: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Oct 26 17:49:43.103: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Oct 26 17:49:43.103: INFO: Checking APIGroup: scheduling.k8s.io
Oct 26 17:49:43.107: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Oct 26 17:49:43.107: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
Oct 26 17:49:43.107: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Oct 26 17:49:43.107: INFO: Checking APIGroup: coordination.k8s.io
Oct 26 17:49:43.110: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Oct 26 17:49:43.111: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Oct 26 17:49:43.111: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Oct 26 17:49:43.111: INFO: Checking APIGroup: discovery.k8s.io
Oct 26 17:49:43.115: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
Oct 26 17:49:43.116: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
Oct 26 17:49:43.116: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
Oct 26 17:49:43.116: INFO: Checking APIGroup: crd.projectcalico.org
Oct 26 17:49:43.120: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Oct 26 17:49:43.120: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Oct 26 17:49:43.120: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Oct 26 17:49:43.120: INFO: Checking APIGroup: operators.coreos.com
Oct 26 17:49:43.125: INFO: PreferredVersion.GroupVersion: operators.coreos.com/v1
Oct 26 17:49:43.125: INFO: Versions found [{operators.coreos.com/v1 v1} {operators.coreos.com/v1alpha2 v1alpha2} {operators.coreos.com/v1alpha1 v1alpha1}]
Oct 26 17:49:43.125: INFO: operators.coreos.com/v1 matches operators.coreos.com/v1
Oct 26 17:49:43.125: INFO: Checking APIGroup: ibm.com
Oct 26 17:49:43.132: INFO: PreferredVersion.GroupVersion: ibm.com/v1alpha1
Oct 26 17:49:43.132: INFO: Versions found [{ibm.com/v1alpha1 v1alpha1}]
Oct 26 17:49:43.132: INFO: ibm.com/v1alpha1 matches ibm.com/v1alpha1
Oct 26 17:49:43.132: INFO: Checking APIGroup: metrics.k8s.io
Oct 26 17:49:43.136: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Oct 26 17:49:43.136: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Oct 26 17:49:43.136: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:49:43.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-709" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":305,"completed":208,"skipped":3407,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:49:43.187: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9298
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 26 17:49:43.434: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Oct 26 17:49:48.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 --namespace=crd-publish-openapi-9298 create -f -'
Oct 26 17:49:49.152: INFO: stderr: ""
Oct 26 17:49:49.152: INFO: stdout: "e2e-test-crd-publish-openapi-6162-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Oct 26 17:49:49.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 --namespace=crd-publish-openapi-9298 delete e2e-test-crd-publish-openapi-6162-crds test-foo'
Oct 26 17:49:49.431: INFO: stderr: ""
Oct 26 17:49:49.431: INFO: stdout: "e2e-test-crd-publish-openapi-6162-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Oct 26 17:49:49.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 --namespace=crd-publish-openapi-9298 apply -f -'
Oct 26 17:49:49.774: INFO: stderr: ""
Oct 26 17:49:49.774: INFO: stdout: "e2e-test-crd-publish-openapi-6162-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Oct 26 17:49:49.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 --namespace=crd-publish-openapi-9298 delete e2e-test-crd-publish-openapi-6162-crds test-foo'
Oct 26 17:49:49.961: INFO: stderr: ""
Oct 26 17:49:49.961: INFO: stdout: "e2e-test-crd-publish-openapi-6162-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Oct 26 17:49:49.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 --namespace=crd-publish-openapi-9298 create -f -'
Oct 26 17:49:50.396: INFO: rc: 1
Oct 26 17:49:50.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 --namespace=crd-publish-openapi-9298 apply -f -'
Oct 26 17:49:50.693: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Oct 26 17:49:50.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 --namespace=crd-publish-openapi-9298 create -f -'
Oct 26 17:49:51.239: INFO: rc: 1
Oct 26 17:49:51.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 --namespace=crd-publish-openapi-9298 apply -f -'
Oct 26 17:49:51.752: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Oct 26 17:49:51.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 explain e2e-test-crd-publish-openapi-6162-crds'
Oct 26 17:49:52.257: INFO: stderr: ""
Oct 26 17:49:52.257: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6162-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Oct 26 17:49:52.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 explain e2e-test-crd-publish-openapi-6162-crds.metadata'
Oct 26 17:49:52.530: INFO: stderr: ""
Oct 26 17:49:52.530: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6162-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Oct 26 17:49:52.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 explain e2e-test-crd-publish-openapi-6162-crds.spec'
Oct 26 17:49:52.927: INFO: stderr: ""
Oct 26 17:49:52.927: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6162-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Oct 26 17:49:52.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 explain e2e-test-crd-publish-openapi-6162-crds.spec.bars'
Oct 26 17:49:53.221: INFO: stderr: ""
Oct 26 17:49:53.221: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6162-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Oct 26 17:49:53.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 explain e2e-test-crd-publish-openapi-6162-crds.spec.bars2'
Oct 26 17:49:53.760: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:49:59.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9298" for this suite.

• [SLOW TEST:16.556 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":305,"completed":209,"skipped":3411,"failed":0}
SSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:49:59.743: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8620
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-aa374303-fb1b-41da-9ddb-00ceede6b36e in namespace container-probe-8620
Oct 26 17:50:04.055: INFO: Started pod busybox-aa374303-fb1b-41da-9ddb-00ceede6b36e in namespace container-probe-8620
STEP: checking the pod's current state and verifying that restartCount is present
Oct 26 17:50:04.065: INFO: Initial restart count of pod busybox-aa374303-fb1b-41da-9ddb-00ceede6b36e is 0
Oct 26 17:50:54.410: INFO: Restart count of pod container-probe-8620/busybox-aa374303-fb1b-41da-9ddb-00ceede6b36e is now 1 (50.344617843s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:50:54.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8620" for this suite.

• [SLOW TEST:54.752 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":305,"completed":210,"skipped":3416,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:50:54.495: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9406
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-cbae6c71-dda1-4223-b24e-5cdb2f3c541f
STEP: Creating a pod to test consume configMaps
Oct 26 17:50:54.758: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5d127fba-d1f5-4304-97e6-ba7fffe2fa26" in namespace "projected-9406" to be "Succeeded or Failed"
Oct 26 17:50:54.781: INFO: Pod "pod-projected-configmaps-5d127fba-d1f5-4304-97e6-ba7fffe2fa26": Phase="Pending", Reason="", readiness=false. Elapsed: 23.489651ms
Oct 26 17:50:56.795: INFO: Pod "pod-projected-configmaps-5d127fba-d1f5-4304-97e6-ba7fffe2fa26": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.03745223s
STEP: Saw pod success
Oct 26 17:50:56.795: INFO: Pod "pod-projected-configmaps-5d127fba-d1f5-4304-97e6-ba7fffe2fa26" satisfied condition "Succeeded or Failed"
Oct 26 17:50:56.806: INFO: Trying to get logs from node 10.112.67.203 pod pod-projected-configmaps-5d127fba-d1f5-4304-97e6-ba7fffe2fa26 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Oct 26 17:50:56.927: INFO: Waiting for pod pod-projected-configmaps-5d127fba-d1f5-4304-97e6-ba7fffe2fa26 to disappear
Oct 26 17:50:56.939: INFO: Pod pod-projected-configmaps-5d127fba-d1f5-4304-97e6-ba7fffe2fa26 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:50:56.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9406" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":305,"completed":211,"skipped":3423,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:50:57.002: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-1145
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
Oct 26 17:50:57.252: INFO: created test-event-1
Oct 26 17:50:57.285: INFO: created test-event-2
Oct 26 17:50:57.311: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Oct 26 17:50:57.339: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Oct 26 17:50:57.621: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:50:57.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1145" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":305,"completed":212,"skipped":3432,"failed":0}

------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:50:57.677: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8891
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service nodeport-test with type=NodePort in namespace services-8891
STEP: creating replication controller nodeport-test in namespace services-8891
I1026 17:50:57.985492      27 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-8891, replica count: 2
Oct 26 17:51:01.036: INFO: Creating new exec pod
I1026 17:51:01.036342      27 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 26 17:51:06.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-8891 execpodx2xnv -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Oct 26 17:51:06.472: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Oct 26 17:51:06.472: INFO: stdout: ""
Oct 26 17:51:06.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-8891 execpodx2xnv -- /bin/sh -x -c nc -zv -t -w 2 172.21.245.37 80'
Oct 26 17:51:06.815: INFO: stderr: "+ nc -zv -t -w 2 172.21.245.37 80\nConnection to 172.21.245.37 80 port [tcp/http] succeeded!\n"
Oct 26 17:51:06.815: INFO: stdout: ""
Oct 26 17:51:06.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-8891 execpodx2xnv -- /bin/sh -x -c nc -zv -t -w 2 10.112.67.207 31434'
Oct 26 17:51:07.188: INFO: stderr: "+ nc -zv -t -w 2 10.112.67.207 31434\nConnection to 10.112.67.207 31434 port [tcp/31434] succeeded!\n"
Oct 26 17:51:07.188: INFO: stdout: ""
Oct 26 17:51:07.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-8891 execpodx2xnv -- /bin/sh -x -c nc -zv -t -w 2 10.112.67.203 31434'
Oct 26 17:51:07.570: INFO: stderr: "+ nc -zv -t -w 2 10.112.67.203 31434\nConnection to 10.112.67.203 31434 port [tcp/31434] succeeded!\n"
Oct 26 17:51:07.570: INFO: stdout: ""
Oct 26 17:51:07.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-8891 execpodx2xnv -- /bin/sh -x -c nc -zv -t -w 2 5.10.101.150 31434'
Oct 26 17:51:07.914: INFO: stderr: "+ nc -zv -t -w 2 5.10.101.150 31434\nConnection to 5.10.101.150 31434 port [tcp/31434] succeeded!\n"
Oct 26 17:51:07.914: INFO: stdout: ""
Oct 26 17:51:07.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-8891 execpodx2xnv -- /bin/sh -x -c nc -zv -t -w 2 5.10.101.149 31434'
Oct 26 17:51:08.284: INFO: stderr: "+ nc -zv -t -w 2 5.10.101.149 31434\nConnection to 5.10.101.149 31434 port [tcp/31434] succeeded!\n"
Oct 26 17:51:08.284: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:51:08.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8891" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:10.656 seconds]
[sig-network] Services
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":305,"completed":213,"skipped":3432,"failed":0}
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:51:08.334: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-6232
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Oct 26 17:51:08.596: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Oct 26 17:51:08.628: INFO: Waiting for terminating namespaces to be deleted...
Oct 26 17:51:08.642: INFO: 
Logging pods the apiserver thinks is on node 10.112.67.201 before test
Oct 26 17:51:08.678: INFO: addon-catalog-source-wnsgb from ibm-system started at 2020-10-26 17:45:39 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:08.678: INFO: 	Container configmap-registry-server ready: true, restart count 0
Oct 26 17:51:08.678: INFO: catalog-operator-7654f857d5-ntsw4 from ibm-system started at 2020-10-26 17:45:31 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:08.678: INFO: 	Container catalog-operator ready: true, restart count 0
Oct 26 17:51:08.678: INFO: ibm-cloud-provider-ip-159-8-176-190-5d7d4b6856-tq96m from ibm-system started at 2020-10-26 15:04:09 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:08.678: INFO: 	Container ibm-cloud-provider-ip-159-8-176-190 ready: true, restart count 0
Oct 26 17:51:08.678: INFO: olm-operator-5cbbb5c89d-9gzjm from ibm-system started at 2020-10-26 15:03:06 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:08.678: INFO: 	Container olm-operator ready: true, restart count 0
Oct 26 17:51:08.678: INFO: calico-kube-controllers-68ddfff8d5-klh68 from kube-system started at 2020-10-26 15:03:05 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:08.678: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Oct 26 17:51:08.678: INFO: calico-node-9zj7g from kube-system started at 2020-10-26 15:02:45 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:08.678: INFO: 	Container calico-node ready: true, restart count 0
Oct 26 17:51:08.678: INFO: calico-typha-d497c4cc8-gzkpz from kube-system started at 2020-10-26 15:03:05 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:08.678: INFO: 	Container calico-typha ready: true, restart count 0
Oct 26 17:51:08.678: INFO: coredns-658bf88df8-4cjw8 from kube-system started at 2020-10-26 15:11:47 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:08.678: INFO: 	Container coredns ready: true, restart count 0
Oct 26 17:51:08.678: INFO: dashboard-metrics-scraper-f99788cf9-h8dpn from kube-system started at 2020-10-26 15:03:06 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:08.678: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Oct 26 17:51:08.678: INFO: ibm-file-plugin-847d88c686-zvgjj from kube-system started at 2020-10-26 17:45:31 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:08.678: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Oct 26 17:51:08.678: INFO: ibm-keepalived-watcher-2tk7w from kube-system started at 2020-10-26 15:02:45 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:08.678: INFO: 	Container keepalived-watcher ready: true, restart count 0
Oct 26 17:51:08.678: INFO: ibm-master-proxy-static-10.112.67.201 from kube-system started at 2020-10-26 15:02:31 +0000 UTC (2 container statuses recorded)
Oct 26 17:51:08.678: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Oct 26 17:51:08.678: INFO: 	Container pause ready: true, restart count 0
Oct 26 17:51:08.678: INFO: ibm-storage-watcher-567779df5-xjzzb from kube-system started at 2020-10-26 15:03:06 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:08.678: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Oct 26 17:51:08.678: INFO: public-crbube2gcl0spv5v02m2cg-alb1-69897d7d5-wd5xh from kube-system started at 2020-10-26 15:07:34 +0000 UTC (4 container statuses recorded)
Oct 26 17:51:08.678: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Oct 26 17:51:08.678: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Oct 26 17:51:08.678: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Oct 26 17:51:08.678: INFO: 	Container nginx-ingress ready: true, restart count 0
Oct 26 17:51:08.678: INFO: nodeport-test-84g5w from services-8891 started at 2020-10-26 17:50:58 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:08.678: INFO: 	Container nodeport-test ready: true, restart count 0
Oct 26 17:51:08.678: INFO: sonobuoy-e2e-job-bb5d7dc03d4b45a5 from sonobuoy started at 2020-10-26 16:37:14 +0000 UTC (2 container statuses recorded)
Oct 26 17:51:08.678: INFO: 	Container e2e ready: true, restart count 0
Oct 26 17:51:08.678: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Oct 26 17:51:08.678: INFO: sonobuoy-systemd-logs-daemon-set-4f27bf473b6a4e03-6rq9w from sonobuoy started at 2020-10-26 16:37:14 +0000 UTC (2 container statuses recorded)
Oct 26 17:51:08.678: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Oct 26 17:51:08.678: INFO: 	Container systemd-logs ready: true, restart count 0
Oct 26 17:51:08.678: INFO: 
Logging pods the apiserver thinks is on node 10.112.67.203 before test
Oct 26 17:51:08.698: INFO: calico-node-dcxtp from kube-system started at 2020-10-26 15:02:43 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:08.698: INFO: 	Container calico-node ready: true, restart count 0
Oct 26 17:51:08.698: INFO: calico-typha-d497c4cc8-dlxtt from kube-system started at 2020-10-26 17:45:59 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:08.698: INFO: 	Container calico-typha ready: true, restart count 0
Oct 26 17:51:08.698: INFO: ibm-keepalived-watcher-wlxd7 from kube-system started at 2020-10-26 15:02:43 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:08.698: INFO: 	Container keepalived-watcher ready: true, restart count 0
Oct 26 17:51:08.698: INFO: ibm-master-proxy-static-10.112.67.203 from kube-system started at 2020-10-26 15:02:39 +0000 UTC (2 container statuses recorded)
Oct 26 17:51:08.698: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Oct 26 17:51:08.698: INFO: 	Container pause ready: true, restart count 0
Oct 26 17:51:08.698: INFO: execpodx2xnv from services-8891 started at 2020-10-26 17:51:01 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:08.698: INFO: 	Container agnhost-pause ready: true, restart count 0
Oct 26 17:51:08.698: INFO: nodeport-test-rxn7f from services-8891 started at 2020-10-26 17:50:58 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:08.698: INFO: 	Container nodeport-test ready: true, restart count 0
Oct 26 17:51:08.698: INFO: sonobuoy from sonobuoy started at 2020-10-26 16:37:06 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:08.698: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Oct 26 17:51:08.699: INFO: sonobuoy-systemd-logs-daemon-set-4f27bf473b6a4e03-75n8p from sonobuoy started at 2020-10-26 16:37:14 +0000 UTC (2 container statuses recorded)
Oct 26 17:51:08.699: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Oct 26 17:51:08.699: INFO: 	Container systemd-logs ready: true, restart count 0
Oct 26 17:51:08.699: INFO: 
Logging pods the apiserver thinks is on node 10.112.67.207 before test
Oct 26 17:51:08.720: INFO: test-k8s-e2e-pvg-master-verification from default started at 2020-10-26 15:05:30 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:08.720: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Oct 26 17:51:08.720: INFO: ibm-cloud-provider-ip-159-8-176-190-5d7d4b6856-77vh5 from ibm-system started at 2020-10-26 15:04:09 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:08.720: INFO: 	Container ibm-cloud-provider-ip-159-8-176-190 ready: true, restart count 0
Oct 26 17:51:08.720: INFO: calico-node-k5p5w from kube-system started at 2020-10-26 15:02:53 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:08.720: INFO: 	Container calico-node ready: true, restart count 0
Oct 26 17:51:08.720: INFO: calico-typha-d497c4cc8-6p86s from kube-system started at 2020-10-26 15:03:16 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:08.720: INFO: 	Container calico-typha ready: true, restart count 0
Oct 26 17:51:08.720: INFO: coredns-658bf88df8-pn52m from kube-system started at 2020-10-26 15:11:47 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:08.720: INFO: 	Container coredns ready: true, restart count 0
Oct 26 17:51:08.720: INFO: coredns-658bf88df8-q4587 from kube-system started at 2020-10-26 17:45:31 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:08.720: INFO: 	Container coredns ready: true, restart count 0
Oct 26 17:51:08.720: INFO: coredns-autoscaler-65b4b99bb7-5p2hq from kube-system started at 2020-10-26 17:45:31 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:08.720: INFO: 	Container autoscaler ready: true, restart count 0
Oct 26 17:51:08.720: INFO: ibm-keepalived-watcher-drkhn from kube-system started at 2020-10-26 15:02:53 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:08.720: INFO: 	Container keepalived-watcher ready: true, restart count 0
Oct 26 17:51:08.720: INFO: ibm-master-proxy-static-10.112.67.207 from kube-system started at 2020-10-26 15:02:38 +0000 UTC (2 container statuses recorded)
Oct 26 17:51:08.720: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Oct 26 17:51:08.720: INFO: 	Container pause ready: true, restart count 0
Oct 26 17:51:08.720: INFO: kubernetes-dashboard-7c8884c686-tqpxt from kube-system started at 2020-10-26 17:45:31 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:08.720: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Oct 26 17:51:08.720: INFO: metrics-server-66957c64f5-kdpjh from kube-system started at 2020-10-26 17:45:31 +0000 UTC (2 container statuses recorded)
Oct 26 17:51:08.720: INFO: 	Container metrics-server ready: true, restart count 0
Oct 26 17:51:08.720: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Oct 26 17:51:08.720: INFO: public-crbube2gcl0spv5v02m2cg-alb1-69897d7d5-rh4l5 from kube-system started at 2020-10-26 15:07:34 +0000 UTC (4 container statuses recorded)
Oct 26 17:51:08.720: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Oct 26 17:51:08.720: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Oct 26 17:51:08.720: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Oct 26 17:51:08.720: INFO: 	Container nginx-ingress ready: true, restart count 0
Oct 26 17:51:08.720: INFO: vpn-7d76994fc5-n94t8 from kube-system started at 2020-10-26 17:45:31 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:08.720: INFO: 	Container vpn ready: true, restart count 0
Oct 26 17:51:08.720: INFO: sonobuoy-systemd-logs-daemon-set-4f27bf473b6a4e03-pnpc9 from sonobuoy started at 2020-10-26 16:37:14 +0000 UTC (2 container statuses recorded)
Oct 26 17:51:08.720: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Oct 26 17:51:08.721: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-5ae9beab-fa81-4323-9819-7b3f72faddd5 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-5ae9beab-fa81-4323-9819-7b3f72faddd5 off the node 10.112.67.203
STEP: verifying the node doesn't have the label kubernetes.io/e2e-5ae9beab-fa81-4323-9819-7b3f72faddd5
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:51:12.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6232" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":305,"completed":214,"skipped":3436,"failed":0}

------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:51:13.002: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9644
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Oct 26 17:51:17.830: INFO: Successfully updated pod "pod-update-activedeadlineseconds-830bf868-0fa7-41a2-87b6-2cd2bf75dd6b"
Oct 26 17:51:17.830: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-830bf868-0fa7-41a2-87b6-2cd2bf75dd6b" in namespace "pods-9644" to be "terminated due to deadline exceeded"
Oct 26 17:51:17.841: INFO: Pod "pod-update-activedeadlineseconds-830bf868-0fa7-41a2-87b6-2cd2bf75dd6b": Phase="Running", Reason="", readiness=true. Elapsed: 11.364956ms
Oct 26 17:51:19.853: INFO: Pod "pod-update-activedeadlineseconds-830bf868-0fa7-41a2-87b6-2cd2bf75dd6b": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.02323684s
Oct 26 17:51:19.853: INFO: Pod "pod-update-activedeadlineseconds-830bf868-0fa7-41a2-87b6-2cd2bf75dd6b" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:51:19.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9644" for this suite.

• [SLOW TEST:6.896 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":305,"completed":215,"skipped":3436,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:51:19.898: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-8164
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Oct 26 17:51:20.230: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Oct 26 17:51:20.263: INFO: Waiting for terminating namespaces to be deleted...
Oct 26 17:51:20.282: INFO: 
Logging pods the apiserver thinks is on node 10.112.67.201 before test
Oct 26 17:51:20.319: INFO: addon-catalog-source-wnsgb from ibm-system started at 2020-10-26 17:45:39 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:20.320: INFO: 	Container configmap-registry-server ready: true, restart count 0
Oct 26 17:51:20.320: INFO: catalog-operator-7654f857d5-ntsw4 from ibm-system started at 2020-10-26 17:45:31 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:20.320: INFO: 	Container catalog-operator ready: true, restart count 0
Oct 26 17:51:20.320: INFO: ibm-cloud-provider-ip-159-8-176-190-5d7d4b6856-tq96m from ibm-system started at 2020-10-26 15:04:09 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:20.320: INFO: 	Container ibm-cloud-provider-ip-159-8-176-190 ready: true, restart count 0
Oct 26 17:51:20.320: INFO: olm-operator-5cbbb5c89d-9gzjm from ibm-system started at 2020-10-26 15:03:06 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:20.320: INFO: 	Container olm-operator ready: true, restart count 0
Oct 26 17:51:20.321: INFO: calico-kube-controllers-68ddfff8d5-klh68 from kube-system started at 2020-10-26 15:03:05 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:20.321: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Oct 26 17:51:20.321: INFO: calico-node-9zj7g from kube-system started at 2020-10-26 15:02:45 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:20.321: INFO: 	Container calico-node ready: true, restart count 0
Oct 26 17:51:20.322: INFO: calico-typha-d497c4cc8-gzkpz from kube-system started at 2020-10-26 15:03:05 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:20.322: INFO: 	Container calico-typha ready: true, restart count 0
Oct 26 17:51:20.322: INFO: coredns-658bf88df8-4cjw8 from kube-system started at 2020-10-26 15:11:47 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:20.322: INFO: 	Container coredns ready: true, restart count 0
Oct 26 17:51:20.322: INFO: dashboard-metrics-scraper-f99788cf9-h8dpn from kube-system started at 2020-10-26 15:03:06 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:20.322: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Oct 26 17:51:20.323: INFO: ibm-file-plugin-847d88c686-zvgjj from kube-system started at 2020-10-26 17:45:31 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:20.323: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Oct 26 17:51:20.323: INFO: ibm-keepalived-watcher-2tk7w from kube-system started at 2020-10-26 15:02:45 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:20.323: INFO: 	Container keepalived-watcher ready: true, restart count 0
Oct 26 17:51:20.323: INFO: ibm-master-proxy-static-10.112.67.201 from kube-system started at 2020-10-26 15:02:31 +0000 UTC (2 container statuses recorded)
Oct 26 17:51:20.323: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Oct 26 17:51:20.323: INFO: 	Container pause ready: true, restart count 0
Oct 26 17:51:20.323: INFO: ibm-storage-watcher-567779df5-xjzzb from kube-system started at 2020-10-26 15:03:06 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:20.323: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Oct 26 17:51:20.323: INFO: public-crbube2gcl0spv5v02m2cg-alb1-69897d7d5-wd5xh from kube-system started at 2020-10-26 15:07:34 +0000 UTC (4 container statuses recorded)
Oct 26 17:51:20.323: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Oct 26 17:51:20.323: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Oct 26 17:51:20.324: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Oct 26 17:51:20.324: INFO: 	Container nginx-ingress ready: true, restart count 0
Oct 26 17:51:20.324: INFO: sonobuoy-e2e-job-bb5d7dc03d4b45a5 from sonobuoy started at 2020-10-26 16:37:14 +0000 UTC (2 container statuses recorded)
Oct 26 17:51:20.324: INFO: 	Container e2e ready: true, restart count 0
Oct 26 17:51:20.324: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Oct 26 17:51:20.324: INFO: sonobuoy-systemd-logs-daemon-set-4f27bf473b6a4e03-6rq9w from sonobuoy started at 2020-10-26 16:37:14 +0000 UTC (2 container statuses recorded)
Oct 26 17:51:20.324: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Oct 26 17:51:20.324: INFO: 	Container systemd-logs ready: true, restart count 0
Oct 26 17:51:20.324: INFO: 
Logging pods the apiserver thinks is on node 10.112.67.203 before test
Oct 26 17:51:20.353: INFO: calico-node-dcxtp from kube-system started at 2020-10-26 15:02:43 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:20.353: INFO: 	Container calico-node ready: true, restart count 0
Oct 26 17:51:20.353: INFO: calico-typha-d497c4cc8-dlxtt from kube-system started at 2020-10-26 17:45:59 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:20.353: INFO: 	Container calico-typha ready: true, restart count 0
Oct 26 17:51:20.353: INFO: ibm-keepalived-watcher-wlxd7 from kube-system started at 2020-10-26 15:02:43 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:20.353: INFO: 	Container keepalived-watcher ready: true, restart count 0
Oct 26 17:51:20.353: INFO: ibm-master-proxy-static-10.112.67.203 from kube-system started at 2020-10-26 15:02:39 +0000 UTC (2 container statuses recorded)
Oct 26 17:51:20.353: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Oct 26 17:51:20.353: INFO: 	Container pause ready: true, restart count 0
Oct 26 17:51:20.353: INFO: pod-update-activedeadlineseconds-830bf868-0fa7-41a2-87b6-2cd2bf75dd6b from pods-9644 started at 2020-10-26 17:51:13 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:20.353: INFO: 	Container nginx ready: false, restart count 0
Oct 26 17:51:20.353: INFO: with-labels from sched-pred-6232 started at 2020-10-26 17:51:10 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:20.353: INFO: 	Container with-labels ready: true, restart count 0
Oct 26 17:51:20.353: INFO: sonobuoy from sonobuoy started at 2020-10-26 16:37:06 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:20.353: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Oct 26 17:51:20.353: INFO: sonobuoy-systemd-logs-daemon-set-4f27bf473b6a4e03-75n8p from sonobuoy started at 2020-10-26 16:37:14 +0000 UTC (2 container statuses recorded)
Oct 26 17:51:20.353: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Oct 26 17:51:20.353: INFO: 	Container systemd-logs ready: true, restart count 0
Oct 26 17:51:20.353: INFO: 
Logging pods the apiserver thinks is on node 10.112.67.207 before test
Oct 26 17:51:20.382: INFO: test-k8s-e2e-pvg-master-verification from default started at 2020-10-26 15:05:30 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:20.382: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Oct 26 17:51:20.382: INFO: ibm-cloud-provider-ip-159-8-176-190-5d7d4b6856-77vh5 from ibm-system started at 2020-10-26 15:04:09 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:20.382: INFO: 	Container ibm-cloud-provider-ip-159-8-176-190 ready: true, restart count 0
Oct 26 17:51:20.382: INFO: calico-node-k5p5w from kube-system started at 2020-10-26 15:02:53 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:20.382: INFO: 	Container calico-node ready: true, restart count 0
Oct 26 17:51:20.382: INFO: calico-typha-d497c4cc8-6p86s from kube-system started at 2020-10-26 15:03:16 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:20.382: INFO: 	Container calico-typha ready: true, restart count 0
Oct 26 17:51:20.382: INFO: coredns-658bf88df8-pn52m from kube-system started at 2020-10-26 15:11:47 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:20.382: INFO: 	Container coredns ready: true, restart count 0
Oct 26 17:51:20.382: INFO: coredns-658bf88df8-q4587 from kube-system started at 2020-10-26 17:45:31 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:20.382: INFO: 	Container coredns ready: true, restart count 0
Oct 26 17:51:20.382: INFO: coredns-autoscaler-65b4b99bb7-5p2hq from kube-system started at 2020-10-26 17:45:31 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:20.382: INFO: 	Container autoscaler ready: true, restart count 0
Oct 26 17:51:20.382: INFO: ibm-keepalived-watcher-drkhn from kube-system started at 2020-10-26 15:02:53 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:20.382: INFO: 	Container keepalived-watcher ready: true, restart count 0
Oct 26 17:51:20.382: INFO: ibm-master-proxy-static-10.112.67.207 from kube-system started at 2020-10-26 15:02:38 +0000 UTC (2 container statuses recorded)
Oct 26 17:51:20.382: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Oct 26 17:51:20.382: INFO: 	Container pause ready: true, restart count 0
Oct 26 17:51:20.382: INFO: kubernetes-dashboard-7c8884c686-tqpxt from kube-system started at 2020-10-26 17:45:31 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:20.382: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Oct 26 17:51:20.382: INFO: metrics-server-66957c64f5-kdpjh from kube-system started at 2020-10-26 17:45:31 +0000 UTC (2 container statuses recorded)
Oct 26 17:51:20.382: INFO: 	Container metrics-server ready: true, restart count 0
Oct 26 17:51:20.382: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Oct 26 17:51:20.382: INFO: public-crbube2gcl0spv5v02m2cg-alb1-69897d7d5-rh4l5 from kube-system started at 2020-10-26 15:07:34 +0000 UTC (4 container statuses recorded)
Oct 26 17:51:20.382: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Oct 26 17:51:20.382: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Oct 26 17:51:20.382: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Oct 26 17:51:20.382: INFO: 	Container nginx-ingress ready: true, restart count 0
Oct 26 17:51:20.382: INFO: vpn-7d76994fc5-n94t8 from kube-system started at 2020-10-26 17:45:31 +0000 UTC (1 container statuses recorded)
Oct 26 17:51:20.383: INFO: 	Container vpn ready: true, restart count 0
Oct 26 17:51:20.383: INFO: sonobuoy-systemd-logs-daemon-set-4f27bf473b6a4e03-pnpc9 from sonobuoy started at 2020-10-26 16:37:14 +0000 UTC (2 container statuses recorded)
Oct 26 17:51:20.383: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Oct 26 17:51:20.383: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16419c31e76de142], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:51:21.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8164" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":305,"completed":216,"skipped":3439,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:51:21.514: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6232
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service nodeport-service with the type=NodePort in namespace services-6232
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-6232
STEP: creating replication controller externalsvc in namespace services-6232
I1026 17:51:21.859128      27 runners.go:190] Created replication controller with name: externalsvc, namespace: services-6232, replica count: 2
I1026 17:51:24.909611      27 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Oct 26 17:51:25.019: INFO: Creating new exec pod
Oct 26 17:51:29.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-6232 execpodsx6cd -- /bin/sh -x -c nslookup nodeport-service.services-6232.svc.cluster.local'
Oct 26 17:51:29.463: INFO: stderr: "+ nslookup nodeport-service.services-6232.svc.cluster.local\n"
Oct 26 17:51:29.463: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-6232.svc.cluster.local\tcanonical name = externalsvc.services-6232.svc.cluster.local.\nName:\texternalsvc.services-6232.svc.cluster.local\nAddress: 172.21.40.177\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6232, will wait for the garbage collector to delete the pods
Oct 26 17:51:29.568: INFO: Deleting ReplicationController externalsvc took: 40.592364ms
Oct 26 17:51:29.668: INFO: Terminating ReplicationController externalsvc pods took: 100.282219ms
Oct 26 17:51:41.959: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:51:42.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6232" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:20.560 seconds]
[sig-network] Services
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":305,"completed":217,"skipped":3446,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:51:42.076: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2129
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:51:49.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2129" for this suite.

• [SLOW TEST:7.308 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":305,"completed":218,"skipped":3469,"failed":0}
S
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:51:49.385: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6488
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6488.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6488.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6488.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6488.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6488.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6488.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct 26 17:51:53.924: INFO: DNS probes using dns-6488/dns-test-a4281827-c8ef-4178-955d-e8a472359d0f succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:51:53.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6488" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":305,"completed":219,"skipped":3470,"failed":0}

------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:51:54.012: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9139
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:51:58.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9139" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":305,"completed":220,"skipped":3470,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:51:58.351: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5914
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-82e4c816-ea44-49de-9ee5-4defe71d32eb
STEP: Creating a pod to test consume secrets
Oct 26 17:51:58.624: INFO: Waiting up to 5m0s for pod "pod-secrets-e27c002e-d93e-40d4-88e5-1b6db59b6fb3" in namespace "secrets-5914" to be "Succeeded or Failed"
Oct 26 17:51:58.636: INFO: Pod "pod-secrets-e27c002e-d93e-40d4-88e5-1b6db59b6fb3": Phase="Pending", Reason="", readiness=false. Elapsed: 11.760382ms
Oct 26 17:52:00.650: INFO: Pod "pod-secrets-e27c002e-d93e-40d4-88e5-1b6db59b6fb3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025714154s
Oct 26 17:52:02.663: INFO: Pod "pod-secrets-e27c002e-d93e-40d4-88e5-1b6db59b6fb3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03884946s
STEP: Saw pod success
Oct 26 17:52:02.663: INFO: Pod "pod-secrets-e27c002e-d93e-40d4-88e5-1b6db59b6fb3" satisfied condition "Succeeded or Failed"
Oct 26 17:52:02.673: INFO: Trying to get logs from node 10.112.67.203 pod pod-secrets-e27c002e-d93e-40d4-88e5-1b6db59b6fb3 container secret-env-test: <nil>
STEP: delete the pod
Oct 26 17:52:02.743: INFO: Waiting for pod pod-secrets-e27c002e-d93e-40d4-88e5-1b6db59b6fb3 to disappear
Oct 26 17:52:02.752: INFO: Pod pod-secrets-e27c002e-d93e-40d4-88e5-1b6db59b6fb3 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:52:02.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5914" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":305,"completed":221,"skipped":3509,"failed":0}

------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:52:02.801: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-6515
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 26 17:52:03.228: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:52:04.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6515" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":305,"completed":222,"skipped":3509,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:52:04.700: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9698
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 26 17:52:06.086: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 26 17:52:08.134: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739331526, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739331526, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739331526, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739331526, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 26 17:52:11.210: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 26 17:52:11.244: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8202-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:52:12.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9698" for this suite.
STEP: Destroying namespace "webhook-9698-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.210 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":305,"completed":223,"skipped":3523,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:52:12.911: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9014
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:52:30.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9014" for this suite.

• [SLOW TEST:17.452 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":305,"completed":224,"skipped":3558,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:52:30.364: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1224
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Oct 26 17:52:33.692: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:52:33.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1224" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":225,"skipped":3573,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:52:33.786: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-2732
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 26 17:52:34.034: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-a420eec0-bc35-44b1-8351-93406903761e" in namespace "security-context-test-2732" to be "Succeeded or Failed"
Oct 26 17:52:34.045: INFO: Pod "busybox-privileged-false-a420eec0-bc35-44b1-8351-93406903761e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.783298ms
Oct 26 17:52:36.128: INFO: Pod "busybox-privileged-false-a420eec0-bc35-44b1-8351-93406903761e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.094014469s
Oct 26 17:52:36.128: INFO: Pod "busybox-privileged-false-a420eec0-bc35-44b1-8351-93406903761e" satisfied condition "Succeeded or Failed"
Oct 26 17:52:36.159: INFO: Got logs for pod "busybox-privileged-false-a420eec0-bc35-44b1-8351-93406903761e": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:52:36.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2732" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":226,"skipped":3590,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:52:36.205: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-38
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Oct 26 17:52:36.566: INFO: Number of nodes with available pods: 0
Oct 26 17:52:36.566: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 17:52:37.602: INFO: Number of nodes with available pods: 0
Oct 26 17:52:37.602: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 17:52:38.595: INFO: Number of nodes with available pods: 0
Oct 26 17:52:38.595: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 17:52:39.595: INFO: Number of nodes with available pods: 3
Oct 26 17:52:39.595: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Oct 26 17:52:39.710: INFO: Number of nodes with available pods: 2
Oct 26 17:52:39.710: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 17:52:40.740: INFO: Number of nodes with available pods: 2
Oct 26 17:52:40.740: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 17:52:41.752: INFO: Number of nodes with available pods: 2
Oct 26 17:52:41.752: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 17:52:42.743: INFO: Number of nodes with available pods: 2
Oct 26 17:52:42.743: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 17:52:43.752: INFO: Number of nodes with available pods: 2
Oct 26 17:52:43.752: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 17:52:44.740: INFO: Number of nodes with available pods: 2
Oct 26 17:52:44.740: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 17:52:45.739: INFO: Number of nodes with available pods: 2
Oct 26 17:52:45.739: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 17:52:46.739: INFO: Number of nodes with available pods: 2
Oct 26 17:52:46.739: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 17:52:47.738: INFO: Number of nodes with available pods: 2
Oct 26 17:52:47.738: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 17:52:48.743: INFO: Number of nodes with available pods: 2
Oct 26 17:52:48.743: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 17:52:49.743: INFO: Number of nodes with available pods: 2
Oct 26 17:52:49.743: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 17:52:50.761: INFO: Number of nodes with available pods: 2
Oct 26 17:52:50.761: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 17:52:51.742: INFO: Number of nodes with available pods: 2
Oct 26 17:52:51.742: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 17:52:52.740: INFO: Number of nodes with available pods: 2
Oct 26 17:52:52.740: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 17:52:53.743: INFO: Number of nodes with available pods: 2
Oct 26 17:52:53.743: INFO: Node 10.112.67.201 is running more than one daemon pod
Oct 26 17:52:54.737: INFO: Number of nodes with available pods: 3
Oct 26 17:52:54.737: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-38, will wait for the garbage collector to delete the pods
Oct 26 17:52:54.839: INFO: Deleting DaemonSet.extensions daemon-set took: 29.651266ms
Oct 26 17:52:54.939: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.362649ms
Oct 26 17:53:08.054: INFO: Number of nodes with available pods: 0
Oct 26 17:53:08.054: INFO: Number of running nodes: 0, number of available pods: 0
Oct 26 17:53:08.064: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-38/daemonsets","resourceVersion":"53471"},"items":null}

Oct 26 17:53:08.087: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-38/pods","resourceVersion":"53471"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:53:08.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-38" for this suite.

• [SLOW TEST:31.985 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":305,"completed":227,"skipped":3608,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:53:08.198: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-9324
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 26 17:53:08.443: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:53:09.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9324" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":305,"completed":228,"skipped":3659,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:53:09.358: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4469
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-614cf1eb-a4c0-4bbb-b025-778738feb672
STEP: Creating a pod to test consume secrets
Oct 26 17:53:09.632: INFO: Waiting up to 5m0s for pod "pod-secrets-03d42ac0-14fd-4ff6-86c9-460ee283874b" in namespace "secrets-4469" to be "Succeeded or Failed"
Oct 26 17:53:09.643: INFO: Pod "pod-secrets-03d42ac0-14fd-4ff6-86c9-460ee283874b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.203974ms
Oct 26 17:53:11.654: INFO: Pod "pod-secrets-03d42ac0-14fd-4ff6-86c9-460ee283874b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02205335s
Oct 26 17:53:13.668: INFO: Pod "pod-secrets-03d42ac0-14fd-4ff6-86c9-460ee283874b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03555369s
STEP: Saw pod success
Oct 26 17:53:13.668: INFO: Pod "pod-secrets-03d42ac0-14fd-4ff6-86c9-460ee283874b" satisfied condition "Succeeded or Failed"
Oct 26 17:53:13.680: INFO: Trying to get logs from node 10.112.67.203 pod pod-secrets-03d42ac0-14fd-4ff6-86c9-460ee283874b container secret-volume-test: <nil>
STEP: delete the pod
Oct 26 17:53:13.797: INFO: Waiting for pod pod-secrets-03d42ac0-14fd-4ff6-86c9-460ee283874b to disappear
Oct 26 17:53:13.808: INFO: Pod pod-secrets-03d42ac0-14fd-4ff6-86c9-460ee283874b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:53:13.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4469" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":229,"skipped":3743,"failed":0}
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:53:13.870: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6118
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-6db88380-494c-4e55-bdbd-49d4beae1f33
STEP: Creating a pod to test consume secrets
Oct 26 17:53:14.152: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4850ef90-29db-4281-8143-f70d68805463" in namespace "projected-6118" to be "Succeeded or Failed"
Oct 26 17:53:14.162: INFO: Pod "pod-projected-secrets-4850ef90-29db-4281-8143-f70d68805463": Phase="Pending", Reason="", readiness=false. Elapsed: 9.755883ms
Oct 26 17:53:16.175: INFO: Pod "pod-projected-secrets-4850ef90-29db-4281-8143-f70d68805463": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022712252s
STEP: Saw pod success
Oct 26 17:53:16.175: INFO: Pod "pod-projected-secrets-4850ef90-29db-4281-8143-f70d68805463" satisfied condition "Succeeded or Failed"
Oct 26 17:53:16.200: INFO: Trying to get logs from node 10.112.67.203 pod pod-projected-secrets-4850ef90-29db-4281-8143-f70d68805463 container projected-secret-volume-test: <nil>
STEP: delete the pod
Oct 26 17:53:16.293: INFO: Waiting for pod pod-projected-secrets-4850ef90-29db-4281-8143-f70d68805463 to disappear
Oct 26 17:53:16.306: INFO: Pod pod-projected-secrets-4850ef90-29db-4281-8143-f70d68805463 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:53:16.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6118" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":230,"skipped":3744,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:53:16.351: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5507
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create deployment with httpd image
Oct 26 17:53:16.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 create -f -'
Oct 26 17:53:17.053: INFO: stderr: ""
Oct 26 17:53:17.053: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Oct 26 17:53:17.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 diff -f -'
Oct 26 17:53:17.737: INFO: rc: 1
Oct 26 17:53:17.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 delete -f -'
Oct 26 17:53:17.915: INFO: stderr: ""
Oct 26 17:53:17.915: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:53:17.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5507" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":305,"completed":231,"skipped":3754,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:53:17.959: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-606
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating replication controller my-hostname-basic-b52e0c97-cc1b-4b01-8074-3affb22d1450
Oct 26 17:53:18.220: INFO: Pod name my-hostname-basic-b52e0c97-cc1b-4b01-8074-3affb22d1450: Found 0 pods out of 1
Oct 26 17:53:23.235: INFO: Pod name my-hostname-basic-b52e0c97-cc1b-4b01-8074-3affb22d1450: Found 1 pods out of 1
Oct 26 17:53:23.235: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-b52e0c97-cc1b-4b01-8074-3affb22d1450" are running
Oct 26 17:53:23.250: INFO: Pod "my-hostname-basic-b52e0c97-cc1b-4b01-8074-3affb22d1450-kss56" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-10-26 17:53:18 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-10-26 17:53:19 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-10-26 17:53:19 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-10-26 17:53:18 +0000 UTC Reason: Message:}])
Oct 26 17:53:23.250: INFO: Trying to dial the pod
Oct 26 17:53:28.306: INFO: Controller my-hostname-basic-b52e0c97-cc1b-4b01-8074-3affb22d1450: Got expected result from replica 1 [my-hostname-basic-b52e0c97-cc1b-4b01-8074-3affb22d1450-kss56]: "my-hostname-basic-b52e0c97-cc1b-4b01-8074-3affb22d1450-kss56", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:53:28.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-606" for this suite.

• [SLOW TEST:10.401 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":305,"completed":232,"skipped":3806,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:53:28.366: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7299
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 26 17:53:28.591: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Oct 26 17:53:33.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 --namespace=crd-publish-openapi-7299 create -f -'
Oct 26 17:53:34.210: INFO: stderr: ""
Oct 26 17:53:34.210: INFO: stdout: "e2e-test-crd-publish-openapi-6934-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Oct 26 17:53:34.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 --namespace=crd-publish-openapi-7299 delete e2e-test-crd-publish-openapi-6934-crds test-cr'
Oct 26 17:53:34.374: INFO: stderr: ""
Oct 26 17:53:34.374: INFO: stdout: "e2e-test-crd-publish-openapi-6934-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Oct 26 17:53:34.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 --namespace=crd-publish-openapi-7299 apply -f -'
Oct 26 17:53:35.071: INFO: stderr: ""
Oct 26 17:53:35.071: INFO: stdout: "e2e-test-crd-publish-openapi-6934-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Oct 26 17:53:35.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 --namespace=crd-publish-openapi-7299 delete e2e-test-crd-publish-openapi-6934-crds test-cr'
Oct 26 17:53:35.251: INFO: stderr: ""
Oct 26 17:53:35.251: INFO: stdout: "e2e-test-crd-publish-openapi-6934-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Oct 26 17:53:35.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 explain e2e-test-crd-publish-openapi-6934-crds'
Oct 26 17:53:35.545: INFO: stderr: ""
Oct 26 17:53:35.545: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6934-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:53:41.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7299" for this suite.

• [SLOW TEST:13.573 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":305,"completed":233,"skipped":3833,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:53:41.946: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2516
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2516
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-2516
I1026 17:53:42.279840      27 runners.go:190] Created replication controller with name: externalname-service, namespace: services-2516, replica count: 2
Oct 26 17:53:45.330: INFO: Creating new exec pod
I1026 17:53:45.330364      27 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 26 17:53:48.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-2516 execpod847l7 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Oct 26 17:53:48.776: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Oct 26 17:53:48.776: INFO: stdout: ""
Oct 26 17:53:48.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-2516 execpod847l7 -- /bin/sh -x -c nc -zv -t -w 2 172.21.236.161 80'
Oct 26 17:53:49.141: INFO: stderr: "+ nc -zv -t -w 2 172.21.236.161 80\nConnection to 172.21.236.161 80 port [tcp/http] succeeded!\n"
Oct 26 17:53:49.141: INFO: stdout: ""
Oct 26 17:53:49.141: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:53:49.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2516" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:7.348 seconds]
[sig-network] Services
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":305,"completed":234,"skipped":3844,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:53:49.295: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1658
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Oct 26 17:53:49.555: INFO: Waiting up to 5m0s for pod "pod-4a9d47fe-85bf-4006-b37a-39fff5b36428" in namespace "emptydir-1658" to be "Succeeded or Failed"
Oct 26 17:53:49.566: INFO: Pod "pod-4a9d47fe-85bf-4006-b37a-39fff5b36428": Phase="Pending", Reason="", readiness=false. Elapsed: 11.269095ms
Oct 26 17:53:51.578: INFO: Pod "pod-4a9d47fe-85bf-4006-b37a-39fff5b36428": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022738559s
Oct 26 17:53:53.590: INFO: Pod "pod-4a9d47fe-85bf-4006-b37a-39fff5b36428": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035076628s
STEP: Saw pod success
Oct 26 17:53:53.590: INFO: Pod "pod-4a9d47fe-85bf-4006-b37a-39fff5b36428" satisfied condition "Succeeded or Failed"
Oct 26 17:53:53.602: INFO: Trying to get logs from node 10.112.67.203 pod pod-4a9d47fe-85bf-4006-b37a-39fff5b36428 container test-container: <nil>
STEP: delete the pod
Oct 26 17:53:53.693: INFO: Waiting for pod pod-4a9d47fe-85bf-4006-b37a-39fff5b36428 to disappear
Oct 26 17:53:53.713: INFO: Pod pod-4a9d47fe-85bf-4006-b37a-39fff5b36428 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:53:53.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1658" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":235,"skipped":3872,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:53:53.760: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-963
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:54:10.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-963" for this suite.

• [SLOW TEST:16.574 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":305,"completed":236,"skipped":3891,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:54:10.335: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4568
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Oct 26 17:54:10.587: INFO: Waiting up to 5m0s for pod "downwardapi-volume-307d4225-2b03-41e8-842c-193a4346e6c6" in namespace "downward-api-4568" to be "Succeeded or Failed"
Oct 26 17:54:10.597: INFO: Pod "downwardapi-volume-307d4225-2b03-41e8-842c-193a4346e6c6": Phase="Pending", Reason="", readiness=false. Elapsed: 9.938168ms
Oct 26 17:54:12.611: INFO: Pod "downwardapi-volume-307d4225-2b03-41e8-842c-193a4346e6c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02352914s
Oct 26 17:54:14.628: INFO: Pod "downwardapi-volume-307d4225-2b03-41e8-842c-193a4346e6c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0411783s
STEP: Saw pod success
Oct 26 17:54:14.629: INFO: Pod "downwardapi-volume-307d4225-2b03-41e8-842c-193a4346e6c6" satisfied condition "Succeeded or Failed"
Oct 26 17:54:14.643: INFO: Trying to get logs from node 10.112.67.203 pod downwardapi-volume-307d4225-2b03-41e8-842c-193a4346e6c6 container client-container: <nil>
STEP: delete the pod
Oct 26 17:54:14.725: INFO: Waiting for pod downwardapi-volume-307d4225-2b03-41e8-842c-193a4346e6c6 to disappear
Oct 26 17:54:14.737: INFO: Pod downwardapi-volume-307d4225-2b03-41e8-842c-193a4346e6c6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:54:14.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4568" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":305,"completed":237,"skipped":3916,"failed":0}
SS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:54:14.783: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-7017
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 26 17:54:15.215: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: creating replication controller svc-latency-rc in namespace svc-latency-7017
I1026 17:54:15.245196      27 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7017, replica count: 1
I1026 17:54:16.295585      27 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1026 17:54:17.295988      27 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 26 17:54:17.434: INFO: Created: latency-svc-bb6jx
Oct 26 17:54:17.450: INFO: Got endpoints: latency-svc-bb6jx [54.13622ms]
Oct 26 17:54:17.491: INFO: Created: latency-svc-xmjhv
Oct 26 17:54:17.504: INFO: Got endpoints: latency-svc-xmjhv [53.743012ms]
Oct 26 17:54:17.525: INFO: Created: latency-svc-48zrt
Oct 26 17:54:17.537: INFO: Got endpoints: latency-svc-48zrt [87.065334ms]
Oct 26 17:54:17.550: INFO: Created: latency-svc-rcpv5
Oct 26 17:54:17.573: INFO: Got endpoints: latency-svc-rcpv5 [122.021343ms]
Oct 26 17:54:17.603: INFO: Created: latency-svc-h5gd2
Oct 26 17:54:17.615: INFO: Got endpoints: latency-svc-h5gd2 [164.274175ms]
Oct 26 17:54:17.625: INFO: Created: latency-svc-pnvb8
Oct 26 17:54:17.640: INFO: Got endpoints: latency-svc-pnvb8 [188.866026ms]
Oct 26 17:54:17.659: INFO: Created: latency-svc-44np9
Oct 26 17:54:17.676: INFO: Got endpoints: latency-svc-44np9 [224.946689ms]
Oct 26 17:54:17.700: INFO: Created: latency-svc-9njzk
Oct 26 17:54:17.702: INFO: Got endpoints: latency-svc-9njzk [251.314825ms]
Oct 26 17:54:17.707: INFO: Created: latency-svc-gtrtn
Oct 26 17:54:17.726: INFO: Got endpoints: latency-svc-gtrtn [274.979822ms]
Oct 26 17:54:17.735: INFO: Created: latency-svc-4nmxm
Oct 26 17:54:17.749: INFO: Got endpoints: latency-svc-4nmxm [297.925408ms]
Oct 26 17:54:17.770: INFO: Created: latency-svc-dkd22
Oct 26 17:54:17.798: INFO: Got endpoints: latency-svc-dkd22 [346.487504ms]
Oct 26 17:54:17.814: INFO: Created: latency-svc-mqkhp
Oct 26 17:54:17.827: INFO: Got endpoints: latency-svc-mqkhp [375.745607ms]
Oct 26 17:54:17.840: INFO: Created: latency-svc-v6gqj
Oct 26 17:54:17.855: INFO: Got endpoints: latency-svc-v6gqj [403.836888ms]
Oct 26 17:54:17.869: INFO: Created: latency-svc-6s7b5
Oct 26 17:54:17.887: INFO: Got endpoints: latency-svc-6s7b5 [435.584442ms]
Oct 26 17:54:17.895: INFO: Created: latency-svc-5msgt
Oct 26 17:54:17.910: INFO: Got endpoints: latency-svc-5msgt [459.657998ms]
Oct 26 17:54:17.933: INFO: Created: latency-svc-qjtp4
Oct 26 17:54:17.946: INFO: Got endpoints: latency-svc-qjtp4 [495.261268ms]
Oct 26 17:54:17.946: INFO: Created: latency-svc-xflhd
Oct 26 17:54:17.969: INFO: Got endpoints: latency-svc-xflhd [464.846086ms]
Oct 26 17:54:17.971: INFO: Created: latency-svc-7fd7l
Oct 26 17:54:17.986: INFO: Got endpoints: latency-svc-7fd7l [448.631272ms]
Oct 26 17:54:17.997: INFO: Created: latency-svc-fcgc6
Oct 26 17:54:18.012: INFO: Got endpoints: latency-svc-fcgc6 [438.965001ms]
Oct 26 17:54:18.022: INFO: Created: latency-svc-9t75g
Oct 26 17:54:18.031: INFO: Got endpoints: latency-svc-9t75g [416.365181ms]
Oct 26 17:54:18.045: INFO: Created: latency-svc-fzdzv
Oct 26 17:54:18.058: INFO: Got endpoints: latency-svc-fzdzv [418.079567ms]
Oct 26 17:54:18.075: INFO: Created: latency-svc-dtgnw
Oct 26 17:54:18.093: INFO: Got endpoints: latency-svc-dtgnw [417.347475ms]
Oct 26 17:54:18.118: INFO: Created: latency-svc-w4w6k
Oct 26 17:54:18.128: INFO: Got endpoints: latency-svc-w4w6k [425.278627ms]
Oct 26 17:54:18.133: INFO: Created: latency-svc-bk6b8
Oct 26 17:54:18.149: INFO: Got endpoints: latency-svc-bk6b8 [56.020444ms]
Oct 26 17:54:18.153: INFO: Created: latency-svc-ttn5l
Oct 26 17:54:18.169: INFO: Got endpoints: latency-svc-ttn5l [442.617052ms]
Oct 26 17:54:18.187: INFO: Created: latency-svc-c2cgn
Oct 26 17:54:18.198: INFO: Got endpoints: latency-svc-c2cgn [449.505487ms]
Oct 26 17:54:18.220: INFO: Created: latency-svc-zlbvw
Oct 26 17:54:18.230: INFO: Got endpoints: latency-svc-zlbvw [432.520504ms]
Oct 26 17:54:18.251: INFO: Created: latency-svc-576c7
Oct 26 17:54:18.260: INFO: Got endpoints: latency-svc-576c7 [433.321472ms]
Oct 26 17:54:18.273: INFO: Created: latency-svc-tgb7t
Oct 26 17:54:18.289: INFO: Got endpoints: latency-svc-tgb7t [434.070729ms]
Oct 26 17:54:18.306: INFO: Created: latency-svc-79mdt
Oct 26 17:54:18.318: INFO: Got endpoints: latency-svc-79mdt [431.305453ms]
Oct 26 17:54:18.326: INFO: Created: latency-svc-f8vwx
Oct 26 17:54:18.341: INFO: Got endpoints: latency-svc-f8vwx [430.497581ms]
Oct 26 17:54:18.352: INFO: Created: latency-svc-f6vdr
Oct 26 17:54:18.366: INFO: Got endpoints: latency-svc-f6vdr [419.967374ms]
Oct 26 17:54:18.378: INFO: Created: latency-svc-fd77p
Oct 26 17:54:18.391: INFO: Got endpoints: latency-svc-fd77p [421.660771ms]
Oct 26 17:54:18.403: INFO: Created: latency-svc-2np8b
Oct 26 17:54:18.418: INFO: Got endpoints: latency-svc-2np8b [432.143294ms]
Oct 26 17:54:18.427: INFO: Created: latency-svc-ctxfj
Oct 26 17:54:18.447: INFO: Got endpoints: latency-svc-ctxfj [435.857816ms]
Oct 26 17:54:18.460: INFO: Created: latency-svc-w95rl
Oct 26 17:54:18.474: INFO: Got endpoints: latency-svc-w95rl [442.795341ms]
Oct 26 17:54:18.489: INFO: Created: latency-svc-8cn7d
Oct 26 17:54:18.513: INFO: Got endpoints: latency-svc-8cn7d [455.268311ms]
Oct 26 17:54:18.525: INFO: Created: latency-svc-gr8lz
Oct 26 17:54:18.535: INFO: Got endpoints: latency-svc-gr8lz [407.213166ms]
Oct 26 17:54:18.564: INFO: Created: latency-svc-4zp5z
Oct 26 17:54:18.583: INFO: Got endpoints: latency-svc-4zp5z [433.646056ms]
Oct 26 17:54:18.588: INFO: Created: latency-svc-hkbp9
Oct 26 17:54:18.604: INFO: Got endpoints: latency-svc-hkbp9 [435.590839ms]
Oct 26 17:54:18.629: INFO: Created: latency-svc-m7kjw
Oct 26 17:54:18.630: INFO: Got endpoints: latency-svc-m7kjw [431.092654ms]
Oct 26 17:54:18.637: INFO: Created: latency-svc-szt6p
Oct 26 17:54:18.653: INFO: Got endpoints: latency-svc-szt6p [422.462548ms]
Oct 26 17:54:18.663: INFO: Created: latency-svc-v4pt4
Oct 26 17:54:18.689: INFO: Got endpoints: latency-svc-v4pt4 [428.622392ms]
Oct 26 17:54:18.691: INFO: Created: latency-svc-f5zwk
Oct 26 17:54:18.714: INFO: Got endpoints: latency-svc-f5zwk [425.197657ms]
Oct 26 17:54:18.715: INFO: Created: latency-svc-dm249
Oct 26 17:54:18.728: INFO: Got endpoints: latency-svc-dm249 [409.784339ms]
Oct 26 17:54:18.743: INFO: Created: latency-svc-46jhv
Oct 26 17:54:18.756: INFO: Got endpoints: latency-svc-46jhv [415.028679ms]
Oct 26 17:54:18.775: INFO: Created: latency-svc-d8qxl
Oct 26 17:54:18.783: INFO: Got endpoints: latency-svc-d8qxl [416.537274ms]
Oct 26 17:54:18.788: INFO: Created: latency-svc-n29vm
Oct 26 17:54:18.802: INFO: Got endpoints: latency-svc-n29vm [411.248164ms]
Oct 26 17:54:18.815: INFO: Created: latency-svc-d8dz2
Oct 26 17:54:18.829: INFO: Got endpoints: latency-svc-d8dz2 [410.686976ms]
Oct 26 17:54:18.836: INFO: Created: latency-svc-h6f7b
Oct 26 17:54:18.858: INFO: Got endpoints: latency-svc-h6f7b [410.359302ms]
Oct 26 17:54:18.879: INFO: Created: latency-svc-vbfdx
Oct 26 17:54:18.895: INFO: Got endpoints: latency-svc-vbfdx [420.977596ms]
Oct 26 17:54:18.926: INFO: Created: latency-svc-hclzv
Oct 26 17:54:18.944: INFO: Got endpoints: latency-svc-hclzv [430.858288ms]
Oct 26 17:54:18.953: INFO: Created: latency-svc-s9zlk
Oct 26 17:54:18.976: INFO: Got endpoints: latency-svc-s9zlk [441.397303ms]
Oct 26 17:54:19.001: INFO: Created: latency-svc-bjshn
Oct 26 17:54:19.012: INFO: Got endpoints: latency-svc-bjshn [428.395417ms]
Oct 26 17:54:19.027: INFO: Created: latency-svc-nfpbc
Oct 26 17:54:19.037: INFO: Got endpoints: latency-svc-nfpbc [432.864625ms]
Oct 26 17:54:19.053: INFO: Created: latency-svc-zc9wq
Oct 26 17:54:19.064: INFO: Got endpoints: latency-svc-zc9wq [434.617712ms]
Oct 26 17:54:19.087: INFO: Created: latency-svc-qr2z6
Oct 26 17:54:19.099: INFO: Got endpoints: latency-svc-qr2z6 [446.119685ms]
Oct 26 17:54:19.111: INFO: Created: latency-svc-7ztpz
Oct 26 17:54:19.127: INFO: Got endpoints: latency-svc-7ztpz [437.908822ms]
Oct 26 17:54:19.136: INFO: Created: latency-svc-25t7n
Oct 26 17:54:19.145: INFO: Got endpoints: latency-svc-25t7n [430.166957ms]
Oct 26 17:54:19.167: INFO: Created: latency-svc-6cr99
Oct 26 17:54:19.180: INFO: Got endpoints: latency-svc-6cr99 [451.786078ms]
Oct 26 17:54:19.189: INFO: Created: latency-svc-fgt7r
Oct 26 17:54:19.199: INFO: Got endpoints: latency-svc-fgt7r [442.645782ms]
Oct 26 17:54:19.224: INFO: Created: latency-svc-nztfs
Oct 26 17:54:19.238: INFO: Got endpoints: latency-svc-nztfs [455.744126ms]
Oct 26 17:54:19.259: INFO: Created: latency-svc-vs6kp
Oct 26 17:54:19.264: INFO: Got endpoints: latency-svc-vs6kp [461.986385ms]
Oct 26 17:54:19.274: INFO: Created: latency-svc-wl7qx
Oct 26 17:54:19.299: INFO: Got endpoints: latency-svc-wl7qx [470.111255ms]
Oct 26 17:54:19.308: INFO: Created: latency-svc-d4djw
Oct 26 17:54:19.315: INFO: Got endpoints: latency-svc-d4djw [457.38915ms]
Oct 26 17:54:19.332: INFO: Created: latency-svc-ppbtr
Oct 26 17:54:19.346: INFO: Got endpoints: latency-svc-ppbtr [450.252142ms]
Oct 26 17:54:19.357: INFO: Created: latency-svc-4swmv
Oct 26 17:54:19.376: INFO: Got endpoints: latency-svc-4swmv [431.947153ms]
Oct 26 17:54:19.388: INFO: Created: latency-svc-vvk5b
Oct 26 17:54:19.402: INFO: Got endpoints: latency-svc-vvk5b [425.295061ms]
Oct 26 17:54:19.410: INFO: Created: latency-svc-s8qmh
Oct 26 17:54:19.421: INFO: Got endpoints: latency-svc-s8qmh [409.426762ms]
Oct 26 17:54:19.433: INFO: Created: latency-svc-jkrjz
Oct 26 17:54:19.453: INFO: Got endpoints: latency-svc-jkrjz [415.345078ms]
Oct 26 17:54:19.462: INFO: Created: latency-svc-kxqkl
Oct 26 17:54:19.481: INFO: Got endpoints: latency-svc-kxqkl [417.233544ms]
Oct 26 17:54:19.494: INFO: Created: latency-svc-qk427
Oct 26 17:54:19.506: INFO: Got endpoints: latency-svc-qk427 [406.480021ms]
Oct 26 17:54:19.524: INFO: Created: latency-svc-6w9kl
Oct 26 17:54:19.536: INFO: Got endpoints: latency-svc-6w9kl [409.130683ms]
Oct 26 17:54:19.551: INFO: Created: latency-svc-xtgtd
Oct 26 17:54:19.566: INFO: Got endpoints: latency-svc-xtgtd [421.287674ms]
Oct 26 17:54:19.576: INFO: Created: latency-svc-cp25m
Oct 26 17:54:19.595: INFO: Got endpoints: latency-svc-cp25m [415.517159ms]
Oct 26 17:54:19.610: INFO: Created: latency-svc-ft2c8
Oct 26 17:54:19.621: INFO: Got endpoints: latency-svc-ft2c8 [421.878863ms]
Oct 26 17:54:19.627: INFO: Created: latency-svc-8nxg9
Oct 26 17:54:19.653: INFO: Got endpoints: latency-svc-8nxg9 [414.435257ms]
Oct 26 17:54:19.667: INFO: Created: latency-svc-pqks7
Oct 26 17:54:19.684: INFO: Got endpoints: latency-svc-pqks7 [419.773848ms]
Oct 26 17:54:19.716: INFO: Created: latency-svc-pjd9v
Oct 26 17:54:19.731: INFO: Got endpoints: latency-svc-pjd9v [431.136653ms]
Oct 26 17:54:19.743: INFO: Created: latency-svc-pm7k6
Oct 26 17:54:19.759: INFO: Got endpoints: latency-svc-pm7k6 [444.065032ms]
Oct 26 17:54:19.793: INFO: Created: latency-svc-7dqgd
Oct 26 17:54:19.820: INFO: Got endpoints: latency-svc-7dqgd [474.354641ms]
Oct 26 17:54:19.852: INFO: Created: latency-svc-58bts
Oct 26 17:54:19.860: INFO: Got endpoints: latency-svc-58bts [483.361593ms]
Oct 26 17:54:19.870: INFO: Created: latency-svc-lk6fr
Oct 26 17:54:19.889: INFO: Got endpoints: latency-svc-lk6fr [487.680451ms]
Oct 26 17:54:19.901: INFO: Created: latency-svc-6s88g
Oct 26 17:54:19.914: INFO: Got endpoints: latency-svc-6s88g [492.578981ms]
Oct 26 17:54:19.932: INFO: Created: latency-svc-4z78r
Oct 26 17:54:19.949: INFO: Got endpoints: latency-svc-4z78r [496.645068ms]
Oct 26 17:54:19.973: INFO: Created: latency-svc-txv8z
Oct 26 17:54:19.990: INFO: Got endpoints: latency-svc-txv8z [508.566208ms]
Oct 26 17:54:20.000: INFO: Created: latency-svc-rwnq9
Oct 26 17:54:20.022: INFO: Got endpoints: latency-svc-rwnq9 [516.478901ms]
Oct 26 17:54:20.035: INFO: Created: latency-svc-vzw2p
Oct 26 17:54:20.058: INFO: Got endpoints: latency-svc-vzw2p [521.329469ms]
Oct 26 17:54:20.065: INFO: Created: latency-svc-rvbd8
Oct 26 17:54:20.079: INFO: Got endpoints: latency-svc-rvbd8 [513.27057ms]
Oct 26 17:54:20.104: INFO: Created: latency-svc-tnm4c
Oct 26 17:54:20.114: INFO: Got endpoints: latency-svc-tnm4c [518.442015ms]
Oct 26 17:54:20.137: INFO: Created: latency-svc-rr5h8
Oct 26 17:54:20.151: INFO: Got endpoints: latency-svc-rr5h8 [529.977157ms]
Oct 26 17:54:20.165: INFO: Created: latency-svc-w98h8
Oct 26 17:54:20.181: INFO: Got endpoints: latency-svc-w98h8 [527.836851ms]
Oct 26 17:54:20.205: INFO: Created: latency-svc-4twxs
Oct 26 17:54:20.218: INFO: Got endpoints: latency-svc-4twxs [534.40866ms]
Oct 26 17:54:20.247: INFO: Created: latency-svc-wwd8n
Oct 26 17:54:20.262: INFO: Got endpoints: latency-svc-wwd8n [531.09371ms]
Oct 26 17:54:20.282: INFO: Created: latency-svc-v2htt
Oct 26 17:54:20.298: INFO: Got endpoints: latency-svc-v2htt [538.240169ms]
Oct 26 17:54:20.315: INFO: Created: latency-svc-jrtnn
Oct 26 17:54:20.359: INFO: Created: latency-svc-wv2nc
Oct 26 17:54:20.404: INFO: Created: latency-svc-c6cts
Oct 26 17:54:20.445: INFO: Created: latency-svc-8mmnf
Oct 26 17:54:20.479: INFO: Created: latency-svc-bv4vw
Oct 26 17:54:20.485: INFO: Got endpoints: latency-svc-c6cts [595.627596ms]
Oct 26 17:54:20.485: INFO: Got endpoints: latency-svc-jrtnn [664.953099ms]
Oct 26 17:54:20.486: INFO: Got endpoints: latency-svc-wv2nc [626.422002ms]
Oct 26 17:54:20.486: INFO: Got endpoints: latency-svc-8mmnf [572.336595ms]
Oct 26 17:54:20.493: INFO: Got endpoints: latency-svc-bv4vw [543.817655ms]
Oct 26 17:54:20.505: INFO: Created: latency-svc-dzr6p
Oct 26 17:54:20.518: INFO: Got endpoints: latency-svc-dzr6p [528.019255ms]
Oct 26 17:54:20.543: INFO: Created: latency-svc-brxq9
Oct 26 17:54:20.554: INFO: Got endpoints: latency-svc-brxq9 [531.757768ms]
Oct 26 17:54:20.569: INFO: Created: latency-svc-gh47s
Oct 26 17:54:20.581: INFO: Got endpoints: latency-svc-gh47s [523.032012ms]
Oct 26 17:54:20.598: INFO: Created: latency-svc-vqzv4
Oct 26 17:54:20.614: INFO: Got endpoints: latency-svc-vqzv4 [534.445469ms]
Oct 26 17:54:20.630: INFO: Created: latency-svc-mltqb
Oct 26 17:54:20.645: INFO: Got endpoints: latency-svc-mltqb [530.644715ms]
Oct 26 17:54:20.660: INFO: Created: latency-svc-79mlq
Oct 26 17:54:20.671: INFO: Got endpoints: latency-svc-79mlq [520.066825ms]
Oct 26 17:54:20.694: INFO: Created: latency-svc-r64ld
Oct 26 17:54:20.702: INFO: Got endpoints: latency-svc-r64ld [520.598465ms]
Oct 26 17:54:20.727: INFO: Created: latency-svc-lqrfm
Oct 26 17:54:20.741: INFO: Got endpoints: latency-svc-lqrfm [522.383726ms]
Oct 26 17:54:20.758: INFO: Created: latency-svc-xxtjz
Oct 26 17:54:20.781: INFO: Got endpoints: latency-svc-xxtjz [518.439465ms]
Oct 26 17:54:20.792: INFO: Created: latency-svc-98zqx
Oct 26 17:54:20.807: INFO: Got endpoints: latency-svc-98zqx [509.240795ms]
Oct 26 17:54:20.836: INFO: Created: latency-svc-bfhhb
Oct 26 17:54:20.862: INFO: Got endpoints: latency-svc-bfhhb [375.753524ms]
Oct 26 17:54:20.870: INFO: Created: latency-svc-l49sg
Oct 26 17:54:20.890: INFO: Got endpoints: latency-svc-l49sg [403.530928ms]
Oct 26 17:54:20.902: INFO: Created: latency-svc-m4xrx
Oct 26 17:54:20.920: INFO: Got endpoints: latency-svc-m4xrx [434.556665ms]
Oct 26 17:54:20.932: INFO: Created: latency-svc-8pff6
Oct 26 17:54:20.950: INFO: Got endpoints: latency-svc-8pff6 [464.185386ms]
Oct 26 17:54:20.978: INFO: Created: latency-svc-zr7cv
Oct 26 17:54:20.993: INFO: Got endpoints: latency-svc-zr7cv [500.093783ms]
Oct 26 17:54:21.001: INFO: Created: latency-svc-j26nm
Oct 26 17:54:21.015: INFO: Got endpoints: latency-svc-j26nm [496.141575ms]
Oct 26 17:54:21.025: INFO: Created: latency-svc-v86q4
Oct 26 17:54:21.040: INFO: Got endpoints: latency-svc-v86q4 [486.506995ms]
Oct 26 17:54:21.051: INFO: Created: latency-svc-574n5
Oct 26 17:54:21.067: INFO: Got endpoints: latency-svc-574n5 [485.774102ms]
Oct 26 17:54:21.078: INFO: Created: latency-svc-jv4l4
Oct 26 17:54:21.088: INFO: Got endpoints: latency-svc-jv4l4 [474.591323ms]
Oct 26 17:54:21.105: INFO: Created: latency-svc-ssfkw
Oct 26 17:54:21.121: INFO: Got endpoints: latency-svc-ssfkw [476.832665ms]
Oct 26 17:54:21.137: INFO: Created: latency-svc-74g5c
Oct 26 17:54:21.159: INFO: Got endpoints: latency-svc-74g5c [487.702932ms]
Oct 26 17:54:21.165: INFO: Created: latency-svc-zrdqr
Oct 26 17:54:21.179: INFO: Got endpoints: latency-svc-zrdqr [477.341123ms]
Oct 26 17:54:21.209: INFO: Created: latency-svc-mzrnx
Oct 26 17:54:21.209: INFO: Got endpoints: latency-svc-mzrnx [468.615809ms]
Oct 26 17:54:21.214: INFO: Created: latency-svc-z9dcf
Oct 26 17:54:21.227: INFO: Got endpoints: latency-svc-z9dcf [446.169559ms]
Oct 26 17:54:21.239: INFO: Created: latency-svc-25f82
Oct 26 17:54:21.249: INFO: Got endpoints: latency-svc-25f82 [442.080721ms]
Oct 26 17:54:21.264: INFO: Created: latency-svc-nvb6m
Oct 26 17:54:21.275: INFO: Got endpoints: latency-svc-nvb6m [412.988875ms]
Oct 26 17:54:21.300: INFO: Created: latency-svc-zndsq
Oct 26 17:54:21.308: INFO: Got endpoints: latency-svc-zndsq [417.640741ms]
Oct 26 17:54:21.333: INFO: Created: latency-svc-6l8t5
Oct 26 17:54:21.344: INFO: Got endpoints: latency-svc-6l8t5 [423.535123ms]
Oct 26 17:54:21.373: INFO: Created: latency-svc-45h6w
Oct 26 17:54:21.385: INFO: Got endpoints: latency-svc-45h6w [435.211324ms]
Oct 26 17:54:21.391: INFO: Created: latency-svc-45hrc
Oct 26 17:54:21.414: INFO: Got endpoints: latency-svc-45hrc [420.601795ms]
Oct 26 17:54:21.414: INFO: Created: latency-svc-7mlpt
Oct 26 17:54:21.420: INFO: Got endpoints: latency-svc-7mlpt [404.845106ms]
Oct 26 17:54:21.429: INFO: Created: latency-svc-4ls96
Oct 26 17:54:21.443: INFO: Got endpoints: latency-svc-4ls96 [402.293157ms]
Oct 26 17:54:21.453: INFO: Created: latency-svc-5pf6t
Oct 26 17:54:21.470: INFO: Got endpoints: latency-svc-5pf6t [403.093463ms]
Oct 26 17:54:21.479: INFO: Created: latency-svc-sthv9
Oct 26 17:54:21.491: INFO: Got endpoints: latency-svc-sthv9 [402.079424ms]
Oct 26 17:54:21.509: INFO: Created: latency-svc-tdvv8
Oct 26 17:54:21.523: INFO: Got endpoints: latency-svc-tdvv8 [401.427399ms]
Oct 26 17:54:21.537: INFO: Created: latency-svc-66svx
Oct 26 17:54:21.550: INFO: Got endpoints: latency-svc-66svx [390.982421ms]
Oct 26 17:54:21.563: INFO: Created: latency-svc-7wffl
Oct 26 17:54:21.574: INFO: Got endpoints: latency-svc-7wffl [394.580533ms]
Oct 26 17:54:21.585: INFO: Created: latency-svc-fvlv2
Oct 26 17:54:21.600: INFO: Got endpoints: latency-svc-fvlv2 [390.38267ms]
Oct 26 17:54:21.614: INFO: Created: latency-svc-tsdmr
Oct 26 17:54:21.627: INFO: Got endpoints: latency-svc-tsdmr [399.868968ms]
Oct 26 17:54:21.637: INFO: Created: latency-svc-cqdzb
Oct 26 17:54:21.647: INFO: Got endpoints: latency-svc-cqdzb [397.260224ms]
Oct 26 17:54:21.671: INFO: Created: latency-svc-hd9jt
Oct 26 17:54:21.675: INFO: Got endpoints: latency-svc-hd9jt [399.698098ms]
Oct 26 17:54:21.685: INFO: Created: latency-svc-p6kpt
Oct 26 17:54:21.698: INFO: Got endpoints: latency-svc-p6kpt [389.671515ms]
Oct 26 17:54:21.710: INFO: Created: latency-svc-xvhp2
Oct 26 17:54:21.731: INFO: Got endpoints: latency-svc-xvhp2 [387.173092ms]
Oct 26 17:54:21.740: INFO: Created: latency-svc-fsm6r
Oct 26 17:54:21.761: INFO: Got endpoints: latency-svc-fsm6r [375.695068ms]
Oct 26 17:54:21.784: INFO: Created: latency-svc-zwshw
Oct 26 17:54:21.802: INFO: Got endpoints: latency-svc-zwshw [388.215841ms]
Oct 26 17:54:21.810: INFO: Created: latency-svc-l2xtp
Oct 26 17:54:21.824: INFO: Got endpoints: latency-svc-l2xtp [404.566303ms]
Oct 26 17:54:21.837: INFO: Created: latency-svc-qq6x9
Oct 26 17:54:21.850: INFO: Got endpoints: latency-svc-qq6x9 [407.222399ms]
Oct 26 17:54:21.863: INFO: Created: latency-svc-m2z9h
Oct 26 17:54:21.880: INFO: Got endpoints: latency-svc-m2z9h [409.710684ms]
Oct 26 17:54:21.890: INFO: Created: latency-svc-fpphj
Oct 26 17:54:21.902: INFO: Got endpoints: latency-svc-fpphj [411.549434ms]
Oct 26 17:54:21.913: INFO: Created: latency-svc-tsnws
Oct 26 17:54:21.925: INFO: Got endpoints: latency-svc-tsnws [401.550703ms]
Oct 26 17:54:21.944: INFO: Created: latency-svc-bpr5q
Oct 26 17:54:21.955: INFO: Got endpoints: latency-svc-bpr5q [404.924154ms]
Oct 26 17:54:21.967: INFO: Created: latency-svc-gzq4j
Oct 26 17:54:21.977: INFO: Got endpoints: latency-svc-gzq4j [403.546668ms]
Oct 26 17:54:21.994: INFO: Created: latency-svc-p6dj8
Oct 26 17:54:22.006: INFO: Got endpoints: latency-svc-p6dj8 [405.937376ms]
Oct 26 17:54:22.012: INFO: Created: latency-svc-9zcgc
Oct 26 17:54:22.025: INFO: Got endpoints: latency-svc-9zcgc [398.068257ms]
Oct 26 17:54:22.035: INFO: Created: latency-svc-z2rx4
Oct 26 17:54:22.047: INFO: Got endpoints: latency-svc-z2rx4 [399.770674ms]
Oct 26 17:54:22.062: INFO: Created: latency-svc-whjnh
Oct 26 17:54:22.080: INFO: Got endpoints: latency-svc-whjnh [404.58742ms]
Oct 26 17:54:22.083: INFO: Created: latency-svc-f77fq
Oct 26 17:54:22.097: INFO: Got endpoints: latency-svc-f77fq [399.008812ms]
Oct 26 17:54:22.110: INFO: Created: latency-svc-p2h5t
Oct 26 17:54:22.122: INFO: Got endpoints: latency-svc-p2h5t [390.577521ms]
Oct 26 17:54:22.130: INFO: Created: latency-svc-xnzxc
Oct 26 17:54:22.142: INFO: Got endpoints: latency-svc-xnzxc [381.527072ms]
Oct 26 17:54:22.163: INFO: Created: latency-svc-8zhtv
Oct 26 17:54:22.175: INFO: Got endpoints: latency-svc-8zhtv [372.623257ms]
Oct 26 17:54:22.205: INFO: Created: latency-svc-gsjp9
Oct 26 17:54:22.213: INFO: Got endpoints: latency-svc-gsjp9 [388.266866ms]
Oct 26 17:54:22.214: INFO: Created: latency-svc-p2mlp
Oct 26 17:54:22.233: INFO: Got endpoints: latency-svc-p2mlp [383.198987ms]
Oct 26 17:54:22.243: INFO: Created: latency-svc-pgjvw
Oct 26 17:54:22.259: INFO: Got endpoints: latency-svc-pgjvw [378.92212ms]
Oct 26 17:54:22.271: INFO: Created: latency-svc-cp9d7
Oct 26 17:54:22.286: INFO: Got endpoints: latency-svc-cp9d7 [384.043336ms]
Oct 26 17:54:22.303: INFO: Created: latency-svc-zkzn6
Oct 26 17:54:22.314: INFO: Got endpoints: latency-svc-zkzn6 [388.977081ms]
Oct 26 17:54:22.338: INFO: Created: latency-svc-w8hh7
Oct 26 17:54:22.351: INFO: Got endpoints: latency-svc-w8hh7 [395.932813ms]
Oct 26 17:54:22.355: INFO: Created: latency-svc-nfqrj
Oct 26 17:54:22.372: INFO: Got endpoints: latency-svc-nfqrj [394.028836ms]
Oct 26 17:54:22.382: INFO: Created: latency-svc-4nndx
Oct 26 17:54:22.398: INFO: Got endpoints: latency-svc-4nndx [392.194822ms]
Oct 26 17:54:22.409: INFO: Created: latency-svc-7jwmh
Oct 26 17:54:22.427: INFO: Got endpoints: latency-svc-7jwmh [402.148587ms]
Oct 26 17:54:22.435: INFO: Created: latency-svc-2xfdz
Oct 26 17:54:22.460: INFO: Got endpoints: latency-svc-2xfdz [413.046961ms]
Oct 26 17:54:22.461: INFO: Created: latency-svc-xdpms
Oct 26 17:54:22.467: INFO: Got endpoints: latency-svc-xdpms [387.282398ms]
Oct 26 17:54:22.483: INFO: Created: latency-svc-6xmk5
Oct 26 17:54:22.500: INFO: Got endpoints: latency-svc-6xmk5 [403.666308ms]
Oct 26 17:54:22.523: INFO: Created: latency-svc-c8glp
Oct 26 17:54:22.539: INFO: Got endpoints: latency-svc-c8glp [417.694829ms]
Oct 26 17:54:22.549: INFO: Created: latency-svc-tnrqw
Oct 26 17:54:22.571: INFO: Got endpoints: latency-svc-tnrqw [428.297851ms]
Oct 26 17:54:22.575: INFO: Created: latency-svc-89gr9
Oct 26 17:54:22.587: INFO: Got endpoints: latency-svc-89gr9 [411.954535ms]
Oct 26 17:54:22.599: INFO: Created: latency-svc-489r2
Oct 26 17:54:22.619: INFO: Got endpoints: latency-svc-489r2 [406.494291ms]
Oct 26 17:54:22.627: INFO: Created: latency-svc-hg7ll
Oct 26 17:54:22.648: INFO: Got endpoints: latency-svc-hg7ll [414.174594ms]
Oct 26 17:54:22.659: INFO: Created: latency-svc-9hh2v
Oct 26 17:54:22.677: INFO: Got endpoints: latency-svc-9hh2v [417.847745ms]
Oct 26 17:54:22.692: INFO: Created: latency-svc-ggg7c
Oct 26 17:54:22.712: INFO: Got endpoints: latency-svc-ggg7c [425.804602ms]
Oct 26 17:54:22.727: INFO: Created: latency-svc-8wl82
Oct 26 17:54:22.745: INFO: Got endpoints: latency-svc-8wl82 [431.669332ms]
Oct 26 17:54:22.756: INFO: Created: latency-svc-xt9g8
Oct 26 17:54:22.770: INFO: Got endpoints: latency-svc-xt9g8 [419.615034ms]
Oct 26 17:54:22.781: INFO: Created: latency-svc-fx7zs
Oct 26 17:54:22.801: INFO: Got endpoints: latency-svc-fx7zs [428.784346ms]
Oct 26 17:54:22.805: INFO: Created: latency-svc-jnjd5
Oct 26 17:54:22.829: INFO: Got endpoints: latency-svc-jnjd5 [431.090455ms]
Oct 26 17:54:22.834: INFO: Created: latency-svc-bwpv8
Oct 26 17:54:22.848: INFO: Got endpoints: latency-svc-bwpv8 [420.002185ms]
Oct 26 17:54:22.865: INFO: Created: latency-svc-blnqb
Oct 26 17:54:22.880: INFO: Got endpoints: latency-svc-blnqb [419.887645ms]
Oct 26 17:54:22.889: INFO: Created: latency-svc-n4c6m
Oct 26 17:54:22.906: INFO: Got endpoints: latency-svc-n4c6m [438.61606ms]
Oct 26 17:54:22.910: INFO: Created: latency-svc-9rs4z
Oct 26 17:54:22.933: INFO: Got endpoints: latency-svc-9rs4z [432.708049ms]
Oct 26 17:54:22.941: INFO: Created: latency-svc-g9l9b
Oct 26 17:54:22.964: INFO: Got endpoints: latency-svc-g9l9b [424.872614ms]
Oct 26 17:54:22.977: INFO: Created: latency-svc-vp6c5
Oct 26 17:54:22.991: INFO: Got endpoints: latency-svc-vp6c5 [420.358437ms]
Oct 26 17:54:23.005: INFO: Created: latency-svc-cwcwb
Oct 26 17:54:23.023: INFO: Got endpoints: latency-svc-cwcwb [435.570578ms]
Oct 26 17:54:23.035: INFO: Created: latency-svc-5fr7k
Oct 26 17:54:23.049: INFO: Got endpoints: latency-svc-5fr7k [429.143758ms]
Oct 26 17:54:23.063: INFO: Created: latency-svc-ksf9j
Oct 26 17:54:23.074: INFO: Got endpoints: latency-svc-ksf9j [426.088618ms]
Oct 26 17:54:23.085: INFO: Created: latency-svc-nmxnn
Oct 26 17:54:23.096: INFO: Got endpoints: latency-svc-nmxnn [419.063445ms]
Oct 26 17:54:23.104: INFO: Created: latency-svc-w244w
Oct 26 17:54:23.119: INFO: Got endpoints: latency-svc-w244w [406.291878ms]
Oct 26 17:54:23.132: INFO: Created: latency-svc-p8dwb
Oct 26 17:54:23.144: INFO: Got endpoints: latency-svc-p8dwb [398.503057ms]
Oct 26 17:54:23.161: INFO: Created: latency-svc-n29z6
Oct 26 17:54:23.177: INFO: Got endpoints: latency-svc-n29z6 [406.193634ms]
Oct 26 17:54:23.184: INFO: Created: latency-svc-9n7pr
Oct 26 17:54:23.195: INFO: Got endpoints: latency-svc-9n7pr [393.902798ms]
Oct 26 17:54:23.220: INFO: Created: latency-svc-plb42
Oct 26 17:54:23.230: INFO: Got endpoints: latency-svc-plb42 [400.341401ms]
Oct 26 17:54:23.242: INFO: Created: latency-svc-bk77c
Oct 26 17:54:23.272: INFO: Got endpoints: latency-svc-bk77c [424.112093ms]
Oct 26 17:54:23.275: INFO: Created: latency-svc-244zd
Oct 26 17:54:23.289: INFO: Got endpoints: latency-svc-244zd [408.99535ms]
Oct 26 17:54:23.301: INFO: Created: latency-svc-kfkz4
Oct 26 17:54:23.313: INFO: Got endpoints: latency-svc-kfkz4 [406.909822ms]
Oct 26 17:54:23.313: INFO: Latencies: [53.743012ms 56.020444ms 87.065334ms 122.021343ms 164.274175ms 188.866026ms 224.946689ms 251.314825ms 274.979822ms 297.925408ms 346.487504ms 372.623257ms 375.695068ms 375.745607ms 375.753524ms 378.92212ms 381.527072ms 383.198987ms 384.043336ms 387.173092ms 387.282398ms 388.215841ms 388.266866ms 388.977081ms 389.671515ms 390.38267ms 390.577521ms 390.982421ms 392.194822ms 393.902798ms 394.028836ms 394.580533ms 395.932813ms 397.260224ms 398.068257ms 398.503057ms 399.008812ms 399.698098ms 399.770674ms 399.868968ms 400.341401ms 401.427399ms 401.550703ms 402.079424ms 402.148587ms 402.293157ms 403.093463ms 403.530928ms 403.546668ms 403.666308ms 403.836888ms 404.566303ms 404.58742ms 404.845106ms 404.924154ms 405.937376ms 406.193634ms 406.291878ms 406.480021ms 406.494291ms 406.909822ms 407.213166ms 407.222399ms 408.99535ms 409.130683ms 409.426762ms 409.710684ms 409.784339ms 410.359302ms 410.686976ms 411.248164ms 411.549434ms 411.954535ms 412.988875ms 413.046961ms 414.174594ms 414.435257ms 415.028679ms 415.345078ms 415.517159ms 416.365181ms 416.537274ms 417.233544ms 417.347475ms 417.640741ms 417.694829ms 417.847745ms 418.079567ms 419.063445ms 419.615034ms 419.773848ms 419.887645ms 419.967374ms 420.002185ms 420.358437ms 420.601795ms 420.977596ms 421.287674ms 421.660771ms 421.878863ms 422.462548ms 423.535123ms 424.112093ms 424.872614ms 425.197657ms 425.278627ms 425.295061ms 425.804602ms 426.088618ms 428.297851ms 428.395417ms 428.622392ms 428.784346ms 429.143758ms 430.166957ms 430.497581ms 430.858288ms 431.090455ms 431.092654ms 431.136653ms 431.305453ms 431.669332ms 431.947153ms 432.143294ms 432.520504ms 432.708049ms 432.864625ms 433.321472ms 433.646056ms 434.070729ms 434.556665ms 434.617712ms 435.211324ms 435.570578ms 435.584442ms 435.590839ms 435.857816ms 437.908822ms 438.61606ms 438.965001ms 441.397303ms 442.080721ms 442.617052ms 442.645782ms 442.795341ms 444.065032ms 446.119685ms 446.169559ms 448.631272ms 449.505487ms 450.252142ms 451.786078ms 455.268311ms 455.744126ms 457.38915ms 459.657998ms 461.986385ms 464.185386ms 464.846086ms 468.615809ms 470.111255ms 474.354641ms 474.591323ms 476.832665ms 477.341123ms 483.361593ms 485.774102ms 486.506995ms 487.680451ms 487.702932ms 492.578981ms 495.261268ms 496.141575ms 496.645068ms 500.093783ms 508.566208ms 509.240795ms 513.27057ms 516.478901ms 518.439465ms 518.442015ms 520.066825ms 520.598465ms 521.329469ms 522.383726ms 523.032012ms 527.836851ms 528.019255ms 529.977157ms 530.644715ms 531.09371ms 531.757768ms 534.40866ms 534.445469ms 538.240169ms 543.817655ms 572.336595ms 595.627596ms 626.422002ms 664.953099ms]
Oct 26 17:54:23.313: INFO: 50 %ile: 422.462548ms
Oct 26 17:54:23.313: INFO: 90 %ile: 518.442015ms
Oct 26 17:54:23.313: INFO: 99 %ile: 626.422002ms
Oct 26 17:54:23.313: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:54:23.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-7017" for this suite.

• [SLOW TEST:8.580 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":305,"completed":238,"skipped":3918,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:54:23.366: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3752
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W1026 17:54:24.817743      27 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1026 17:54:24.817979      27 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1026 17:54:24.818090      27 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Oct 26 17:54:24.818: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:54:24.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3752" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":305,"completed":239,"skipped":3934,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:54:24.861: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5462
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Oct 26 17:54:26.293: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1026 17:54:26.293829      27 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1026 17:54:26.293871      27 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1026 17:54:26.293880      27 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:54:26.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5462" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":305,"completed":240,"skipped":3937,"failed":0}

------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:54:26.338: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-6979
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Oct 26 17:54:34.743: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Oct 26 17:54:34.762: INFO: Pod pod-with-poststart-exec-hook still exists
Oct 26 17:54:36.763: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Oct 26 17:54:36.779: INFO: Pod pod-with-poststart-exec-hook still exists
Oct 26 17:54:38.763: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Oct 26 17:54:38.778: INFO: Pod pod-with-poststart-exec-hook still exists
Oct 26 17:54:40.763: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Oct 26 17:54:40.777: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:54:40.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6979" for this suite.

• [SLOW TEST:14.480 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":305,"completed":241,"skipped":3937,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:54:40.821: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-1961
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod with failed condition
STEP: updating the pod
Oct 26 17:56:41.739: INFO: Successfully updated pod "var-expansion-5e78233c-19ad-47a2-bcc5-98d96f5ff106"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Oct 26 17:56:43.765: INFO: Deleting pod "var-expansion-5e78233c-19ad-47a2-bcc5-98d96f5ff106" in namespace "var-expansion-1961"
Oct 26 17:56:43.876: INFO: Wait up to 5m0s for pod "var-expansion-5e78233c-19ad-47a2-bcc5-98d96f5ff106" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:57:19.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1961" for this suite.

• [SLOW TEST:159.123 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":305,"completed":242,"skipped":3982,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:57:19.946: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4224
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-9fb87d95-bfd4-4f9e-a1f3-b0582877598e
STEP: Creating a pod to test consume secrets
Oct 26 17:57:20.234: INFO: Waiting up to 5m0s for pod "pod-secrets-c3c8f554-42ab-4c58-8039-26e4b0d8e1c5" in namespace "secrets-4224" to be "Succeeded or Failed"
Oct 26 17:57:20.246: INFO: Pod "pod-secrets-c3c8f554-42ab-4c58-8039-26e4b0d8e1c5": Phase="Pending", Reason="", readiness=false. Elapsed: 11.879074ms
Oct 26 17:57:22.259: INFO: Pod "pod-secrets-c3c8f554-42ab-4c58-8039-26e4b0d8e1c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02451746s
Oct 26 17:57:24.269: INFO: Pod "pod-secrets-c3c8f554-42ab-4c58-8039-26e4b0d8e1c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035261297s
STEP: Saw pod success
Oct 26 17:57:24.270: INFO: Pod "pod-secrets-c3c8f554-42ab-4c58-8039-26e4b0d8e1c5" satisfied condition "Succeeded or Failed"
Oct 26 17:57:24.279: INFO: Trying to get logs from node 10.112.67.203 pod pod-secrets-c3c8f554-42ab-4c58-8039-26e4b0d8e1c5 container secret-volume-test: <nil>
STEP: delete the pod
Oct 26 17:57:24.399: INFO: Waiting for pod pod-secrets-c3c8f554-42ab-4c58-8039-26e4b0d8e1c5 to disappear
Oct 26 17:57:24.409: INFO: Pod pod-secrets-c3c8f554-42ab-4c58-8039-26e4b0d8e1c5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:57:24.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4224" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":243,"skipped":4031,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:57:24.457: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-415
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:57:40.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-415" for this suite.

• [SLOW TEST:16.422 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":305,"completed":244,"skipped":4040,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:57:40.880: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3967
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 26 17:57:41.772: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 26 17:57:43.816: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739331861, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739331861, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739331861, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739331861, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 26 17:57:46.872: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:57:47.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3967" for this suite.
STEP: Destroying namespace "webhook-3967-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.597 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":305,"completed":245,"skipped":4054,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:57:47.477: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1322
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Oct 26 17:57:47.724: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c196a994-3333-43f1-9bac-68494608a45d" in namespace "downward-api-1322" to be "Succeeded or Failed"
Oct 26 17:57:47.734: INFO: Pod "downwardapi-volume-c196a994-3333-43f1-9bac-68494608a45d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.611948ms
Oct 26 17:57:49.747: INFO: Pod "downwardapi-volume-c196a994-3333-43f1-9bac-68494608a45d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022338013s
STEP: Saw pod success
Oct 26 17:57:49.747: INFO: Pod "downwardapi-volume-c196a994-3333-43f1-9bac-68494608a45d" satisfied condition "Succeeded or Failed"
Oct 26 17:57:49.765: INFO: Trying to get logs from node 10.112.67.203 pod downwardapi-volume-c196a994-3333-43f1-9bac-68494608a45d container client-container: <nil>
STEP: delete the pod
Oct 26 17:57:49.841: INFO: Waiting for pod downwardapi-volume-c196a994-3333-43f1-9bac-68494608a45d to disappear
Oct 26 17:57:49.856: INFO: Pod downwardapi-volume-c196a994-3333-43f1-9bac-68494608a45d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:57:49.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1322" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":246,"skipped":4062,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:57:49.902: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-4964
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's args
Oct 26 17:57:50.168: INFO: Waiting up to 5m0s for pod "var-expansion-51b5bcc5-6122-4dd6-a2e4-8191f24515cb" in namespace "var-expansion-4964" to be "Succeeded or Failed"
Oct 26 17:57:50.179: INFO: Pod "var-expansion-51b5bcc5-6122-4dd6-a2e4-8191f24515cb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.371498ms
Oct 26 17:57:52.191: INFO: Pod "var-expansion-51b5bcc5-6122-4dd6-a2e4-8191f24515cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022802588s
Oct 26 17:57:54.205: INFO: Pod "var-expansion-51b5bcc5-6122-4dd6-a2e4-8191f24515cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037024249s
STEP: Saw pod success
Oct 26 17:57:54.206: INFO: Pod "var-expansion-51b5bcc5-6122-4dd6-a2e4-8191f24515cb" satisfied condition "Succeeded or Failed"
Oct 26 17:57:54.220: INFO: Trying to get logs from node 10.112.67.203 pod var-expansion-51b5bcc5-6122-4dd6-a2e4-8191f24515cb container dapi-container: <nil>
STEP: delete the pod
Oct 26 17:57:54.295: INFO: Waiting for pod var-expansion-51b5bcc5-6122-4dd6-a2e4-8191f24515cb to disappear
Oct 26 17:57:54.307: INFO: Pod var-expansion-51b5bcc5-6122-4dd6-a2e4-8191f24515cb no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:57:54.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4964" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":305,"completed":247,"skipped":4071,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:57:54.370: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2984
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:57:54.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2984" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":305,"completed":248,"skipped":4087,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:57:54.838: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-1545
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-1545
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Oct 26 17:57:55.086: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Oct 26 17:57:55.217: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Oct 26 17:57:57.235: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Oct 26 17:57:59.234: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 26 17:58:01.229: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 26 17:58:03.229: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 26 17:58:05.230: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 26 17:58:07.229: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 26 17:58:09.230: INFO: The status of Pod netserver-0 is Running (Ready = true)
Oct 26 17:58:09.251: INFO: The status of Pod netserver-1 is Running (Ready = false)
Oct 26 17:58:11.264: INFO: The status of Pod netserver-1 is Running (Ready = false)
Oct 26 17:58:13.266: INFO: The status of Pod netserver-1 is Running (Ready = false)
Oct 26 17:58:15.266: INFO: The status of Pod netserver-1 is Running (Ready = true)
Oct 26 17:58:15.286: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Oct 26 17:58:19.361: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.31.245:8080/dial?request=hostname&protocol=http&host=172.30.41.205&port=8080&tries=1'] Namespace:pod-network-test-1545 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 17:58:19.361: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
Oct 26 17:58:19.613: INFO: Waiting for responses: map[]
Oct 26 17:58:19.625: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.31.245:8080/dial?request=hostname&protocol=http&host=172.30.31.207&port=8080&tries=1'] Namespace:pod-network-test-1545 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 17:58:19.625: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
Oct 26 17:58:19.850: INFO: Waiting for responses: map[]
Oct 26 17:58:19.862: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.31.245:8080/dial?request=hostname&protocol=http&host=172.30.67.230&port=8080&tries=1'] Namespace:pod-network-test-1545 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 17:58:19.862: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
Oct 26 17:58:20.111: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:58:20.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1545" for this suite.

• [SLOW TEST:25.329 seconds]
[sig-network] Networking
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":305,"completed":249,"skipped":4107,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:58:20.168: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-967
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Oct 26 17:58:21.472: INFO: Pod name wrapped-volume-race-18e8f5d4-f8ed-4b0a-8f2c-2cc425782c4f: Found 0 pods out of 5
Oct 26 17:58:26.499: INFO: Pod name wrapped-volume-race-18e8f5d4-f8ed-4b0a-8f2c-2cc425782c4f: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-18e8f5d4-f8ed-4b0a-8f2c-2cc425782c4f in namespace emptydir-wrapper-967, will wait for the garbage collector to delete the pods
Oct 26 17:58:26.719: INFO: Deleting ReplicationController wrapped-volume-race-18e8f5d4-f8ed-4b0a-8f2c-2cc425782c4f took: 54.078475ms
Oct 26 17:58:26.819: INFO: Terminating ReplicationController wrapped-volume-race-18e8f5d4-f8ed-4b0a-8f2c-2cc425782c4f pods took: 100.565902ms
STEP: Creating RC which spawns configmap-volume pods
Oct 26 17:58:42.288: INFO: Pod name wrapped-volume-race-b38c447c-e72e-4084-8064-492a08eb8716: Found 0 pods out of 5
Oct 26 17:58:47.312: INFO: Pod name wrapped-volume-race-b38c447c-e72e-4084-8064-492a08eb8716: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-b38c447c-e72e-4084-8064-492a08eb8716 in namespace emptydir-wrapper-967, will wait for the garbage collector to delete the pods
Oct 26 17:58:47.490: INFO: Deleting ReplicationController wrapped-volume-race-b38c447c-e72e-4084-8064-492a08eb8716 took: 45.146153ms
Oct 26 17:58:47.590: INFO: Terminating ReplicationController wrapped-volume-race-b38c447c-e72e-4084-8064-492a08eb8716 pods took: 100.261347ms
STEP: Creating RC which spawns configmap-volume pods
Oct 26 17:59:00.381: INFO: Pod name wrapped-volume-race-7a329137-598e-42fe-8abd-28fef8e3836a: Found 0 pods out of 5
Oct 26 17:59:05.408: INFO: Pod name wrapped-volume-race-7a329137-598e-42fe-8abd-28fef8e3836a: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-7a329137-598e-42fe-8abd-28fef8e3836a in namespace emptydir-wrapper-967, will wait for the garbage collector to delete the pods
Oct 26 17:59:05.577: INFO: Deleting ReplicationController wrapped-volume-race-7a329137-598e-42fe-8abd-28fef8e3836a took: 38.741802ms
Oct 26 17:59:05.678: INFO: Terminating ReplicationController wrapped-volume-race-7a329137-598e-42fe-8abd-28fef8e3836a pods took: 100.385248ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:59:22.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-967" for this suite.

• [SLOW TEST:61.934 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":305,"completed":250,"skipped":4121,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:59:22.103: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9048
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Oct 26 17:59:22.377: INFO: Waiting up to 5m0s for pod "downwardapi-volume-27793d10-6c50-46f4-ba76-952de05b8120" in namespace "downward-api-9048" to be "Succeeded or Failed"
Oct 26 17:59:22.388: INFO: Pod "downwardapi-volume-27793d10-6c50-46f4-ba76-952de05b8120": Phase="Pending", Reason="", readiness=false. Elapsed: 10.645307ms
Oct 26 17:59:24.401: INFO: Pod "downwardapi-volume-27793d10-6c50-46f4-ba76-952de05b8120": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024140741s
STEP: Saw pod success
Oct 26 17:59:24.401: INFO: Pod "downwardapi-volume-27793d10-6c50-46f4-ba76-952de05b8120" satisfied condition "Succeeded or Failed"
Oct 26 17:59:24.413: INFO: Trying to get logs from node 10.112.67.203 pod downwardapi-volume-27793d10-6c50-46f4-ba76-952de05b8120 container client-container: <nil>
STEP: delete the pod
Oct 26 17:59:24.555: INFO: Waiting for pod downwardapi-volume-27793d10-6c50-46f4-ba76-952de05b8120 to disappear
Oct 26 17:59:24.569: INFO: Pod downwardapi-volume-27793d10-6c50-46f4-ba76-952de05b8120 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:59:24.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9048" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":251,"skipped":4145,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:59:24.625: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-6119
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:59:29.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6119" for this suite.

• [SLOW TEST:5.424 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":305,"completed":252,"skipped":4169,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:59:30.049: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3463
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Oct 26 17:59:30.317: INFO: Waiting up to 5m0s for pod "pod-64e026cf-8443-40f9-888a-10fd92467ee6" in namespace "emptydir-3463" to be "Succeeded or Failed"
Oct 26 17:59:30.329: INFO: Pod "pod-64e026cf-8443-40f9-888a-10fd92467ee6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.340842ms
Oct 26 17:59:32.342: INFO: Pod "pod-64e026cf-8443-40f9-888a-10fd92467ee6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024981457s
STEP: Saw pod success
Oct 26 17:59:32.342: INFO: Pod "pod-64e026cf-8443-40f9-888a-10fd92467ee6" satisfied condition "Succeeded or Failed"
Oct 26 17:59:32.354: INFO: Trying to get logs from node 10.112.67.203 pod pod-64e026cf-8443-40f9-888a-10fd92467ee6 container test-container: <nil>
STEP: delete the pod
Oct 26 17:59:32.422: INFO: Waiting for pod pod-64e026cf-8443-40f9-888a-10fd92467ee6 to disappear
Oct 26 17:59:32.434: INFO: Pod pod-64e026cf-8443-40f9-888a-10fd92467ee6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:59:32.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3463" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":253,"skipped":4174,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:59:32.474: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1256
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Oct 26 17:59:37.334: INFO: Successfully updated pod "annotationupdateadd00adc-20eb-4e9e-be16-3f1f45078e1f"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 17:59:39.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1256" for this suite.

• [SLOW TEST:6.990 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":305,"completed":254,"skipped":4213,"failed":0}
SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 17:59:39.464: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5563
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-5563
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-5563
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5563
Oct 26 17:59:39.807: INFO: Found 0 stateful pods, waiting for 1
Oct 26 17:59:49.819: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Oct 26 17:59:49.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=statefulset-5563 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 26 17:59:50.201: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 26 17:59:50.201: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 26 17:59:50.201: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Oct 26 17:59:50.211: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Oct 26 18:00:00.226: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Oct 26 18:00:00.227: INFO: Waiting for statefulset status.replicas updated to 0
Oct 26 18:00:00.285: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999798s
Oct 26 18:00:01.299: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.988969856s
Oct 26 18:00:02.330: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.974033667s
Oct 26 18:00:03.344: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.943360558s
Oct 26 18:00:04.358: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.928793668s
Oct 26 18:00:05.371: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.915087858s
Oct 26 18:00:06.395: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.901475444s
Oct 26 18:00:07.411: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.87807872s
Oct 26 18:00:08.425: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.862219537s
Oct 26 18:00:09.563: INFO: Verifying statefulset ss doesn't scale past 1 for another 848.924994ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5563
Oct 26 18:00:10.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=statefulset-5563 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 18:00:10.992: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Oct 26 18:00:10.992: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Oct 26 18:00:10.992: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Oct 26 18:00:11.006: INFO: Found 1 stateful pods, waiting for 3
Oct 26 18:00:21.024: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Oct 26 18:00:21.024: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Oct 26 18:00:21.024: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Oct 26 18:00:21.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=statefulset-5563 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 26 18:00:21.454: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 26 18:00:21.454: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 26 18:00:21.454: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Oct 26 18:00:21.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=statefulset-5563 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 26 18:00:21.879: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 26 18:00:21.879: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 26 18:00:21.879: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Oct 26 18:00:21.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=statefulset-5563 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 26 18:00:22.291: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 26 18:00:22.291: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 26 18:00:22.291: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Oct 26 18:00:22.291: INFO: Waiting for statefulset status.replicas updated to 0
Oct 26 18:00:22.304: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Oct 26 18:00:32.337: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Oct 26 18:00:32.337: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Oct 26 18:00:32.337: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Oct 26 18:00:32.400: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999997697s
Oct 26 18:00:33.413: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.984835232s
Oct 26 18:00:34.459: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.97242271s
Oct 26 18:00:35.479: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.926021236s
Oct 26 18:00:36.496: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.906360295s
Oct 26 18:00:37.512: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.888941339s
Oct 26 18:00:38.553: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.872712431s
Oct 26 18:00:39.577: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.832289689s
Oct 26 18:00:40.599: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.80765613s
Oct 26 18:00:41.620: INFO: Verifying statefulset ss doesn't scale past 3 for another 785.661052ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5563
Oct 26 18:00:42.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=statefulset-5563 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 18:00:43.001: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Oct 26 18:00:43.001: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Oct 26 18:00:43.001: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Oct 26 18:00:43.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=statefulset-5563 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 18:00:43.402: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Oct 26 18:00:43.402: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Oct 26 18:00:43.402: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Oct 26 18:00:43.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=statefulset-5563 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 18:00:43.766: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Oct 26 18:00:43.766: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Oct 26 18:00:43.766: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Oct 26 18:00:43.766: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Oct 26 18:01:03.860: INFO: Deleting all statefulset in ns statefulset-5563
Oct 26 18:01:03.879: INFO: Scaling statefulset ss to 0
Oct 26 18:01:03.947: INFO: Waiting for statefulset status.replicas updated to 0
Oct 26 18:01:03.967: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:01:04.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5563" for this suite.

• [SLOW TEST:84.669 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":305,"completed":255,"skipped":4217,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:01:04.133: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6156
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-db39e9b0-1228-42d7-92f7-5e7983389f80
STEP: Creating a pod to test consume configMaps
Oct 26 18:01:04.458: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-01d579fc-6661-4410-ad5d-30414dc264ff" in namespace "projected-6156" to be "Succeeded or Failed"
Oct 26 18:01:04.475: INFO: Pod "pod-projected-configmaps-01d579fc-6661-4410-ad5d-30414dc264ff": Phase="Pending", Reason="", readiness=false. Elapsed: 17.440152ms
Oct 26 18:01:06.493: INFO: Pod "pod-projected-configmaps-01d579fc-6661-4410-ad5d-30414dc264ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034554317s
Oct 26 18:01:08.508: INFO: Pod "pod-projected-configmaps-01d579fc-6661-4410-ad5d-30414dc264ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049874169s
STEP: Saw pod success
Oct 26 18:01:08.508: INFO: Pod "pod-projected-configmaps-01d579fc-6661-4410-ad5d-30414dc264ff" satisfied condition "Succeeded or Failed"
Oct 26 18:01:08.522: INFO: Trying to get logs from node 10.112.67.203 pod pod-projected-configmaps-01d579fc-6661-4410-ad5d-30414dc264ff container projected-configmap-volume-test: <nil>
STEP: delete the pod
Oct 26 18:01:08.617: INFO: Waiting for pod pod-projected-configmaps-01d579fc-6661-4410-ad5d-30414dc264ff to disappear
Oct 26 18:01:08.630: INFO: Pod pod-projected-configmaps-01d579fc-6661-4410-ad5d-30414dc264ff no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:01:08.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6156" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":256,"skipped":4222,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:01:08.688: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4901
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 26 18:01:09.645: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 26 18:01:12.764: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:01:20.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4901" for this suite.
STEP: Destroying namespace "webhook-4901-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:12.315 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":305,"completed":257,"skipped":4226,"failed":0}
SSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:01:21.004: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2537
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating pod
Oct 26 18:01:25.380: INFO: Pod pod-hostip-a7c432bf-21ab-481d-8b4a-0d9408516b8c has hostIP: 10.112.67.203
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:01:25.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2537" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":305,"completed":258,"skipped":4232,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:01:25.449: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-1320
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Oct 26 18:01:25.753: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1320 /api/v1/namespaces/watch-1320/configmaps/e2e-watch-test-configmap-a d6e9416a-9d9b-4c5b-a6d8-10b9b3de7a7f 58939 0 2020-10-26 18:01:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-10-26 18:01:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 26 18:01:25.753: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1320 /api/v1/namespaces/watch-1320/configmaps/e2e-watch-test-configmap-a d6e9416a-9d9b-4c5b-a6d8-10b9b3de7a7f 58939 0 2020-10-26 18:01:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-10-26 18:01:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Oct 26 18:01:35.807: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1320 /api/v1/namespaces/watch-1320/configmaps/e2e-watch-test-configmap-a d6e9416a-9d9b-4c5b-a6d8-10b9b3de7a7f 59001 0 2020-10-26 18:01:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-10-26 18:01:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 26 18:01:35.807: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1320 /api/v1/namespaces/watch-1320/configmaps/e2e-watch-test-configmap-a d6e9416a-9d9b-4c5b-a6d8-10b9b3de7a7f 59001 0 2020-10-26 18:01:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-10-26 18:01:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Oct 26 18:01:45.847: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1320 /api/v1/namespaces/watch-1320/configmaps/e2e-watch-test-configmap-a d6e9416a-9d9b-4c5b-a6d8-10b9b3de7a7f 59028 0 2020-10-26 18:01:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-10-26 18:01:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 26 18:01:45.847: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1320 /api/v1/namespaces/watch-1320/configmaps/e2e-watch-test-configmap-a d6e9416a-9d9b-4c5b-a6d8-10b9b3de7a7f 59028 0 2020-10-26 18:01:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-10-26 18:01:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Oct 26 18:01:55.886: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1320 /api/v1/namespaces/watch-1320/configmaps/e2e-watch-test-configmap-a d6e9416a-9d9b-4c5b-a6d8-10b9b3de7a7f 59055 0 2020-10-26 18:01:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-10-26 18:01:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 26 18:01:55.886: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1320 /api/v1/namespaces/watch-1320/configmaps/e2e-watch-test-configmap-a d6e9416a-9d9b-4c5b-a6d8-10b9b3de7a7f 59055 0 2020-10-26 18:01:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-10-26 18:01:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Oct 26 18:02:05.925: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1320 /api/v1/namespaces/watch-1320/configmaps/e2e-watch-test-configmap-b e28f18dc-65e4-4520-b740-77daa5aac8f8 59080 0 2020-10-26 18:02:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-10-26 18:02:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 26 18:02:05.925: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1320 /api/v1/namespaces/watch-1320/configmaps/e2e-watch-test-configmap-b e28f18dc-65e4-4520-b740-77daa5aac8f8 59080 0 2020-10-26 18:02:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-10-26 18:02:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Oct 26 18:02:15.979: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1320 /api/v1/namespaces/watch-1320/configmaps/e2e-watch-test-configmap-b e28f18dc-65e4-4520-b740-77daa5aac8f8 59110 0 2020-10-26 18:02:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-10-26 18:02:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 26 18:02:15.979: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1320 /api/v1/namespaces/watch-1320/configmaps/e2e-watch-test-configmap-b e28f18dc-65e4-4520-b740-77daa5aac8f8 59110 0 2020-10-26 18:02:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-10-26 18:02:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:02:25.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1320" for this suite.

• [SLOW TEST:60.592 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":305,"completed":259,"skipped":4237,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:02:26.041: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-5392
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override all
Oct 26 18:02:26.346: INFO: Waiting up to 5m0s for pod "client-containers-83cee8ef-43e5-492a-a8ff-553caa555d34" in namespace "containers-5392" to be "Succeeded or Failed"
Oct 26 18:02:26.361: INFO: Pod "client-containers-83cee8ef-43e5-492a-a8ff-553caa555d34": Phase="Pending", Reason="", readiness=false. Elapsed: 15.507746ms
Oct 26 18:02:28.377: INFO: Pod "client-containers-83cee8ef-43e5-492a-a8ff-553caa555d34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031221038s
Oct 26 18:02:30.394: INFO: Pod "client-containers-83cee8ef-43e5-492a-a8ff-553caa555d34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048562986s
STEP: Saw pod success
Oct 26 18:02:30.394: INFO: Pod "client-containers-83cee8ef-43e5-492a-a8ff-553caa555d34" satisfied condition "Succeeded or Failed"
Oct 26 18:02:30.409: INFO: Trying to get logs from node 10.112.67.203 pod client-containers-83cee8ef-43e5-492a-a8ff-553caa555d34 container test-container: <nil>
STEP: delete the pod
Oct 26 18:02:30.512: INFO: Waiting for pod client-containers-83cee8ef-43e5-492a-a8ff-553caa555d34 to disappear
Oct 26 18:02:30.530: INFO: Pod client-containers-83cee8ef-43e5-492a-a8ff-553caa555d34 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:02:30.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5392" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":305,"completed":260,"skipped":4292,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:02:30.596: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-4892
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Oct 26 18:02:32.936: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-4892 PodName:var-expansion-989f29b7-7237-4ae8-9255-0d4d6543f142 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 18:02:32.936: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: test for file in mounted path
Oct 26 18:02:33.178: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-4892 PodName:var-expansion-989f29b7-7237-4ae8-9255-0d4d6543f142 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 18:02:33.178: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: updating the annotation value
Oct 26 18:02:33.975: INFO: Successfully updated pod "var-expansion-989f29b7-7237-4ae8-9255-0d4d6543f142"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Oct 26 18:02:33.989: INFO: Deleting pod "var-expansion-989f29b7-7237-4ae8-9255-0d4d6543f142" in namespace "var-expansion-4892"
Oct 26 18:02:34.018: INFO: Wait up to 5m0s for pod "var-expansion-989f29b7-7237-4ae8-9255-0d4d6543f142" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:03:10.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4892" for this suite.

• [SLOW TEST:39.529 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":305,"completed":261,"skipped":4309,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:03:10.126: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4410
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4410.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4410.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct 26 18:03:14.735: INFO: DNS probes using dns-4410/dns-test-0441817d-aa83-4259-8c0f-988c317cd8c5 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:03:14.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4410" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":305,"completed":262,"skipped":4318,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:03:14.846: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9678
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 26 18:03:15.102: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Oct 26 18:03:15.138: INFO: Pod name sample-pod: Found 0 pods out of 1
Oct 26 18:03:20.156: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Oct 26 18:03:20.156: INFO: Creating deployment "test-rolling-update-deployment"
Oct 26 18:03:20.181: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Oct 26 18:03:20.237: INFO: deployment "test-rolling-update-deployment" doesn't have the required revision set
Oct 26 18:03:22.274: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Oct 26 18:03:22.290: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739332200, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739332200, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739332200, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739332200, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-c4cb8d6d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 26 18:03:24.312: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Oct 26 18:03:24.371: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-9678 /apis/apps/v1/namespaces/deployment-9678/deployments/test-rolling-update-deployment 5d5ac829-8396-4ae0-85c0-0781d17b3171 59466 1 2020-10-26 18:03:20 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2020-10-26 18:03:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-10-26 18:03:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a83588 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-10-26 18:03:20 +0000 UTC,LastTransitionTime:2020-10-26 18:03:20 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" has successfully progressed.,LastUpdateTime:2020-10-26 18:03:22 +0000 UTC,LastTransitionTime:2020-10-26 18:03:20 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Oct 26 18:03:24.387: INFO: New ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9  deployment-9678 /apis/apps/v1/namespaces/deployment-9678/replicasets/test-rolling-update-deployment-c4cb8d6d9 d6364e9a-36a5-4218-bb4f-3137aca968d8 59455 1 2020-10-26 18:03:20 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 5d5ac829-8396-4ae0-85c0-0781d17b3171 0xc003a83af0 0xc003a83af1}] []  [{kube-controller-manager Update apps/v1 2020-10-26 18:03:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5d5ac829-8396-4ae0-85c0-0781d17b3171\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: c4cb8d6d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a83b68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Oct 26 18:03:24.387: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Oct 26 18:03:24.388: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-9678 /apis/apps/v1/namespaces/deployment-9678/replicasets/test-rolling-update-controller 1b9836b4-102f-4f98-89dc-2c10a333341e 59464 2 2020-10-26 18:03:15 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 5d5ac829-8396-4ae0-85c0-0781d17b3171 0xc003a839e7 0xc003a839e8}] []  [{e2e.test Update apps/v1 2020-10-26 18:03:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-10-26 18:03:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5d5ac829-8396-4ae0-85c0-0781d17b3171\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003a83a88 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Oct 26 18:03:24.403: INFO: Pod "test-rolling-update-deployment-c4cb8d6d9-mv6t9" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9-mv6t9 test-rolling-update-deployment-c4cb8d6d9- deployment-9678 /api/v1/namespaces/deployment-9678/pods/test-rolling-update-deployment-c4cb8d6d9-mv6t9 b9bbbce6-f7b9-4f48-b0a2-061b3aee66fe 59454 0 2020-10-26 18:03:20 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[cni.projectcalico.org/podIP:172.30.31.215/32 cni.projectcalico.org/podIPs:172.30.31.215/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rolling-update-deployment-c4cb8d6d9 d6364e9a-36a5-4218-bb4f-3137aca968d8 0xc005525a00 0xc005525a01}] []  [{kube-controller-manager Update v1 2020-10-26 18:03:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d6364e9a-36a5-4218-bb4f-3137aca968d8\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-10-26 18:03:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-10-26 18:03:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.31.215\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qc88x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qc88x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qc88x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.112.67.203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 18:03:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 18:03:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 18:03:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 18:03:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.112.67.203,PodIP:172.30.31.215,StartTime:2020-10-26 18:03:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-10-26 18:03:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:containerd://ed9861b8b9f5de1771b4582dc4297fd09be788ecf55950621a45319397fc5018,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.31.215,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:03:24.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9678" for this suite.

• [SLOW TEST:9.612 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":305,"completed":263,"skipped":4335,"failed":0}
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:03:24.464: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-6848
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-6848
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Oct 26 18:03:24.705: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Oct 26 18:03:24.878: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Oct 26 18:03:26.893: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 26 18:03:28.895: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 26 18:03:30.894: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 26 18:03:32.895: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 26 18:03:34.894: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 26 18:03:36.895: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 26 18:03:38.893: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 26 18:03:40.894: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 26 18:03:42.894: INFO: The status of Pod netserver-0 is Running (Ready = true)
Oct 26 18:03:42.925: INFO: The status of Pod netserver-1 is Running (Ready = true)
Oct 26 18:03:42.953: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Oct 26 18:03:45.121: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.41.212:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6848 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 18:03:45.121: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
Oct 26 18:03:45.332: INFO: Found all expected endpoints: [netserver-0]
Oct 26 18:03:45.347: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.31.220:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6848 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 18:03:45.347: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
Oct 26 18:03:45.607: INFO: Found all expected endpoints: [netserver-1]
Oct 26 18:03:45.623: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.67.232:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6848 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct 26 18:03:45.623: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
Oct 26 18:03:45.856: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:03:45.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6848" for this suite.

• [SLOW TEST:21.477 seconds]
[sig-network] Networking
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":264,"skipped":4337,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:03:45.941: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9459
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:03:46.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9459" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":305,"completed":265,"skipped":4347,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:03:46.538: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2799
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting the proxy server
Oct 26 18:03:46.784: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-940878786 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:03:46.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2799" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":305,"completed":266,"skipped":4348,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:03:46.939: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-7880
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
STEP: reading a file in the container
Oct 26 18:03:49.931: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7880 pod-service-account-d37cc8a1-74e4-4244-bdbd-38997b735c9b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Oct 26 18:03:50.445: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7880 pod-service-account-d37cc8a1-74e4-4244-bdbd-38997b735c9b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Oct 26 18:03:50.803: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7880 pod-service-account-d37cc8a1-74e4-4244-bdbd-38997b735c9b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:03:51.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7880" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":305,"completed":267,"skipped":4380,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:03:51.229: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9645
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 26 18:03:52.586: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 26 18:03:54.641: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739332232, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739332232, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739332232, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739332232, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 26 18:03:57.721: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:03:57.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9645" for this suite.
STEP: Destroying namespace "webhook-9645-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.011 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":305,"completed":268,"skipped":4384,"failed":0}
SSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:03:58.241: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-529
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap that has name configmap-test-emptyKey-5a696bf2-387e-450e-b2a0-222d348a047f
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:03:58.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-529" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":305,"completed":269,"skipped":4387,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:03:58.572: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1829
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Oct 26 18:03:58.861: INFO: Waiting up to 5m0s for pod "downwardapi-volume-10e0d826-1981-4ae9-af13-be7540433b71" in namespace "projected-1829" to be "Succeeded or Failed"
Oct 26 18:03:58.884: INFO: Pod "downwardapi-volume-10e0d826-1981-4ae9-af13-be7540433b71": Phase="Pending", Reason="", readiness=false. Elapsed: 23.348469ms
Oct 26 18:04:00.900: INFO: Pod "downwardapi-volume-10e0d826-1981-4ae9-af13-be7540433b71": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.039211074s
STEP: Saw pod success
Oct 26 18:04:00.900: INFO: Pod "downwardapi-volume-10e0d826-1981-4ae9-af13-be7540433b71" satisfied condition "Succeeded or Failed"
Oct 26 18:04:00.916: INFO: Trying to get logs from node 10.112.67.203 pod downwardapi-volume-10e0d826-1981-4ae9-af13-be7540433b71 container client-container: <nil>
STEP: delete the pod
Oct 26 18:04:01.040: INFO: Waiting for pod downwardapi-volume-10e0d826-1981-4ae9-af13-be7540433b71 to disappear
Oct 26 18:04:01.058: INFO: Pod downwardapi-volume-10e0d826-1981-4ae9-af13-be7540433b71 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:04:01.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1829" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":270,"skipped":4399,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:04:01.127: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1037
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Starting the proxy
Oct 26 18:04:01.393: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-940878786 proxy --unix-socket=/tmp/kubectl-proxy-unix517225037/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:04:01.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1037" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":305,"completed":271,"skipped":4403,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:04:01.537: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1646
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:04:13.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1646" for this suite.

• [SLOW TEST:11.522 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":305,"completed":272,"skipped":4424,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:04:13.059: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8272
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-16b4b7b3-94f1-4ed5-99e4-1de09a2ba7a6
STEP: Creating a pod to test consume configMaps
Oct 26 18:04:13.358: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d82a0a1c-0e38-4488-beaf-cff473bb8458" in namespace "projected-8272" to be "Succeeded or Failed"
Oct 26 18:04:13.382: INFO: Pod "pod-projected-configmaps-d82a0a1c-0e38-4488-beaf-cff473bb8458": Phase="Pending", Reason="", readiness=false. Elapsed: 23.543659ms
Oct 26 18:04:15.399: INFO: Pod "pod-projected-configmaps-d82a0a1c-0e38-4488-beaf-cff473bb8458": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041108407s
Oct 26 18:04:17.414: INFO: Pod "pod-projected-configmaps-d82a0a1c-0e38-4488-beaf-cff473bb8458": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055606189s
STEP: Saw pod success
Oct 26 18:04:17.414: INFO: Pod "pod-projected-configmaps-d82a0a1c-0e38-4488-beaf-cff473bb8458" satisfied condition "Succeeded or Failed"
Oct 26 18:04:17.431: INFO: Trying to get logs from node 10.112.67.203 pod pod-projected-configmaps-d82a0a1c-0e38-4488-beaf-cff473bb8458 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Oct 26 18:04:17.511: INFO: Waiting for pod pod-projected-configmaps-d82a0a1c-0e38-4488-beaf-cff473bb8458 to disappear
Oct 26 18:04:17.533: INFO: Pod pod-projected-configmaps-d82a0a1c-0e38-4488-beaf-cff473bb8458 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:04:17.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8272" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":273,"skipped":4426,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:04:17.615: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-5873
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Oct 26 18:04:19.967: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:04:20.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5873" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":274,"skipped":4467,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:04:20.091: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5937
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 26 18:04:20.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 create -f - --namespace=kubectl-5937'
Oct 26 18:04:21.016: INFO: stderr: ""
Oct 26 18:04:21.016: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Oct 26 18:04:21.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 create -f - --namespace=kubectl-5937'
Oct 26 18:04:21.424: INFO: stderr: ""
Oct 26 18:04:21.424: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Oct 26 18:04:22.440: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 26 18:04:22.440: INFO: Found 0 / 1
Oct 26 18:04:23.441: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 26 18:04:23.441: INFO: Found 0 / 1
Oct 26 18:04:24.443: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 26 18:04:24.443: INFO: Found 1 / 1
Oct 26 18:04:24.443: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Oct 26 18:04:24.458: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 26 18:04:24.458: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Oct 26 18:04:24.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 describe pod agnhost-primary-7rp55 --namespace=kubectl-5937'
Oct 26 18:04:24.643: INFO: stderr: ""
Oct 26 18:04:24.643: INFO: stdout: "Name:         agnhost-primary-7rp55\nNamespace:    kubectl-5937\nPriority:     0\nNode:         10.112.67.203/10.112.67.203\nStart Time:   Mon, 26 Oct 2020 18:04:21 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/podIP: 172.30.31.210/32\n              cni.projectcalico.org/podIPs: 172.30.31.210/32\n              kubernetes.io/psp: e2e-test-privileged-psp\nStatus:       Running\nIP:           172.30.31.210\nIPs:\n  IP:           172.30.31.210\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://4e4c60620d6a6a636e664f06a7e1583bc1c325355ed66e5304fb6efd30a4e27e\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 26 Oct 2020 18:04:23 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-dzxtz (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-dzxtz:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-dzxtz\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 600s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 600s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-5937/agnhost-primary-7rp55 to 10.112.67.203\n  Normal  Pulled     2s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.20\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Oct 26 18:04:24.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 describe rc agnhost-primary --namespace=kubectl-5937'
Oct 26 18:04:24.873: INFO: stderr: ""
Oct 26 18:04:24.873: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-5937\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-7rp55\n"
Oct 26 18:04:24.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 describe service agnhost-primary --namespace=kubectl-5937'
Oct 26 18:04:25.066: INFO: stderr: ""
Oct 26 18:04:25.066: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-5937\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP:                172.21.77.49\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.30.31.210:6379\nSession Affinity:  None\nEvents:            <none>\n"
Oct 26 18:04:25.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 describe node 10.112.67.201'
Oct 26 18:04:25.358: INFO: stderr: ""
Oct 26 18:04:25.358: INFO: stdout: "Name:               10.112.67.201\nRoles:              <none>\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-gb\n                    failure-domain.beta.kubernetes.io/zone=lon02\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=5.10.101.152\n                    ibm-cloud.kubernetes.io/ha-worker=true\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.112.67.201\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=UBUNTU_18_64\n                    ibm-cloud.kubernetes.io/region=eu-gb\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=kube-bube2gcl0spv5v02m2cg-kubee2epvgp-default-0000025f\n                    ibm-cloud.kubernetes.io/worker-pool-id=bube2gcl0spv5v02m2cg-53f8c7e\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=1.19.2_1524\n                    ibm-cloud.kubernetes.io/zone=lon02\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.112.67.201\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    privateVLAN=2747290\n                    publicVLAN=2747292\n                    topology.kubernetes.io/region=eu-gb\n                    topology.kubernetes.io/zone=lon02\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.112.67.201/26\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.30.41.192\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 26 Oct 2020 15:02:44 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.112.67.201\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 26 Oct 2020 18:04:22 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 26 Oct 2020 15:03:19 +0000   Mon, 26 Oct 2020 15:03:19 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Mon, 26 Oct 2020 18:01:09 +0000   Mon, 26 Oct 2020 15:02:44 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 26 Oct 2020 18:01:09 +0000   Mon, 26 Oct 2020 15:02:44 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 26 Oct 2020 18:01:09 +0000   Mon, 26 Oct 2020 15:02:44 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 26 Oct 2020 18:01:09 +0000   Mon, 26 Oct 2020 15:03:04 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.112.67.201\n  ExternalIP:  5.10.101.152\n  Hostname:    10.112.67.201\nCapacity:\n  cpu:                4\n  ephemeral-storage:  102685624Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16419664Ki\n  pods:               110\nAllocatable:\n  cpu:                3910m\n  ephemeral-storage:  99892574949\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             13627216Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 4ae3d3d9f6134ad28c193a861912651e\n  System UUID:                D866D6EB-F404-9034-3EA9-F8A6F79B36FB\n  Boot ID:                    9471a938-b132-4863-94d1-f7200a5ec581\n  Kernel Version:             4.15.0-118-generic\n  OS Image:                   Ubuntu 18.04.5 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.4.1\n  Kubelet Version:            v1.19.2+IKS\n  Kube-Proxy Version:         v1.19.2+IKS\nProviderID:                   ibm://fee034388aa6435883a1f720010ab3a2///bube2gcl0spv5v02m2cg/kube-bube2gcl0spv5v02m2cg-kubee2epvgp-default-0000025f\nNon-terminated Pods:          (16 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  ibm-system                  addon-catalog-source-wnsgb                                 10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         18m\n  ibm-system                  catalog-operator-7654f857d5-ntsw4                          10m (0%)      0 (0%)      80Mi (0%)        0 (0%)         18m\n  ibm-system                  ibm-cloud-provider-ip-159-8-176-190-5d7d4b6856-tq96m       5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         3h\n  ibm-system                  olm-operator-5cbbb5c89d-9gzjm                              10m (0%)      0 (0%)      160Mi (1%)       0 (0%)         3h3m\n  kube-system                 calico-kube-controllers-68ddfff8d5-klh68                   10m (0%)      0 (0%)      25Mi (0%)        3Gi (23%)      3h6m\n  kube-system                 calico-node-9zj7g                                          250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         3h1m\n  kube-system                 calico-typha-d497c4cc8-gzkpz                               250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         3h6m\n  kube-system                 coredns-658bf88df8-4cjw8                                   100m (2%)     0 (0%)      70Mi (0%)        400Mi (3%)     172m\n  kube-system                 dashboard-metrics-scraper-f99788cf9-h8dpn                  1m (0%)       0 (0%)      10Mi (0%)        0 (0%)         3h3m\n  kube-system                 ibm-file-plugin-847d88c686-zvgjj                           50m (1%)      200m (5%)   100Mi (0%)       200Mi (1%)     18m\n  kube-system                 ibm-keepalived-watcher-2tk7w                               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         3h1m\n  kube-system                 ibm-master-proxy-static-10.112.67.201                      25m (0%)      300m (7%)   32M (0%)         512M (3%)      3h\n  kube-system                 ibm-storage-watcher-567779df5-xjzzb                        50m (1%)      200m (5%)   100Mi (0%)       200Mi (1%)     3h5m\n  kube-system                 public-crbube2gcl0spv5v02m2cg-alb1-69897d7d5-wd5xh         10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         176m\n  sonobuoy                    sonobuoy-e2e-job-bb5d7dc03d4b45a5                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         87m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-4f27bf473b6a4e03-6rq9w    0 (0%)        0 (0%)      0 (0%)           0 (0%)         87m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests       Limits\n  --------           --------       ------\n  cpu                786m (20%)     700m (17%)\n  memory             927250Ki (6%)  4464928Ki (32%)\n  ephemeral-storage  0 (0%)         0 (0%)\n  hugepages-1Gi      0 (0%)         0 (0%)\n  hugepages-2Mi      0 (0%)         0 (0%)\nEvents:              <none>\n"
Oct 26 18:04:25.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 describe namespace kubectl-5937'
Oct 26 18:04:25.527: INFO: stderr: ""
Oct 26 18:04:25.527: INFO: stdout: "Name:         kubectl-5937\nLabels:       e2e-framework=kubectl\n              e2e-run=abecb1f6-f360-40a4-a88c-8a3ae39152ae\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:04:25.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5937" for this suite.

• [SLOW TEST:5.498 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1105
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":305,"completed":275,"skipped":4481,"failed":0}
SSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:04:25.590: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-9429
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Oct 26 18:04:25.835: INFO: Waiting up to 1m0s for all nodes to be ready
Oct 26 18:05:25.983: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 26 18:05:26.009: INFO: Starting informer...
STEP: Starting pod...
Oct 26 18:05:26.287: INFO: Pod is running on 10.112.67.203. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Oct 26 18:05:26.342: INFO: Pod wasn't evicted. Proceeding
Oct 26 18:05:26.342: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Oct 26 18:06:41.406: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:06:41.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-9429" for this suite.

• [SLOW TEST:135.874 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":305,"completed":276,"skipped":4484,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:06:41.464: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3561
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-cb91c959-4b37-43e5-9b01-b45869bedae7
STEP: Creating a pod to test consume configMaps
Oct 26 18:06:41.769: INFO: Waiting up to 5m0s for pod "pod-configmaps-a00666c9-c166-4785-9b15-2608a00f4f9b" in namespace "configmap-3561" to be "Succeeded or Failed"
Oct 26 18:06:41.783: INFO: Pod "pod-configmaps-a00666c9-c166-4785-9b15-2608a00f4f9b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.461068ms
Oct 26 18:06:43.801: INFO: Pod "pod-configmaps-a00666c9-c166-4785-9b15-2608a00f4f9b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031711695s
Oct 26 18:06:45.879: INFO: Pod "pod-configmaps-a00666c9-c166-4785-9b15-2608a00f4f9b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.109857162s
STEP: Saw pod success
Oct 26 18:06:45.879: INFO: Pod "pod-configmaps-a00666c9-c166-4785-9b15-2608a00f4f9b" satisfied condition "Succeeded or Failed"
Oct 26 18:06:45.893: INFO: Trying to get logs from node 10.112.67.203 pod pod-configmaps-a00666c9-c166-4785-9b15-2608a00f4f9b container configmap-volume-test: <nil>
STEP: delete the pod
Oct 26 18:06:46.030: INFO: Waiting for pod pod-configmaps-a00666c9-c166-4785-9b15-2608a00f4f9b to disappear
Oct 26 18:06:46.043: INFO: Pod pod-configmaps-a00666c9-c166-4785-9b15-2608a00f4f9b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:06:46.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3561" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":277,"skipped":4494,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:06:46.098: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5896
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Oct 26 18:06:46.368: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8ecc1237-a3c7-4eaa-ab50-e6a3c01b77e9" in namespace "projected-5896" to be "Succeeded or Failed"
Oct 26 18:06:46.383: INFO: Pod "downwardapi-volume-8ecc1237-a3c7-4eaa-ab50-e6a3c01b77e9": Phase="Pending", Reason="", readiness=false. Elapsed: 15.617898ms
Oct 26 18:06:48.399: INFO: Pod "downwardapi-volume-8ecc1237-a3c7-4eaa-ab50-e6a3c01b77e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031242316s
STEP: Saw pod success
Oct 26 18:06:48.399: INFO: Pod "downwardapi-volume-8ecc1237-a3c7-4eaa-ab50-e6a3c01b77e9" satisfied condition "Succeeded or Failed"
Oct 26 18:06:48.415: INFO: Trying to get logs from node 10.112.67.203 pod downwardapi-volume-8ecc1237-a3c7-4eaa-ab50-e6a3c01b77e9 container client-container: <nil>
STEP: delete the pod
Oct 26 18:06:48.506: INFO: Waiting for pod downwardapi-volume-8ecc1237-a3c7-4eaa-ab50-e6a3c01b77e9 to disappear
Oct 26 18:06:48.521: INFO: Pod downwardapi-volume-8ecc1237-a3c7-4eaa-ab50-e6a3c01b77e9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:06:48.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5896" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":305,"completed":278,"skipped":4520,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:06:48.575: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename certificates
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in certificates-5921
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Oct 26 18:06:50.836: INFO: starting watch
STEP: patching
STEP: updating
Oct 26 18:06:50.891: INFO: waiting for watch events with expected annotations
Oct 26 18:06:50.891: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:06:51.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-5921" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":305,"completed":279,"skipped":4545,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:06:51.281: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1758
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-18cf478c-7b12-4e9b-9235-f7d47798e8db
STEP: Creating a pod to test consume configMaps
Oct 26 18:06:51.601: INFO: Waiting up to 5m0s for pod "pod-configmaps-f5c3d0d6-14fc-4f98-8942-46e160ba30fd" in namespace "configmap-1758" to be "Succeeded or Failed"
Oct 26 18:06:51.625: INFO: Pod "pod-configmaps-f5c3d0d6-14fc-4f98-8942-46e160ba30fd": Phase="Pending", Reason="", readiness=false. Elapsed: 23.856446ms
Oct 26 18:06:53.644: INFO: Pod "pod-configmaps-f5c3d0d6-14fc-4f98-8942-46e160ba30fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.042872285s
STEP: Saw pod success
Oct 26 18:06:53.644: INFO: Pod "pod-configmaps-f5c3d0d6-14fc-4f98-8942-46e160ba30fd" satisfied condition "Succeeded or Failed"
Oct 26 18:06:53.662: INFO: Trying to get logs from node 10.112.67.203 pod pod-configmaps-f5c3d0d6-14fc-4f98-8942-46e160ba30fd container configmap-volume-test: <nil>
STEP: delete the pod
Oct 26 18:06:53.739: INFO: Waiting for pod pod-configmaps-f5c3d0d6-14fc-4f98-8942-46e160ba30fd to disappear
Oct 26 18:06:53.756: INFO: Pod pod-configmaps-f5c3d0d6-14fc-4f98-8942-46e160ba30fd no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:06:53.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1758" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":280,"skipped":4590,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:06:53.824: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2957
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:07:10.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2957" for this suite.

• [SLOW TEST:16.686 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":305,"completed":281,"skipped":4602,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:07:10.511: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5816
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Oct 26 18:07:10.787: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Oct 26 18:07:33.158: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
Oct 26 18:07:39.569: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:08:03.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5816" for this suite.

• [SLOW TEST:53.179 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":305,"completed":282,"skipped":4630,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:08:03.692: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4588
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:08:15.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4588" for this suite.

• [SLOW TEST:11.539 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":305,"completed":283,"skipped":4638,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:08:15.232: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1329
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on tmpfs
Oct 26 18:08:15.525: INFO: Waiting up to 5m0s for pod "pod-d7b5718b-ab51-44a4-9fdd-13a055dd9a13" in namespace "emptydir-1329" to be "Succeeded or Failed"
Oct 26 18:08:15.538: INFO: Pod "pod-d7b5718b-ab51-44a4-9fdd-13a055dd9a13": Phase="Pending", Reason="", readiness=false. Elapsed: 13.330631ms
Oct 26 18:08:17.554: INFO: Pod "pod-d7b5718b-ab51-44a4-9fdd-13a055dd9a13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029150248s
Oct 26 18:08:19.574: INFO: Pod "pod-d7b5718b-ab51-44a4-9fdd-13a055dd9a13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04922114s
STEP: Saw pod success
Oct 26 18:08:19.574: INFO: Pod "pod-d7b5718b-ab51-44a4-9fdd-13a055dd9a13" satisfied condition "Succeeded or Failed"
Oct 26 18:08:19.589: INFO: Trying to get logs from node 10.112.67.203 pod pod-d7b5718b-ab51-44a4-9fdd-13a055dd9a13 container test-container: <nil>
STEP: delete the pod
Oct 26 18:08:19.698: INFO: Waiting for pod pod-d7b5718b-ab51-44a4-9fdd-13a055dd9a13 to disappear
Oct 26 18:08:19.725: INFO: Pod pod-d7b5718b-ab51-44a4-9fdd-13a055dd9a13 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:08:19.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1329" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":284,"skipped":4660,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:08:19.790: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-8871
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:08:24.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8871" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":305,"completed":285,"skipped":4682,"failed":0}

------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:08:24.254: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6500
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-bbf2edf6-fccf-4e8a-98a5-ae207c105251
STEP: Creating configMap with name cm-test-opt-upd-680bb0ee-b0f8-4f5e-a0c0-b64d46fad0f9
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-bbf2edf6-fccf-4e8a-98a5-ae207c105251
STEP: Updating configmap cm-test-opt-upd-680bb0ee-b0f8-4f5e-a0c0-b64d46fad0f9
STEP: Creating configMap with name cm-test-opt-create-851a98f4-abf6-4af6-99cf-201da1882085
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:09:52.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6500" for this suite.

• [SLOW TEST:88.369 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":286,"skipped":4682,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:09:52.623: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-7867
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:09:52.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-7867" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":305,"completed":287,"skipped":4693,"failed":0}
S
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:09:52.921: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-8618
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 26 18:09:53.163: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Oct 26 18:09:54.268: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:09:54.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8618" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":305,"completed":288,"skipped":4694,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:09:54.353: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-812
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:09:54.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-812" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":305,"completed":289,"skipped":4697,"failed":0}
SSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:09:54.722: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-5062
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Oct 26 18:09:54.950: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:09:58.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5062" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":305,"completed":290,"skipped":4701,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:09:58.794: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-1749
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-7mj9
STEP: Creating a pod to test atomic-volume-subpath
Oct 26 18:09:59.083: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-7mj9" in namespace "subpath-1749" to be "Succeeded or Failed"
Oct 26 18:09:59.107: INFO: Pod "pod-subpath-test-configmap-7mj9": Phase="Pending", Reason="", readiness=false. Elapsed: 24.622125ms
Oct 26 18:10:01.118: INFO: Pod "pod-subpath-test-configmap-7mj9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035697301s
Oct 26 18:10:03.137: INFO: Pod "pod-subpath-test-configmap-7mj9": Phase="Running", Reason="", readiness=true. Elapsed: 4.054786802s
Oct 26 18:10:05.159: INFO: Pod "pod-subpath-test-configmap-7mj9": Phase="Running", Reason="", readiness=true. Elapsed: 6.076241633s
Oct 26 18:10:07.173: INFO: Pod "pod-subpath-test-configmap-7mj9": Phase="Running", Reason="", readiness=true. Elapsed: 8.090284716s
Oct 26 18:10:09.189: INFO: Pod "pod-subpath-test-configmap-7mj9": Phase="Running", Reason="", readiness=true. Elapsed: 10.105967894s
Oct 26 18:10:11.201: INFO: Pod "pod-subpath-test-configmap-7mj9": Phase="Running", Reason="", readiness=true. Elapsed: 12.118420112s
Oct 26 18:10:13.214: INFO: Pod "pod-subpath-test-configmap-7mj9": Phase="Running", Reason="", readiness=true. Elapsed: 14.131833984s
Oct 26 18:10:15.227: INFO: Pod "pod-subpath-test-configmap-7mj9": Phase="Running", Reason="", readiness=true. Elapsed: 16.1448891s
Oct 26 18:10:17.239: INFO: Pod "pod-subpath-test-configmap-7mj9": Phase="Running", Reason="", readiness=true. Elapsed: 18.156207793s
Oct 26 18:10:19.250: INFO: Pod "pod-subpath-test-configmap-7mj9": Phase="Running", Reason="", readiness=true. Elapsed: 20.167841997s
Oct 26 18:10:21.269: INFO: Pod "pod-subpath-test-configmap-7mj9": Phase="Running", Reason="", readiness=true. Elapsed: 22.18652706s
Oct 26 18:10:23.280: INFO: Pod "pod-subpath-test-configmap-7mj9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.19779875s
STEP: Saw pod success
Oct 26 18:10:23.280: INFO: Pod "pod-subpath-test-configmap-7mj9" satisfied condition "Succeeded or Failed"
Oct 26 18:10:23.290: INFO: Trying to get logs from node 10.112.67.203 pod pod-subpath-test-configmap-7mj9 container test-container-subpath-configmap-7mj9: <nil>
STEP: delete the pod
Oct 26 18:10:23.368: INFO: Waiting for pod pod-subpath-test-configmap-7mj9 to disappear
Oct 26 18:10:23.378: INFO: Pod pod-subpath-test-configmap-7mj9 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-7mj9
Oct 26 18:10:23.378: INFO: Deleting pod "pod-subpath-test-configmap-7mj9" in namespace "subpath-1749"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:10:23.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1749" for this suite.

• [SLOW TEST:24.657 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":305,"completed":291,"skipped":4717,"failed":0}
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:10:23.453: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-1081
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-1081, will wait for the garbage collector to delete the pods
Oct 26 18:10:27.817: INFO: Deleting Job.batch foo took: 42.501316ms
Oct 26 18:10:27.918: INFO: Terminating Job.batch foo pods took: 100.242182ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:11:10.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1081" for this suite.

• [SLOW TEST:46.652 seconds]
[sig-apps] Job
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":305,"completed":292,"skipped":4717,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:11:10.106: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-4349
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in volume subpath
Oct 26 18:11:10.372: INFO: Waiting up to 5m0s for pod "var-expansion-c30fd411-f4fb-4fce-abbe-fcc5115a5cdd" in namespace "var-expansion-4349" to be "Succeeded or Failed"
Oct 26 18:11:10.382: INFO: Pod "var-expansion-c30fd411-f4fb-4fce-abbe-fcc5115a5cdd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.217399ms
Oct 26 18:11:12.393: INFO: Pod "var-expansion-c30fd411-f4fb-4fce-abbe-fcc5115a5cdd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021482733s
Oct 26 18:11:14.406: INFO: Pod "var-expansion-c30fd411-f4fb-4fce-abbe-fcc5115a5cdd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034495009s
STEP: Saw pod success
Oct 26 18:11:14.406: INFO: Pod "var-expansion-c30fd411-f4fb-4fce-abbe-fcc5115a5cdd" satisfied condition "Succeeded or Failed"
Oct 26 18:11:14.416: INFO: Trying to get logs from node 10.112.67.203 pod var-expansion-c30fd411-f4fb-4fce-abbe-fcc5115a5cdd container dapi-container: <nil>
STEP: delete the pod
Oct 26 18:11:14.487: INFO: Waiting for pod var-expansion-c30fd411-f4fb-4fce-abbe-fcc5115a5cdd to disappear
Oct 26 18:11:14.500: INFO: Pod var-expansion-c30fd411-f4fb-4fce-abbe-fcc5115a5cdd no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:11:14.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4349" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":305,"completed":293,"skipped":4724,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:11:14.554: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4261
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Oct 26 18:11:14.789: INFO: namespace kubectl-4261
Oct 26 18:11:14.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 create -f - --namespace=kubectl-4261'
Oct 26 18:11:15.326: INFO: stderr: ""
Oct 26 18:11:15.326: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Oct 26 18:11:16.338: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 26 18:11:16.338: INFO: Found 0 / 1
Oct 26 18:11:17.338: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 26 18:11:17.338: INFO: Found 1 / 1
Oct 26 18:11:17.338: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Oct 26 18:11:17.352: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 26 18:11:17.352: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Oct 26 18:11:17.352: INFO: wait on agnhost-primary startup in kubectl-4261 
Oct 26 18:11:17.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 logs agnhost-primary-jp5rp agnhost-primary --namespace=kubectl-4261'
Oct 26 18:11:17.534: INFO: stderr: ""
Oct 26 18:11:17.534: INFO: stdout: "Paused\n"
STEP: exposing RC
Oct 26 18:11:17.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-4261'
Oct 26 18:11:17.724: INFO: stderr: ""
Oct 26 18:11:17.724: INFO: stdout: "service/rm2 exposed\n"
Oct 26 18:11:17.736: INFO: Service rm2 in namespace kubectl-4261 found.
STEP: exposing service
Oct 26 18:11:19.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-4261'
Oct 26 18:11:19.993: INFO: stderr: ""
Oct 26 18:11:19.993: INFO: stdout: "service/rm3 exposed\n"
Oct 26 18:11:20.014: INFO: Service rm3 in namespace kubectl-4261 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:11:22.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4261" for this suite.

• [SLOW TEST:7.543 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1246
    should create services for rc  [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":305,"completed":294,"skipped":4725,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:11:22.098: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-9188
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Oct 26 18:11:28.512: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Oct 26 18:11:28.529: INFO: Pod pod-with-poststart-http-hook still exists
Oct 26 18:11:30.534: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Oct 26 18:11:30.550: INFO: Pod pod-with-poststart-http-hook still exists
Oct 26 18:11:32.530: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Oct 26 18:11:32.558: INFO: Pod pod-with-poststart-http-hook still exists
Oct 26 18:11:34.530: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Oct 26 18:11:34.541: INFO: Pod pod-with-poststart-http-hook still exists
Oct 26 18:11:36.529: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Oct 26 18:11:36.542: INFO: Pod pod-with-poststart-http-hook still exists
Oct 26 18:11:38.530: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Oct 26 18:11:38.543: INFO: Pod pod-with-poststart-http-hook still exists
Oct 26 18:11:40.530: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Oct 26 18:11:40.543: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:11:40.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9188" for this suite.

• [SLOW TEST:18.493 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":305,"completed":295,"skipped":4745,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:11:40.592: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5441
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service multi-endpoint-test in namespace services-5441
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5441 to expose endpoints map[]
Oct 26 18:11:40.876: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Oct 26 18:11:41.905: INFO: successfully validated that service multi-endpoint-test in namespace services-5441 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-5441
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5441 to expose endpoints map[pod1:[100]]
Oct 26 18:11:44.993: INFO: successfully validated that service multi-endpoint-test in namespace services-5441 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-5441
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5441 to expose endpoints map[pod1:[100] pod2:[101]]
Oct 26 18:11:48.088: INFO: successfully validated that service multi-endpoint-test in namespace services-5441 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-5441
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5441 to expose endpoints map[pod2:[101]]
Oct 26 18:11:48.178: INFO: successfully validated that service multi-endpoint-test in namespace services-5441 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-5441
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5441 to expose endpoints map[]
Oct 26 18:11:49.249: INFO: successfully validated that service multi-endpoint-test in namespace services-5441 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:11:49.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5441" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:8.787 seconds]
[sig-network] Services
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":305,"completed":296,"skipped":4770,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:11:49.380: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9571
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Oct 26 18:11:49.604: INFO: Creating deployment "test-recreate-deployment"
Oct 26 18:11:49.623: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Oct 26 18:11:49.669: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Oct 26 18:11:51.701: INFO: Waiting deployment "test-recreate-deployment" to complete
Oct 26 18:11:51.714: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739332709, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739332709, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63739332709, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63739332709, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 26 18:11:53.736: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Oct 26 18:11:53.766: INFO: Updating deployment test-recreate-deployment
Oct 26 18:11:53.766: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Oct 26 18:11:54.025: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-9571 /apis/apps/v1/namespaces/deployment-9571/deployments/test-recreate-deployment 28f98c2f-7045-4a5a-adb1-92425303282c 62201 2 2020-10-26 18:11:49 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-10-26 18:11:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-10-26 18:11:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0044e36e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-10-26 18:11:53 +0000 UTC,LastTransitionTime:2020-10-26 18:11:53 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f79dd4667" is progressing.,LastUpdateTime:2020-10-26 18:11:53 +0000 UTC,LastTransitionTime:2020-10-26 18:11:49 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Oct 26 18:11:54.038: INFO: New ReplicaSet "test-recreate-deployment-f79dd4667" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f79dd4667  deployment-9571 /apis/apps/v1/namespaces/deployment-9571/replicasets/test-recreate-deployment-f79dd4667 6168d1ab-9ccc-4225-aa61-f2d45c5ab3a4 62200 1 2020-10-26 18:11:53 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 28f98c2f-7045-4a5a-adb1-92425303282c 0xc00374c850 0xc00374c851}] []  [{kube-controller-manager Update apps/v1 2020-10-26 18:11:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"28f98c2f-7045-4a5a-adb1-92425303282c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f79dd4667,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00374c8c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Oct 26 18:11:54.038: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Oct 26 18:11:54.038: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-c96cf48f  deployment-9571 /apis/apps/v1/namespaces/deployment-9571/replicasets/test-recreate-deployment-c96cf48f bb7947ce-bbf4-447c-8de8-f8156d646646 62189 2 2020-10-26 18:11:49 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 28f98c2f-7045-4a5a-adb1-92425303282c 0xc00374c75f 0xc00374c770}] []  [{kube-controller-manager Update apps/v1 2020-10-26 18:11:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"28f98c2f-7045-4a5a-adb1-92425303282c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c96cf48f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00374c7e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Oct 26 18:11:54.048: INFO: Pod "test-recreate-deployment-f79dd4667-vg558" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f79dd4667-vg558 test-recreate-deployment-f79dd4667- deployment-9571 /api/v1/namespaces/deployment-9571/pods/test-recreate-deployment-f79dd4667-vg558 b18d3f4f-6397-4315-9b12-0dc2ab7007a4 62199 0 2020-10-26 18:11:53 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-recreate-deployment-f79dd4667 6168d1ab-9ccc-4225-aa61-f2d45c5ab3a4 0xc00374cdd0 0xc00374cdd1}] []  [{kube-controller-manager Update v1 2020-10-26 18:11:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6168d1ab-9ccc-4225-aa61-f2d45c5ab3a4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-10-26 18:11:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-j2l8w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-j2l8w,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-j2l8w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.112.67.203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 18:11:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 18:11:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 18:11:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-10-26 18:11:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.112.67.203,PodIP:,StartTime:2020-10-26 18:11:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:11:54.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9571" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":305,"completed":297,"skipped":4798,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:11:54.105: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6086
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-6086
Oct 26 18:11:56.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-6086 kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Oct 26 18:11:56.876: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Oct 26 18:11:56.876: INFO: stdout: "iptables"
Oct 26 18:11:56.876: INFO: proxyMode: iptables
Oct 26 18:11:56.899: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Oct 26 18:11:56.910: INFO: Pod kube-proxy-mode-detector still exists
Oct 26 18:11:58.910: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Oct 26 18:11:58.922: INFO: Pod kube-proxy-mode-detector still exists
Oct 26 18:12:00.910: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Oct 26 18:12:00.921: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-6086
STEP: creating replication controller affinity-nodeport-timeout in namespace services-6086
I1026 18:12:01.010926      27 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-6086, replica count: 3
I1026 18:12:04.061967      27 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 26 18:12:04.106: INFO: Creating new exec pod
Oct 26 18:12:07.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-6086 execpod-affinityglrzm -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
Oct 26 18:12:07.557: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Oct 26 18:12:07.557: INFO: stdout: ""
Oct 26 18:12:07.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-6086 execpod-affinityglrzm -- /bin/sh -x -c nc -zv -t -w 2 172.21.133.105 80'
Oct 26 18:12:07.932: INFO: stderr: "+ nc -zv -t -w 2 172.21.133.105 80\nConnection to 172.21.133.105 80 port [tcp/http] succeeded!\n"
Oct 26 18:12:07.932: INFO: stdout: ""
Oct 26 18:12:07.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-6086 execpod-affinityglrzm -- /bin/sh -x -c nc -zv -t -w 2 10.112.67.207 30705'
Oct 26 18:12:08.346: INFO: stderr: "+ nc -zv -t -w 2 10.112.67.207 30705\nConnection to 10.112.67.207 30705 port [tcp/30705] succeeded!\n"
Oct 26 18:12:08.346: INFO: stdout: ""
Oct 26 18:12:08.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-6086 execpod-affinityglrzm -- /bin/sh -x -c nc -zv -t -w 2 10.112.67.201 30705'
Oct 26 18:12:08.849: INFO: stderr: "+ nc -zv -t -w 2 10.112.67.201 30705\nConnection to 10.112.67.201 30705 port [tcp/30705] succeeded!\n"
Oct 26 18:12:08.849: INFO: stdout: ""
Oct 26 18:12:08.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-6086 execpod-affinityglrzm -- /bin/sh -x -c nc -zv -t -w 2 5.10.101.150 30705'
Oct 26 18:12:09.267: INFO: stderr: "+ nc -zv -t -w 2 5.10.101.150 30705\nConnection to 5.10.101.150 30705 port [tcp/30705] succeeded!\n"
Oct 26 18:12:09.267: INFO: stdout: ""
Oct 26 18:12:09.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-6086 execpod-affinityglrzm -- /bin/sh -x -c nc -zv -t -w 2 5.10.101.152 30705'
Oct 26 18:12:09.601: INFO: stderr: "+ nc -zv -t -w 2 5.10.101.152 30705\nConnection to 5.10.101.152 30705 port [tcp/30705] succeeded!\n"
Oct 26 18:12:09.601: INFO: stdout: ""
Oct 26 18:12:09.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-6086 execpod-affinityglrzm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.112.67.201:30705/ ; done'
Oct 26 18:12:10.128: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30705/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30705/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30705/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30705/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30705/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30705/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30705/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30705/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30705/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30705/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30705/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30705/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30705/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30705/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30705/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.112.67.201:30705/\n"
Oct 26 18:12:10.128: INFO: stdout: "\naffinity-nodeport-timeout-4kf8c\naffinity-nodeport-timeout-4kf8c\naffinity-nodeport-timeout-4kf8c\naffinity-nodeport-timeout-4kf8c\naffinity-nodeport-timeout-4kf8c\naffinity-nodeport-timeout-4kf8c\naffinity-nodeport-timeout-4kf8c\naffinity-nodeport-timeout-4kf8c\naffinity-nodeport-timeout-4kf8c\naffinity-nodeport-timeout-4kf8c\naffinity-nodeport-timeout-4kf8c\naffinity-nodeport-timeout-4kf8c\naffinity-nodeport-timeout-4kf8c\naffinity-nodeport-timeout-4kf8c\naffinity-nodeport-timeout-4kf8c\naffinity-nodeport-timeout-4kf8c"
Oct 26 18:12:10.128: INFO: Received response from host: affinity-nodeport-timeout-4kf8c
Oct 26 18:12:10.128: INFO: Received response from host: affinity-nodeport-timeout-4kf8c
Oct 26 18:12:10.128: INFO: Received response from host: affinity-nodeport-timeout-4kf8c
Oct 26 18:12:10.128: INFO: Received response from host: affinity-nodeport-timeout-4kf8c
Oct 26 18:12:10.128: INFO: Received response from host: affinity-nodeport-timeout-4kf8c
Oct 26 18:12:10.128: INFO: Received response from host: affinity-nodeport-timeout-4kf8c
Oct 26 18:12:10.128: INFO: Received response from host: affinity-nodeport-timeout-4kf8c
Oct 26 18:12:10.128: INFO: Received response from host: affinity-nodeport-timeout-4kf8c
Oct 26 18:12:10.128: INFO: Received response from host: affinity-nodeport-timeout-4kf8c
Oct 26 18:12:10.128: INFO: Received response from host: affinity-nodeport-timeout-4kf8c
Oct 26 18:12:10.128: INFO: Received response from host: affinity-nodeport-timeout-4kf8c
Oct 26 18:12:10.128: INFO: Received response from host: affinity-nodeport-timeout-4kf8c
Oct 26 18:12:10.128: INFO: Received response from host: affinity-nodeport-timeout-4kf8c
Oct 26 18:12:10.128: INFO: Received response from host: affinity-nodeport-timeout-4kf8c
Oct 26 18:12:10.128: INFO: Received response from host: affinity-nodeport-timeout-4kf8c
Oct 26 18:12:10.128: INFO: Received response from host: affinity-nodeport-timeout-4kf8c
Oct 26 18:12:10.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-6086 execpod-affinityglrzm -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.112.67.201:30705/'
Oct 26 18:12:10.563: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.112.67.201:30705/\n"
Oct 26 18:12:10.563: INFO: stdout: "affinity-nodeport-timeout-4kf8c"
Oct 26 18:12:25.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=services-6086 execpod-affinityglrzm -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.112.67.201:30705/'
Oct 26 18:12:25.934: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.112.67.201:30705/\n"
Oct 26 18:12:25.934: INFO: stdout: "affinity-nodeport-timeout-jqnbl"
Oct 26 18:12:25.934: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-6086, will wait for the garbage collector to delete the pods
Oct 26 18:12:26.073: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 34.660934ms
Oct 26 18:12:26.173: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.248667ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:12:39.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6086" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:45.927 seconds]
[sig-network] Services
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":298,"skipped":4812,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:12:40.033: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1208
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating cluster-info
Oct 26 18:12:40.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 cluster-info'
Oct 26 18:12:40.400: INFO: stderr: ""
Oct 26 18:12:40.400: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mkubernetes-dashboard\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\x1b[0m\n\x1b[0;32mMetrics-server\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\x1b[0m\n\x1b[0;32mNodeLocalDNS\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443/api/v1/namespaces/kube-system/services/node-local-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:12:40.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1208" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":305,"completed":299,"skipped":4823,"failed":0}
SSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:12:40.451: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-5830
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:12:42.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5830" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":300,"skipped":4829,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:12:42.842: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1449
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-1449
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating stateful set ss in namespace statefulset-1449
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1449
Oct 26 18:12:43.124: INFO: Found 0 stateful pods, waiting for 1
Oct 26 18:12:53.142: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Oct 26 18:12:53.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=statefulset-1449 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 26 18:12:53.514: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 26 18:12:53.515: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 26 18:12:53.515: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Oct 26 18:12:53.527: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Oct 26 18:13:03.540: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Oct 26 18:13:03.540: INFO: Waiting for statefulset status.replicas updated to 0
Oct 26 18:13:03.606: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Oct 26 18:13:03.607: INFO: ss-0  10.112.67.203  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:12:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:12:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:12:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:12:43 +0000 UTC  }]
Oct 26 18:13:03.607: INFO: 
Oct 26 18:13:03.607: INFO: StatefulSet ss has not reached scale 3, at 1
Oct 26 18:13:04.621: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.98893468s
Oct 26 18:13:05.633: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.974922005s
Oct 26 18:13:06.644: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.962662991s
Oct 26 18:13:07.658: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.950943984s
Oct 26 18:13:08.670: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.937444263s
Oct 26 18:13:09.683: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.925364935s
Oct 26 18:13:10.699: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.912757151s
Oct 26 18:13:11.719: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.896653456s
Oct 26 18:13:12.742: INFO: Verifying statefulset ss doesn't scale past 3 for another 876.607427ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1449
Oct 26 18:13:13.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=statefulset-1449 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 18:13:14.093: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Oct 26 18:13:14.093: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Oct 26 18:13:14.093: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Oct 26 18:13:14.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=statefulset-1449 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 18:13:14.490: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Oct 26 18:13:14.490: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Oct 26 18:13:14.490: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Oct 26 18:13:14.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=statefulset-1449 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 26 18:13:14.887: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Oct 26 18:13:14.887: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Oct 26 18:13:14.887: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Oct 26 18:13:14.903: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Oct 26 18:13:14.903: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Oct 26 18:13:14.903: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Oct 26 18:13:14.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=statefulset-1449 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 26 18:13:15.288: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 26 18:13:15.288: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 26 18:13:15.288: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Oct 26 18:13:15.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=statefulset-1449 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 26 18:13:15.615: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 26 18:13:15.615: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 26 18:13:15.615: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Oct 26 18:13:15.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-940878786 exec --namespace=statefulset-1449 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 26 18:13:15.954: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 26 18:13:15.954: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 26 18:13:15.954: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Oct 26 18:13:15.954: INFO: Waiting for statefulset status.replicas updated to 0
Oct 26 18:13:15.973: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Oct 26 18:13:26.003: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Oct 26 18:13:26.003: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Oct 26 18:13:26.003: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Oct 26 18:13:26.057: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Oct 26 18:13:26.057: INFO: ss-0  10.112.67.203  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:12:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:13:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:13:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:12:43 +0000 UTC  }]
Oct 26 18:13:26.057: INFO: ss-1  10.112.67.201  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:13:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:13:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:13:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:13:03 +0000 UTC  }]
Oct 26 18:13:26.057: INFO: ss-2  10.112.67.207  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:13:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:13:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:13:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:13:03 +0000 UTC  }]
Oct 26 18:13:26.057: INFO: 
Oct 26 18:13:26.057: INFO: StatefulSet ss has not reached scale 0, at 3
Oct 26 18:13:27.069: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Oct 26 18:13:27.069: INFO: ss-0  10.112.67.203  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:12:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:13:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:13:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:12:43 +0000 UTC  }]
Oct 26 18:13:27.069: INFO: ss-1  10.112.67.201  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:13:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:13:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:13:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:13:03 +0000 UTC  }]
Oct 26 18:13:27.069: INFO: ss-2  10.112.67.207  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:13:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:13:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:13:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:13:03 +0000 UTC  }]
Oct 26 18:13:27.069: INFO: 
Oct 26 18:13:27.069: INFO: StatefulSet ss has not reached scale 0, at 3
Oct 26 18:13:28.090: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Oct 26 18:13:28.090: INFO: ss-0  10.112.67.203  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:12:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:13:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:13:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:12:43 +0000 UTC  }]
Oct 26 18:13:28.090: INFO: ss-1  10.112.67.201  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:13:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:13:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:13:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:13:03 +0000 UTC  }]
Oct 26 18:13:28.090: INFO: ss-2  10.112.67.207  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:13:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:13:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:13:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:13:03 +0000 UTC  }]
Oct 26 18:13:28.090: INFO: 
Oct 26 18:13:28.090: INFO: StatefulSet ss has not reached scale 0, at 3
Oct 26 18:13:29.103: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Oct 26 18:13:29.103: INFO: ss-1  10.112.67.201  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:13:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:13:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:13:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:13:03 +0000 UTC  }]
Oct 26 18:13:29.103: INFO: 
Oct 26 18:13:29.103: INFO: StatefulSet ss has not reached scale 0, at 1
Oct 26 18:13:30.114: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Oct 26 18:13:30.114: INFO: ss-1  10.112.67.201  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:13:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:13:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:13:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:13:03 +0000 UTC  }]
Oct 26 18:13:30.114: INFO: 
Oct 26 18:13:30.114: INFO: StatefulSet ss has not reached scale 0, at 1
Oct 26 18:13:31.130: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Oct 26 18:13:31.130: INFO: ss-1  10.112.67.201  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:13:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:13:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:13:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-10-26 18:13:03 +0000 UTC  }]
Oct 26 18:13:31.130: INFO: 
Oct 26 18:13:31.130: INFO: StatefulSet ss has not reached scale 0, at 1
Oct 26 18:13:32.141: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.913365712s
Oct 26 18:13:33.154: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.902301124s
Oct 26 18:13:34.165: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.88985329s
Oct 26 18:13:35.179: INFO: Verifying statefulset ss doesn't scale past 0 for another 878.001557ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1449
Oct 26 18:13:36.192: INFO: Scaling statefulset ss to 0
Oct 26 18:13:36.229: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Oct 26 18:13:36.244: INFO: Deleting all statefulset in ns statefulset-1449
Oct 26 18:13:36.256: INFO: Scaling statefulset ss to 0
Oct 26 18:13:36.293: INFO: Waiting for statefulset status.replicas updated to 0
Oct 26 18:13:36.305: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:13:36.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1449" for this suite.

• [SLOW TEST:53.566 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":305,"completed":301,"skipped":4842,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:13:36.410: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4952
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-4952
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating statefulset ss in namespace statefulset-4952
Oct 26 18:13:36.691: INFO: Found 0 stateful pods, waiting for 1
Oct 26 18:13:46.707: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Oct 26 18:13:46.779: INFO: Deleting all statefulset in ns statefulset-4952
Oct 26 18:13:46.800: INFO: Scaling statefulset ss to 0
Oct 26 18:14:06.868: INFO: Waiting for statefulset status.replicas updated to 0
Oct 26 18:14:06.884: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:14:06.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4952" for this suite.

• [SLOW TEST:30.593 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":305,"completed":302,"skipped":4880,"failed":0}
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:14:07.004: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-5721
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Oct 26 18:14:07.266: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Oct 26 18:14:07.301: INFO: Waiting for terminating namespaces to be deleted...
Oct 26 18:14:07.315: INFO: 
Logging pods the apiserver thinks is on node 10.112.67.201 before test
Oct 26 18:14:07.353: INFO: addon-catalog-source-wnsgb from ibm-system started at 2020-10-26 17:45:39 +0000 UTC (1 container statuses recorded)
Oct 26 18:14:07.353: INFO: 	Container configmap-registry-server ready: true, restart count 0
Oct 26 18:14:07.354: INFO: catalog-operator-7654f857d5-ntsw4 from ibm-system started at 2020-10-26 17:45:31 +0000 UTC (1 container statuses recorded)
Oct 26 18:14:07.354: INFO: 	Container catalog-operator ready: true, restart count 0
Oct 26 18:14:07.354: INFO: ibm-cloud-provider-ip-159-8-176-190-5d7d4b6856-tq96m from ibm-system started at 2020-10-26 15:04:09 +0000 UTC (1 container statuses recorded)
Oct 26 18:14:07.354: INFO: 	Container ibm-cloud-provider-ip-159-8-176-190 ready: true, restart count 0
Oct 26 18:14:07.354: INFO: olm-operator-5cbbb5c89d-9gzjm from ibm-system started at 2020-10-26 15:03:06 +0000 UTC (1 container statuses recorded)
Oct 26 18:14:07.354: INFO: 	Container olm-operator ready: true, restart count 0
Oct 26 18:14:07.354: INFO: calico-kube-controllers-68ddfff8d5-klh68 from kube-system started at 2020-10-26 15:03:05 +0000 UTC (1 container statuses recorded)
Oct 26 18:14:07.355: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Oct 26 18:14:07.355: INFO: calico-node-9zj7g from kube-system started at 2020-10-26 15:02:45 +0000 UTC (1 container statuses recorded)
Oct 26 18:14:07.355: INFO: 	Container calico-node ready: true, restart count 0
Oct 26 18:14:07.355: INFO: calico-typha-d497c4cc8-gzkpz from kube-system started at 2020-10-26 15:03:05 +0000 UTC (1 container statuses recorded)
Oct 26 18:14:07.355: INFO: 	Container calico-typha ready: true, restart count 0
Oct 26 18:14:07.355: INFO: coredns-658bf88df8-4cjw8 from kube-system started at 2020-10-26 15:11:47 +0000 UTC (1 container statuses recorded)
Oct 26 18:14:07.355: INFO: 	Container coredns ready: true, restart count 0
Oct 26 18:14:07.356: INFO: dashboard-metrics-scraper-f99788cf9-h8dpn from kube-system started at 2020-10-26 15:03:06 +0000 UTC (1 container statuses recorded)
Oct 26 18:14:07.356: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Oct 26 18:14:07.356: INFO: ibm-file-plugin-847d88c686-zvgjj from kube-system started at 2020-10-26 17:45:31 +0000 UTC (1 container statuses recorded)
Oct 26 18:14:07.356: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Oct 26 18:14:07.356: INFO: ibm-keepalived-watcher-2tk7w from kube-system started at 2020-10-26 15:02:45 +0000 UTC (1 container statuses recorded)
Oct 26 18:14:07.356: INFO: 	Container keepalived-watcher ready: true, restart count 0
Oct 26 18:14:07.356: INFO: ibm-master-proxy-static-10.112.67.201 from kube-system started at 2020-10-26 15:02:31 +0000 UTC (2 container statuses recorded)
Oct 26 18:14:07.356: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Oct 26 18:14:07.357: INFO: 	Container pause ready: true, restart count 0
Oct 26 18:14:07.357: INFO: ibm-storage-watcher-567779df5-xjzzb from kube-system started at 2020-10-26 15:03:06 +0000 UTC (1 container statuses recorded)
Oct 26 18:14:07.357: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Oct 26 18:14:07.357: INFO: public-crbube2gcl0spv5v02m2cg-alb1-69897d7d5-wd5xh from kube-system started at 2020-10-26 15:07:34 +0000 UTC (4 container statuses recorded)
Oct 26 18:14:07.357: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Oct 26 18:14:07.357: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Oct 26 18:14:07.357: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Oct 26 18:14:07.358: INFO: 	Container nginx-ingress ready: true, restart count 0
Oct 26 18:14:07.358: INFO: sonobuoy-e2e-job-bb5d7dc03d4b45a5 from sonobuoy started at 2020-10-26 16:37:14 +0000 UTC (2 container statuses recorded)
Oct 26 18:14:07.358: INFO: 	Container e2e ready: true, restart count 0
Oct 26 18:14:07.358: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Oct 26 18:14:07.358: INFO: sonobuoy-systemd-logs-daemon-set-4f27bf473b6a4e03-6rq9w from sonobuoy started at 2020-10-26 16:37:14 +0000 UTC (2 container statuses recorded)
Oct 26 18:14:07.358: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Oct 26 18:14:07.358: INFO: 	Container systemd-logs ready: true, restart count 0
Oct 26 18:14:07.358: INFO: 
Logging pods the apiserver thinks is on node 10.112.67.203 before test
Oct 26 18:14:07.385: INFO: calico-node-dcxtp from kube-system started at 2020-10-26 15:02:43 +0000 UTC (1 container statuses recorded)
Oct 26 18:14:07.385: INFO: 	Container calico-node ready: true, restart count 0
Oct 26 18:14:07.385: INFO: calico-typha-d497c4cc8-bbvcx from kube-system started at 2020-10-26 18:05:28 +0000 UTC (1 container statuses recorded)
Oct 26 18:14:07.385: INFO: 	Container calico-typha ready: true, restart count 0
Oct 26 18:14:07.385: INFO: ibm-keepalived-watcher-wlxd7 from kube-system started at 2020-10-26 15:02:43 +0000 UTC (1 container statuses recorded)
Oct 26 18:14:07.385: INFO: 	Container keepalived-watcher ready: true, restart count 0
Oct 26 18:14:07.385: INFO: ibm-master-proxy-static-10.112.67.203 from kube-system started at 2020-10-26 15:02:39 +0000 UTC (2 container statuses recorded)
Oct 26 18:14:07.385: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Oct 26 18:14:07.385: INFO: 	Container pause ready: true, restart count 0
Oct 26 18:14:07.385: INFO: sonobuoy from sonobuoy started at 2020-10-26 16:37:06 +0000 UTC (1 container statuses recorded)
Oct 26 18:14:07.385: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Oct 26 18:14:07.385: INFO: sonobuoy-systemd-logs-daemon-set-4f27bf473b6a4e03-75n8p from sonobuoy started at 2020-10-26 16:37:14 +0000 UTC (2 container statuses recorded)
Oct 26 18:14:07.385: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Oct 26 18:14:07.385: INFO: 	Container systemd-logs ready: true, restart count 0
Oct 26 18:14:07.385: INFO: 
Logging pods the apiserver thinks is on node 10.112.67.207 before test
Oct 26 18:14:07.411: INFO: test-k8s-e2e-pvg-master-verification from default started at 2020-10-26 15:05:30 +0000 UTC (1 container statuses recorded)
Oct 26 18:14:07.411: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Oct 26 18:14:07.411: INFO: ibm-cloud-provider-ip-159-8-176-190-5d7d4b6856-77vh5 from ibm-system started at 2020-10-26 15:04:09 +0000 UTC (1 container statuses recorded)
Oct 26 18:14:07.411: INFO: 	Container ibm-cloud-provider-ip-159-8-176-190 ready: true, restart count 0
Oct 26 18:14:07.411: INFO: calico-node-k5p5w from kube-system started at 2020-10-26 15:02:53 +0000 UTC (1 container statuses recorded)
Oct 26 18:14:07.411: INFO: 	Container calico-node ready: true, restart count 0
Oct 26 18:14:07.411: INFO: calico-typha-d497c4cc8-6p86s from kube-system started at 2020-10-26 15:03:16 +0000 UTC (1 container statuses recorded)
Oct 26 18:14:07.411: INFO: 	Container calico-typha ready: true, restart count 0
Oct 26 18:14:07.411: INFO: coredns-658bf88df8-pn52m from kube-system started at 2020-10-26 15:11:47 +0000 UTC (1 container statuses recorded)
Oct 26 18:14:07.411: INFO: 	Container coredns ready: true, restart count 0
Oct 26 18:14:07.411: INFO: coredns-658bf88df8-q4587 from kube-system started at 2020-10-26 17:45:31 +0000 UTC (1 container statuses recorded)
Oct 26 18:14:07.411: INFO: 	Container coredns ready: true, restart count 0
Oct 26 18:14:07.411: INFO: coredns-autoscaler-65b4b99bb7-5p2hq from kube-system started at 2020-10-26 17:45:31 +0000 UTC (1 container statuses recorded)
Oct 26 18:14:07.411: INFO: 	Container autoscaler ready: true, restart count 0
Oct 26 18:14:07.411: INFO: ibm-keepalived-watcher-drkhn from kube-system started at 2020-10-26 15:02:53 +0000 UTC (1 container statuses recorded)
Oct 26 18:14:07.411: INFO: 	Container keepalived-watcher ready: true, restart count 0
Oct 26 18:14:07.411: INFO: ibm-master-proxy-static-10.112.67.207 from kube-system started at 2020-10-26 15:02:38 +0000 UTC (2 container statuses recorded)
Oct 26 18:14:07.411: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Oct 26 18:14:07.411: INFO: 	Container pause ready: true, restart count 0
Oct 26 18:14:07.411: INFO: kubernetes-dashboard-7c8884c686-tqpxt from kube-system started at 2020-10-26 17:45:31 +0000 UTC (1 container statuses recorded)
Oct 26 18:14:07.411: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Oct 26 18:14:07.411: INFO: metrics-server-66957c64f5-kdpjh from kube-system started at 2020-10-26 17:45:31 +0000 UTC (2 container statuses recorded)
Oct 26 18:14:07.411: INFO: 	Container metrics-server ready: true, restart count 0
Oct 26 18:14:07.411: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Oct 26 18:14:07.411: INFO: public-crbube2gcl0spv5v02m2cg-alb1-69897d7d5-rh4l5 from kube-system started at 2020-10-26 15:07:34 +0000 UTC (4 container statuses recorded)
Oct 26 18:14:07.411: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Oct 26 18:14:07.411: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Oct 26 18:14:07.411: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Oct 26 18:14:07.411: INFO: 	Container nginx-ingress ready: true, restart count 0
Oct 26 18:14:07.411: INFO: vpn-7d76994fc5-n94t8 from kube-system started at 2020-10-26 17:45:31 +0000 UTC (1 container statuses recorded)
Oct 26 18:14:07.411: INFO: 	Container vpn ready: true, restart count 0
Oct 26 18:14:07.411: INFO: sonobuoy-systemd-logs-daemon-set-4f27bf473b6a4e03-pnpc9 from sonobuoy started at 2020-10-26 16:37:14 +0000 UTC (2 container statuses recorded)
Oct 26 18:14:07.411: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Oct 26 18:14:07.411: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-4657c290-1f81-4ec9-85ff-622b7f672a62 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-4657c290-1f81-4ec9-85ff-622b7f672a62 off the node 10.112.67.203
STEP: verifying the node doesn't have the label kubernetes.io/e2e-4657c290-1f81-4ec9-85ff-622b7f672a62
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:14:23.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5721" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:16.789 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":305,"completed":303,"skipped":4888,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:14:23.794: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6314
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-d54b941a-b747-4cb0-8943-2048fe532abd
STEP: Creating a pod to test consume configMaps
Oct 26 18:14:24.074: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f3c3c4a8-e2c7-48a8-80a6-2e03038740b9" in namespace "projected-6314" to be "Succeeded or Failed"
Oct 26 18:14:24.084: INFO: Pod "pod-projected-configmaps-f3c3c4a8-e2c7-48a8-80a6-2e03038740b9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.109761ms
Oct 26 18:14:26.095: INFO: Pod "pod-projected-configmaps-f3c3c4a8-e2c7-48a8-80a6-2e03038740b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021603009s
Oct 26 18:14:28.108: INFO: Pod "pod-projected-configmaps-f3c3c4a8-e2c7-48a8-80a6-2e03038740b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034051382s
STEP: Saw pod success
Oct 26 18:14:28.108: INFO: Pod "pod-projected-configmaps-f3c3c4a8-e2c7-48a8-80a6-2e03038740b9" satisfied condition "Succeeded or Failed"
Oct 26 18:14:28.118: INFO: Trying to get logs from node 10.112.67.203 pod pod-projected-configmaps-f3c3c4a8-e2c7-48a8-80a6-2e03038740b9 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Oct 26 18:14:28.229: INFO: Waiting for pod pod-projected-configmaps-f3c3c4a8-e2c7-48a8-80a6-2e03038740b9 to disappear
Oct 26 18:14:28.239: INFO: Pod pod-projected-configmaps-f3c3c4a8-e2c7-48a8-80a6-2e03038740b9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:14:28.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6314" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":304,"skipped":4905,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Oct 26 18:14:28.293: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9223
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Oct 26 18:14:28.531: INFO: >>> kubeConfig: /tmp/kubeconfig-940878786
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Oct 26 18:15:01.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9223" for this suite.

• [SLOW TEST:33.003 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":305,"completed":305,"skipped":4920,"failed":0}
SSSSSSSSOct 26 18:15:01.296: INFO: Running AfterSuite actions on all nodes
Oct 26 18:15:01.296: INFO: Running AfterSuite actions on node 1
Oct 26 18:15:01.296: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":305,"completed":305,"skipped":4928,"failed":0}

Ran 305 of 5233 Specs in 5845.671 seconds
SUCCESS! -- 305 Passed | 0 Failed | 0 Pending | 4928 Skipped
PASS

Ginkgo ran 1 suite in 1h37m27.66963355s
Test Suite Passed
