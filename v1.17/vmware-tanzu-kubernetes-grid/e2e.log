I0123 22:02:32.538610      21 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-837620349
I0123 22:02:32.538995      21 test_context.go:419] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0123 22:02:32.539421      21 e2e.go:109] Starting e2e run "88e364ac-9d03-442f-b032-9a9c415c90f9" on Ginkgo node 1
{"msg":"Test Suite starting","total":280,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1579816950 - Will randomize all specs
Will run 280 of 4843 specs

Jan 23 22:02:32.557: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
Jan 23 22:02:32.566: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
E0123 22:02:32.566747      21 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post http://localhost:8099/progress: dial tcp [::1]:8099: connect: connection refused
Jan 23 22:02:32.611: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jan 23 22:02:32.696: INFO: 21 / 21 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jan 23 22:02:32.696: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Jan 23 22:02:32.696: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jan 23 22:02:32.709: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jan 23 22:02:32.709: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jan 23 22:02:32.709: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'vsphere-cloud-controller-manager' (0 seconds elapsed)
Jan 23 22:02:32.709: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'vsphere-csi-node' (0 seconds elapsed)
Jan 23 22:02:32.709: INFO: e2e test version: v1.17.2
Jan 23 22:02:32.711: INFO: kube-apiserver version: v1.17.2+vmware.1
Jan 23 22:02:32.711: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
Jan 23 22:02:32.718: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:02:32.719: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename sched-pred
Jan 23 22:02:32.874: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Jan 23 22:02:32.879: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 23 22:02:32.935: INFO: Waiting for terminating namespaces to be deleted...
Jan 23 22:02:32.939: INFO: 
Logging pods the kubelet thinks is on node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz before test
Jan 23 22:02:33.095: INFO: kube-proxy-qfmln from kube-system started at 2020-01-22 22:09:02 +0000 UTC (1 container statuses recorded)
Jan 23 22:02:33.095: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 23 22:02:33.095: INFO: sonobuoy-systemd-logs-daemon-set-82eb3c000b9d48f1-m9c8t from sonobuoy started at 2020-01-23 22:01:22 +0000 UTC (2 container statuses recorded)
Jan 23 22:02:33.095: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 23 22:02:33.095: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 23 22:02:33.095: INFO: vsphere-csi-node-mp8xr from kube-system started at 2020-01-22 22:09:24 +0000 UTC (3 container statuses recorded)
Jan 23 22:02:33.095: INFO: 	Container liveness-probe ready: true, restart count 0
Jan 23 22:02:33.095: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 23 22:02:33.095: INFO: 	Container vsphere-csi-node ready: true, restart count 0
Jan 23 22:02:33.095: INFO: calico-node-zvhsz from kube-system started at 2020-01-22 22:09:02 +0000 UTC (1 container statuses recorded)
Jan 23 22:02:33.095: INFO: 	Container calico-node ready: true, restart count 0
Jan 23 22:02:33.095: INFO: 
Logging pods the kubelet thinks is on node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f before test
Jan 23 22:02:33.157: INFO: calico-node-27bq2 from kube-system started at 2020-01-22 22:09:05 +0000 UTC (1 container statuses recorded)
Jan 23 22:02:33.157: INFO: 	Container calico-node ready: true, restart count 0
Jan 23 22:02:33.157: INFO: vsphere-csi-node-rd47x from kube-system started at 2020-01-22 22:09:27 +0000 UTC (3 container statuses recorded)
Jan 23 22:02:33.157: INFO: 	Container liveness-probe ready: true, restart count 0
Jan 23 22:02:33.157: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 23 22:02:33.157: INFO: 	Container vsphere-csi-node ready: true, restart count 0
Jan 23 22:02:33.157: INFO: sonobuoy from sonobuoy started at 2020-01-23 22:01:11 +0000 UTC (1 container statuses recorded)
Jan 23 22:02:33.157: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 23 22:02:33.157: INFO: kube-proxy-n9fnt from kube-system started at 2020-01-22 22:09:05 +0000 UTC (1 container statuses recorded)
Jan 23 22:02:33.157: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 23 22:02:33.157: INFO: sonobuoy-systemd-logs-daemon-set-82eb3c000b9d48f1-mkq6m from sonobuoy started at 2020-01-23 22:01:23 +0000 UTC (2 container statuses recorded)
Jan 23 22:02:33.158: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 23 22:02:33.158: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 23 22:02:33.158: INFO: 
Logging pods the kubelet thinks is on node management-cluster-1-17-2-md-0-5f64bb5777-v477d before test
Jan 23 22:02:33.234: INFO: vsphere-csi-node-h9rth from kube-system started at 2020-01-22 22:09:25 +0000 UTC (3 container statuses recorded)
Jan 23 22:02:33.234: INFO: 	Container liveness-probe ready: true, restart count 0
Jan 23 22:02:33.234: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 23 22:02:33.234: INFO: 	Container vsphere-csi-node ready: true, restart count 0
Jan 23 22:02:33.234: INFO: calico-node-lbpft from kube-system started at 2020-01-22 22:09:03 +0000 UTC (1 container statuses recorded)
Jan 23 22:02:33.234: INFO: 	Container calico-node ready: true, restart count 0
Jan 23 22:02:33.234: INFO: sonobuoy-systemd-logs-daemon-set-82eb3c000b9d48f1-fwpld from sonobuoy started at 2020-01-23 22:01:22 +0000 UTC (2 container statuses recorded)
Jan 23 22:02:33.234: INFO: 	Container sonobuoy-worker ready: false, restart count 0
Jan 23 22:02:33.234: INFO: 	Container systemd-logs ready: false, restart count 0
Jan 23 22:02:33.234: INFO: sonobuoy-e2e-job-61735b81990a421a from sonobuoy started at 2020-01-23 22:01:22 +0000 UTC (2 container statuses recorded)
Jan 23 22:02:33.234: INFO: 	Container e2e ready: false, restart count 0
Jan 23 22:02:33.234: INFO: 	Container sonobuoy-worker ready: false, restart count 0
Jan 23 22:02:33.234: INFO: kube-proxy-7w2h9 from kube-system started at 2020-01-22 22:09:03 +0000 UTC (1 container statuses recorded)
Jan 23 22:02:33.234: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-92cc1428-5993-4784-939f-1a530c4b19c3 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-92cc1428-5993-4784-939f-1a530c4b19c3 off the node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz
STEP: verifying the node doesn't have the label kubernetes.io/e2e-92cc1428-5993-4784-939f-1a530c4b19c3
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:02:41.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5626" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:8.695 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":280,"completed":1,"skipped":31,"failed":0}
SSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:02:41.414: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod test-webserver-8edf08a1-4367-4cc2-a776-1da1c12102f2 in namespace container-probe-7265
Jan 23 22:02:45.533: INFO: Started pod test-webserver-8edf08a1-4367-4cc2-a776-1da1c12102f2 in namespace container-probe-7265
STEP: checking the pod's current state and verifying that restartCount is present
Jan 23 22:02:45.537: INFO: Initial restart count of pod test-webserver-8edf08a1-4367-4cc2-a776-1da1c12102f2 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:06:58.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7265" for this suite.

• [SLOW TEST:244.911 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":280,"completed":2,"skipped":35,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:06:58.153: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 23 22:06:58.238: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-ef8dc983-0c42-41f2-b610-16112c22f216" in namespace "security-context-test-5641" to be "success or failure"
Jan 23 22:06:58.249: INFO: Pod "busybox-readonly-false-ef8dc983-0c42-41f2-b610-16112c22f216": Phase="Pending", Reason="", readiness=false. Elapsed: 11.146865ms
Jan 23 22:07:00.253: INFO: Pod "busybox-readonly-false-ef8dc983-0c42-41f2-b610-16112c22f216": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015314805s
Jan 23 22:07:00.254: INFO: Pod "busybox-readonly-false-ef8dc983-0c42-41f2-b610-16112c22f216" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:07:00.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5641" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":280,"completed":3,"skipped":55,"failed":0}

------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:07:00.266: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jan 23 22:07:04.398: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9840 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 23 22:07:04.398: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
Jan 23 22:07:04.659: INFO: Exec stderr: ""
Jan 23 22:07:04.660: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9840 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 23 22:07:04.660: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
Jan 23 22:07:04.850: INFO: Exec stderr: ""
Jan 23 22:07:04.850: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9840 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 23 22:07:04.850: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
Jan 23 22:07:05.017: INFO: Exec stderr: ""
Jan 23 22:07:05.018: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9840 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 23 22:07:05.018: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
Jan 23 22:07:05.183: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jan 23 22:07:05.183: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9840 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 23 22:07:05.183: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
Jan 23 22:07:05.381: INFO: Exec stderr: ""
Jan 23 22:07:05.381: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9840 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 23 22:07:05.381: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
Jan 23 22:07:05.580: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jan 23 22:07:05.580: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9840 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 23 22:07:05.580: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
Jan 23 22:07:05.790: INFO: Exec stderr: ""
Jan 23 22:07:05.790: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9840 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 23 22:07:05.790: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
Jan 23 22:07:05.944: INFO: Exec stderr: ""
Jan 23 22:07:05.945: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9840 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 23 22:07:05.945: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
Jan 23 22:07:06.139: INFO: Exec stderr: ""
Jan 23 22:07:06.139: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9840 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 23 22:07:06.139: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
Jan 23 22:07:06.325: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:07:06.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-9840" for this suite.

• [SLOW TEST:6.073 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":4,"skipped":55,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:07:06.343: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 23 22:07:06.384: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:07:07.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1089" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":280,"completed":5,"skipped":58,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:07:07.041: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 23 22:07:07.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 create -f - --namespace=kubectl-1859'
Jan 23 22:07:08.017: INFO: stderr: ""
Jan 23 22:07:08.017: INFO: stdout: "replicationcontroller/agnhost-master created\n"
Jan 23 22:07:08.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 create -f - --namespace=kubectl-1859'
Jan 23 22:07:08.435: INFO: stderr: ""
Jan 23 22:07:08.435: INFO: stdout: "service/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Jan 23 22:07:09.440: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 23 22:07:09.440: INFO: Found 1 / 1
Jan 23 22:07:09.440: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 23 22:07:09.444: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 23 22:07:09.444: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 23 22:07:09.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 describe pod agnhost-master-jxh76 --namespace=kubectl-1859'
Jan 23 22:07:09.593: INFO: stderr: ""
Jan 23 22:07:09.593: INFO: stdout: "Name:         agnhost-master-jxh76\nNamespace:    kubectl-1859\nPriority:     0\nNode:         management-cluster-1-17-2-md-0-5f64bb5777-v477d/10.78.209.78\nStart Time:   Thu, 23 Jan 2020 22:07:08 +0000\nLabels:       app=agnhost\n              role=master\nAnnotations:  cni.projectcalico.org/podIP: 100.105.119.86/32\nStatus:       Running\nIP:           100.105.119.86\nIPs:\n  IP:           100.105.119.86\nControlled By:  ReplicationController/agnhost-master\nContainers:\n  agnhost-master:\n    Container ID:   containerd://506b8c513e30a0411c3d767d2735c74895c5fcbd3e328a21eb39d9101dad8d0b\n    Image:          gcr.io/kubernetes-e2e-test-images/agnhost:2.8\n    Image ID:       gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 23 Jan 2020 22:07:09 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-wcj66 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-wcj66:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-wcj66\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From                                                      Message\n  ----    ------     ----       ----                                                      -------\n  Normal  Scheduled  <unknown>  default-scheduler                                         Successfully assigned kubectl-1859/agnhost-master-jxh76 to management-cluster-1-17-2-md-0-5f64bb5777-v477d\n  Normal  Pulled     0s         kubelet, management-cluster-1-17-2-md-0-5f64bb5777-v477d  Container image \"gcr.io/kubernetes-e2e-test-images/agnhost:2.8\" already present on machine\n  Normal  Created    0s         kubelet, management-cluster-1-17-2-md-0-5f64bb5777-v477d  Created container agnhost-master\n  Normal  Started    0s         kubelet, management-cluster-1-17-2-md-0-5f64bb5777-v477d  Started container agnhost-master\n"
Jan 23 22:07:09.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 describe rc agnhost-master --namespace=kubectl-1859'
Jan 23 22:07:09.719: INFO: stderr: ""
Jan 23 22:07:09.719: INFO: stdout: "Name:         agnhost-master\nNamespace:    kubectl-1859\nSelector:     app=agnhost,role=master\nLabels:       app=agnhost\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=master\n  Containers:\n   agnhost-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/agnhost:2.8\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-master-jxh76\n"
Jan 23 22:07:09.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 describe service agnhost-master --namespace=kubectl-1859'
Jan 23 22:07:09.814: INFO: stderr: ""
Jan 23 22:07:09.814: INFO: stdout: "Name:              agnhost-master\nNamespace:         kubectl-1859\nLabels:            app=agnhost\n                   role=master\nAnnotations:       <none>\nSelector:          app=agnhost,role=master\nType:              ClusterIP\nIP:                100.67.227.110\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         100.105.119.86:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jan 23 22:07:09.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 describe node management-cluster-1-17-2-controlplane-0'
Jan 23 22:07:09.950: INFO: stderr: ""
Jan 23 22:07:09.951: INFO: stdout: "Name:               management-cluster-1-17-2-controlplane-0\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=vsphere-vm.cpu-2.mem-2gb.os-linux\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=management-cluster-1-17-2-controlplane-0\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"csi.vsphere.vmware.com\":\"management-cluster-1-17-2-controlplane-0\"}\n                    kubeadm.alpha.kubernetes.io/cri-socket: /var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.78.220.50/20\n                    projectcalico.org/IPv4IPIPTunnelAddr: 100.124.46.192\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 22 Jan 2020 22:05:46 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  management-cluster-1-17-2-controlplane-0\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 23 Jan 2020 22:07:02 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 22 Jan 2020 22:06:54 +0000   Wed, 22 Jan 2020 22:06:54 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Thu, 23 Jan 2020 22:02:44 +0000   Wed, 22 Jan 2020 22:05:45 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 23 Jan 2020 22:02:44 +0000   Wed, 22 Jan 2020 22:05:45 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 23 Jan 2020 22:02:44 +0000   Wed, 22 Jan 2020 22:05:45 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 23 Jan 2020 22:02:44 +0000   Wed, 22 Jan 2020 22:07:00 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  Hostname:    management-cluster-1-17-2-controlplane-0\n  ExternalIP:  10.78.220.50\n  InternalIP:  10.78.220.50\nCapacity:\n  cpu:                2\n  ephemeral-storage:  30817168Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             2039552Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  28401101982\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             1937152Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 8d480bfdf71c404b9e5c34723b3c585d\n  System UUID:                616b1a42-250b-2d97-d100-31d2109ec997\n  Boot ID:                    f66152e1-8846-410e-a680-ba94b4f51ec9\n  Kernel Version:             4.19.79-2.ph3\n  OS Image:                   VMware Photon OS/Linux\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.2.9\n  Kubelet Version:            v1.17.2+vmware.1\n  Kube-Proxy Version:         v1.17.2+vmware.1\nPodCIDR:                      100.96.0.0/24\nPodCIDRs:                     100.96.0.0/24\nProviderID:                   vsphere://421a6b61-0b25-972d-d100-31d2109ec997\nNon-terminated Pods:          (16 in total)\n  Namespace                   Name                                                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                                ------------  ----------  ---------------  -------------  ---\n  cabpk-system                cabpk-controller-manager-674b45d495-6rrvl                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         24h\n  capi-system                 capi-controller-manager-6d7bfb9d66-nwvm2                            0 (0%)        0 (0%)      100Mi (5%)       100Mi (5%)     24h\n  capv-system                 capv-controller-manager-b9fcc858b-886zf                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         24h\n  kube-system                 calico-kube-controllers-ff95847f5-x494v                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         24h\n  kube-system                 calico-node-nl9tz                                                   250m (12%)    0 (0%)      0 (0%)           0 (0%)         24h\n  kube-system                 coredns-5c6dd84487-jmbhs                                            100m (5%)     0 (0%)      70Mi (3%)        170Mi (8%)     24h\n  kube-system                 coredns-5c6dd84487-mf8s5                                            100m (5%)     0 (0%)      70Mi (3%)        170Mi (8%)     24h\n  kube-system                 etcd-management-cluster-1-17-2-controlplane-0                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         24h\n  kube-system                 kube-apiserver-management-cluster-1-17-2-controlplane-0             250m (12%)    0 (0%)      0 (0%)           0 (0%)         24h\n  kube-system                 kube-controller-manager-management-cluster-1-17-2-controlplane-0    200m (10%)    0 (0%)      0 (0%)           0 (0%)         24h\n  kube-system                 kube-proxy-t6tk6                                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         24h\n  kube-system                 kube-scheduler-management-cluster-1-17-2-controlplane-0             100m (5%)     0 (0%)      0 (0%)           0 (0%)         24h\n  kube-system                 vsphere-cloud-controller-manager-wtrgx                              200m (10%)    0 (0%)      0 (0%)           0 (0%)         24h\n  kube-system                 vsphere-csi-controller-0                                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         24h\n  kube-system                 vsphere-csi-node-5ml62                                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         24h\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-82eb3c000b9d48f1-8d9jx             0 (0%)        0 (0%)      0 (0%)           0 (0%)         5m48s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                1200m (60%)  0 (0%)\n  memory             240Mi (12%)  440Mi (23%)\n  ephemeral-storage  0 (0%)       0 (0%)\nEvents:              <none>\n"
Jan 23 22:07:09.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 describe namespace kubectl-1859'
Jan 23 22:07:10.052: INFO: stderr: ""
Jan 23 22:07:10.052: INFO: stdout: "Name:         kubectl-1859\nLabels:       e2e-framework=kubectl\n              e2e-run=88e364ac-9d03-442f-b032-9a9c415c90f9\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:07:10.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1859" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":280,"completed":6,"skipped":58,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:07:10.066: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-d578386b-5fad-4d18-83a9-5bc7913d428d
STEP: Creating a pod to test consume configMaps
Jan 23 22:07:10.125: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-90059194-43f9-4de5-8733-6d4f19b7b706" in namespace "projected-6942" to be "success or failure"
Jan 23 22:07:10.129: INFO: Pod "pod-projected-configmaps-90059194-43f9-4de5-8733-6d4f19b7b706": Phase="Pending", Reason="", readiness=false. Elapsed: 3.764157ms
Jan 23 22:07:12.133: INFO: Pod "pod-projected-configmaps-90059194-43f9-4de5-8733-6d4f19b7b706": Phase="Running", Reason="", readiness=true. Elapsed: 2.008357146s
Jan 23 22:07:14.137: INFO: Pod "pod-projected-configmaps-90059194-43f9-4de5-8733-6d4f19b7b706": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012134264s
STEP: Saw pod success
Jan 23 22:07:14.137: INFO: Pod "pod-projected-configmaps-90059194-43f9-4de5-8733-6d4f19b7b706" satisfied condition "success or failure"
Jan 23 22:07:14.141: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod pod-projected-configmaps-90059194-43f9-4de5-8733-6d4f19b7b706 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 23 22:07:14.173: INFO: Waiting for pod pod-projected-configmaps-90059194-43f9-4de5-8733-6d4f19b7b706 to disappear
Jan 23 22:07:14.178: INFO: Pod pod-projected-configmaps-90059194-43f9-4de5-8733-6d4f19b7b706 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:07:14.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6942" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":280,"completed":7,"skipped":67,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:07:14.203: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test substitution in container's args
Jan 23 22:07:14.253: INFO: Waiting up to 5m0s for pod "var-expansion-716044d6-4f82-45f1-a744-cd7cc4b87183" in namespace "var-expansion-6674" to be "success or failure"
Jan 23 22:07:14.262: INFO: Pod "var-expansion-716044d6-4f82-45f1-a744-cd7cc4b87183": Phase="Pending", Reason="", readiness=false. Elapsed: 8.833361ms
Jan 23 22:07:16.267: INFO: Pod "var-expansion-716044d6-4f82-45f1-a744-cd7cc4b87183": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01359214s
Jan 23 22:07:18.270: INFO: Pod "var-expansion-716044d6-4f82-45f1-a744-cd7cc4b87183": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017320336s
STEP: Saw pod success
Jan 23 22:07:18.270: INFO: Pod "var-expansion-716044d6-4f82-45f1-a744-cd7cc4b87183" satisfied condition "success or failure"
Jan 23 22:07:18.275: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-v477d pod var-expansion-716044d6-4f82-45f1-a744-cd7cc4b87183 container dapi-container: <nil>
STEP: delete the pod
Jan 23 22:07:18.310: INFO: Waiting for pod var-expansion-716044d6-4f82-45f1-a744-cd7cc4b87183 to disappear
Jan 23 22:07:18.313: INFO: Pod var-expansion-716044d6-4f82-45f1-a744-cd7cc4b87183 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:07:18.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6674" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":280,"completed":8,"skipped":107,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:07:18.341: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 23 22:07:20.421: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:07:20.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3633" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":280,"completed":9,"skipped":140,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:07:20.454: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 23 22:07:20.512: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dabdb35a-2b50-4a99-9e68-a6420a04d9a1" in namespace "projected-2287" to be "success or failure"
Jan 23 22:07:20.528: INFO: Pod "downwardapi-volume-dabdb35a-2b50-4a99-9e68-a6420a04d9a1": Phase="Pending", Reason="", readiness=false. Elapsed: 15.804763ms
Jan 23 22:07:22.705: INFO: Pod "downwardapi-volume-dabdb35a-2b50-4a99-9e68-a6420a04d9a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.193285913s
STEP: Saw pod success
Jan 23 22:07:22.705: INFO: Pod "downwardapi-volume-dabdb35a-2b50-4a99-9e68-a6420a04d9a1" satisfied condition "success or failure"
Jan 23 22:07:22.729: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-v477d pod downwardapi-volume-dabdb35a-2b50-4a99-9e68-a6420a04d9a1 container client-container: <nil>
STEP: delete the pod
Jan 23 22:07:22.780: INFO: Waiting for pod downwardapi-volume-dabdb35a-2b50-4a99-9e68-a6420a04d9a1 to disappear
Jan 23 22:07:22.784: INFO: Pod downwardapi-volume-dabdb35a-2b50-4a99-9e68-a6420a04d9a1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:07:22.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2287" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":10,"skipped":150,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:07:22.799: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 23 22:07:22.850: INFO: Waiting up to 5m0s for pod "downwardapi-volume-345eb475-fc7c-405c-96de-fc6f5a480b99" in namespace "downward-api-2707" to be "success or failure"
Jan 23 22:07:22.856: INFO: Pod "downwardapi-volume-345eb475-fc7c-405c-96de-fc6f5a480b99": Phase="Pending", Reason="", readiness=false. Elapsed: 5.301293ms
Jan 23 22:07:24.886: INFO: Pod "downwardapi-volume-345eb475-fc7c-405c-96de-fc6f5a480b99": Phase="Running", Reason="", readiness=true. Elapsed: 2.034976741s
Jan 23 22:07:26.892: INFO: Pod "downwardapi-volume-345eb475-fc7c-405c-96de-fc6f5a480b99": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041215776s
STEP: Saw pod success
Jan 23 22:07:26.892: INFO: Pod "downwardapi-volume-345eb475-fc7c-405c-96de-fc6f5a480b99" satisfied condition "success or failure"
Jan 23 22:07:26.896: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-v477d pod downwardapi-volume-345eb475-fc7c-405c-96de-fc6f5a480b99 container client-container: <nil>
STEP: delete the pod
Jan 23 22:07:26.932: INFO: Waiting for pod downwardapi-volume-345eb475-fc7c-405c-96de-fc6f5a480b99 to disappear
Jan 23 22:07:26.938: INFO: Pod downwardapi-volume-345eb475-fc7c-405c-96de-fc6f5a480b99 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:07:26.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2707" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":280,"completed":11,"skipped":154,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:07:26.960: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:07:44.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4658" for this suite.

• [SLOW TEST:16.148 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":280,"completed":12,"skipped":159,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:07:44.730: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Jan 23 22:07:47.348: INFO: Successfully updated pod "annotationupdateb44c741c-a01c-4ab2-9fcb-323da100a2d2"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:07:51.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4558" for this suite.

• [SLOW TEST:6.663 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":280,"completed":13,"skipped":170,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:07:51.400: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Jan 23 22:07:52.518: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:07:52.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0123 22:07:52.517887      21 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-6495" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":280,"completed":14,"skipped":197,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:07:52.536: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 23 22:07:53.548: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 23 22:07:56.616: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:07:56.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3471" for this suite.
STEP: Destroying namespace "webhook-3471-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":280,"completed":15,"skipped":201,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:07:57.086: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: validating api versions
Jan 23 22:07:57.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 api-versions'
Jan 23 22:07:57.365: INFO: stderr: ""
Jan 23 22:07:57.365: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbootstrap.cluster.x-k8s.io/v1alpha2\ncertificates.k8s.io/v1beta1\ncluster.x-k8s.io/v1alpha2\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\ninfrastructure.cluster.x-k8s.io/v1alpha2\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:07:57.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-991" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":280,"completed":16,"skipped":210,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:07:57.382: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 23 22:07:57.456: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8fa463f6-cbcd-46f4-944d-22b3bfc3513b" in namespace "downward-api-1538" to be "success or failure"
Jan 23 22:07:57.467: INFO: Pod "downwardapi-volume-8fa463f6-cbcd-46f4-944d-22b3bfc3513b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.575001ms
Jan 23 22:07:59.473: INFO: Pod "downwardapi-volume-8fa463f6-cbcd-46f4-944d-22b3bfc3513b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016941326s
STEP: Saw pod success
Jan 23 22:07:59.474: INFO: Pod "downwardapi-volume-8fa463f6-cbcd-46f4-944d-22b3bfc3513b" satisfied condition "success or failure"
Jan 23 22:07:59.481: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod downwardapi-volume-8fa463f6-cbcd-46f4-944d-22b3bfc3513b container client-container: <nil>
STEP: delete the pod
Jan 23 22:07:59.527: INFO: Waiting for pod downwardapi-volume-8fa463f6-cbcd-46f4-944d-22b3bfc3513b to disappear
Jan 23 22:07:59.538: INFO: Pod downwardapi-volume-8fa463f6-cbcd-46f4-944d-22b3bfc3513b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:07:59.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1538" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":280,"completed":17,"skipped":212,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:07:59.563: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:08:01.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-917" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":280,"completed":18,"skipped":225,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:08:01.660: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:08:20.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4451" for this suite.

• [SLOW TEST:17.117 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":280,"completed":19,"skipped":237,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:08:20.451: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service externalname-service with the type=ExternalName in namespace services-9901
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-9901
I0123 22:08:20.549575      21 runners.go:189] Created replication controller with name: externalname-service, namespace: services-9901, replica count: 2
I0123 22:08:23.602191      21 runners.go:189] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 23 22:08:23.602: INFO: Creating new exec pod
Jan 23 22:08:26.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 exec --namespace=services-9901 execpodjw8gs -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jan 23 22:08:27.042: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 23 22:08:27.042: INFO: stdout: ""
Jan 23 22:08:27.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 exec --namespace=services-9901 execpodjw8gs -- /bin/sh -x -c nc -zv -t -w 2 100.66.57.58 80'
Jan 23 22:08:27.680: INFO: stderr: "+ nc -zv -t -w 2 100.66.57.58 80\nConnection to 100.66.57.58 80 port [tcp/http] succeeded!\n"
Jan 23 22:08:27.686: INFO: stdout: ""
Jan 23 22:08:27.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 exec --namespace=services-9901 execpodjw8gs -- /bin/sh -x -c nc -zv -t -w 2 10.78.220.23 31363'
Jan 23 22:08:28.113: INFO: stderr: "+ nc -zv -t -w 2 10.78.220.23 31363\nConnection to 10.78.220.23 31363 port [tcp/31363] succeeded!\n"
Jan 23 22:08:28.114: INFO: stdout: ""
Jan 23 22:08:28.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 exec --namespace=services-9901 execpodjw8gs -- /bin/sh -x -c nc -zv -t -w 2 10.78.209.78 31363'
Jan 23 22:08:28.571: INFO: stderr: "+ nc -zv -t -w 2 10.78.209.78 31363\nConnection to 10.78.209.78 31363 port [tcp/31363] succeeded!\n"
Jan 23 22:08:28.571: INFO: stdout: ""
Jan 23 22:08:28.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 exec --namespace=services-9901 execpodjw8gs -- /bin/sh -x -c nc -zv -t -w 2 10.78.220.23 31363'
Jan 23 22:08:28.946: INFO: stderr: "+ nc -zv -t -w 2 10.78.220.23 31363\nConnection to 10.78.220.23 31363 port [tcp/31363] succeeded!\n"
Jan 23 22:08:28.946: INFO: stdout: ""
Jan 23 22:08:28.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 exec --namespace=services-9901 execpodjw8gs -- /bin/sh -x -c nc -zv -t -w 2 10.78.209.78 31363'
Jan 23 22:08:29.240: INFO: stderr: "+ nc -zv -t -w 2 10.78.209.78 31363\nConnection to 10.78.209.78 31363 port [tcp/31363] succeeded!\n"
Jan 23 22:08:29.241: INFO: stdout: ""
Jan 23 22:08:29.241: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:08:29.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9901" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:9.056 seconds]
[sig-network] Services
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":280,"completed":20,"skipped":242,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:08:29.532: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 23 22:08:29.670: INFO: Waiting up to 5m0s for pod "busybox-user-65534-b532eefb-c43e-4a94-8a6f-6cb1e1cea5e7" in namespace "security-context-test-5436" to be "success or failure"
Jan 23 22:08:29.698: INFO: Pod "busybox-user-65534-b532eefb-c43e-4a94-8a6f-6cb1e1cea5e7": Phase="Pending", Reason="", readiness=false. Elapsed: 27.712909ms
Jan 23 22:08:31.703: INFO: Pod "busybox-user-65534-b532eefb-c43e-4a94-8a6f-6cb1e1cea5e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033258691s
Jan 23 22:08:33.712: INFO: Pod "busybox-user-65534-b532eefb-c43e-4a94-8a6f-6cb1e1cea5e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041415879s
Jan 23 22:08:33.712: INFO: Pod "busybox-user-65534-b532eefb-c43e-4a94-8a6f-6cb1e1cea5e7" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:08:33.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5436" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":21,"skipped":261,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:08:33.739: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:125
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jan 23 22:08:34.198: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jan 23 22:08:37.891: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715414114, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715414114, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715414114, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715414114, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 23 22:08:40.924: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 23 22:08:40.939: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:08:43.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-2895" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:136

• [SLOW TEST:7.944 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":280,"completed":22,"skipped":306,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:08:43.366: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-a2174910-a343-46bf-902b-8f5312196b10
STEP: Creating a pod to test consume configMaps
Jan 23 22:08:43.578: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ccea8679-164c-48b9-a4a8-7dcde9476db5" in namespace "projected-5859" to be "success or failure"
Jan 23 22:08:43.590: INFO: Pod "pod-projected-configmaps-ccea8679-164c-48b9-a4a8-7dcde9476db5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.381578ms
Jan 23 22:08:45.594: INFO: Pod "pod-projected-configmaps-ccea8679-164c-48b9-a4a8-7dcde9476db5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016500534s
Jan 23 22:08:47.600: INFO: Pod "pod-projected-configmaps-ccea8679-164c-48b9-a4a8-7dcde9476db5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022173065s
STEP: Saw pod success
Jan 23 22:08:47.600: INFO: Pod "pod-projected-configmaps-ccea8679-164c-48b9-a4a8-7dcde9476db5" satisfied condition "success or failure"
Jan 23 22:08:47.604: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod pod-projected-configmaps-ccea8679-164c-48b9-a4a8-7dcde9476db5 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 23 22:08:47.639: INFO: Waiting for pod pod-projected-configmaps-ccea8679-164c-48b9-a4a8-7dcde9476db5 to disappear
Jan 23 22:08:47.644: INFO: Pod pod-projected-configmaps-ccea8679-164c-48b9-a4a8-7dcde9476db5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:08:47.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5859" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":23,"skipped":326,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:08:47.664: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Update Demo
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:330
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the initial replication controller
Jan 23 22:08:47.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 create -f - --namespace=kubectl-1928'
Jan 23 22:08:48.040: INFO: stderr: ""
Jan 23 22:08:48.040: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 23 22:08:48.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1928'
Jan 23 22:08:48.122: INFO: stderr: ""
Jan 23 22:08:48.122: INFO: stdout: "update-demo-nautilus-2pn8x update-demo-nautilus-htrsh "
Jan 23 22:08:48.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get pods update-demo-nautilus-2pn8x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1928'
Jan 23 22:08:48.215: INFO: stderr: ""
Jan 23 22:08:48.215: INFO: stdout: ""
Jan 23 22:08:48.215: INFO: update-demo-nautilus-2pn8x is created but not running
Jan 23 22:08:53.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1928'
Jan 23 22:08:53.298: INFO: stderr: ""
Jan 23 22:08:53.298: INFO: stdout: "update-demo-nautilus-2pn8x update-demo-nautilus-htrsh "
Jan 23 22:08:53.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get pods update-demo-nautilus-2pn8x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1928'
Jan 23 22:08:53.379: INFO: stderr: ""
Jan 23 22:08:53.379: INFO: stdout: "true"
Jan 23 22:08:53.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get pods update-demo-nautilus-2pn8x -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1928'
Jan 23 22:08:53.492: INFO: stderr: ""
Jan 23 22:08:53.492: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 23 22:08:53.492: INFO: validating pod update-demo-nautilus-2pn8x
Jan 23 22:08:53.512: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 23 22:08:53.512: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 23 22:08:53.512: INFO: update-demo-nautilus-2pn8x is verified up and running
Jan 23 22:08:53.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get pods update-demo-nautilus-htrsh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1928'
Jan 23 22:08:53.597: INFO: stderr: ""
Jan 23 22:08:53.597: INFO: stdout: "true"
Jan 23 22:08:53.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get pods update-demo-nautilus-htrsh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1928'
Jan 23 22:08:53.689: INFO: stderr: ""
Jan 23 22:08:53.689: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 23 22:08:53.689: INFO: validating pod update-demo-nautilus-htrsh
Jan 23 22:08:53.705: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 23 22:08:53.705: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 23 22:08:53.705: INFO: update-demo-nautilus-htrsh is verified up and running
STEP: rolling-update to new replication controller
Jan 23 22:08:53.709: INFO: scanned /root for discovery docs: <nil>
Jan 23 22:08:53.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-1928'
Jan 23 22:09:18.030: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jan 23 22:09:18.031: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 23 22:09:18.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1928'
Jan 23 22:09:18.151: INFO: stderr: ""
Jan 23 22:09:18.151: INFO: stdout: "update-demo-kitten-l9ktl update-demo-kitten-v2q6s "
Jan 23 22:09:18.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get pods update-demo-kitten-l9ktl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1928'
Jan 23 22:09:18.237: INFO: stderr: ""
Jan 23 22:09:18.237: INFO: stdout: "true"
Jan 23 22:09:18.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get pods update-demo-kitten-l9ktl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1928'
Jan 23 22:09:18.325: INFO: stderr: ""
Jan 23 22:09:18.325: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jan 23 22:09:18.325: INFO: validating pod update-demo-kitten-l9ktl
Jan 23 22:09:18.337: INFO: got data: {
  "image": "kitten.jpg"
}

Jan 23 22:09:18.338: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jan 23 22:09:18.338: INFO: update-demo-kitten-l9ktl is verified up and running
Jan 23 22:09:18.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get pods update-demo-kitten-v2q6s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1928'
Jan 23 22:09:18.432: INFO: stderr: ""
Jan 23 22:09:18.432: INFO: stdout: "true"
Jan 23 22:09:18.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get pods update-demo-kitten-v2q6s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1928'
Jan 23 22:09:18.522: INFO: stderr: ""
Jan 23 22:09:18.522: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jan 23 22:09:18.522: INFO: validating pod update-demo-kitten-v2q6s
Jan 23 22:09:18.529: INFO: got data: {
  "image": "kitten.jpg"
}

Jan 23 22:09:18.529: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jan 23 22:09:18.529: INFO: update-demo-kitten-v2q6s is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:09:18.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1928" for this suite.

• [SLOW TEST:29.134 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:328
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should do a rolling update of a replication controller  [Conformance]","total":280,"completed":24,"skipped":356,"failed":0}
S
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:09:18.546: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test substitution in container's command
Jan 23 22:09:18.609: INFO: Waiting up to 5m0s for pod "var-expansion-c9e5aec0-5f61-4e8b-a665-94c9acdd78c6" in namespace "var-expansion-3451" to be "success or failure"
Jan 23 22:09:18.615: INFO: Pod "var-expansion-c9e5aec0-5f61-4e8b-a665-94c9acdd78c6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.092383ms
Jan 23 22:09:20.620: INFO: Pod "var-expansion-c9e5aec0-5f61-4e8b-a665-94c9acdd78c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011345955s
STEP: Saw pod success
Jan 23 22:09:20.620: INFO: Pod "var-expansion-c9e5aec0-5f61-4e8b-a665-94c9acdd78c6" satisfied condition "success or failure"
Jan 23 22:09:20.624: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod var-expansion-c9e5aec0-5f61-4e8b-a665-94c9acdd78c6 container dapi-container: <nil>
STEP: delete the pod
Jan 23 22:09:20.657: INFO: Waiting for pod var-expansion-c9e5aec0-5f61-4e8b-a665-94c9acdd78c6 to disappear
Jan 23 22:09:20.665: INFO: Pod var-expansion-c9e5aec0-5f61-4e8b-a665-94c9acdd78c6 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:09:20.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3451" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":280,"completed":25,"skipped":357,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:09:20.686: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 23 22:09:21.769: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 23 22:09:24.816: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:09:24.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3465" for this suite.
STEP: Destroying namespace "webhook-3465-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":280,"completed":26,"skipped":376,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:09:25.037: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 23 22:09:29.345: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:09:29.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9132" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":280,"completed":27,"skipped":396,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:09:29.416: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 23 22:09:29.631: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:09:29.640: INFO: Number of nodes with available pods: 0
Jan 23 22:09:29.640: INFO: Node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz is running more than one daemon pod
Jan 23 22:09:30.661: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:09:30.669: INFO: Number of nodes with available pods: 0
Jan 23 22:09:30.669: INFO: Node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz is running more than one daemon pod
Jan 23 22:09:31.650: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:09:31.655: INFO: Number of nodes with available pods: 1
Jan 23 22:09:31.655: INFO: Node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz is running more than one daemon pod
Jan 23 22:09:32.647: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:09:32.653: INFO: Number of nodes with available pods: 3
Jan 23 22:09:32.653: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jan 23 22:09:32.728: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:09:32.742: INFO: Number of nodes with available pods: 2
Jan 23 22:09:32.742: INFO: Node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz is running more than one daemon pod
Jan 23 22:09:33.751: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:09:33.757: INFO: Number of nodes with available pods: 2
Jan 23 22:09:33.757: INFO: Node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz is running more than one daemon pod
Jan 23 22:09:34.751: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:09:34.762: INFO: Number of nodes with available pods: 3
Jan 23 22:09:34.762: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9421, will wait for the garbage collector to delete the pods
Jan 23 22:09:34.840: INFO: Deleting DaemonSet.extensions daemon-set took: 9.311625ms
Jan 23 22:09:34.942: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.481996ms
Jan 23 22:09:46.862: INFO: Number of nodes with available pods: 0
Jan 23 22:09:46.862: INFO: Number of running nodes: 0, number of available pods: 0
Jan 23 22:09:46.876: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9421/daemonsets","resourceVersion":"399098"},"items":null}

Jan 23 22:09:46.881: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9421/pods","resourceVersion":"399098"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:09:46.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9421" for this suite.

• [SLOW TEST:15.788 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":280,"completed":28,"skipped":419,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:09:46.926: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 23 22:09:49.004: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:09:49.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6080" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":280,"completed":29,"skipped":446,"failed":0}

------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:09:49.044: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-8312
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a new StatefulSet
Jan 23 22:09:49.152: INFO: Found 0 stateful pods, waiting for 3
Jan 23 22:09:59.159: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 23 22:09:59.159: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 23 22:09:59.159: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jan 23 22:09:59.192: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jan 23 22:10:09.246: INFO: Updating stateful set ss2
Jan 23 22:10:09.274: INFO: Waiting for Pod statefulset-8312/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Jan 23 22:10:20.673: INFO: Found 2 stateful pods, waiting for 3
Jan 23 22:10:30.679: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 23 22:10:30.679: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 23 22:10:30.679: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jan 23 22:10:30.707: INFO: Updating stateful set ss2
Jan 23 22:10:30.719: INFO: Waiting for Pod statefulset-8312/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jan 23 22:10:40.728: INFO: Waiting for Pod statefulset-8312/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jan 23 22:10:50.747: INFO: Updating stateful set ss2
Jan 23 22:10:50.765: INFO: Waiting for StatefulSet statefulset-8312/ss2 to complete update
Jan 23 22:10:50.765: INFO: Waiting for Pod statefulset-8312/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jan 23 22:11:02.121: INFO: Deleting all statefulset in ns statefulset-8312
Jan 23 22:11:02.125: INFO: Scaling statefulset ss2 to 0
Jan 23 22:11:33.638: INFO: Waiting for statefulset status.replicas updated to 0
Jan 23 22:11:33.651: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:11:33.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8312" for this suite.

• [SLOW TEST:100.582 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":280,"completed":30,"skipped":446,"failed":0}
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:11:33.756: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:344
Jan 23 22:11:33.935: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 23 22:12:36.989: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 23 22:12:36.996: INFO: Starting informer...
STEP: Starting pods...
Jan 23 22:12:37.231: INFO: Pod1 is running on management-cluster-1-17-2-md-0-5f64bb5777-dlwcz. Tainting Node
Jan 23 22:12:39.497: INFO: Pod2 is running on management-cluster-1-17-2-md-0-5f64bb5777-dlwcz. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Jan 23 22:12:53.988: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jan 23 22:13:06.268: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:13:07.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-9038" for this suite.

• [SLOW TEST:89.625 seconds]
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":280,"completed":31,"skipped":446,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:13:07.915: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating Agnhost RC
Jan 23 22:13:08.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 create -f - --namespace=kubectl-2986'
Jan 23 22:13:08.526: INFO: stderr: ""
Jan 23 22:13:08.526: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Jan 23 22:13:09.533: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 23 22:13:09.533: INFO: Found 0 / 1
Jan 23 22:13:10.531: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 23 22:13:10.531: INFO: Found 1 / 1
Jan 23 22:13:10.531: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jan 23 22:13:10.536: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 23 22:13:10.536: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 23 22:13:10.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 patch pod agnhost-master-wtmb2 --namespace=kubectl-2986 -p {"metadata":{"annotations":{"x":"y"}}}'
Jan 23 22:13:10.630: INFO: stderr: ""
Jan 23 22:13:10.630: INFO: stdout: "pod/agnhost-master-wtmb2 patched\n"
STEP: checking annotations
Jan 23 22:13:10.634: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 23 22:13:10.634: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:13:10.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2986" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":280,"completed":32,"skipped":456,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:13:10.655: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-map-87b0ca80-eda9-4ec7-9b81-028aada4e438
STEP: Creating a pod to test consume secrets
Jan 23 22:13:10.732: INFO: Waiting up to 5m0s for pod "pod-secrets-22383c01-7ac0-431e-b079-09fddcacc8e7" in namespace "secrets-5289" to be "success or failure"
Jan 23 22:13:10.742: INFO: Pod "pod-secrets-22383c01-7ac0-431e-b079-09fddcacc8e7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.272ms
Jan 23 22:13:12.747: INFO: Pod "pod-secrets-22383c01-7ac0-431e-b079-09fddcacc8e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014836902s
STEP: Saw pod success
Jan 23 22:13:12.747: INFO: Pod "pod-secrets-22383c01-7ac0-431e-b079-09fddcacc8e7" satisfied condition "success or failure"
Jan 23 22:13:12.752: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f pod pod-secrets-22383c01-7ac0-431e-b079-09fddcacc8e7 container secret-volume-test: <nil>
STEP: delete the pod
Jan 23 22:13:12.808: INFO: Waiting for pod pod-secrets-22383c01-7ac0-431e-b079-09fddcacc8e7 to disappear
Jan 23 22:13:12.813: INFO: Pod pod-secrets-22383c01-7ac0-431e-b079-09fddcacc8e7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:13:12.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5289" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":33,"skipped":474,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:13:12.838: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Jan 23 22:13:12.885: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
Jan 23 22:13:17.103: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:13:30.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7061" for this suite.

• [SLOW TEST:18.084 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":280,"completed":34,"skipped":495,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:13:30.924: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 23 22:13:31.434: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 23 22:13:34.508: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Jan 23 22:13:36.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 attach --namespace=webhook-2814 to-be-attached-pod -i -c=container1'
Jan 23 22:13:36.668: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:13:36.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2814" for this suite.
STEP: Destroying namespace "webhook-2814-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.865 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":280,"completed":35,"skipped":533,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:13:36.799: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 23 22:13:36.884: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:13:36.888: INFO: Number of nodes with available pods: 0
Jan 23 22:13:36.888: INFO: Node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz is running more than one daemon pod
Jan 23 22:13:37.899: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:13:37.912: INFO: Number of nodes with available pods: 0
Jan 23 22:13:37.913: INFO: Node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz is running more than one daemon pod
Jan 23 22:13:38.894: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:13:38.899: INFO: Number of nodes with available pods: 3
Jan 23 22:13:38.899: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jan 23 22:13:38.918: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:13:38.921: INFO: Number of nodes with available pods: 2
Jan 23 22:13:38.921: INFO: Node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f is running more than one daemon pod
Jan 23 22:13:39.928: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:13:39.933: INFO: Number of nodes with available pods: 2
Jan 23 22:13:39.933: INFO: Node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f is running more than one daemon pod
Jan 23 22:13:42.489: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:13:42.492: INFO: Number of nodes with available pods: 2
Jan 23 22:13:42.492: INFO: Node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f is running more than one daemon pod
Jan 23 22:13:43.491: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:13:43.496: INFO: Number of nodes with available pods: 2
Jan 23 22:13:43.496: INFO: Node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f is running more than one daemon pod
Jan 23 22:13:44.490: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:13:44.495: INFO: Number of nodes with available pods: 2
Jan 23 22:13:44.495: INFO: Node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f is running more than one daemon pod
Jan 23 22:13:45.489: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:13:45.495: INFO: Number of nodes with available pods: 2
Jan 23 22:13:45.495: INFO: Node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f is running more than one daemon pod
Jan 23 22:13:46.494: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:13:46.499: INFO: Number of nodes with available pods: 2
Jan 23 22:13:46.499: INFO: Node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f is running more than one daemon pod
Jan 23 22:13:47.490: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:13:47.494: INFO: Number of nodes with available pods: 2
Jan 23 22:13:47.494: INFO: Node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f is running more than one daemon pod
Jan 23 22:13:48.488: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:13:48.493: INFO: Number of nodes with available pods: 2
Jan 23 22:13:48.493: INFO: Node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f is running more than one daemon pod
Jan 23 22:13:49.491: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:13:49.496: INFO: Number of nodes with available pods: 2
Jan 23 22:13:49.496: INFO: Node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f is running more than one daemon pod
Jan 23 22:13:50.489: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:13:50.492: INFO: Number of nodes with available pods: 2
Jan 23 22:13:50.492: INFO: Node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f is running more than one daemon pod
Jan 23 22:13:51.497: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:13:51.505: INFO: Number of nodes with available pods: 2
Jan 23 22:13:51.505: INFO: Node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f is running more than one daemon pod
Jan 23 22:13:52.491: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:13:52.495: INFO: Number of nodes with available pods: 2
Jan 23 22:13:52.496: INFO: Node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f is running more than one daemon pod
Jan 23 22:13:53.489: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:13:53.493: INFO: Number of nodes with available pods: 3
Jan 23 22:13:53.493: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5190, will wait for the garbage collector to delete the pods
Jan 23 22:13:53.560: INFO: Deleting DaemonSet.extensions daemon-set took: 9.693324ms
Jan 23 22:13:53.660: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.450704ms
Jan 23 22:13:58.166: INFO: Number of nodes with available pods: 0
Jan 23 22:13:58.166: INFO: Number of running nodes: 0, number of available pods: 0
Jan 23 22:13:58.170: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5190/daemonsets","resourceVersion":"400950"},"items":null}

Jan 23 22:13:58.174: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5190/pods","resourceVersion":"400950"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:13:58.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5190" for this suite.

• [SLOW TEST:19.842 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":280,"completed":36,"skipped":540,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:13:58.207: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: getting the auto-created API token
Jan 23 22:13:58.830: INFO: created pod pod-service-account-defaultsa
Jan 23 22:13:58.830: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jan 23 22:13:58.836: INFO: created pod pod-service-account-mountsa
Jan 23 22:13:58.836: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jan 23 22:13:58.861: INFO: created pod pod-service-account-nomountsa
Jan 23 22:13:58.861: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jan 23 22:13:58.866: INFO: created pod pod-service-account-defaultsa-mountspec
Jan 23 22:13:58.866: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jan 23 22:13:58.888: INFO: created pod pod-service-account-mountsa-mountspec
Jan 23 22:13:58.889: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jan 23 22:13:58.926: INFO: created pod pod-service-account-nomountsa-mountspec
Jan 23 22:13:58.926: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jan 23 22:13:58.962: INFO: created pod pod-service-account-defaultsa-nomountspec
Jan 23 22:13:58.962: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jan 23 22:13:58.998: INFO: created pod pod-service-account-mountsa-nomountspec
Jan 23 22:13:58.998: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jan 23 22:13:59.027: INFO: created pod pod-service-account-nomountsa-nomountspec
Jan 23 22:13:59.027: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:13:59.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8903" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":280,"completed":37,"skipped":555,"failed":0}
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:13:59.084: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-configmap-b9s7
STEP: Creating a pod to test atomic-volume-subpath
Jan 23 22:13:59.162: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-b9s7" in namespace "subpath-9299" to be "success or failure"
Jan 23 22:13:59.170: INFO: Pod "pod-subpath-test-configmap-b9s7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.623122ms
Jan 23 22:14:01.174: INFO: Pod "pod-subpath-test-configmap-b9s7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011595174s
Jan 23 22:14:03.179: INFO: Pod "pod-subpath-test-configmap-b9s7": Phase="Running", Reason="", readiness=true. Elapsed: 4.016093926s
Jan 23 22:14:05.188: INFO: Pod "pod-subpath-test-configmap-b9s7": Phase="Running", Reason="", readiness=true. Elapsed: 6.025029414s
Jan 23 22:14:07.194: INFO: Pod "pod-subpath-test-configmap-b9s7": Phase="Running", Reason="", readiness=true. Elapsed: 8.031143216s
Jan 23 22:14:09.202: INFO: Pod "pod-subpath-test-configmap-b9s7": Phase="Running", Reason="", readiness=true. Elapsed: 10.039163111s
Jan 23 22:14:11.206: INFO: Pod "pod-subpath-test-configmap-b9s7": Phase="Running", Reason="", readiness=true. Elapsed: 12.043133126s
Jan 23 22:14:13.210: INFO: Pod "pod-subpath-test-configmap-b9s7": Phase="Running", Reason="", readiness=true. Elapsed: 14.047797818s
Jan 23 22:14:16.815: INFO: Pod "pod-subpath-test-configmap-b9s7": Phase="Running", Reason="", readiness=true. Elapsed: 16.052288393s
Jan 23 22:14:18.819: INFO: Pod "pod-subpath-test-configmap-b9s7": Phase="Running", Reason="", readiness=true. Elapsed: 18.056535103s
Jan 23 22:14:20.823: INFO: Pod "pod-subpath-test-configmap-b9s7": Phase="Running", Reason="", readiness=true. Elapsed: 20.061004443s
Jan 23 22:14:22.828: INFO: Pod "pod-subpath-test-configmap-b9s7": Phase="Running", Reason="", readiness=true. Elapsed: 22.06569312s
Jan 23 22:14:24.837: INFO: Pod "pod-subpath-test-configmap-b9s7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.075143979s
STEP: Saw pod success
Jan 23 22:14:24.838: INFO: Pod "pod-subpath-test-configmap-b9s7" satisfied condition "success or failure"
Jan 23 22:14:24.845: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod pod-subpath-test-configmap-b9s7 container test-container-subpath-configmap-b9s7: <nil>
STEP: delete the pod
Jan 23 22:14:24.921: INFO: Waiting for pod pod-subpath-test-configmap-b9s7 to disappear
Jan 23 22:14:24.932: INFO: Pod pod-subpath-test-configmap-b9s7 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-b9s7
Jan 23 22:14:24.933: INFO: Deleting pod "pod-subpath-test-configmap-b9s7" in namespace "subpath-9299"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:14:24.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9299" for this suite.

• [SLOW TEST:24.297 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":280,"completed":38,"skipped":556,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:14:24.985: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: set up a multi version CRD
Jan 23 22:14:25.052: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:14:44.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3802" for this suite.

• [SLOW TEST:19.665 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":280,"completed":39,"skipped":562,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:14:44.662: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Jan 23 22:14:44.727: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:14:50.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7008" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":280,"completed":40,"skipped":606,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:14:51.013: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service endpoint-test2 in namespace services-6853
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6853 to expose endpoints map[]
Jan 23 22:14:51.114: INFO: successfully validated that service endpoint-test2 in namespace services-6853 exposes endpoints map[] (10.956035ms elapsed)
STEP: Creating pod pod1 in namespace services-6853
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6853 to expose endpoints map[pod1:[80]]
Jan 23 22:14:53.164: INFO: successfully validated that service endpoint-test2 in namespace services-6853 exposes endpoints map[pod1:[80]] (2.032106378s elapsed)
STEP: Creating pod pod2 in namespace services-6853
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6853 to expose endpoints map[pod1:[80] pod2:[80]]
Jan 23 22:14:55.223: INFO: successfully validated that service endpoint-test2 in namespace services-6853 exposes endpoints map[pod1:[80] pod2:[80]] (2.050807217s elapsed)
STEP: Deleting pod pod1 in namespace services-6853
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6853 to expose endpoints map[pod2:[80]]
Jan 23 22:14:56.280: INFO: successfully validated that service endpoint-test2 in namespace services-6853 exposes endpoints map[pod2:[80]] (1.03718916s elapsed)
STEP: Deleting pod pod2 in namespace services-6853
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6853 to expose endpoints map[]
Jan 23 22:14:57.305: INFO: successfully validated that service endpoint-test2 in namespace services-6853 exposes endpoints map[] (1.012283278s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:14:57.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6853" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:6.409 seconds]
[sig-network] Services
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":280,"completed":41,"skipped":611,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:14:57.428: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-5008
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 23 22:14:57.516: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jan 23 22:15:23.394: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.117.222.253:8080/dial?request=hostname&protocol=http&host=100.117.222.252&port=8080&tries=1'] Namespace:pod-network-test-5008 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 23 22:15:23.395: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
Jan 23 22:15:23.687: INFO: Waiting for responses: map[]
Jan 23 22:15:23.691: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.117.222.253:8080/dial?request=hostname&protocol=http&host=100.120.200.22&port=8080&tries=1'] Namespace:pod-network-test-5008 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 23 22:15:23.691: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
Jan 23 22:15:23.897: INFO: Waiting for responses: map[]
Jan 23 22:15:23.902: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.117.222.253:8080/dial?request=hostname&protocol=http&host=100.105.119.100&port=8080&tries=1'] Namespace:pod-network-test-5008 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 23 22:15:23.902: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
Jan 23 22:15:24.096: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:15:24.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5008" for this suite.

• [SLOW TEST:25.013 seconds]
[sig-network] Networking
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":42,"skipped":619,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:15:24.116: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 23 22:15:24.173: INFO: Waiting up to 5m0s for pod "downwardapi-volume-938d47c2-086f-4437-bb43-70ee269f7052" in namespace "downward-api-2923" to be "success or failure"
Jan 23 22:15:24.201: INFO: Pod "downwardapi-volume-938d47c2-086f-4437-bb43-70ee269f7052": Phase="Pending", Reason="", readiness=false. Elapsed: 28.478695ms
Jan 23 22:15:26.206: INFO: Pod "downwardapi-volume-938d47c2-086f-4437-bb43-70ee269f7052": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.03343051s
STEP: Saw pod success
Jan 23 22:15:26.206: INFO: Pod "downwardapi-volume-938d47c2-086f-4437-bb43-70ee269f7052" satisfied condition "success or failure"
Jan 23 22:15:26.211: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f pod downwardapi-volume-938d47c2-086f-4437-bb43-70ee269f7052 container client-container: <nil>
STEP: delete the pod
Jan 23 22:15:26.276: INFO: Waiting for pod downwardapi-volume-938d47c2-086f-4437-bb43-70ee269f7052 to disappear
Jan 23 22:15:26.281: INFO: Pod downwardapi-volume-938d47c2-086f-4437-bb43-70ee269f7052 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:15:26.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2923" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":43,"skipped":633,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:15:26.302: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Jan 23 22:15:26.351: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 23 22:15:26.378: INFO: Waiting for terminating namespaces to be deleted...
Jan 23 22:15:26.382: INFO: 
Logging pods the kubelet thinks is on node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz before test
Jan 23 22:15:26.400: INFO: test-container-pod from pod-network-test-5008 started at 2020-01-23 22:15:20 +0000 UTC (1 container statuses recorded)
Jan 23 22:15:26.400: INFO: 	Container webserver ready: true, restart count 0
Jan 23 22:15:26.400: INFO: netserver-0 from pod-network-test-5008 started at 2020-01-23 22:14:57 +0000 UTC (1 container statuses recorded)
Jan 23 22:15:26.401: INFO: 	Container webserver ready: true, restart count 0
Jan 23 22:15:26.401: INFO: kube-proxy-qfmln from kube-system started at 2020-01-22 22:09:02 +0000 UTC (1 container statuses recorded)
Jan 23 22:15:26.401: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 23 22:15:26.401: INFO: sonobuoy-systemd-logs-daemon-set-82eb3c000b9d48f1-m9c8t from sonobuoy started at 2020-01-23 22:01:22 +0000 UTC (2 container statuses recorded)
Jan 23 22:15:26.401: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 23 22:15:26.401: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 23 22:15:26.401: INFO: vsphere-csi-node-4knv2 from kube-system started at 2020-01-23 22:13:07 +0000 UTC (3 container statuses recorded)
Jan 23 22:15:26.401: INFO: 	Container liveness-probe ready: true, restart count 0
Jan 23 22:15:26.401: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 23 22:15:26.401: INFO: 	Container vsphere-csi-node ready: true, restart count 0
Jan 23 22:15:26.401: INFO: host-test-container-pod from pod-network-test-5008 started at 2020-01-23 22:15:20 +0000 UTC (1 container statuses recorded)
Jan 23 22:15:26.401: INFO: 	Container agnhost ready: true, restart count 0
Jan 23 22:15:26.401: INFO: calico-node-zvhsz from kube-system started at 2020-01-22 22:09:02 +0000 UTC (1 container statuses recorded)
Jan 23 22:15:26.401: INFO: 	Container calico-node ready: true, restart count 0
Jan 23 22:15:26.401: INFO: 
Logging pods the kubelet thinks is on node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f before test
Jan 23 22:15:26.412: INFO: vsphere-csi-node-rd47x from kube-system started at 2020-01-22 22:09:27 +0000 UTC (3 container statuses recorded)
Jan 23 22:15:26.413: INFO: 	Container liveness-probe ready: true, restart count 0
Jan 23 22:15:26.413: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 23 22:15:26.413: INFO: 	Container vsphere-csi-node ready: true, restart count 0
Jan 23 22:15:26.413: INFO: sonobuoy from sonobuoy started at 2020-01-23 22:01:11 +0000 UTC (1 container statuses recorded)
Jan 23 22:15:26.413: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 23 22:15:26.413: INFO: kube-proxy-n9fnt from kube-system started at 2020-01-22 22:09:05 +0000 UTC (1 container statuses recorded)
Jan 23 22:15:26.413: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 23 22:15:26.413: INFO: sonobuoy-systemd-logs-daemon-set-82eb3c000b9d48f1-mkq6m from sonobuoy started at 2020-01-23 22:01:23 +0000 UTC (2 container statuses recorded)
Jan 23 22:15:26.413: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 23 22:15:26.413: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 23 22:15:26.413: INFO: calico-node-27bq2 from kube-system started at 2020-01-22 22:09:05 +0000 UTC (1 container statuses recorded)
Jan 23 22:15:26.413: INFO: 	Container calico-node ready: true, restart count 0
Jan 23 22:15:26.413: INFO: netserver-1 from pod-network-test-5008 started at 2020-01-23 22:14:57 +0000 UTC (1 container statuses recorded)
Jan 23 22:15:26.413: INFO: 	Container webserver ready: true, restart count 0
Jan 23 22:15:26.413: INFO: 
Logging pods the kubelet thinks is on node management-cluster-1-17-2-md-0-5f64bb5777-v477d before test
Jan 23 22:15:26.443: INFO: vsphere-csi-node-h9rth from kube-system started at 2020-01-22 22:09:25 +0000 UTC (3 container statuses recorded)
Jan 23 22:15:26.443: INFO: 	Container liveness-probe ready: true, restart count 0
Jan 23 22:15:26.443: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 23 22:15:26.443: INFO: 	Container vsphere-csi-node ready: true, restart count 0
Jan 23 22:15:26.443: INFO: sonobuoy-systemd-logs-daemon-set-82eb3c000b9d48f1-fwpld from sonobuoy started at 2020-01-23 22:01:22 +0000 UTC (2 container statuses recorded)
Jan 23 22:15:26.443: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 23 22:15:26.443: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 23 22:15:26.443: INFO: calico-node-lbpft from kube-system started at 2020-01-22 22:09:03 +0000 UTC (1 container statuses recorded)
Jan 23 22:15:26.443: INFO: 	Container calico-node ready: true, restart count 0
Jan 23 22:15:26.443: INFO: sonobuoy-e2e-job-61735b81990a421a from sonobuoy started at 2020-01-23 22:01:22 +0000 UTC (2 container statuses recorded)
Jan 23 22:15:26.443: INFO: 	Container e2e ready: true, restart count 0
Jan 23 22:15:26.443: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 23 22:15:26.443: INFO: netserver-2 from pod-network-test-5008 started at 2020-01-23 22:14:57 +0000 UTC (1 container statuses recorded)
Jan 23 22:15:26.443: INFO: 	Container webserver ready: true, restart count 0
Jan 23 22:15:26.443: INFO: kube-proxy-7w2h9 from kube-system started at 2020-01-22 22:09:03 +0000 UTC (1 container statuses recorded)
Jan 23 22:15:26.443: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-3131f908-1e97-461b-941f-70b0135cc679 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-3131f908-1e97-461b-941f-70b0135cc679 off the node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f
STEP: verifying the node doesn't have the label kubernetes.io/e2e-3131f908-1e97-461b-941f-70b0135cc679
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:20:44.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8483" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:304.433 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":280,"completed":44,"skipped":648,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:20:44.563: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jan 23 22:20:48.748: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 23 22:20:48.753: INFO: Pod pod-with-prestop-http-hook still exists
Jan 23 22:20:50.754: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 23 22:20:50.768: INFO: Pod pod-with-prestop-http-hook still exists
Jan 23 22:20:52.754: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 23 22:20:52.762: INFO: Pod pod-with-prestop-http-hook still exists
Jan 23 22:20:54.754: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 23 22:20:54.759: INFO: Pod pod-with-prestop-http-hook still exists
Jan 23 22:20:56.754: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 23 22:20:56.758: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:20:56.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1858" for this suite.

• [SLOW TEST:12.272 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":280,"completed":45,"skipped":677,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:20:56.838: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-configmap-xg5b
STEP: Creating a pod to test atomic-volume-subpath
Jan 23 22:20:56.917: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-xg5b" in namespace "subpath-5587" to be "success or failure"
Jan 23 22:20:56.923: INFO: Pod "pod-subpath-test-configmap-xg5b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.760646ms
Jan 23 22:20:58.929: INFO: Pod "pod-subpath-test-configmap-xg5b": Phase="Running", Reason="", readiness=true. Elapsed: 2.011885015s
Jan 23 22:21:02.499: INFO: Pod "pod-subpath-test-configmap-xg5b": Phase="Running", Reason="", readiness=true. Elapsed: 4.017693148s
Jan 23 22:21:04.504: INFO: Pod "pod-subpath-test-configmap-xg5b": Phase="Running", Reason="", readiness=true. Elapsed: 6.022544571s
Jan 23 22:21:06.508: INFO: Pod "pod-subpath-test-configmap-xg5b": Phase="Running", Reason="", readiness=true. Elapsed: 8.026715199s
Jan 23 22:21:08.519: INFO: Pod "pod-subpath-test-configmap-xg5b": Phase="Running", Reason="", readiness=true. Elapsed: 10.037851404s
Jan 23 22:21:10.560: INFO: Pod "pod-subpath-test-configmap-xg5b": Phase="Running", Reason="", readiness=true. Elapsed: 12.078132854s
Jan 23 22:21:12.565: INFO: Pod "pod-subpath-test-configmap-xg5b": Phase="Running", Reason="", readiness=true. Elapsed: 14.083916838s
Jan 23 22:21:14.571: INFO: Pod "pod-subpath-test-configmap-xg5b": Phase="Running", Reason="", readiness=true. Elapsed: 16.089085788s
Jan 23 22:21:16.577: INFO: Pod "pod-subpath-test-configmap-xg5b": Phase="Running", Reason="", readiness=true. Elapsed: 18.095571421s
Jan 23 22:21:18.585: INFO: Pod "pod-subpath-test-configmap-xg5b": Phase="Running", Reason="", readiness=true. Elapsed: 20.103007595s
Jan 23 22:21:20.590: INFO: Pod "pod-subpath-test-configmap-xg5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.108850239s
STEP: Saw pod success
Jan 23 22:21:20.590: INFO: Pod "pod-subpath-test-configmap-xg5b" satisfied condition "success or failure"
Jan 23 22:21:20.598: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod pod-subpath-test-configmap-xg5b container test-container-subpath-configmap-xg5b: <nil>
STEP: delete the pod
Jan 23 22:21:20.634: INFO: Waiting for pod pod-subpath-test-configmap-xg5b to disappear
Jan 23 22:21:20.641: INFO: Pod pod-subpath-test-configmap-xg5b no longer exists
STEP: Deleting pod pod-subpath-test-configmap-xg5b
Jan 23 22:21:20.641: INFO: Deleting pod "pod-subpath-test-configmap-xg5b" in namespace "subpath-5587"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:21:20.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5587" for this suite.

• [SLOW TEST:22.254 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":280,"completed":46,"skipped":694,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:21:20.666: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 23 22:21:20.709: INFO: Creating ReplicaSet my-hostname-basic-ce66af55-dcf1-41ab-bca5-a8620a480ead
Jan 23 22:21:20.730: INFO: Pod name my-hostname-basic-ce66af55-dcf1-41ab-bca5-a8620a480ead: Found 0 pods out of 1
Jan 23 22:21:25.735: INFO: Pod name my-hostname-basic-ce66af55-dcf1-41ab-bca5-a8620a480ead: Found 1 pods out of 1
Jan 23 22:21:25.735: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-ce66af55-dcf1-41ab-bca5-a8620a480ead" is running
Jan 23 22:21:25.740: INFO: Pod "my-hostname-basic-ce66af55-dcf1-41ab-bca5-a8620a480ead-2w9j2" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-01-23 22:21:20 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-01-23 22:21:21 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-01-23 22:21:21 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-01-23 22:21:21 +0000 UTC Reason: Message:}])
Jan 23 22:21:25.741: INFO: Trying to dial the pod
Jan 23 22:21:30.757: INFO: Controller my-hostname-basic-ce66af55-dcf1-41ab-bca5-a8620a480ead: Got expected result from replica 1 [my-hostname-basic-ce66af55-dcf1-41ab-bca5-a8620a480ead-2w9j2]: "my-hostname-basic-ce66af55-dcf1-41ab-bca5-a8620a480ead-2w9j2", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:21:30.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2686" for this suite.

• [SLOW TEST:10.106 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":280,"completed":47,"skipped":720,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:21:30.779: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jan 23 22:21:42.591: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:21:42.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0123 22:21:42.590645      21 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-5811" for this suite.

• [SLOW TEST:10.231 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":280,"completed":48,"skipped":726,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:21:42.616: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 23 22:21:43.368: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 23 22:21:45.382: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715414902, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715414902, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715414902, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715414902, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 23 22:21:48.406: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:21:48.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4460" for this suite.
STEP: Destroying namespace "webhook-4460-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.024 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":280,"completed":49,"skipped":749,"failed":0}
S
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:21:48.644: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jan 23 22:21:49.144: INFO: Pod name wrapped-volume-race-d141474b-a488-4fc1-b47b-d79339dc8b2a: Found 0 pods out of 5
Jan 23 22:21:54.158: INFO: Pod name wrapped-volume-race-d141474b-a488-4fc1-b47b-d79339dc8b2a: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-d141474b-a488-4fc1-b47b-d79339dc8b2a in namespace emptydir-wrapper-4408, will wait for the garbage collector to delete the pods
Jan 23 22:22:04.268: INFO: Deleting ReplicationController wrapped-volume-race-d141474b-a488-4fc1-b47b-d79339dc8b2a took: 16.539568ms
Jan 23 22:22:04.869: INFO: Terminating ReplicationController wrapped-volume-race-d141474b-a488-4fc1-b47b-d79339dc8b2a pods took: 601.167536ms
STEP: Creating RC which spawns configmap-volume pods
Jan 23 22:22:12.633: INFO: Pod name wrapped-volume-race-27491b54-056a-44f7-aa1b-8ed727c41870: Found 1 pods out of 5
Jan 23 22:22:17.687: INFO: Pod name wrapped-volume-race-27491b54-056a-44f7-aa1b-8ed727c41870: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-27491b54-056a-44f7-aa1b-8ed727c41870 in namespace emptydir-wrapper-4408, will wait for the garbage collector to delete the pods
Jan 23 22:22:29.804: INFO: Deleting ReplicationController wrapped-volume-race-27491b54-056a-44f7-aa1b-8ed727c41870 took: 11.314299ms
Jan 23 22:22:30.405: INFO: Terminating ReplicationController wrapped-volume-race-27491b54-056a-44f7-aa1b-8ed727c41870 pods took: 601.020238ms
STEP: Creating RC which spawns configmap-volume pods
Jan 23 22:22:35.865: INFO: Pod name wrapped-volume-race-f0d9640d-2003-44b9-b6d0-ed62844d5e35: Found 0 pods out of 5
Jan 23 22:22:40.879: INFO: Pod name wrapped-volume-race-f0d9640d-2003-44b9-b6d0-ed62844d5e35: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-f0d9640d-2003-44b9-b6d0-ed62844d5e35 in namespace emptydir-wrapper-4408, will wait for the garbage collector to delete the pods
Jan 23 22:22:52.652: INFO: Deleting ReplicationController wrapped-volume-race-f0d9640d-2003-44b9-b6d0-ed62844d5e35 took: 12.427086ms
Jan 23 22:22:52.853: INFO: Terminating ReplicationController wrapped-volume-race-f0d9640d-2003-44b9-b6d0-ed62844d5e35 pods took: 200.822151ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:23:00.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4408" for this suite.

• [SLOW TEST:68.267 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":280,"completed":50,"skipped":750,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:23:00.213: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 23 22:23:02.327: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:23:02.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-517" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":280,"completed":51,"skipped":776,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:23:02.369: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on node default medium
Jan 23 22:23:02.427: INFO: Waiting up to 5m0s for pod "pod-25ee6218-86b2-45a7-8c8d-5a815e43d9cb" in namespace "emptydir-9934" to be "success or failure"
Jan 23 22:23:02.435: INFO: Pod "pod-25ee6218-86b2-45a7-8c8d-5a815e43d9cb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.793928ms
Jan 23 22:23:04.442: INFO: Pod "pod-25ee6218-86b2-45a7-8c8d-5a815e43d9cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014544076s
STEP: Saw pod success
Jan 23 22:23:04.442: INFO: Pod "pod-25ee6218-86b2-45a7-8c8d-5a815e43d9cb" satisfied condition "success or failure"
Jan 23 22:23:04.445: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod pod-25ee6218-86b2-45a7-8c8d-5a815e43d9cb container test-container: <nil>
STEP: delete the pod
Jan 23 22:23:04.504: INFO: Waiting for pod pod-25ee6218-86b2-45a7-8c8d-5a815e43d9cb to disappear
Jan 23 22:23:04.509: INFO: Pod pod-25ee6218-86b2-45a7-8c8d-5a815e43d9cb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:23:04.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9934" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":52,"skipped":785,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:23:04.523: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name cm-test-opt-del-b72429ab-bdbd-4054-8a13-ffb461beb1af
STEP: Creating configMap with name cm-test-opt-upd-b5cc94ed-bf0f-460e-8797-5351ca998fb5
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-b72429ab-bdbd-4054-8a13-ffb461beb1af
STEP: Updating configmap cm-test-opt-upd-b5cc94ed-bf0f-460e-8797-5351ca998fb5
STEP: Creating configMap with name cm-test-opt-create-31a3641a-452f-41b8-8027-a92279f6fd5d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:24:22.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-78" for this suite.

• [SLOW TEST:75.022 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":53,"skipped":788,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:24:24.733: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9146 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9146;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9146 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9146;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9146.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9146.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9146.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9146.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9146.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9146.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9146.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9146.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9146.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9146.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9146.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9146.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9146.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 163.255.67.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.67.255.163_udp@PTR;check="$$(dig +tcp +noall +answer +search 163.255.67.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.67.255.163_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9146 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9146;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9146 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9146;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9146.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9146.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9146.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9146.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9146.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9146.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9146.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9146.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9146.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9146.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9146.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9146.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9146.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 163.255.67.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.67.255.163_udp@PTR;check="$$(dig +tcp +noall +answer +search 163.255.67.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.67.255.163_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 23 22:24:29.034: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:29.040: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:29.046: INFO: Unable to read wheezy_udp@dns-test-service.dns-9146 from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:29.052: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9146 from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:29.058: INFO: Unable to read wheezy_udp@dns-test-service.dns-9146.svc from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:29.062: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9146.svc from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:29.118: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:29.125: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:29.131: INFO: Unable to read jessie_udp@dns-test-service.dns-9146 from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:29.138: INFO: Unable to read jessie_tcp@dns-test-service.dns-9146 from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:29.145: INFO: Unable to read jessie_udp@dns-test-service.dns-9146.svc from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:29.151: INFO: Unable to read jessie_tcp@dns-test-service.dns-9146.svc from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:29.158: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9146.svc from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:29.197: INFO: Lookups using dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9146 wheezy_tcp@dns-test-service.dns-9146 wheezy_udp@dns-test-service.dns-9146.svc wheezy_tcp@dns-test-service.dns-9146.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9146 jessie_tcp@dns-test-service.dns-9146 jessie_udp@dns-test-service.dns-9146.svc jessie_tcp@dns-test-service.dns-9146.svc jessie_udp@_http._tcp.dns-test-service.dns-9146.svc]

Jan 23 22:24:34.205: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:34.212: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:34.218: INFO: Unable to read wheezy_udp@dns-test-service.dns-9146 from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:34.227: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9146 from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:34.234: INFO: Unable to read wheezy_udp@dns-test-service.dns-9146.svc from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:34.241: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9146.svc from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:34.299: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:34.307: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:34.313: INFO: Unable to read jessie_udp@dns-test-service.dns-9146 from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:34.318: INFO: Unable to read jessie_tcp@dns-test-service.dns-9146 from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:34.325: INFO: Unable to read jessie_udp@dns-test-service.dns-9146.svc from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:34.331: INFO: Unable to read jessie_tcp@dns-test-service.dns-9146.svc from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:34.377: INFO: Lookups using dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9146 wheezy_tcp@dns-test-service.dns-9146 wheezy_udp@dns-test-service.dns-9146.svc wheezy_tcp@dns-test-service.dns-9146.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9146 jessie_tcp@dns-test-service.dns-9146 jessie_udp@dns-test-service.dns-9146.svc jessie_tcp@dns-test-service.dns-9146.svc]

Jan 23 22:24:39.208: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:39.214: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:39.221: INFO: Unable to read wheezy_udp@dns-test-service.dns-9146 from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:39.229: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9146 from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:39.236: INFO: Unable to read wheezy_udp@dns-test-service.dns-9146.svc from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:39.242: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9146.svc from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:39.312: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:39.320: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:39.327: INFO: Unable to read jessie_udp@dns-test-service.dns-9146 from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:39.334: INFO: Unable to read jessie_tcp@dns-test-service.dns-9146 from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:39.342: INFO: Unable to read jessie_udp@dns-test-service.dns-9146.svc from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:39.349: INFO: Unable to read jessie_tcp@dns-test-service.dns-9146.svc from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:39.407: INFO: Lookups using dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9146 wheezy_tcp@dns-test-service.dns-9146 wheezy_udp@dns-test-service.dns-9146.svc wheezy_tcp@dns-test-service.dns-9146.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9146 jessie_tcp@dns-test-service.dns-9146 jessie_udp@dns-test-service.dns-9146.svc jessie_tcp@dns-test-service.dns-9146.svc]

Jan 23 22:24:44.205: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:44.213: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:44.220: INFO: Unable to read wheezy_udp@dns-test-service.dns-9146 from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:44.226: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9146 from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:44.234: INFO: Unable to read wheezy_udp@dns-test-service.dns-9146.svc from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:44.244: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9146.svc from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:44.294: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:44.300: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:44.305: INFO: Unable to read jessie_udp@dns-test-service.dns-9146 from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:44.314: INFO: Unable to read jessie_tcp@dns-test-service.dns-9146 from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:44.322: INFO: Unable to read jessie_udp@dns-test-service.dns-9146.svc from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:44.328: INFO: Unable to read jessie_tcp@dns-test-service.dns-9146.svc from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:44.379: INFO: Lookups using dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9146 wheezy_tcp@dns-test-service.dns-9146 wheezy_udp@dns-test-service.dns-9146.svc wheezy_tcp@dns-test-service.dns-9146.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9146 jessie_tcp@dns-test-service.dns-9146 jessie_udp@dns-test-service.dns-9146.svc jessie_tcp@dns-test-service.dns-9146.svc]

Jan 23 22:24:49.262: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:49.278: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:49.289: INFO: Unable to read wheezy_udp@dns-test-service.dns-9146 from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:49.303: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9146 from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:49.312: INFO: Unable to read wheezy_udp@dns-test-service.dns-9146.svc from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:49.321: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9146.svc from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:49.393: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:49.401: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:49.418: INFO: Unable to read jessie_udp@dns-test-service.dns-9146 from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:49.426: INFO: Unable to read jessie_tcp@dns-test-service.dns-9146 from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:49.441: INFO: Unable to read jessie_udp@dns-test-service.dns-9146.svc from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:49.452: INFO: Unable to read jessie_tcp@dns-test-service.dns-9146.svc from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:49.534: INFO: Lookups using dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9146 wheezy_tcp@dns-test-service.dns-9146 wheezy_udp@dns-test-service.dns-9146.svc wheezy_tcp@dns-test-service.dns-9146.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9146 jessie_tcp@dns-test-service.dns-9146 jessie_udp@dns-test-service.dns-9146.svc jessie_tcp@dns-test-service.dns-9146.svc]

Jan 23 22:24:54.205: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:54.223: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:54.236: INFO: Unable to read wheezy_udp@dns-test-service.dns-9146 from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:54.248: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9146 from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:54.273: INFO: Unable to read wheezy_udp@dns-test-service.dns-9146.svc from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:54.292: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9146.svc from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:54.369: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:54.377: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:54.385: INFO: Unable to read jessie_udp@dns-test-service.dns-9146 from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:54.393: INFO: Unable to read jessie_tcp@dns-test-service.dns-9146 from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:54.400: INFO: Unable to read jessie_udp@dns-test-service.dns-9146.svc from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:54.406: INFO: Unable to read jessie_tcp@dns-test-service.dns-9146.svc from pod dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0: the server could not find the requested resource (get pods dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0)
Jan 23 22:24:54.461: INFO: Lookups using dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9146 wheezy_tcp@dns-test-service.dns-9146 wheezy_udp@dns-test-service.dns-9146.svc wheezy_tcp@dns-test-service.dns-9146.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9146 jessie_tcp@dns-test-service.dns-9146 jessie_udp@dns-test-service.dns-9146.svc jessie_tcp@dns-test-service.dns-9146.svc]

Jan 23 22:25:00.763: INFO: DNS probes using dns-9146/dns-test-ae57887b-3b74-4719-b0a2-ff4ddd141db0 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:25:01.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9146" for this suite.

• [SLOW TEST:35.132 seconds]
[sig-network] DNS
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":280,"completed":54,"skipped":797,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:25:01.202: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-11d540c5-efce-47bb-8e01-a754c319f3f5
STEP: Creating a pod to test consume secrets
Jan 23 22:25:01.368: INFO: Waiting up to 5m0s for pod "pod-secrets-e5a7e294-9a83-48ea-8347-da784532b5f4" in namespace "secrets-6765" to be "success or failure"
Jan 23 22:25:01.401: INFO: Pod "pod-secrets-e5a7e294-9a83-48ea-8347-da784532b5f4": Phase="Pending", Reason="", readiness=false. Elapsed: 32.376768ms
Jan 23 22:25:03.406: INFO: Pod "pod-secrets-e5a7e294-9a83-48ea-8347-da784532b5f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037353765s
Jan 23 22:25:05.411: INFO: Pod "pod-secrets-e5a7e294-9a83-48ea-8347-da784532b5f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042295814s
STEP: Saw pod success
Jan 23 22:25:05.411: INFO: Pod "pod-secrets-e5a7e294-9a83-48ea-8347-da784532b5f4" satisfied condition "success or failure"
Jan 23 22:25:05.415: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod pod-secrets-e5a7e294-9a83-48ea-8347-da784532b5f4 container secret-volume-test: <nil>
STEP: delete the pod
Jan 23 22:25:05.460: INFO: Waiting for pod pod-secrets-e5a7e294-9a83-48ea-8347-da784532b5f4 to disappear
Jan 23 22:25:05.467: INFO: Pod pod-secrets-e5a7e294-9a83-48ea-8347-da784532b5f4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:25:05.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6765" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":280,"completed":55,"skipped":818,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:25:05.498: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 23 22:25:05.561: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3781bf79-6dca-4a08-b5f8-5bf863daf091" in namespace "downward-api-7161" to be "success or failure"
Jan 23 22:25:05.569: INFO: Pod "downwardapi-volume-3781bf79-6dca-4a08-b5f8-5bf863daf091": Phase="Pending", Reason="", readiness=false. Elapsed: 7.533096ms
Jan 23 22:25:07.574: INFO: Pod "downwardapi-volume-3781bf79-6dca-4a08-b5f8-5bf863daf091": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011976299s
STEP: Saw pod success
Jan 23 22:25:07.574: INFO: Pod "downwardapi-volume-3781bf79-6dca-4a08-b5f8-5bf863daf091" satisfied condition "success or failure"
Jan 23 22:25:07.578: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod downwardapi-volume-3781bf79-6dca-4a08-b5f8-5bf863daf091 container client-container: <nil>
STEP: delete the pod
Jan 23 22:25:07.601: INFO: Waiting for pod downwardapi-volume-3781bf79-6dca-4a08-b5f8-5bf863daf091 to disappear
Jan 23 22:25:07.606: INFO: Pod downwardapi-volume-3781bf79-6dca-4a08-b5f8-5bf863daf091 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:25:07.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7161" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":56,"skipped":831,"failed":0}
SSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:25:07.628: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override all
Jan 23 22:25:07.702: INFO: Waiting up to 5m0s for pod "client-containers-4607c754-37dd-4804-95c7-860ed884f8b0" in namespace "containers-8816" to be "success or failure"
Jan 23 22:25:07.713: INFO: Pod "client-containers-4607c754-37dd-4804-95c7-860ed884f8b0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.508677ms
Jan 23 22:25:09.723: INFO: Pod "client-containers-4607c754-37dd-4804-95c7-860ed884f8b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020955266s
STEP: Saw pod success
Jan 23 22:25:09.723: INFO: Pod "client-containers-4607c754-37dd-4804-95c7-860ed884f8b0" satisfied condition "success or failure"
Jan 23 22:25:09.728: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod client-containers-4607c754-37dd-4804-95c7-860ed884f8b0 container test-container: <nil>
STEP: delete the pod
Jan 23 22:25:09.750: INFO: Waiting for pod client-containers-4607c754-37dd-4804-95c7-860ed884f8b0 to disappear
Jan 23 22:25:09.754: INFO: Pod client-containers-4607c754-37dd-4804-95c7-860ed884f8b0 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:25:09.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8816" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":280,"completed":57,"skipped":835,"failed":0}
SSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:25:09.772: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: getting the auto-created API token
STEP: reading a file in the container
Jan 23 22:25:12.396: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7952 pod-service-account-3f2deac7-f66d-49e9-b9cb-a67f4c9e524e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jan 23 22:25:13.328: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7952 pod-service-account-3f2deac7-f66d-49e9-b9cb-a67f4c9e524e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jan 23 22:25:13.645: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7952 pod-service-account-3f2deac7-f66d-49e9-b9cb-a67f4c9e524e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:25:13.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7952" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":280,"completed":58,"skipped":838,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:25:13.992: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 23 22:25:14.065: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8cc92d68-4bfa-4eeb-855d-184a5ba9eb46" in namespace "projected-1145" to be "success or failure"
Jan 23 22:25:14.076: INFO: Pod "downwardapi-volume-8cc92d68-4bfa-4eeb-855d-184a5ba9eb46": Phase="Pending", Reason="", readiness=false. Elapsed: 10.673167ms
Jan 23 22:25:16.081: INFO: Pod "downwardapi-volume-8cc92d68-4bfa-4eeb-855d-184a5ba9eb46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01627183s
STEP: Saw pod success
Jan 23 22:25:16.081: INFO: Pod "downwardapi-volume-8cc92d68-4bfa-4eeb-855d-184a5ba9eb46" satisfied condition "success or failure"
Jan 23 22:25:16.085: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod downwardapi-volume-8cc92d68-4bfa-4eeb-855d-184a5ba9eb46 container client-container: <nil>
STEP: delete the pod
Jan 23 22:25:16.109: INFO: Waiting for pod downwardapi-volume-8cc92d68-4bfa-4eeb-855d-184a5ba9eb46 to disappear
Jan 23 22:25:16.118: INFO: Pod downwardapi-volume-8cc92d68-4bfa-4eeb-855d-184a5ba9eb46 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:25:16.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1145" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":280,"completed":59,"skipped":839,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:25:16.144: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-c0d39da0-489b-43c6-9eba-a35f9b329a10
STEP: Creating a pod to test consume configMaps
Jan 23 22:25:16.199: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-05c0821a-73e2-42cc-a5e6-78b63ba04feb" in namespace "projected-5589" to be "success or failure"
Jan 23 22:25:16.210: INFO: Pod "pod-projected-configmaps-05c0821a-73e2-42cc-a5e6-78b63ba04feb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.96657ms
Jan 23 22:25:18.217: INFO: Pod "pod-projected-configmaps-05c0821a-73e2-42cc-a5e6-78b63ba04feb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017614299s
STEP: Saw pod success
Jan 23 22:25:18.217: INFO: Pod "pod-projected-configmaps-05c0821a-73e2-42cc-a5e6-78b63ba04feb" satisfied condition "success or failure"
Jan 23 22:25:18.226: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f pod pod-projected-configmaps-05c0821a-73e2-42cc-a5e6-78b63ba04feb container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 23 22:25:18.285: INFO: Waiting for pod pod-projected-configmaps-05c0821a-73e2-42cc-a5e6-78b63ba04feb to disappear
Jan 23 22:25:18.290: INFO: Pod pod-projected-configmaps-05c0821a-73e2-42cc-a5e6-78b63ba04feb no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:25:18.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5589" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":60,"skipped":882,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:25:18.322: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-map-20bc174b-42aa-4106-b995-6236c22ab72d
STEP: Creating a pod to test consume secrets
Jan 23 22:25:18.388: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f230aa39-fa59-4389-a746-d2c84d689592" in namespace "projected-5340" to be "success or failure"
Jan 23 22:25:18.394: INFO: Pod "pod-projected-secrets-f230aa39-fa59-4389-a746-d2c84d689592": Phase="Pending", Reason="", readiness=false. Elapsed: 5.903952ms
Jan 23 22:25:20.399: INFO: Pod "pod-projected-secrets-f230aa39-fa59-4389-a746-d2c84d689592": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01083179s
STEP: Saw pod success
Jan 23 22:25:20.399: INFO: Pod "pod-projected-secrets-f230aa39-fa59-4389-a746-d2c84d689592" satisfied condition "success or failure"
Jan 23 22:25:20.402: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod pod-projected-secrets-f230aa39-fa59-4389-a746-d2c84d689592 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 23 22:25:20.427: INFO: Waiting for pod pod-projected-secrets-f230aa39-fa59-4389-a746-d2c84d689592 to disappear
Jan 23 22:25:20.431: INFO: Pod pod-projected-secrets-f230aa39-fa59-4389-a746-d2c84d689592 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:25:20.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5340" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":61,"skipped":910,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:25:20.448: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 23 22:25:20.494: INFO: Creating deployment "test-recreate-deployment"
Jan 23 22:25:20.503: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jan 23 22:25:20.516: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Jan 23 22:25:22.527: INFO: Waiting deployment "test-recreate-deployment" to complete
Jan 23 22:25:22.530: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715415121, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715415121, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715415121, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715415121, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-799c574856\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 23 22:25:24.535: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jan 23 22:25:24.546: INFO: Updating deployment test-recreate-deployment
Jan 23 22:25:24.547: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Jan 23 22:25:24.660: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-3286 /apis/apps/v1/namespaces/deployment-3286/deployments/test-recreate-deployment be8506ef-ad95-4cc3-8715-047e25d0ae38 406357 2 2020-01-23 22:25:21 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001a04d68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-01-23 22:25:25 +0000 UTC,LastTransitionTime:2020-01-23 22:25:25 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2020-01-23 22:25:25 +0000 UTC,LastTransitionTime:2020-01-23 22:25:21 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jan 23 22:25:24.680: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-3286 /apis/apps/v1/namespaces/deployment-3286/replicasets/test-recreate-deployment-5f94c574ff 4c0bf31d-71f8-41af-a943-bbfc24e34406 406356 1 2020-01-23 22:25:25 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment be8506ef-ad95-4cc3-8715-047e25d0ae38 0xc001a05147 0xc001a05148}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001a051a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 23 22:25:24.680: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jan 23 22:25:24.681: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-799c574856  deployment-3286 /apis/apps/v1/namespaces/deployment-3286/replicasets/test-recreate-deployment-799c574856 319af6e3-20e0-47f9-aa76-5cf68bb0d88a 406346 2 2020-01-23 22:25:21 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:799c574856] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment be8506ef-ad95-4cc3-8715-047e25d0ae38 0xc001a05217 0xc001a05218}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 799c574856,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:799c574856] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001a05288 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 23 22:25:24.689: INFO: Pod "test-recreate-deployment-5f94c574ff-j5rvv" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-j5rvv test-recreate-deployment-5f94c574ff- deployment-3286 /api/v1/namespaces/deployment-3286/pods/test-recreate-deployment-5f94c574ff-j5rvv 3ca30cf3-98cf-4c52-a464-111239bc2c68 406359 0 2020-01-23 22:25:25 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff 4c0bf31d-71f8-41af-a943-bbfc24e34406 0xc001a05707 0xc001a05708}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c6drm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c6drm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c6drm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:management-cluster-1-17-2-md-0-5f64bb5777-dlwcz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:25:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:25:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:25:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:25:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.220.23,PodIP:,StartTime:2020-01-23 22:25:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:25:24.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3286" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":280,"completed":62,"skipped":917,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:25:24.712: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 23 22:25:24.764: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-c9c1bf55-ec90-4c17-8986-35e22adf5622" in namespace "security-context-test-1665" to be "success or failure"
Jan 23 22:25:24.769: INFO: Pod "busybox-privileged-false-c9c1bf55-ec90-4c17-8986-35e22adf5622": Phase="Pending", Reason="", readiness=false. Elapsed: 5.118193ms
Jan 23 22:25:26.775: INFO: Pod "busybox-privileged-false-c9c1bf55-ec90-4c17-8986-35e22adf5622": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010301462s
Jan 23 22:25:26.775: INFO: Pod "busybox-privileged-false-c9c1bf55-ec90-4c17-8986-35e22adf5622" satisfied condition "success or failure"
Jan 23 22:25:26.805: INFO: Got logs for pod "busybox-privileged-false-c9c1bf55-ec90-4c17-8986-35e22adf5622": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:25:26.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1665" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":63,"skipped":925,"failed":0}
S
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:25:26.821: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Jan 23 22:25:29.464: INFO: Successfully updated pod "adopt-release-9xp2l"
STEP: Checking that the Job readopts the Pod
Jan 23 22:25:29.465: INFO: Waiting up to 15m0s for pod "adopt-release-9xp2l" in namespace "job-2956" to be "adopted"
Jan 23 22:25:29.471: INFO: Pod "adopt-release-9xp2l": Phase="Running", Reason="", readiness=true. Elapsed: 6.785087ms
Jan 23 22:25:32.819: INFO: Pod "adopt-release-9xp2l": Phase="Running", Reason="", readiness=true. Elapsed: 2.015531475s
Jan 23 22:25:32.819: INFO: Pod "adopt-release-9xp2l" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Jan 23 22:25:33.336: INFO: Successfully updated pod "adopt-release-9xp2l"
STEP: Checking that the Job releases the Pod
Jan 23 22:25:33.336: INFO: Waiting up to 15m0s for pod "adopt-release-9xp2l" in namespace "job-2956" to be "released"
Jan 23 22:25:33.349: INFO: Pod "adopt-release-9xp2l": Phase="Running", Reason="", readiness=true. Elapsed: 13.40464ms
Jan 23 22:25:35.354: INFO: Pod "adopt-release-9xp2l": Phase="Running", Reason="", readiness=true. Elapsed: 2.018028758s
Jan 23 22:25:35.354: INFO: Pod "adopt-release-9xp2l" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:25:35.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2956" for this suite.

• [SLOW TEST:7.206 seconds]
[sig-apps] Job
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":280,"completed":64,"skipped":926,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:25:35.368: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 23 22:25:35.424: INFO: Waiting up to 5m0s for pod "downwardapi-volume-74d85816-8809-4796-a4fb-3114c3487dbd" in namespace "downward-api-6024" to be "success or failure"
Jan 23 22:25:35.432: INFO: Pod "downwardapi-volume-74d85816-8809-4796-a4fb-3114c3487dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 7.173286ms
Jan 23 22:25:37.438: INFO: Pod "downwardapi-volume-74d85816-8809-4796-a4fb-3114c3487dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01281432s
Jan 23 22:25:39.448: INFO: Pod "downwardapi-volume-74d85816-8809-4796-a4fb-3114c3487dbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023071841s
STEP: Saw pod success
Jan 23 22:25:39.449: INFO: Pod "downwardapi-volume-74d85816-8809-4796-a4fb-3114c3487dbd" satisfied condition "success or failure"
Jan 23 22:25:39.456: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f pod downwardapi-volume-74d85816-8809-4796-a4fb-3114c3487dbd container client-container: <nil>
STEP: delete the pod
Jan 23 22:25:39.495: INFO: Waiting for pod downwardapi-volume-74d85816-8809-4796-a4fb-3114c3487dbd to disappear
Jan 23 22:25:39.498: INFO: Pod downwardapi-volume-74d85816-8809-4796-a4fb-3114c3487dbd no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:25:39.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6024" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":65,"skipped":941,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:25:39.510: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 23 22:25:40.539: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 23 22:25:43.576: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:25:43.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2264" for this suite.
STEP: Destroying namespace "webhook-2264-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":280,"completed":66,"skipped":957,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:25:43.935: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 23 22:25:44.021: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3d8cea82-c6b2-443a-bf9f-726a7fef284d" in namespace "projected-4405" to be "success or failure"
Jan 23 22:25:44.027: INFO: Pod "downwardapi-volume-3d8cea82-c6b2-443a-bf9f-726a7fef284d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.725737ms
Jan 23 22:25:46.032: INFO: Pod "downwardapi-volume-3d8cea82-c6b2-443a-bf9f-726a7fef284d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011090071s
STEP: Saw pod success
Jan 23 22:25:46.032: INFO: Pod "downwardapi-volume-3d8cea82-c6b2-443a-bf9f-726a7fef284d" satisfied condition "success or failure"
Jan 23 22:25:46.036: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f pod downwardapi-volume-3d8cea82-c6b2-443a-bf9f-726a7fef284d container client-container: <nil>
STEP: delete the pod
Jan 23 22:25:46.060: INFO: Waiting for pod downwardapi-volume-3d8cea82-c6b2-443a-bf9f-726a7fef284d to disappear
Jan 23 22:25:46.064: INFO: Pod downwardapi-volume-3d8cea82-c6b2-443a-bf9f-726a7fef284d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:25:46.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4405" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":67,"skipped":967,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:25:46.084: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 23 22:25:46.124: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:25:53.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2141" for this suite.

• [SLOW TEST:7.675 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:47
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":280,"completed":68,"skipped":974,"failed":0}
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:25:53.760: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with configMap that has name projected-configmap-test-upd-6083d06a-5aa3-4b19-8b3f-04fa231323c4
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-6083d06a-5aa3-4b19-8b3f-04fa231323c4
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:27:26.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1099" for this suite.

• [SLOW TEST:87.810 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":69,"skipped":974,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:27:26.091: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-map-d4a39c56-420e-4065-b22f-cfacb5fe05f5
STEP: Creating a pod to test consume secrets
Jan 23 22:27:26.208: INFO: Waiting up to 5m0s for pod "pod-secrets-9f9bbe72-8af9-4afe-84e1-22111d0e832e" in namespace "secrets-2074" to be "success or failure"
Jan 23 22:27:26.220: INFO: Pod "pod-secrets-9f9bbe72-8af9-4afe-84e1-22111d0e832e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.397432ms
Jan 23 22:27:28.228: INFO: Pod "pod-secrets-9f9bbe72-8af9-4afe-84e1-22111d0e832e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019820428s
STEP: Saw pod success
Jan 23 22:27:28.228: INFO: Pod "pod-secrets-9f9bbe72-8af9-4afe-84e1-22111d0e832e" satisfied condition "success or failure"
Jan 23 22:27:28.233: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod pod-secrets-9f9bbe72-8af9-4afe-84e1-22111d0e832e container secret-volume-test: <nil>
STEP: delete the pod
Jan 23 22:27:28.300: INFO: Waiting for pod pod-secrets-9f9bbe72-8af9-4afe-84e1-22111d0e832e to disappear
Jan 23 22:27:28.307: INFO: Pod pod-secrets-9f9bbe72-8af9-4afe-84e1-22111d0e832e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:27:28.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2074" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":70,"skipped":1012,"failed":0}

------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:27:28.328: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 23 22:27:28.376: INFO: Creating deployment "webserver-deployment"
Jan 23 22:27:28.384: INFO: Waiting for observed generation 1
Jan 23 22:27:31.219: INFO: Waiting for all required pods to come up
Jan 23 22:27:31.713: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Jan 23 22:27:33.896: INFO: Waiting for deployment "webserver-deployment" to complete
Jan 23 22:27:33.910: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jan 23 22:27:33.921: INFO: Updating deployment webserver-deployment
Jan 23 22:27:33.921: INFO: Waiting for observed generation 2
Jan 23 22:27:35.935: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jan 23 22:27:35.940: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jan 23 22:27:35.946: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 23 22:27:35.958: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jan 23 22:27:35.958: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jan 23 22:27:35.962: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 23 22:27:35.972: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jan 23 22:27:35.973: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jan 23 22:27:35.982: INFO: Updating deployment webserver-deployment
Jan 23 22:27:35.982: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jan 23 22:27:35.992: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jan 23 22:27:36.043: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Jan 23 22:27:36.165: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-7425 /apis/apps/v1/namespaces/deployment-7425/deployments/webserver-deployment f99dd6ce-d56a-4f6f-90e9-b28e8fc26c21 407628 3 2020-01-23 22:27:29 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0020391a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2020-01-23 22:27:34 +0000 UTC,LastTransitionTime:2020-01-23 22:27:29 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-01-23 22:27:36 +0000 UTC,LastTransitionTime:2020-01-23 22:27:36 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jan 23 22:27:36.249: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-7425 /apis/apps/v1/namespaces/deployment-7425/replicasets/webserver-deployment-c7997dcc8 47e7f75a-cf60-4c25-9e54-899089929e31 407616 3 2020-01-23 22:27:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment f99dd6ce-d56a-4f6f-90e9-b28e8fc26c21 0xc002039677 0xc002039678}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0020396e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 23 22:27:36.249: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jan 23 22:27:36.249: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-7425 /apis/apps/v1/namespaces/deployment-7425/replicasets/webserver-deployment-595b5b9587 4a60c846-89a8-44cf-ab2f-e986ab1cca08 407613 3 2020-01-23 22:27:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment f99dd6ce-d56a-4f6f-90e9-b28e8fc26c21 0xc0020395b7 0xc0020395b8}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002039618 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jan 23 22:27:36.318: INFO: Pod "webserver-deployment-595b5b9587-4w26c" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-4w26c webserver-deployment-595b5b9587- deployment-7425 /api/v1/namespaces/deployment-7425/pods/webserver-deployment-595b5b9587-4w26c ec0a9a1f-eda9-4445-9fb0-1834a434be14 407465 0 2020-01-23 22:27:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:100.120.200.52/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 4a60c846-89a8-44cf-ab2f-e986ab1cca08 0xc002095647 0xc002095648}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f84sl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f84sl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f84sl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:management-cluster-1-17-2-md-0-5f64bb5777-fsd6f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.219.138,PodIP:100.120.200.52,StartTime:2020-01-23 22:27:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-23 22:27:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://53923e641fdeac8f87bf987887807ae3c05cf057194d88bd93040367bf3a13d0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.120.200.52,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 22:27:36.319: INFO: Pod "webserver-deployment-595b5b9587-8f9gk" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-8f9gk webserver-deployment-595b5b9587- deployment-7425 /api/v1/namespaces/deployment-7425/pods/webserver-deployment-595b5b9587-8f9gk 8a1e3c2f-c76b-47cc-a05c-d9df2d038813 407470 0 2020-01-23 22:27:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:100.120.200.48/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 4a60c846-89a8-44cf-ab2f-e986ab1cca08 0xc0020957b7 0xc0020957b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f84sl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f84sl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f84sl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:management-cluster-1-17-2-md-0-5f64bb5777-fsd6f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.219.138,PodIP:100.120.200.48,StartTime:2020-01-23 22:27:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-23 22:27:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://f18d2c0a7431fdf2903a675ced1b09993747d810a67f1baacaccf836c93a0060,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.120.200.48,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 22:27:36.320: INFO: Pod "webserver-deployment-595b5b9587-96995" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-96995 webserver-deployment-595b5b9587- deployment-7425 /api/v1/namespaces/deployment-7425/pods/webserver-deployment-595b5b9587-96995 7e0be446-cedd-440c-96a2-2df6b5fbd72e 407481 0 2020-01-23 22:27:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:100.105.119.109/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 4a60c846-89a8-44cf-ab2f-e986ab1cca08 0xc002095ba7 0xc002095ba8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f84sl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f84sl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f84sl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:management-cluster-1-17-2-md-0-5f64bb5777-v477d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.209.78,PodIP:100.105.119.109,StartTime:2020-01-23 22:27:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-23 22:27:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://4733bf55feaf9a2aeb7b7de40af41f340879ec6b582f21a1c0038bd00ff01eb4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.105.119.109,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 22:27:36.321: INFO: Pod "webserver-deployment-595b5b9587-c4j95" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-c4j95 webserver-deployment-595b5b9587- deployment-7425 /api/v1/namespaces/deployment-7425/pods/webserver-deployment-595b5b9587-c4j95 b0450f96-a1ca-479d-bcf5-be9eedec467b 407673 0 2020-01-23 22:27:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 4a60c846-89a8-44cf-ab2f-e986ab1cca08 0xc002095e67 0xc002095e68}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f84sl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f84sl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f84sl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:management-cluster-1-17-2-md-0-5f64bb5777-dlwcz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 22:27:36.322: INFO: Pod "webserver-deployment-595b5b9587-czr8l" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-czr8l webserver-deployment-595b5b9587- deployment-7425 /api/v1/namespaces/deployment-7425/pods/webserver-deployment-595b5b9587-czr8l bce08816-5b99-41b0-8bfa-ee057b3083fc 407447 0 2020-01-23 22:27:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:100.117.222.220/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 4a60c846-89a8-44cf-ab2f-e986ab1cca08 0xc002406090 0xc002406091}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f84sl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f84sl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f84sl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:management-cluster-1-17-2-md-0-5f64bb5777-dlwcz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.220.23,PodIP:100.117.222.220,StartTime:2020-01-23 22:27:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-23 22:27:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://93b1ff9062d746b90a49b3920a2b1216c2941e5ece7b8d98ca485969bc1c63ad,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.117.222.220,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 22:27:36.322: INFO: Pod "webserver-deployment-595b5b9587-dnxxr" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-dnxxr webserver-deployment-595b5b9587- deployment-7425 /api/v1/namespaces/deployment-7425/pods/webserver-deployment-595b5b9587-dnxxr 87552131-6bad-45e0-8260-e1d7508421fa 407672 0 2020-01-23 22:27:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 4a60c846-89a8-44cf-ab2f-e986ab1cca08 0xc0024061f7 0xc0024061f8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f84sl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f84sl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f84sl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:management-cluster-1-17-2-md-0-5f64bb5777-fsd6f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 22:27:36.323: INFO: Pod "webserver-deployment-595b5b9587-gcpdj" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-gcpdj webserver-deployment-595b5b9587- deployment-7425 /api/v1/namespaces/deployment-7425/pods/webserver-deployment-595b5b9587-gcpdj 597591bc-6b72-491a-9075-690bc6127850 407639 0 2020-01-23 22:27:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 4a60c846-89a8-44cf-ab2f-e986ab1cca08 0xc002406300 0xc002406301}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f84sl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f84sl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f84sl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:management-cluster-1-17-2-md-0-5f64bb5777-dlwcz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.220.23,PodIP:,StartTime:2020-01-23 22:27:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 22:27:36.328: INFO: Pod "webserver-deployment-595b5b9587-j5p8d" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-j5p8d webserver-deployment-595b5b9587- deployment-7425 /api/v1/namespaces/deployment-7425/pods/webserver-deployment-595b5b9587-j5p8d 54ec8c65-c057-46e6-a839-d601d9312792 407485 0 2020-01-23 22:27:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:100.105.119.112/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 4a60c846-89a8-44cf-ab2f-e986ab1cca08 0xc002406447 0xc002406448}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f84sl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f84sl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f84sl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:management-cluster-1-17-2-md-0-5f64bb5777-v477d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.209.78,PodIP:100.105.119.112,StartTime:2020-01-23 22:27:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-23 22:27:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://8b300179723923b1a031a5fa16b097d237095429daa826b6223b1578bc42165d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.105.119.112,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 22:27:36.331: INFO: Pod "webserver-deployment-595b5b9587-jdzmx" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-jdzmx webserver-deployment-595b5b9587- deployment-7425 /api/v1/namespaces/deployment-7425/pods/webserver-deployment-595b5b9587-jdzmx ad390a37-7134-49fb-9c6d-6a2727325701 407670 0 2020-01-23 22:27:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 4a60c846-89a8-44cf-ab2f-e986ab1cca08 0xc0024065b7 0xc0024065b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f84sl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f84sl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f84sl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:management-cluster-1-17-2-md-0-5f64bb5777-dlwcz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.220.23,PodIP:,StartTime:2020-01-23 22:27:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 22:27:36.331: INFO: Pod "webserver-deployment-595b5b9587-jpcrz" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-jpcrz webserver-deployment-595b5b9587- deployment-7425 /api/v1/namespaces/deployment-7425/pods/webserver-deployment-595b5b9587-jpcrz d20985d1-afc0-4103-88ad-eb545d1eb3c5 407650 0 2020-01-23 22:27:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 4a60c846-89a8-44cf-ab2f-e986ab1cca08 0xc002406707 0xc002406708}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f84sl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f84sl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f84sl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:management-cluster-1-17-2-md-0-5f64bb5777-v477d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 22:27:36.332: INFO: Pod "webserver-deployment-595b5b9587-jqmnr" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-jqmnr webserver-deployment-595b5b9587- deployment-7425 /api/v1/namespaces/deployment-7425/pods/webserver-deployment-595b5b9587-jqmnr 399fcb21-8df1-4671-945b-f1e708e225a5 407476 0 2020-01-23 22:27:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:100.105.119.110/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 4a60c846-89a8-44cf-ab2f-e986ab1cca08 0xc002406810 0xc002406811}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f84sl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f84sl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f84sl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:management-cluster-1-17-2-md-0-5f64bb5777-v477d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.209.78,PodIP:100.105.119.110,StartTime:2020-01-23 22:27:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-23 22:27:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://e0ae67240f7da51d2fa064dde065eedaaf8e1044a98e8341da365b05f20f4d2a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.105.119.110,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 22:27:36.332: INFO: Pod "webserver-deployment-595b5b9587-kppg7" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-kppg7 webserver-deployment-595b5b9587- deployment-7425 /api/v1/namespaces/deployment-7425/pods/webserver-deployment-595b5b9587-kppg7 341805b7-7294-44c0-a561-99c757bd54a2 407678 0 2020-01-23 22:27:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 4a60c846-89a8-44cf-ab2f-e986ab1cca08 0xc002406977 0xc002406978}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f84sl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f84sl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f84sl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:management-cluster-1-17-2-md-0-5f64bb5777-fsd6f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 22:27:36.333: INFO: Pod "webserver-deployment-595b5b9587-qb85z" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-qb85z webserver-deployment-595b5b9587- deployment-7425 /api/v1/namespaces/deployment-7425/pods/webserver-deployment-595b5b9587-qb85z ffcc7fa6-1d29-4e12-b6d5-3cba343ec6e5 407668 0 2020-01-23 22:27:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 4a60c846-89a8-44cf-ab2f-e986ab1cca08 0xc002406a80 0xc002406a81}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f84sl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f84sl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f84sl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:management-cluster-1-17-2-md-0-5f64bb5777-v477d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 22:27:36.334: INFO: Pod "webserver-deployment-595b5b9587-qpxdj" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-qpxdj webserver-deployment-595b5b9587- deployment-7425 /api/v1/namespaces/deployment-7425/pods/webserver-deployment-595b5b9587-qpxdj 87ad8474-7493-43c8-a0aa-b2f8d1538700 407679 0 2020-01-23 22:27:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 4a60c846-89a8-44cf-ab2f-e986ab1cca08 0xc002406b80 0xc002406b81}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f84sl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f84sl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f84sl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:management-cluster-1-17-2-md-0-5f64bb5777-dlwcz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 22:27:36.337: INFO: Pod "webserver-deployment-595b5b9587-qskz5" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-qskz5 webserver-deployment-595b5b9587- deployment-7425 /api/v1/namespaces/deployment-7425/pods/webserver-deployment-595b5b9587-qskz5 5d9f85f5-f125-4024-92b0-747b7fb82d37 407455 0 2020-01-23 22:27:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:100.117.222.221/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 4a60c846-89a8-44cf-ab2f-e986ab1cca08 0xc002406c80 0xc002406c81}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f84sl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f84sl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f84sl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:management-cluster-1-17-2-md-0-5f64bb5777-dlwcz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.220.23,PodIP:100.117.222.221,StartTime:2020-01-23 22:27:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-23 22:27:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://c55518c8ba461aa85f289fb3308c26c31d6efacb8fb0891ca752a15388f4b7f5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.117.222.221,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 22:27:36.338: INFO: Pod "webserver-deployment-595b5b9587-t7z54" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-t7z54 webserver-deployment-595b5b9587- deployment-7425 /api/v1/namespaces/deployment-7425/pods/webserver-deployment-595b5b9587-t7z54 e616607d-4cf0-4ea7-b181-244009ce5d67 407629 0 2020-01-23 22:27:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 4a60c846-89a8-44cf-ab2f-e986ab1cca08 0xc002406de7 0xc002406de8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f84sl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f84sl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f84sl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:management-cluster-1-17-2-md-0-5f64bb5777-v477d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 22:27:36.339: INFO: Pod "webserver-deployment-595b5b9587-tcxwc" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-tcxwc webserver-deployment-595b5b9587- deployment-7425 /api/v1/namespaces/deployment-7425/pods/webserver-deployment-595b5b9587-tcxwc 737cbec9-ad88-4fd4-82da-38858c0161f6 407662 0 2020-01-23 22:27:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 4a60c846-89a8-44cf-ab2f-e986ab1cca08 0xc002406ef0 0xc002406ef1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f84sl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f84sl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f84sl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:management-cluster-1-17-2-md-0-5f64bb5777-fsd6f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 22:27:36.339: INFO: Pod "webserver-deployment-595b5b9587-tfh75" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-tfh75 webserver-deployment-595b5b9587- deployment-7425 /api/v1/namespaces/deployment-7425/pods/webserver-deployment-595b5b9587-tfh75 ac1ffab1-db19-42d6-9013-4a50ea78c3d3 407645 0 2020-01-23 22:27:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 4a60c846-89a8-44cf-ab2f-e986ab1cca08 0xc002406ff0 0xc002406ff1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f84sl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f84sl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f84sl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:management-cluster-1-17-2-md-0-5f64bb5777-fsd6f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 22:27:36.340: INFO: Pod "webserver-deployment-595b5b9587-xm2mp" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-xm2mp webserver-deployment-595b5b9587- deployment-7425 /api/v1/namespaces/deployment-7425/pods/webserver-deployment-595b5b9587-xm2mp 9fc67457-0de5-42eb-bf36-d012c4197304 407450 0 2020-01-23 22:27:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:100.120.200.49/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 4a60c846-89a8-44cf-ab2f-e986ab1cca08 0xc0024070f0 0xc0024070f1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f84sl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f84sl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f84sl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:management-cluster-1-17-2-md-0-5f64bb5777-fsd6f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.219.138,PodIP:100.120.200.49,StartTime:2020-01-23 22:27:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-23 22:27:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://d0d0c10c002a20a820d4c7b245f4fa7ec952835b400a030cad9724d516765ba3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.120.200.49,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 22:27:36.340: INFO: Pod "webserver-deployment-595b5b9587-zdhjp" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-zdhjp webserver-deployment-595b5b9587- deployment-7425 /api/v1/namespaces/deployment-7425/pods/webserver-deployment-595b5b9587-zdhjp 9154cb9f-53f4-43eb-b300-afe1215a4a30 407646 0 2020-01-23 22:27:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 4a60c846-89a8-44cf-ab2f-e986ab1cca08 0xc002407257 0xc002407258}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f84sl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f84sl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f84sl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:management-cluster-1-17-2-md-0-5f64bb5777-dlwcz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 22:27:36.341: INFO: Pod "webserver-deployment-c7997dcc8-55lsr" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-55lsr webserver-deployment-c7997dcc8- deployment-7425 /api/v1/namespaces/deployment-7425/pods/webserver-deployment-c7997dcc8-55lsr 1668f41e-1af8-4d9c-981c-87dab5920065 407674 0 2020-01-23 22:27:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 47e7f75a-cf60-4c25-9e54-899089929e31 0xc002407360 0xc002407361}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f84sl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f84sl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f84sl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 22:27:36.341: INFO: Pod "webserver-deployment-c7997dcc8-5llk5" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-5llk5 webserver-deployment-c7997dcc8- deployment-7425 /api/v1/namespaces/deployment-7425/pods/webserver-deployment-c7997dcc8-5llk5 3d3fc061-8bfb-4484-a321-e0c406568fb5 407676 0 2020-01-23 22:27:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 47e7f75a-cf60-4c25-9e54-899089929e31 0xc002407460 0xc002407461}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f84sl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f84sl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f84sl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:management-cluster-1-17-2-md-0-5f64bb5777-fsd6f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 22:27:36.342: INFO: Pod "webserver-deployment-c7997dcc8-b7llx" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-b7llx webserver-deployment-c7997dcc8- deployment-7425 /api/v1/namespaces/deployment-7425/pods/webserver-deployment-c7997dcc8-b7llx d8d717d6-1ee0-4f67-95ac-12915b0d31e4 407669 0 2020-01-23 22:27:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 47e7f75a-cf60-4c25-9e54-899089929e31 0xc002407570 0xc002407571}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f84sl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f84sl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f84sl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:management-cluster-1-17-2-md-0-5f64bb5777-v477d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 22:27:36.343: INFO: Pod "webserver-deployment-c7997dcc8-gjrls" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-gjrls webserver-deployment-c7997dcc8- deployment-7425 /api/v1/namespaces/deployment-7425/pods/webserver-deployment-c7997dcc8-gjrls fac20cdd-075a-434c-be03-e07d32b21b85 407647 0 2020-01-23 22:27:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 47e7f75a-cf60-4c25-9e54-899089929e31 0xc002407680 0xc002407681}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f84sl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f84sl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f84sl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:management-cluster-1-17-2-md-0-5f64bb5777-fsd6f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 22:27:36.344: INFO: Pod "webserver-deployment-c7997dcc8-h42v9" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-h42v9 webserver-deployment-c7997dcc8- deployment-7425 /api/v1/namespaces/deployment-7425/pods/webserver-deployment-c7997dcc8-h42v9 12e86b5d-8aa9-42a6-9fde-b1a04e46bf89 407579 0 2020-01-23 22:27:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:100.120.200.51/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 47e7f75a-cf60-4c25-9e54-899089929e31 0xc002407790 0xc002407791}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f84sl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f84sl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f84sl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:management-cluster-1-17-2-md-0-5f64bb5777-fsd6f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.219.138,PodIP:,StartTime:2020-01-23 22:27:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 22:27:36.349: INFO: Pod "webserver-deployment-c7997dcc8-nvhxs" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-nvhxs webserver-deployment-c7997dcc8- deployment-7425 /api/v1/namespaces/deployment-7425/pods/webserver-deployment-c7997dcc8-nvhxs 1b193a49-ad70-489b-8fdf-5631b068e29b 407602 0 2020-01-23 22:27:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:100.117.222.222/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 47e7f75a-cf60-4c25-9e54-899089929e31 0xc0024078f7 0xc0024078f8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f84sl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f84sl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f84sl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:management-cluster-1-17-2-md-0-5f64bb5777-dlwcz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.220.23,PodIP:,StartTime:2020-01-23 22:27:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 22:27:36.349: INFO: Pod "webserver-deployment-c7997dcc8-qfnlt" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-qfnlt webserver-deployment-c7997dcc8- deployment-7425 /api/v1/namespaces/deployment-7425/pods/webserver-deployment-c7997dcc8-qfnlt ea2dd5f1-f12b-4c9a-b852-aafd512482f6 407605 0 2020-01-23 22:27:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:100.117.222.225/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 47e7f75a-cf60-4c25-9e54-899089929e31 0xc002407a67 0xc002407a68}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f84sl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f84sl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f84sl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:management-cluster-1-17-2-md-0-5f64bb5777-dlwcz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.220.23,PodIP:,StartTime:2020-01-23 22:27:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 22:27:36.350: INFO: Pod "webserver-deployment-c7997dcc8-rbgws" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-rbgws webserver-deployment-c7997dcc8- deployment-7425 /api/v1/namespaces/deployment-7425/pods/webserver-deployment-c7997dcc8-rbgws 56c53271-8226-4af0-9cc5-9f96f8ea8c3d 407675 0 2020-01-23 22:27:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 47e7f75a-cf60-4c25-9e54-899089929e31 0xc002407bd7 0xc002407bd8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f84sl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f84sl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f84sl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:management-cluster-1-17-2-md-0-5f64bb5777-v477d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 22:27:36.351: INFO: Pod "webserver-deployment-c7997dcc8-rxrjx" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-rxrjx webserver-deployment-c7997dcc8- deployment-7425 /api/v1/namespaces/deployment-7425/pods/webserver-deployment-c7997dcc8-rxrjx 07ee1315-ed9c-4eac-a343-2a30187eb68a 407640 0 2020-01-23 22:27:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 47e7f75a-cf60-4c25-9e54-899089929e31 0xc002407cf0 0xc002407cf1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f84sl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f84sl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f84sl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:management-cluster-1-17-2-md-0-5f64bb5777-dlwcz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 22:27:36.353: INFO: Pod "webserver-deployment-c7997dcc8-sbzv8" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-sbzv8 webserver-deployment-c7997dcc8- deployment-7425 /api/v1/namespaces/deployment-7425/pods/webserver-deployment-c7997dcc8-sbzv8 e8405589-6c41-4a3d-ad00-406cc119e52e 407654 0 2020-01-23 22:27:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 47e7f75a-cf60-4c25-9e54-899089929e31 0xc002407e20 0xc002407e21}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f84sl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f84sl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f84sl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:management-cluster-1-17-2-md-0-5f64bb5777-v477d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.209.78,PodIP:,StartTime:2020-01-23 22:27:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 22:27:36.354: INFO: Pod "webserver-deployment-c7997dcc8-sckbw" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-sckbw webserver-deployment-c7997dcc8- deployment-7425 /api/v1/namespaces/deployment-7425/pods/webserver-deployment-c7997dcc8-sckbw b8b00ccb-aedf-4da0-82ad-62b7319b44b1 407586 0 2020-01-23 22:27:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:100.120.200.50/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 47e7f75a-cf60-4c25-9e54-899089929e31 0xc002407f87 0xc002407f88}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f84sl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f84sl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f84sl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:management-cluster-1-17-2-md-0-5f64bb5777-fsd6f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.219.138,PodIP:,StartTime:2020-01-23 22:27:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 22:27:36.355: INFO: Pod "webserver-deployment-c7997dcc8-xkndz" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-xkndz webserver-deployment-c7997dcc8- deployment-7425 /api/v1/namespaces/deployment-7425/pods/webserver-deployment-c7997dcc8-xkndz f396635d-cd3c-4bda-8e18-8925b2a94fb5 407671 0 2020-01-23 22:27:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 47e7f75a-cf60-4c25-9e54-899089929e31 0xc002c060f7 0xc002c060f8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f84sl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f84sl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f84sl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:management-cluster-1-17-2-md-0-5f64bb5777-dlwcz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 22:27:36.355: INFO: Pod "webserver-deployment-c7997dcc8-ztfln" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-ztfln webserver-deployment-c7997dcc8- deployment-7425 /api/v1/namespaces/deployment-7425/pods/webserver-deployment-c7997dcc8-ztfln de6882d4-1e0c-49d2-b3c4-44aab2fe967b 407583 0 2020-01-23 22:27:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:100.105.119.111/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 47e7f75a-cf60-4c25-9e54-899089929e31 0xc002c06210 0xc002c06211}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f84sl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f84sl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f84sl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:management-cluster-1-17-2-md-0-5f64bb5777-v477d,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:27:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.209.78,PodIP:,StartTime:2020-01-23 22:27:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:27:36.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7425" for this suite.

• [SLOW TEST:8.108 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":280,"completed":71,"skipped":1012,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:27:36.446: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod busybox-e71dfeb2-8775-49fa-8208-3d651d6f628c in namespace container-probe-3408
Jan 23 22:27:42.730: INFO: Started pod busybox-e71dfeb2-8775-49fa-8208-3d651d6f628c in namespace container-probe-3408
STEP: checking the pod's current state and verifying that restartCount is present
Jan 23 22:27:42.738: INFO: Initial restart count of pod busybox-e71dfeb2-8775-49fa-8208-3d651d6f628c is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:31:57.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3408" for this suite.

• [SLOW TEST:248.383 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":280,"completed":72,"skipped":1047,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:31:58.002: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 23 22:31:58.190: INFO: Create a RollingUpdate DaemonSet
Jan 23 22:31:58.210: INFO: Check that daemon pods launch on every node of the cluster
Jan 23 22:31:58.237: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:31:58.243: INFO: Number of nodes with available pods: 0
Jan 23 22:31:58.243: INFO: Node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz is running more than one daemon pod
Jan 23 22:31:59.253: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:31:59.271: INFO: Number of nodes with available pods: 0
Jan 23 22:31:59.271: INFO: Node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz is running more than one daemon pod
Jan 23 22:32:00.255: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:32:00.269: INFO: Number of nodes with available pods: 1
Jan 23 22:32:00.269: INFO: Node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz is running more than one daemon pod
Jan 23 22:32:01.251: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:32:01.258: INFO: Number of nodes with available pods: 3
Jan 23 22:32:01.258: INFO: Number of running nodes: 3, number of available pods: 3
Jan 23 22:32:01.258: INFO: Update the DaemonSet to trigger a rollout
Jan 23 22:32:01.271: INFO: Updating DaemonSet daemon-set
Jan 23 22:32:12.391: INFO: Roll back the DaemonSet before rollout is complete
Jan 23 22:32:12.405: INFO: Updating DaemonSet daemon-set
Jan 23 22:32:12.405: INFO: Make sure DaemonSet rollback is complete
Jan 23 22:32:12.424: INFO: Wrong image for pod: daemon-set-bpgfj. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jan 23 22:32:12.424: INFO: Pod daemon-set-bpgfj is not available
Jan 23 22:32:12.435: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:32:13.444: INFO: Wrong image for pod: daemon-set-bpgfj. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jan 23 22:32:13.444: INFO: Pod daemon-set-bpgfj is not available
Jan 23 22:32:13.456: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:32:14.447: INFO: Wrong image for pod: daemon-set-bpgfj. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jan 23 22:32:14.448: INFO: Pod daemon-set-bpgfj is not available
Jan 23 22:32:14.457: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:32:15.446: INFO: Pod daemon-set-h5sgt is not available
Jan 23 22:32:15.454: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1194, will wait for the garbage collector to delete the pods
Jan 23 22:32:15.562: INFO: Deleting DaemonSet.extensions daemon-set took: 30.53556ms
Jan 23 22:32:16.062: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.905707ms
Jan 23 22:32:30.074: INFO: Number of nodes with available pods: 0
Jan 23 22:32:30.074: INFO: Number of running nodes: 0, number of available pods: 0
Jan 23 22:32:30.080: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1194/daemonsets","resourceVersion":"409583"},"items":null}

Jan 23 22:32:30.085: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1194/pods","resourceVersion":"409583"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:32:30.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1194" for this suite.

• [SLOW TEST:30.819 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":280,"completed":73,"skipped":1109,"failed":0}
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:32:30.129: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jan 23 22:32:38.305: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 22:32:38.313: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 22:32:40.313: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 22:32:40.321: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 22:32:42.313: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 22:32:42.318: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 23 22:32:44.313: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 23 22:32:44.320: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:32:44.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8262" for this suite.

• [SLOW TEST:14.211 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":280,"completed":74,"skipped":1109,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:32:44.345: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 23 22:32:44.449: INFO: Waiting up to 5m0s for pod "downwardapi-volume-72c3169e-4f43-4c89-a8e2-3c3ef3496afd" in namespace "projected-716" to be "success or failure"
Jan 23 22:32:44.457: INFO: Pod "downwardapi-volume-72c3169e-4f43-4c89-a8e2-3c3ef3496afd": Phase="Pending", Reason="", readiness=false. Elapsed: 7.910361ms
Jan 23 22:32:46.464: INFO: Pod "downwardapi-volume-72c3169e-4f43-4c89-a8e2-3c3ef3496afd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014566176s
Jan 23 22:32:48.470: INFO: Pod "downwardapi-volume-72c3169e-4f43-4c89-a8e2-3c3ef3496afd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020854528s
STEP: Saw pod success
Jan 23 22:32:48.471: INFO: Pod "downwardapi-volume-72c3169e-4f43-4c89-a8e2-3c3ef3496afd" satisfied condition "success or failure"
Jan 23 22:32:48.479: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f pod downwardapi-volume-72c3169e-4f43-4c89-a8e2-3c3ef3496afd container client-container: <nil>
STEP: delete the pod
Jan 23 22:32:48.535: INFO: Waiting for pod downwardapi-volume-72c3169e-4f43-4c89-a8e2-3c3ef3496afd to disappear
Jan 23 22:32:48.550: INFO: Pod downwardapi-volume-72c3169e-4f43-4c89-a8e2-3c3ef3496afd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:32:48.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-716" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":280,"completed":75,"skipped":1130,"failed":0}
SS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:32:48.565: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:178
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:32:48.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-712" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":280,"completed":76,"skipped":1132,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:32:48.677: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod liveness-23999c25-6095-49e0-918f-059053c1ca09 in namespace container-probe-206
Jan 23 22:32:52.113: INFO: Started pod liveness-23999c25-6095-49e0-918f-059053c1ca09 in namespace container-probe-206
STEP: checking the pod's current state and verifying that restartCount is present
Jan 23 22:32:52.117: INFO: Initial restart count of pod liveness-23999c25-6095-49e0-918f-059053c1ca09 is 0
Jan 23 22:33:06.160: INFO: Restart count of pod container-probe-206/liveness-23999c25-6095-49e0-918f-059053c1ca09 is now 1 (14.042475857s elapsed)
Jan 23 22:33:29.834: INFO: Restart count of pod container-probe-206/liveness-23999c25-6095-49e0-918f-059053c1ca09 is now 2 (36.223244813s elapsed)
Jan 23 22:33:47.903: INFO: Restart count of pod container-probe-206/liveness-23999c25-6095-49e0-918f-059053c1ca09 is now 3 (54.29203115s elapsed)
Jan 23 22:34:09.506: INFO: Restart count of pod container-probe-206/liveness-23999c25-6095-49e0-918f-059053c1ca09 is now 4 (1m14.402657531s elapsed)
Jan 23 22:35:22.850: INFO: Restart count of pod container-probe-206/liveness-23999c25-6095-49e0-918f-059053c1ca09 is now 5 (2m24.711160961s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:35:22.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-206" for this suite.

• [SLOW TEST:147.036 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":280,"completed":77,"skipped":1164,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:35:23.096: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-373
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a new StatefulSet
Jan 23 22:35:23.555: INFO: Found 0 stateful pods, waiting for 3
Jan 23 22:35:33.563: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 23 22:35:33.564: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 23 22:35:33.564: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jan 23 22:35:33.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 exec --namespace=statefulset-373 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 23 22:35:34.365: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 23 22:35:34.365: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 23 22:35:34.365: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jan 23 22:35:45.990: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jan 23 22:35:56.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 exec --namespace=statefulset-373 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 23 22:35:56.369: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 23 22:35:56.369: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 23 22:35:56.369: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 23 22:36:06.431: INFO: Waiting for StatefulSet statefulset-373/ss2 to complete update
Jan 23 22:36:06.432: INFO: Waiting for Pod statefulset-373/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jan 23 22:36:18.050: INFO: Waiting for StatefulSet statefulset-373/ss2 to complete update
Jan 23 22:36:18.051: INFO: Waiting for Pod statefulset-373/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
Jan 23 22:36:28.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 exec --namespace=statefulset-373 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 23 22:36:29.286: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 23 22:36:29.289: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 23 22:36:29.290: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 23 22:36:39.864: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jan 23 22:36:51.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 exec --namespace=statefulset-373 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 23 22:36:52.098: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 23 22:36:52.099: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 23 22:36:52.099: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 23 22:37:12.153: INFO: Waiting for StatefulSet statefulset-373/ss2 to complete update
Jan 23 22:37:12.154: INFO: Waiting for Pod statefulset-373/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jan 23 22:37:23.831: INFO: Waiting for StatefulSet statefulset-373/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jan 23 22:37:33.831: INFO: Deleting all statefulset in ns statefulset-373
Jan 23 22:37:33.837: INFO: Scaling statefulset ss2 to 0
Jan 23 22:37:43.869: INFO: Waiting for statefulset status.replicas updated to 0
Jan 23 22:37:43.875: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:37:43.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-373" for this suite.

• [SLOW TEST:134.411 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":280,"completed":78,"skipped":1184,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:37:43.965: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1209.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-1209.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1209.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1209.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-1209.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1209.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 23 22:37:46.185: INFO: DNS probes using dns-1209/dns-test-af1612fd-8cd8-4d95-9598-0160df23a786 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:37:46.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1209" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":280,"completed":79,"skipped":1189,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:37:46.303: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 23 22:37:47.057: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 23 22:37:49.142: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715415867, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715415867, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715415868, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715415867, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 23 22:37:51.151: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715415867, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715415867, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715415868, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715415867, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 23 22:37:55.877: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Jan 23 22:37:55.967: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:37:56.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-661" for this suite.
STEP: Destroying namespace "webhook-661-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.208 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":280,"completed":80,"skipped":1193,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:37:56.204: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jan 23 22:38:02.903: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:38:02.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0123 22:38:02.900823      21 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-2381" for this suite.

• [SLOW TEST:6.865 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":280,"completed":81,"skipped":1214,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:38:03.094: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-map-e6bb0035-4458-4a4e-b292-1f438f6f9f72
STEP: Creating a pod to test consume secrets
Jan 23 22:38:03.398: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0d2b3c43-91d1-43f2-8f5b-4f30e3213791" in namespace "projected-4982" to be "success or failure"
Jan 23 22:38:03.405: INFO: Pod "pod-projected-secrets-0d2b3c43-91d1-43f2-8f5b-4f30e3213791": Phase="Pending", Reason="", readiness=false. Elapsed: 7.177384ms
Jan 23 22:38:05.411: INFO: Pod "pod-projected-secrets-0d2b3c43-91d1-43f2-8f5b-4f30e3213791": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013138144s
STEP: Saw pod success
Jan 23 22:38:05.411: INFO: Pod "pod-projected-secrets-0d2b3c43-91d1-43f2-8f5b-4f30e3213791" satisfied condition "success or failure"
Jan 23 22:38:05.416: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod pod-projected-secrets-0d2b3c43-91d1-43f2-8f5b-4f30e3213791 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 23 22:38:05.499: INFO: Waiting for pod pod-projected-secrets-0d2b3c43-91d1-43f2-8f5b-4f30e3213791 to disappear
Jan 23 22:38:05.503: INFO: Pod pod-projected-secrets-0d2b3c43-91d1-43f2-8f5b-4f30e3213791 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:38:05.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4982" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":82,"skipped":1228,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:38:05.525: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:38:21.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9636" for this suite.

• [SLOW TEST:16.261 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":280,"completed":83,"skipped":1244,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:38:21.794: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-46564846-9767-498e-a715-ba7109c76112
STEP: Creating a pod to test consume secrets
Jan 23 22:38:21.857: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d8475845-f275-4089-a63e-77bfd94406c7" in namespace "projected-896" to be "success or failure"
Jan 23 22:38:21.864: INFO: Pod "pod-projected-secrets-d8475845-f275-4089-a63e-77bfd94406c7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.68877ms
Jan 23 22:38:23.870: INFO: Pod "pod-projected-secrets-d8475845-f275-4089-a63e-77bfd94406c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013846325s
STEP: Saw pod success
Jan 23 22:38:23.871: INFO: Pod "pod-projected-secrets-d8475845-f275-4089-a63e-77bfd94406c7" satisfied condition "success or failure"
Jan 23 22:38:23.877: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod pod-projected-secrets-d8475845-f275-4089-a63e-77bfd94406c7 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 23 22:38:23.913: INFO: Waiting for pod pod-projected-secrets-d8475845-f275-4089-a63e-77bfd94406c7 to disappear
Jan 23 22:38:23.918: INFO: Pod pod-projected-secrets-d8475845-f275-4089-a63e-77bfd94406c7 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:38:23.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-896" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":84,"skipped":1285,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:38:23.940: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-bcf5cf7d-bb9c-4feb-8a9a-5eefe14dda07
STEP: Creating a pod to test consume configMaps
Jan 23 22:38:23.999: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d082c250-37eb-4bac-866c-0592be29357c" in namespace "projected-5201" to be "success or failure"
Jan 23 22:38:24.016: INFO: Pod "pod-projected-configmaps-d082c250-37eb-4bac-866c-0592be29357c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.307847ms
Jan 23 22:38:26.022: INFO: Pod "pod-projected-configmaps-d082c250-37eb-4bac-866c-0592be29357c": Phase="Running", Reason="", readiness=true. Elapsed: 2.022132883s
Jan 23 22:38:29.767: INFO: Pod "pod-projected-configmaps-d082c250-37eb-4bac-866c-0592be29357c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027385675s
STEP: Saw pod success
Jan 23 22:38:29.767: INFO: Pod "pod-projected-configmaps-d082c250-37eb-4bac-866c-0592be29357c" satisfied condition "success or failure"
Jan 23 22:38:29.771: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod pod-projected-configmaps-d082c250-37eb-4bac-866c-0592be29357c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 23 22:38:29.800: INFO: Waiting for pod pod-projected-configmaps-d082c250-37eb-4bac-866c-0592be29357c to disappear
Jan 23 22:38:29.804: INFO: Pod pod-projected-configmaps-d082c250-37eb-4bac-866c-0592be29357c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:38:29.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5201" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":280,"completed":85,"skipped":1310,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:38:29.822: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 23 22:38:29.863: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Creating first CR 
Jan 23 22:38:30.646: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-01-23T22:38:29Z generation:1 name:name1 resourceVersion:412347 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:c1419b08-30e5-470a-8761-976e39ce7629] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Jan 23 22:38:40.654: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-01-23T22:38:39Z generation:1 name:name2 resourceVersion:412413 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:819c5ac5-d18b-415f-8a97-9a91fd342cde] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Jan 23 22:38:50.670: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-01-23T22:38:29Z generation:2 name:name1 resourceVersion:412459 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:c1419b08-30e5-470a-8761-976e39ce7629] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Jan 23 22:39:00.681: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-01-23T22:38:39Z generation:2 name:name2 resourceVersion:412505 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:819c5ac5-d18b-415f-8a97-9a91fd342cde] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Jan 23 22:39:12.443: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-01-23T22:38:29Z generation:2 name:name1 resourceVersion:412550 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:c1419b08-30e5-470a-8761-976e39ce7629] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Jan 23 22:39:22.460: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-01-23T22:38:39Z generation:2 name:name2 resourceVersion:412597 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:819c5ac5-d18b-415f-8a97-9a91fd342cde] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:39:32.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-870" for this suite.

• [SLOW TEST:61.438 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:41
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":280,"completed":86,"skipped":1315,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:39:33.021: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Jan 23 22:39:33.078: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:39:35.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-553" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":280,"completed":87,"skipped":1342,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:39:35.870: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-547ae7b5-f45b-4868-81ef-78ad0d5f589e
STEP: Creating a pod to test consume configMaps
Jan 23 22:39:35.934: INFO: Waiting up to 5m0s for pod "pod-configmaps-a144b842-98be-4639-961b-7beffc0d1051" in namespace "configmap-4250" to be "success or failure"
Jan 23 22:39:35.940: INFO: Pod "pod-configmaps-a144b842-98be-4639-961b-7beffc0d1051": Phase="Pending", Reason="", readiness=false. Elapsed: 5.646859ms
Jan 23 22:39:40.114: INFO: Pod "pod-configmaps-a144b842-98be-4639-961b-7beffc0d1051": Phase="Pending", Reason="", readiness=false. Elapsed: 2.358085861s
Jan 23 22:39:42.128: INFO: Pod "pod-configmaps-a144b842-98be-4639-961b-7beffc0d1051": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.372148902s
STEP: Saw pod success
Jan 23 22:39:42.128: INFO: Pod "pod-configmaps-a144b842-98be-4639-961b-7beffc0d1051" satisfied condition "success or failure"
Jan 23 22:39:42.135: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod pod-configmaps-a144b842-98be-4639-961b-7beffc0d1051 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 23 22:39:42.198: INFO: Waiting for pod pod-configmaps-a144b842-98be-4639-961b-7beffc0d1051 to disappear
Jan 23 22:39:42.207: INFO: Pod pod-configmaps-a144b842-98be-4639-961b-7beffc0d1051 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:39:42.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4250" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":88,"skipped":1343,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:39:42.297: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on node default medium
Jan 23 22:39:42.434: INFO: Waiting up to 5m0s for pod "pod-f0c9e911-e0d1-450f-97f9-b4de6a157430" in namespace "emptydir-7056" to be "success or failure"
Jan 23 22:39:42.453: INFO: Pod "pod-f0c9e911-e0d1-450f-97f9-b4de6a157430": Phase="Pending", Reason="", readiness=false. Elapsed: 18.579065ms
Jan 23 22:39:44.460: INFO: Pod "pod-f0c9e911-e0d1-450f-97f9-b4de6a157430": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025655523s
Jan 23 22:39:46.466: INFO: Pod "pod-f0c9e911-e0d1-450f-97f9-b4de6a157430": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032066825s
STEP: Saw pod success
Jan 23 22:39:46.466: INFO: Pod "pod-f0c9e911-e0d1-450f-97f9-b4de6a157430" satisfied condition "success or failure"
Jan 23 22:39:46.472: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod pod-f0c9e911-e0d1-450f-97f9-b4de6a157430 container test-container: <nil>
STEP: delete the pod
Jan 23 22:39:46.549: INFO: Waiting for pod pod-f0c9e911-e0d1-450f-97f9-b4de6a157430 to disappear
Jan 23 22:39:46.589: INFO: Pod pod-f0c9e911-e0d1-450f-97f9-b4de6a157430 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:39:46.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7056" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":89,"skipped":1362,"failed":0}
SSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:39:46.613: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1178.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-1178.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1178.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-1178.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1178.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1178.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-1178.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1178.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-1178.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1178.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 23 22:39:50.828: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local from pod dns-1178/dns-test-e96479b0-f188-4851-a317-16f2a15b189a: the server could not find the requested resource (get pods dns-test-e96479b0-f188-4851-a317-16f2a15b189a)
Jan 23 22:39:50.835: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local from pod dns-1178/dns-test-e96479b0-f188-4851-a317-16f2a15b189a: the server could not find the requested resource (get pods dns-test-e96479b0-f188-4851-a317-16f2a15b189a)
Jan 23 22:39:50.868: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local from pod dns-1178/dns-test-e96479b0-f188-4851-a317-16f2a15b189a: the server could not find the requested resource (get pods dns-test-e96479b0-f188-4851-a317-16f2a15b189a)
Jan 23 22:39:50.874: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local from pod dns-1178/dns-test-e96479b0-f188-4851-a317-16f2a15b189a: the server could not find the requested resource (get pods dns-test-e96479b0-f188-4851-a317-16f2a15b189a)
Jan 23 22:39:50.899: INFO: Lookups using dns-1178/dns-test-e96479b0-f188-4851-a317-16f2a15b189a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local]

Jan 23 22:39:55.908: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local from pod dns-1178/dns-test-e96479b0-f188-4851-a317-16f2a15b189a: the server could not find the requested resource (get pods dns-test-e96479b0-f188-4851-a317-16f2a15b189a)
Jan 23 22:39:55.914: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local from pod dns-1178/dns-test-e96479b0-f188-4851-a317-16f2a15b189a: the server could not find the requested resource (get pods dns-test-e96479b0-f188-4851-a317-16f2a15b189a)
Jan 23 22:39:55.944: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local from pod dns-1178/dns-test-e96479b0-f188-4851-a317-16f2a15b189a: the server could not find the requested resource (get pods dns-test-e96479b0-f188-4851-a317-16f2a15b189a)
Jan 23 22:39:55.951: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local from pod dns-1178/dns-test-e96479b0-f188-4851-a317-16f2a15b189a: the server could not find the requested resource (get pods dns-test-e96479b0-f188-4851-a317-16f2a15b189a)
Jan 23 22:39:55.974: INFO: Lookups using dns-1178/dns-test-e96479b0-f188-4851-a317-16f2a15b189a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local]

Jan 23 22:40:00.909: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local from pod dns-1178/dns-test-e96479b0-f188-4851-a317-16f2a15b189a: the server could not find the requested resource (get pods dns-test-e96479b0-f188-4851-a317-16f2a15b189a)
Jan 23 22:40:00.919: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local from pod dns-1178/dns-test-e96479b0-f188-4851-a317-16f2a15b189a: the server could not find the requested resource (get pods dns-test-e96479b0-f188-4851-a317-16f2a15b189a)
Jan 23 22:40:00.963: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local from pod dns-1178/dns-test-e96479b0-f188-4851-a317-16f2a15b189a: the server could not find the requested resource (get pods dns-test-e96479b0-f188-4851-a317-16f2a15b189a)
Jan 23 22:40:00.973: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local from pod dns-1178/dns-test-e96479b0-f188-4851-a317-16f2a15b189a: the server could not find the requested resource (get pods dns-test-e96479b0-f188-4851-a317-16f2a15b189a)
Jan 23 22:40:01.007: INFO: Lookups using dns-1178/dns-test-e96479b0-f188-4851-a317-16f2a15b189a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local]

Jan 23 22:40:05.918: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local from pod dns-1178/dns-test-e96479b0-f188-4851-a317-16f2a15b189a: the server could not find the requested resource (get pods dns-test-e96479b0-f188-4851-a317-16f2a15b189a)
Jan 23 22:40:05.934: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local from pod dns-1178/dns-test-e96479b0-f188-4851-a317-16f2a15b189a: the server could not find the requested resource (get pods dns-test-e96479b0-f188-4851-a317-16f2a15b189a)
Jan 23 22:40:06.052: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local from pod dns-1178/dns-test-e96479b0-f188-4851-a317-16f2a15b189a: the server could not find the requested resource (get pods dns-test-e96479b0-f188-4851-a317-16f2a15b189a)
Jan 23 22:40:06.080: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local from pod dns-1178/dns-test-e96479b0-f188-4851-a317-16f2a15b189a: the server could not find the requested resource (get pods dns-test-e96479b0-f188-4851-a317-16f2a15b189a)
Jan 23 22:40:06.203: INFO: Lookups using dns-1178/dns-test-e96479b0-f188-4851-a317-16f2a15b189a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local]

Jan 23 22:40:12.671: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local from pod dns-1178/dns-test-e96479b0-f188-4851-a317-16f2a15b189a: the server could not find the requested resource (get pods dns-test-e96479b0-f188-4851-a317-16f2a15b189a)
Jan 23 22:40:12.677: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local from pod dns-1178/dns-test-e96479b0-f188-4851-a317-16f2a15b189a: the server could not find the requested resource (get pods dns-test-e96479b0-f188-4851-a317-16f2a15b189a)
Jan 23 22:40:12.719: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local from pod dns-1178/dns-test-e96479b0-f188-4851-a317-16f2a15b189a: the server could not find the requested resource (get pods dns-test-e96479b0-f188-4851-a317-16f2a15b189a)
Jan 23 22:40:12.725: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local from pod dns-1178/dns-test-e96479b0-f188-4851-a317-16f2a15b189a: the server could not find the requested resource (get pods dns-test-e96479b0-f188-4851-a317-16f2a15b189a)
Jan 23 22:40:12.752: INFO: Lookups using dns-1178/dns-test-e96479b0-f188-4851-a317-16f2a15b189a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local]

Jan 23 22:40:17.673: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local from pod dns-1178/dns-test-e96479b0-f188-4851-a317-16f2a15b189a: the server could not find the requested resource (get pods dns-test-e96479b0-f188-4851-a317-16f2a15b189a)
Jan 23 22:40:17.681: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local from pod dns-1178/dns-test-e96479b0-f188-4851-a317-16f2a15b189a: the server could not find the requested resource (get pods dns-test-e96479b0-f188-4851-a317-16f2a15b189a)
Jan 23 22:40:17.710: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local from pod dns-1178/dns-test-e96479b0-f188-4851-a317-16f2a15b189a: the server could not find the requested resource (get pods dns-test-e96479b0-f188-4851-a317-16f2a15b189a)
Jan 23 22:40:17.715: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local from pod dns-1178/dns-test-e96479b0-f188-4851-a317-16f2a15b189a: the server could not find the requested resource (get pods dns-test-e96479b0-f188-4851-a317-16f2a15b189a)
Jan 23 22:40:17.755: INFO: Lookups using dns-1178/dns-test-e96479b0-f188-4851-a317-16f2a15b189a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1178.svc.cluster.local]

Jan 23 22:40:22.755: INFO: DNS probes using dns-1178/dns-test-e96479b0-f188-4851-a317-16f2a15b189a succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:40:22.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1178" for this suite.

• [SLOW TEST:34.460 seconds]
[sig-network] DNS
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":280,"completed":90,"skipped":1369,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:40:22.842: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting the proxy server
Jan 23 22:40:22.905: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-837620349 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:40:23.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-894" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":280,"completed":91,"skipped":1389,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:40:23.833: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Jan 23 22:40:28.287: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-720 PodName:pod-sharedvolume-3badb61b-3d9c-4d62-b8ef-b7445c25e9e3 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 23 22:40:28.287: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
Jan 23 22:40:28.583: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:40:28.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-720" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":280,"completed":92,"skipped":1401,"failed":0}
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:40:28.619: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Jan 23 22:40:28.744: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 23 22:40:28.858: INFO: Waiting for terminating namespaces to be deleted...
Jan 23 22:40:28.879: INFO: 
Logging pods the kubelet thinks is on node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz before test
Jan 23 22:40:28.921: INFO: kube-proxy-qfmln from kube-system started at 2020-01-22 22:09:02 +0000 UTC (1 container statuses recorded)
Jan 23 22:40:28.922: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 23 22:40:28.922: INFO: sonobuoy-systemd-logs-daemon-set-82eb3c000b9d48f1-m9c8t from sonobuoy started at 2020-01-23 22:01:22 +0000 UTC (2 container statuses recorded)
Jan 23 22:40:28.922: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 23 22:40:28.922: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 23 22:40:28.922: INFO: vsphere-csi-node-4knv2 from kube-system started at 2020-01-23 22:13:07 +0000 UTC (3 container statuses recorded)
Jan 23 22:40:28.922: INFO: 	Container liveness-probe ready: true, restart count 0
Jan 23 22:40:28.922: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 23 22:40:28.922: INFO: 	Container vsphere-csi-node ready: true, restart count 0
Jan 23 22:40:28.922: INFO: calico-node-zvhsz from kube-system started at 2020-01-22 22:09:02 +0000 UTC (1 container statuses recorded)
Jan 23 22:40:28.922: INFO: 	Container calico-node ready: true, restart count 0
Jan 23 22:40:28.922: INFO: pod-sharedvolume-3badb61b-3d9c-4d62-b8ef-b7445c25e9e3 from emptydir-720 started at 2020-01-23 22:40:23 +0000 UTC (2 container statuses recorded)
Jan 23 22:40:28.922: INFO: 	Container busybox-main-container ready: true, restart count 0
Jan 23 22:40:28.922: INFO: 	Container busybox-sub-container ready: false, restart count 0
Jan 23 22:40:28.922: INFO: 
Logging pods the kubelet thinks is on node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f before test
Jan 23 22:40:28.975: INFO: vsphere-csi-node-rd47x from kube-system started at 2020-01-22 22:09:27 +0000 UTC (3 container statuses recorded)
Jan 23 22:40:28.975: INFO: 	Container liveness-probe ready: true, restart count 0
Jan 23 22:40:28.975: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 23 22:40:28.975: INFO: 	Container vsphere-csi-node ready: true, restart count 0
Jan 23 22:40:28.975: INFO: sonobuoy from sonobuoy started at 2020-01-23 22:01:11 +0000 UTC (1 container statuses recorded)
Jan 23 22:40:28.975: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 23 22:40:28.975: INFO: kube-proxy-n9fnt from kube-system started at 2020-01-22 22:09:05 +0000 UTC (1 container statuses recorded)
Jan 23 22:40:28.975: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 23 22:40:28.975: INFO: sonobuoy-systemd-logs-daemon-set-82eb3c000b9d48f1-mkq6m from sonobuoy started at 2020-01-23 22:01:23 +0000 UTC (2 container statuses recorded)
Jan 23 22:40:28.976: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 23 22:40:28.976: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 23 22:40:28.976: INFO: calico-node-27bq2 from kube-system started at 2020-01-22 22:09:05 +0000 UTC (1 container statuses recorded)
Jan 23 22:40:28.976: INFO: 	Container calico-node ready: true, restart count 0
Jan 23 22:40:28.976: INFO: 
Logging pods the kubelet thinks is on node management-cluster-1-17-2-md-0-5f64bb5777-v477d before test
Jan 23 22:40:29.036: INFO: calico-node-lbpft from kube-system started at 2020-01-22 22:09:03 +0000 UTC (1 container statuses recorded)
Jan 23 22:40:29.036: INFO: 	Container calico-node ready: true, restart count 0
Jan 23 22:40:29.036: INFO: sonobuoy-systemd-logs-daemon-set-82eb3c000b9d48f1-fwpld from sonobuoy started at 2020-01-23 22:01:22 +0000 UTC (2 container statuses recorded)
Jan 23 22:40:29.036: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 23 22:40:29.036: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 23 22:40:29.036: INFO: sonobuoy-e2e-job-61735b81990a421a from sonobuoy started at 2020-01-23 22:01:22 +0000 UTC (2 container statuses recorded)
Jan 23 22:40:29.036: INFO: 	Container e2e ready: true, restart count 0
Jan 23 22:40:29.036: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 23 22:40:29.036: INFO: kube-proxy-7w2h9 from kube-system started at 2020-01-22 22:09:03 +0000 UTC (1 container statuses recorded)
Jan 23 22:40:29.036: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 23 22:40:29.036: INFO: vsphere-csi-node-h9rth from kube-system started at 2020-01-22 22:09:25 +0000 UTC (3 container statuses recorded)
Jan 23 22:40:29.036: INFO: 	Container liveness-probe ready: true, restart count 0
Jan 23 22:40:29.036: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 23 22:40:29.036: INFO: 	Container vsphere-csi-node ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-ed348eae-f721-436a-ab7c-e79b1b117852 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-ed348eae-f721-436a-ab7c-e79b1b117852 off the node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f
STEP: verifying the node doesn't have the label kubernetes.io/e2e-ed348eae-f721-436a-ab7c-e79b1b117852
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:40:41.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6253" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:12.694 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":280,"completed":93,"skipped":1406,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:40:41.319: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 23 22:40:41.371: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jan 23 22:40:47.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 --namespace=crd-publish-openapi-7659 create -f -'
Jan 23 22:40:48.413: INFO: stderr: ""
Jan 23 22:40:48.413: INFO: stdout: "e2e-test-crd-publish-openapi-1590-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 23 22:40:48.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 --namespace=crd-publish-openapi-7659 delete e2e-test-crd-publish-openapi-1590-crds test-cr'
Jan 23 22:40:48.580: INFO: stderr: ""
Jan 23 22:40:48.580: INFO: stdout: "e2e-test-crd-publish-openapi-1590-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jan 23 22:40:48.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 --namespace=crd-publish-openapi-7659 apply -f -'
Jan 23 22:40:49.048: INFO: stderr: ""
Jan 23 22:40:49.048: INFO: stdout: "e2e-test-crd-publish-openapi-1590-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 23 22:40:49.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 --namespace=crd-publish-openapi-7659 delete e2e-test-crd-publish-openapi-1590-crds test-cr'
Jan 23 22:40:49.149: INFO: stderr: ""
Jan 23 22:40:49.149: INFO: stdout: "e2e-test-crd-publish-openapi-1590-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Jan 23 22:40:49.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 explain e2e-test-crd-publish-openapi-1590-crds'
Jan 23 22:40:49.483: INFO: stderr: ""
Jan 23 22:40:49.483: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1590-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:40:53.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7659" for this suite.

• [SLOW TEST:10.584 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":280,"completed":94,"skipped":1414,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:40:53.260: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 23 22:40:53.312: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:40:55.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7026" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":280,"completed":95,"skipped":1425,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:40:55.558: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl logs
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1464
STEP: creating an pod
Jan 23 22:40:55.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.8 --namespace=kubectl-9539 -- logs-generator --log-lines-total 100 --run-duration 20s'
Jan 23 22:40:55.739: INFO: stderr: ""
Jan 23 22:40:55.739: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Waiting for log generator to start.
Jan 23 22:40:55.739: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jan 23 22:40:55.740: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-9539" to be "running and ready, or succeeded"
Jan 23 22:40:55.746: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.833216ms
Jan 23 22:40:57.752: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.01225798s
Jan 23 22:40:57.752: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jan 23 22:40:57.752: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Jan 23 22:40:57.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 logs logs-generator logs-generator --namespace=kubectl-9539'
Jan 23 22:40:57.852: INFO: stderr: ""
Jan 23 22:40:57.852: INFO: stdout: "I0123 22:40:56.440906       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/7zh 325\nI0123 22:40:56.641014       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/pzx 376\nI0123 22:40:56.840980       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/7gc2 239\nI0123 22:40:57.040849       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/hcq 222\nI0123 22:40:57.240858       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/kqv 591\nI0123 22:40:57.440799       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/kjr7 210\n"
STEP: limiting log lines
Jan 23 22:40:57.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 logs logs-generator logs-generator --namespace=kubectl-9539 --tail=1'
Jan 23 22:40:57.979: INFO: stderr: ""
Jan 23 22:40:57.979: INFO: stdout: "I0123 22:40:57.440799       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/kjr7 210\n"
Jan 23 22:40:57.979: INFO: got output "I0123 22:40:57.440799       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/kjr7 210\n"
STEP: limiting log bytes
Jan 23 22:40:57.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 logs logs-generator logs-generator --namespace=kubectl-9539 --limit-bytes=1'
Jan 23 22:40:58.099: INFO: stderr: ""
Jan 23 22:40:58.099: INFO: stdout: "I"
Jan 23 22:40:58.099: INFO: got output "I"
STEP: exposing timestamps
Jan 23 22:40:58.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 logs logs-generator logs-generator --namespace=kubectl-9539 --tail=1 --timestamps'
Jan 23 22:40:58.203: INFO: stderr: ""
Jan 23 22:40:58.203: INFO: stdout: "2020-01-23T22:40:57.84159172Z I0123 22:40:57.841403       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/kqc 202\n"
Jan 23 22:40:58.203: INFO: got output "2020-01-23T22:40:57.84159172Z I0123 22:40:57.841403       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/kqc 202\n"
STEP: restricting to a time range
Jan 23 22:41:00.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 logs logs-generator logs-generator --namespace=kubectl-9539 --since=1s'
Jan 23 22:41:00.820: INFO: stderr: ""
Jan 23 22:41:00.820: INFO: stdout: "I0123 22:40:59.640882       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/default/pods/cxd 587\nI0123 22:40:59.840859       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/s2xx 514\nI0123 22:41:00.040810       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/7snh 415\nI0123 22:41:00.240837       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/p7f 266\nI0123 22:41:00.440889       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/default/pods/ffd 489\n"
Jan 23 22:41:00.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 logs logs-generator logs-generator --namespace=kubectl-9539 --since=24h'
Jan 23 22:41:00.941: INFO: stderr: ""
Jan 23 22:41:00.941: INFO: stdout: "I0123 22:40:56.440906       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/7zh 325\nI0123 22:40:56.641014       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/pzx 376\nI0123 22:40:56.840980       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/7gc2 239\nI0123 22:40:57.040849       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/hcq 222\nI0123 22:40:57.240858       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/kqv 591\nI0123 22:40:57.440799       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/kjr7 210\nI0123 22:40:57.641139       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/tpcc 515\nI0123 22:40:57.841403       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/kqc 202\nI0123 22:40:58.040860       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/9h2 590\nI0123 22:40:58.240881       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/kz4r 426\nI0123 22:40:58.440928       1 logs_generator.go:76] 10 GET /api/v1/namespaces/default/pods/tnp7 288\nI0123 22:40:58.640887       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/n4hn 553\nI0123 22:40:58.840778       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/default/pods/pz5 241\nI0123 22:40:59.040834       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/2zvg 540\nI0123 22:40:59.240869       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/68w 226\nI0123 22:40:59.440836       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/tf2 332\nI0123 22:40:59.640882       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/default/pods/cxd 587\nI0123 22:40:59.840859       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/s2xx 514\nI0123 22:41:00.040810       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/7snh 415\nI0123 22:41:00.240837       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/p7f 266\nI0123 22:41:00.440889       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/default/pods/ffd 489\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1470
Jan 23 22:41:00.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 delete pod logs-generator --namespace=kubectl-9539'
Jan 23 22:41:03.828: INFO: stderr: ""
Jan 23 22:41:03.835: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:41:03.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9539" for this suite.

• [SLOW TEST:8.445 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1460
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":280,"completed":96,"skipped":1452,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:41:04.029: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:41:06.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5333" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":280,"completed":97,"skipped":1458,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:41:06.877: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override arguments
Jan 23 22:41:06.954: INFO: Waiting up to 5m0s for pod "client-containers-9b2368b4-c25e-4f8f-aeae-447ca08bd465" in namespace "containers-5488" to be "success or failure"
Jan 23 22:41:06.962: INFO: Pod "client-containers-9b2368b4-c25e-4f8f-aeae-447ca08bd465": Phase="Pending", Reason="", readiness=false. Elapsed: 7.369233ms
Jan 23 22:41:08.972: INFO: Pod "client-containers-9b2368b4-c25e-4f8f-aeae-447ca08bd465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017642899s
STEP: Saw pod success
Jan 23 22:41:08.972: INFO: Pod "client-containers-9b2368b4-c25e-4f8f-aeae-447ca08bd465" satisfied condition "success or failure"
Jan 23 22:41:08.980: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod client-containers-9b2368b4-c25e-4f8f-aeae-447ca08bd465 container test-container: <nil>
STEP: delete the pod
Jan 23 22:41:09.097: INFO: Waiting for pod client-containers-9b2368b4-c25e-4f8f-aeae-447ca08bd465 to disappear
Jan 23 22:41:09.114: INFO: Pod client-containers-9b2368b4-c25e-4f8f-aeae-447ca08bd465 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:41:09.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5488" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":280,"completed":98,"skipped":1517,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:41:09.150: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service nodeport-service with the type=NodePort in namespace services-6486
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-6486
STEP: creating replication controller externalsvc in namespace services-6486
I0123 22:41:09.474309      21 runners.go:189] Created replication controller with name: externalsvc, namespace: services-6486, replica count: 2
I0123 22:41:12.534244      21 runners.go:189] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Jan 23 22:41:12.582: INFO: Creating new exec pod
Jan 23 22:41:14.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 exec --namespace=services-6486 execpodwqnzw -- /bin/sh -x -c nslookup nodeport-service'
Jan 23 22:41:15.160: INFO: stderr: "+ nslookup nodeport-service\n"
Jan 23 22:41:15.160: INFO: stdout: "Server:\t\t100.64.0.10\nAddress:\t100.64.0.10#53\n\nnodeport-service.services-6486.svc.cluster.local\tcanonical name = externalsvc.services-6486.svc.cluster.local.\nName:\texternalsvc.services-6486.svc.cluster.local\nAddress: 100.71.38.180\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6486, will wait for the garbage collector to delete the pods
Jan 23 22:41:15.226: INFO: Deleting ReplicationController externalsvc took: 10.333767ms
Jan 23 22:41:15.727: INFO: Terminating ReplicationController externalsvc pods took: 500.775039ms
Jan 23 22:41:28.782: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:41:28.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6486" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:18.295 seconds]
[sig-network] Services
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":280,"completed":99,"skipped":1543,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:41:28.859: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Jan 23 22:41:28.995: INFO: Created pod &Pod{ObjectMeta:{dns-6708  dns-6708 /api/v1/namespaces/dns-6708/pods/dns-6708 8243f548-2d80-495e-bbd5-ff074fb0fd86 413672 0 2020-01-23 22:41:28 +0000 UTC <nil> <nil> map[] map[] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lrmmr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lrmmr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lrmmr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
STEP: Verifying customized DNS suffix list is configured on pod...
Jan 23 22:41:31.022: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-6708 PodName:dns-6708 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 23 22:41:31.022: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Verifying customized DNS server is configured on pod...
Jan 23 22:41:31.274: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-6708 PodName:dns-6708 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 23 22:41:31.274: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
Jan 23 22:41:31.475: INFO: Deleting pod dns-6708...
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:41:31.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6708" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":280,"completed":100,"skipped":1561,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:41:31.517: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:41:31.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7624" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":280,"completed":101,"skipped":1574,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:41:31.603: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap configmap-3416/configmap-test-d574d122-fb0d-4961-8ce2-60b22327084c
STEP: Creating a pod to test consume configMaps
Jan 23 22:41:31.680: INFO: Waiting up to 5m0s for pod "pod-configmaps-cc1f8771-1ba2-4dd7-aec6-8ca6ef08e8cc" in namespace "configmap-3416" to be "success or failure"
Jan 23 22:41:31.699: INFO: Pod "pod-configmaps-cc1f8771-1ba2-4dd7-aec6-8ca6ef08e8cc": Phase="Pending", Reason="", readiness=false. Elapsed: 19.048578ms
Jan 23 22:41:33.722: INFO: Pod "pod-configmaps-cc1f8771-1ba2-4dd7-aec6-8ca6ef08e8cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041464054s
Jan 23 22:41:35.726: INFO: Pod "pod-configmaps-cc1f8771-1ba2-4dd7-aec6-8ca6ef08e8cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046269883s
STEP: Saw pod success
Jan 23 22:41:35.727: INFO: Pod "pod-configmaps-cc1f8771-1ba2-4dd7-aec6-8ca6ef08e8cc" satisfied condition "success or failure"
Jan 23 22:41:35.731: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f pod pod-configmaps-cc1f8771-1ba2-4dd7-aec6-8ca6ef08e8cc container env-test: <nil>
STEP: delete the pod
Jan 23 22:41:35.767: INFO: Waiting for pod pod-configmaps-cc1f8771-1ba2-4dd7-aec6-8ca6ef08e8cc to disappear
Jan 23 22:41:35.774: INFO: Pod pod-configmaps-cc1f8771-1ba2-4dd7-aec6-8ca6ef08e8cc no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:41:35.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3416" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":280,"completed":102,"skipped":1599,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:41:35.800: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-f5d69843-25c3-4adc-82a1-c65491314c80
STEP: Creating a pod to test consume secrets
Jan 23 22:41:35.867: INFO: Waiting up to 5m0s for pod "pod-secrets-7b0453ee-045d-4bc3-9870-cadfb378faec" in namespace "secrets-4444" to be "success or failure"
Jan 23 22:41:35.875: INFO: Pod "pod-secrets-7b0453ee-045d-4bc3-9870-cadfb378faec": Phase="Pending", Reason="", readiness=false. Elapsed: 7.346491ms
Jan 23 22:41:37.881: INFO: Pod "pod-secrets-7b0453ee-045d-4bc3-9870-cadfb378faec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01348384s
STEP: Saw pod success
Jan 23 22:41:37.881: INFO: Pod "pod-secrets-7b0453ee-045d-4bc3-9870-cadfb378faec" satisfied condition "success or failure"
Jan 23 22:41:37.887: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod pod-secrets-7b0453ee-045d-4bc3-9870-cadfb378faec container secret-volume-test: <nil>
STEP: delete the pod
Jan 23 22:41:37.915: INFO: Waiting for pod pod-secrets-7b0453ee-045d-4bc3-9870-cadfb378faec to disappear
Jan 23 22:41:37.925: INFO: Pod pod-secrets-7b0453ee-045d-4bc3-9870-cadfb378faec no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:41:37.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4444" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":103,"skipped":1677,"failed":0}
SSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:41:37.946: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:172
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating server pod server in namespace prestop-650
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-650
STEP: Deleting pre-stop pod
Jan 23 22:41:49.496: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:41:49.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-650" for this suite.

• [SLOW TEST:11.614 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":280,"completed":104,"skipped":1681,"failed":0}
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:41:49.567: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-c3c8e1d5-cfa3-4b4e-b847-7f9082d87f53
STEP: Creating a pod to test consume secrets
Jan 23 22:41:49.801: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-444a6ec5-bf00-4777-89d8-b01684539b09" in namespace "projected-832" to be "success or failure"
Jan 23 22:41:49.822: INFO: Pod "pod-projected-secrets-444a6ec5-bf00-4777-89d8-b01684539b09": Phase="Pending", Reason="", readiness=false. Elapsed: 20.38974ms
Jan 23 22:41:53.365: INFO: Pod "pod-projected-secrets-444a6ec5-bf00-4777-89d8-b01684539b09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026349412s
Jan 23 22:41:55.372: INFO: Pod "pod-projected-secrets-444a6ec5-bf00-4777-89d8-b01684539b09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033357426s
STEP: Saw pod success
Jan 23 22:41:55.372: INFO: Pod "pod-projected-secrets-444a6ec5-bf00-4777-89d8-b01684539b09" satisfied condition "success or failure"
Jan 23 22:41:55.387: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f pod pod-projected-secrets-444a6ec5-bf00-4777-89d8-b01684539b09 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 23 22:41:55.459: INFO: Waiting for pod pod-projected-secrets-444a6ec5-bf00-4777-89d8-b01684539b09 to disappear
Jan 23 22:41:55.471: INFO: Pod pod-projected-secrets-444a6ec5-bf00-4777-89d8-b01684539b09 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:41:55.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-832" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":105,"skipped":1685,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:41:55.512: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 23 22:41:55.630: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fd93e40e-78f8-4e72-a841-ae0e27c7eb84" in namespace "projected-6029" to be "success or failure"
Jan 23 22:41:55.668: INFO: Pod "downwardapi-volume-fd93e40e-78f8-4e72-a841-ae0e27c7eb84": Phase="Pending", Reason="", readiness=false. Elapsed: 37.484465ms
Jan 23 22:41:57.674: INFO: Pod "downwardapi-volume-fd93e40e-78f8-4e72-a841-ae0e27c7eb84": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.043441785s
STEP: Saw pod success
Jan 23 22:41:57.674: INFO: Pod "downwardapi-volume-fd93e40e-78f8-4e72-a841-ae0e27c7eb84" satisfied condition "success or failure"
Jan 23 22:41:57.684: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f pod downwardapi-volume-fd93e40e-78f8-4e72-a841-ae0e27c7eb84 container client-container: <nil>
STEP: delete the pod
Jan 23 22:41:57.723: INFO: Waiting for pod downwardapi-volume-fd93e40e-78f8-4e72-a841-ae0e27c7eb84 to disappear
Jan 23 22:41:57.733: INFO: Pod downwardapi-volume-fd93e40e-78f8-4e72-a841-ae0e27c7eb84 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:41:57.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6029" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":106,"skipped":1701,"failed":0}
S
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:41:57.756: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Jan 23 22:41:57.830: INFO: Waiting up to 5m0s for pod "downward-api-46f1da0f-2ff5-4bc1-b346-21c21c79aea6" in namespace "downward-api-8768" to be "success or failure"
Jan 23 22:41:57.843: INFO: Pod "downward-api-46f1da0f-2ff5-4bc1-b346-21c21c79aea6": Phase="Pending", Reason="", readiness=false. Elapsed: 12.91788ms
Jan 23 22:41:59.851: INFO: Pod "downward-api-46f1da0f-2ff5-4bc1-b346-21c21c79aea6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021099881s
STEP: Saw pod success
Jan 23 22:41:59.851: INFO: Pod "downward-api-46f1da0f-2ff5-4bc1-b346-21c21c79aea6" satisfied condition "success or failure"
Jan 23 22:41:59.858: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod downward-api-46f1da0f-2ff5-4bc1-b346-21c21c79aea6 container dapi-container: <nil>
STEP: delete the pod
Jan 23 22:41:59.911: INFO: Waiting for pod downward-api-46f1da0f-2ff5-4bc1-b346-21c21c79aea6 to disappear
Jan 23 22:41:59.921: INFO: Pod downward-api-46f1da0f-2ff5-4bc1-b346-21c21c79aea6 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:41:59.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8768" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":280,"completed":107,"skipped":1702,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:41:59.948: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 23 22:42:00.059: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jan 23 22:42:05.075: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 23 22:42:05.076: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jan 23 22:42:07.082: INFO: Creating deployment "test-rollover-deployment"
Jan 23 22:42:07.102: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jan 23 22:42:09.111: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jan 23 22:42:09.124: INFO: Ensure that both replica sets have 1 created replica
Jan 23 22:42:09.133: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jan 23 22:42:09.145: INFO: Updating deployment test-rollover-deployment
Jan 23 22:42:09.145: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jan 23 22:42:11.159: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jan 23 22:42:11.170: INFO: Make sure deployment "test-rollover-deployment" is complete
Jan 23 22:42:11.179: INFO: all replica sets need to contain the pod-template-hash label
Jan 23 22:42:11.180: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715416127, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715416127, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715416131, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715416127, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 23 22:42:13.194: INFO: all replica sets need to contain the pod-template-hash label
Jan 23 22:42:13.195: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715416127, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715416127, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715416131, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715416127, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 23 22:42:15.199: INFO: all replica sets need to contain the pod-template-hash label
Jan 23 22:42:15.207: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715416127, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715416127, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715416131, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715416127, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 23 22:42:17.191: INFO: all replica sets need to contain the pod-template-hash label
Jan 23 22:42:17.191: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715416127, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715416127, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715416131, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715416127, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 23 22:42:19.191: INFO: all replica sets need to contain the pod-template-hash label
Jan 23 22:42:19.191: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715416127, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715416127, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715416131, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715416127, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 23 22:42:21.191: INFO: 
Jan 23 22:42:21.191: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Jan 23 22:42:21.206: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-9478 /apis/apps/v1/namespaces/deployment-9478/deployments/test-rollover-deployment faf0b1b0-c97b-4148-90e6-8398f0333f93 414266 2 2020-01-23 22:42:07 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004d98f48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-01-23 22:42:07 +0000 UTC,LastTransitionTime:2020-01-23 22:42:07 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-574d6dfbff" has successfully progressed.,LastUpdateTime:2020-01-23 22:42:21 +0000 UTC,LastTransitionTime:2020-01-23 22:42:07 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 23 22:42:21.213: INFO: New ReplicaSet "test-rollover-deployment-574d6dfbff" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-574d6dfbff  deployment-9478 /apis/apps/v1/namespaces/deployment-9478/replicasets/test-rollover-deployment-574d6dfbff dde7c03b-fd30-448f-be4e-6f5920cd07f4 414254 2 2020-01-23 22:42:09 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment faf0b1b0-c97b-4148-90e6-8398f0333f93 0xc00236d817 0xc00236d818}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 574d6dfbff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00236d888 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 23 22:42:21.214: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jan 23 22:42:21.214: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-9478 /apis/apps/v1/namespaces/deployment-9478/replicasets/test-rollover-controller b504c7e1-7761-4b71-98a8-5e012e35c3e3 414263 2 2020-01-23 22:41:59 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment faf0b1b0-c97b-4148-90e6-8398f0333f93 0xc00236d747 0xc00236d748}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00236d7a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 23 22:42:21.215: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-9478 /apis/apps/v1/namespaces/deployment-9478/replicasets/test-rollover-deployment-f6c94f66c b32e6b5c-791f-4cd0-b1a3-8036daa427e2 414184 2 2020-01-23 22:42:07 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment faf0b1b0-c97b-4148-90e6-8398f0333f93 0xc00236d8f0 0xc00236d8f1}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00236d968 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 23 22:42:21.226: INFO: Pod "test-rollover-deployment-574d6dfbff-gxkd2" is available:
&Pod{ObjectMeta:{test-rollover-deployment-574d6dfbff-gxkd2 test-rollover-deployment-574d6dfbff- deployment-9478 /api/v1/namespaces/deployment-9478/pods/test-rollover-deployment-574d6dfbff-gxkd2 2b59be69-fd6b-4291-9f6b-5168d7c75a54 414201 0 2020-01-23 22:42:09 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[cni.projectcalico.org/podIP:100.120.200.20/32] [{apps/v1 ReplicaSet test-rollover-deployment-574d6dfbff dde7c03b-fd30-448f-be4e-6f5920cd07f4 0xc00236df07 0xc00236df08}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-67vrj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-67vrj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-67vrj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:management-cluster-1-17-2-md-0-5f64bb5777-fsd6f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:42:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:42:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:42:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:42:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.219.138,PodIP:100.120.200.20,StartTime:2020-01-23 22:42:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-23 22:42:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:containerd://14e053ffc8ac67429e929b761170d4377485f84364758517385df4385f0889dd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.120.200.20,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:42:21.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9478" for this suite.

• [SLOW TEST:21.298 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":280,"completed":108,"skipped":1715,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:42:21.254: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:42:21.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4091" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":280,"completed":109,"skipped":1750,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:42:21.429: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:42:34.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7888" for this suite.

• [SLOW TEST:11.158 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":280,"completed":110,"skipped":1768,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:42:34.137: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl label
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1382
STEP: creating the pod
Jan 23 22:42:34.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 create -f - --namespace=kubectl-9418'
Jan 23 22:42:34.672: INFO: stderr: ""
Jan 23 22:42:34.672: INFO: stdout: "pod/pause created\n"
Jan 23 22:42:34.672: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jan 23 22:42:34.673: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-9418" to be "running and ready"
Jan 23 22:42:34.680: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 7.338578ms
Jan 23 22:42:36.687: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.01408205s
Jan 23 22:42:36.687: INFO: Pod "pause" satisfied condition "running and ready"
Jan 23 22:42:36.687: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: adding the label testing-label with value testing-label-value to a pod
Jan 23 22:42:36.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 label pods pause testing-label=testing-label-value --namespace=kubectl-9418'
Jan 23 22:42:36.841: INFO: stderr: ""
Jan 23 22:42:36.841: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jan 23 22:42:36.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get pod pause -L testing-label --namespace=kubectl-9418'
Jan 23 22:42:37.017: INFO: stderr: ""
Jan 23 22:42:37.017: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jan 23 22:42:37.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 label pods pause testing-label- --namespace=kubectl-9418'
Jan 23 22:42:37.129: INFO: stderr: ""
Jan 23 22:42:37.129: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jan 23 22:42:37.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get pod pause -L testing-label --namespace=kubectl-9418'
Jan 23 22:42:37.222: INFO: stderr: ""
Jan 23 22:42:37.222: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1389
STEP: using delete to clean up resources
Jan 23 22:42:37.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 delete --grace-period=0 --force -f - --namespace=kubectl-9418'
Jan 23 22:42:37.341: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 23 22:42:37.341: INFO: stdout: "pod \"pause\" force deleted\n"
Jan 23 22:42:37.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get rc,svc -l name=pause --no-headers --namespace=kubectl-9418'
Jan 23 22:42:37.447: INFO: stderr: "No resources found in kubectl-9418 namespace.\n"
Jan 23 22:42:37.447: INFO: stdout: ""
Jan 23 22:42:37.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get pods -l name=pause --namespace=kubectl-9418 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 23 22:42:37.541: INFO: stderr: ""
Jan 23 22:42:37.541: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:42:37.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9418" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":280,"completed":111,"skipped":1774,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:42:37.560: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jan 23 22:42:37.645: INFO: Waiting up to 5m0s for pod "pod-d089fdd8-4dc0-4276-be09-30bd9d4c10e5" in namespace "emptydir-8411" to be "success or failure"
Jan 23 22:42:37.664: INFO: Pod "pod-d089fdd8-4dc0-4276-be09-30bd9d4c10e5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.702926ms
Jan 23 22:42:39.669: INFO: Pod "pod-d089fdd8-4dc0-4276-be09-30bd9d4c10e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02398175s
STEP: Saw pod success
Jan 23 22:42:39.669: INFO: Pod "pod-d089fdd8-4dc0-4276-be09-30bd9d4c10e5" satisfied condition "success or failure"
Jan 23 22:42:39.673: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod pod-d089fdd8-4dc0-4276-be09-30bd9d4c10e5 container test-container: <nil>
STEP: delete the pod
Jan 23 22:42:39.707: INFO: Waiting for pod pod-d089fdd8-4dc0-4276-be09-30bd9d4c10e5 to disappear
Jan 23 22:42:39.713: INFO: Pod pod-d089fdd8-4dc0-4276-be09-30bd9d4c10e5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:42:39.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8411" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":112,"skipped":1786,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:42:39.737: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Starting the proxy
Jan 23 22:42:39.784: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-837620349 proxy --unix-socket=/tmp/kubectl-proxy-unix846910692/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:42:39.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9463" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":280,"completed":113,"skipped":1802,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:42:39.892: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:43:43.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7776" for this suite.

• [SLOW TEST:60.154 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":280,"completed":114,"skipped":1822,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:43:43.229: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 23 22:43:43.323: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jan 23 22:43:43.341: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 23 22:43:48.347: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 23 22:43:48.349: INFO: Creating deployment "test-rolling-update-deployment"
Jan 23 22:43:48.359: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jan 23 22:43:48.373: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jan 23 22:43:50.384: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jan 23 22:43:50.390: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715416229, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715416229, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715416229, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715416229, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67cf4f6444\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 23 22:43:52.403: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Jan 23 22:43:52.426: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-7542 /apis/apps/v1/namespaces/deployment-7542/deployments/test-rolling-update-deployment 6d9e85e1-e5d8-43c4-96b7-3d09c44759d9 414890 1 2020-01-23 22:43:49 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001053488 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-01-23 22:43:49 +0000 UTC,LastTransitionTime:2020-01-23 22:43:49 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-67cf4f6444" has successfully progressed.,LastUpdateTime:2020-01-23 22:43:51 +0000 UTC,LastTransitionTime:2020-01-23 22:43:49 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 23 22:43:52.432: INFO: New ReplicaSet "test-rolling-update-deployment-67cf4f6444" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-67cf4f6444  deployment-7542 /apis/apps/v1/namespaces/deployment-7542/replicasets/test-rolling-update-deployment-67cf4f6444 d44c8cd0-7352-4029-b777-d99bf844326b 414879 1 2020-01-23 22:43:49 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 6d9e85e1-e5d8-43c4-96b7-3d09c44759d9 0xc001053df7 0xc001053df8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67cf4f6444,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001053f28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 23 22:43:52.432: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jan 23 22:43:52.433: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-7542 /apis/apps/v1/namespaces/deployment-7542/replicasets/test-rolling-update-controller 3c3a6828-6892-44e7-ae6e-bf5d809abb64 414889 2 2020-01-23 22:43:42 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 6d9e85e1-e5d8-43c4-96b7-3d09c44759d9 0xc001053c37 0xc001053c38}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc001053d28 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 23 22:43:52.444: INFO: Pod "test-rolling-update-deployment-67cf4f6444-wdd9v" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-67cf4f6444-wdd9v test-rolling-update-deployment-67cf4f6444- deployment-7542 /api/v1/namespaces/deployment-7542/pods/test-rolling-update-deployment-67cf4f6444-wdd9v a6cac3e3-5277-4f0a-9cf8-42aa3af43ffd 414878 0 2020-01-23 22:43:49 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[cni.projectcalico.org/podIP:100.117.222.203/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-67cf4f6444 d44c8cd0-7352-4029-b777-d99bf844326b 0xc002aed577 0xc002aed578}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5tx6h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5tx6h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5tx6h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:management-cluster-1-17-2-md-0-5f64bb5777-dlwcz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:43:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:43:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:43:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 22:43:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.220.23,PodIP:100.117.222.203,StartTime:2020-01-23 22:43:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-23 22:43:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:containerd://ff0627a4a2fd236c6b619dd6c9f2ad6a0f54f283bdb46708ae5f69e44a41d87c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.117.222.203,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:43:52.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7542" for this suite.

• [SLOW TEST:9.242 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":280,"completed":115,"skipped":1863,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:43:52.475: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:44:10.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1899" for this suite.

• [SLOW TEST:16.747 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":280,"completed":116,"skipped":1865,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:44:10.851: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl replace
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1897
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jan 23 22:44:10.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-1740'
Jan 23 22:44:11.113: INFO: stderr: ""
Jan 23 22:44:11.113: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Jan 23 22:44:16.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get pod e2e-test-httpd-pod --namespace=kubectl-1740 -o json'
Jan 23 22:44:16.278: INFO: stderr: ""
Jan 23 22:44:16.278: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"100.117.222.208/32\"\n        },\n        \"creationTimestamp\": \"2020-01-23T22:44:10Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-1740\",\n        \"resourceVersion\": \"415085\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-1740/pods/e2e-test-httpd-pod\",\n        \"uid\": \"375cf228-c2f6-4909-98be-486d5caa8484\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-gmxhr\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"management-cluster-1-17-2-md-0-5f64bb5777-dlwcz\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-gmxhr\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-gmxhr\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-01-23T22:44:10Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-01-23T22:44:12Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-01-23T22:44:12Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-01-23T22:44:10Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://2dd02144584de6f1003222848a365379de57b491a91898954160692a1046d71b\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-01-23T22:44:12Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.78.220.23\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.117.222.208\",\n        \"podIPs\": [\n            {\n                \"ip\": \"100.117.222.208\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-01-23T22:44:10Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jan 23 22:44:16.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 replace -f - --namespace=kubectl-1740'
Jan 23 22:44:16.763: INFO: stderr: ""
Jan 23 22:44:16.764: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1902
Jan 23 22:44:16.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 delete pods e2e-test-httpd-pod --namespace=kubectl-1740'
Jan 23 22:44:22.642: INFO: stderr: ""
Jan 23 22:44:22.642: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:44:22.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1740" for this suite.

• [SLOW TEST:11.819 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1893
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":280,"completed":117,"skipped":1866,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:44:22.671: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-ztsjp in namespace proxy-5749
I0123 22:44:22.814005      21 runners.go:189] Created replication controller with name: proxy-service-ztsjp, namespace: proxy-5749, replica count: 1
I0123 22:44:23.870379      21 runners.go:189] proxy-service-ztsjp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0123 22:44:24.870835      21 runners.go:189] proxy-service-ztsjp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0123 22:44:25.871244      21 runners.go:189] proxy-service-ztsjp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0123 22:44:26.871771      21 runners.go:189] proxy-service-ztsjp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0123 22:44:27.872099      21 runners.go:189] proxy-service-ztsjp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0123 22:44:28.872535      21 runners.go:189] proxy-service-ztsjp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0123 22:44:29.873639      21 runners.go:189] proxy-service-ztsjp Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 23 22:44:29.880: INFO: setup took 7.142078121s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jan 23 22:44:29.927: INFO: (0) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:160/proxy/: foo (200; 41.204347ms)
Jan 23 22:44:29.929: INFO: (0) /api/v1/namespaces/proxy-5749/services/http:proxy-service-ztsjp:portname2/proxy/: bar (200; 46.570197ms)
Jan 23 22:44:29.930: INFO: (0) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:1080/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:1080/proxy/rewriteme">... (200; 46.660596ms)
Jan 23 22:44:29.931: INFO: (0) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:160/proxy/: foo (200; 47.019361ms)
Jan 23 22:44:29.931: INFO: (0) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:1080/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:1080/proxy/rewriteme">test<... (200; 46.361407ms)
Jan 23 22:44:29.931: INFO: (0) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:162/proxy/: bar (200; 46.005268ms)
Jan 23 22:44:29.933: INFO: (0) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:162/proxy/: bar (200; 50.781863ms)
Jan 23 22:44:29.934: INFO: (0) /api/v1/namespaces/proxy-5749/services/proxy-service-ztsjp:portname1/proxy/: foo (200; 52.280829ms)
Jan 23 22:44:29.946: INFO: (0) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf/proxy/rewriteme">test</a> (200; 64.769782ms)
Jan 23 22:44:29.954: INFO: (0) /api/v1/namespaces/proxy-5749/services/proxy-service-ztsjp:portname2/proxy/: bar (200; 70.201977ms)
Jan 23 22:44:29.955: INFO: (0) /api/v1/namespaces/proxy-5749/services/http:proxy-service-ztsjp:portname1/proxy/: foo (200; 70.665587ms)
Jan 23 22:44:29.957: INFO: (0) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:460/proxy/: tls baz (200; 73.029568ms)
Jan 23 22:44:29.959: INFO: (0) /api/v1/namespaces/proxy-5749/services/https:proxy-service-ztsjp:tlsportname1/proxy/: tls baz (200; 76.295445ms)
Jan 23 22:44:29.959: INFO: (0) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:443/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:443/proxy/tlsrewritem... (200; 77.071506ms)
Jan 23 22:44:29.960: INFO: (0) /api/v1/namespaces/proxy-5749/services/https:proxy-service-ztsjp:tlsportname2/proxy/: tls qux (200; 74.592371ms)
Jan 23 22:44:29.960: INFO: (0) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:462/proxy/: tls qux (200; 77.056853ms)
Jan 23 22:44:29.976: INFO: (1) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:162/proxy/: bar (200; 14.779421ms)
Jan 23 22:44:29.976: INFO: (1) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:1080/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:1080/proxy/rewriteme">test<... (200; 14.648095ms)
Jan 23 22:44:29.978: INFO: (1) /api/v1/namespaces/proxy-5749/services/proxy-service-ztsjp:portname2/proxy/: bar (200; 16.77396ms)
Jan 23 22:44:29.978: INFO: (1) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:160/proxy/: foo (200; 17.681108ms)
Jan 23 22:44:29.978: INFO: (1) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:162/proxy/: bar (200; 16.468498ms)
Jan 23 22:44:29.982: INFO: (1) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf/proxy/rewriteme">test</a> (200; 18.735635ms)
Jan 23 22:44:29.982: INFO: (1) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:160/proxy/: foo (200; 17.601296ms)
Jan 23 22:44:29.983: INFO: (1) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:1080/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:1080/proxy/rewriteme">... (200; 19.587189ms)
Jan 23 22:44:29.983: INFO: (1) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:443/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:443/proxy/tlsrewritem... (200; 19.44714ms)
Jan 23 22:44:29.984: INFO: (1) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:462/proxy/: tls qux (200; 21.774897ms)
Jan 23 22:44:29.984: INFO: (1) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:460/proxy/: tls baz (200; 22.056601ms)
Jan 23 22:44:29.988: INFO: (1) /api/v1/namespaces/proxy-5749/services/https:proxy-service-ztsjp:tlsportname1/proxy/: tls baz (200; 24.252851ms)
Jan 23 22:44:29.989: INFO: (1) /api/v1/namespaces/proxy-5749/services/https:proxy-service-ztsjp:tlsportname2/proxy/: tls qux (200; 24.429297ms)
Jan 23 22:44:29.990: INFO: (1) /api/v1/namespaces/proxy-5749/services/http:proxy-service-ztsjp:portname1/proxy/: foo (200; 25.598769ms)
Jan 23 22:44:29.990: INFO: (1) /api/v1/namespaces/proxy-5749/services/http:proxy-service-ztsjp:portname2/proxy/: bar (200; 26.338191ms)
Jan 23 22:44:29.990: INFO: (1) /api/v1/namespaces/proxy-5749/services/proxy-service-ztsjp:portname1/proxy/: foo (200; 27.682979ms)
Jan 23 22:44:30.002: INFO: (2) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:160/proxy/: foo (200; 11.184181ms)
Jan 23 22:44:30.002: INFO: (2) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:1080/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:1080/proxy/rewriteme">test<... (200; 10.099423ms)
Jan 23 22:44:30.013: INFO: (2) /api/v1/namespaces/proxy-5749/services/https:proxy-service-ztsjp:tlsportname2/proxy/: tls qux (200; 21.947121ms)
Jan 23 22:44:30.013: INFO: (2) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:162/proxy/: bar (200; 20.775182ms)
Jan 23 22:44:30.014: INFO: (2) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:162/proxy/: bar (200; 19.779232ms)
Jan 23 22:44:30.014: INFO: (2) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:443/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:443/proxy/tlsrewritem... (200; 19.989588ms)
Jan 23 22:44:30.014: INFO: (2) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:160/proxy/: foo (200; 22.824734ms)
Jan 23 22:44:30.016: INFO: (2) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:1080/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:1080/proxy/rewriteme">... (200; 22.338181ms)
Jan 23 22:44:30.016: INFO: (2) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf/proxy/rewriteme">test</a> (200; 22.0618ms)
Jan 23 22:44:30.016: INFO: (2) /api/v1/namespaces/proxy-5749/services/proxy-service-ztsjp:portname2/proxy/: bar (200; 24.133386ms)
Jan 23 22:44:30.017: INFO: (2) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:462/proxy/: tls qux (200; 22.177983ms)
Jan 23 22:44:30.017: INFO: (2) /api/v1/namespaces/proxy-5749/services/http:proxy-service-ztsjp:portname1/proxy/: foo (200; 26.234362ms)
Jan 23 22:44:30.018: INFO: (2) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:460/proxy/: tls baz (200; 25.165442ms)
Jan 23 22:44:30.019: INFO: (2) /api/v1/namespaces/proxy-5749/services/proxy-service-ztsjp:portname1/proxy/: foo (200; 25.461709ms)
Jan 23 22:44:30.019: INFO: (2) /api/v1/namespaces/proxy-5749/services/http:proxy-service-ztsjp:portname2/proxy/: bar (200; 24.407741ms)
Jan 23 22:44:30.019: INFO: (2) /api/v1/namespaces/proxy-5749/services/https:proxy-service-ztsjp:tlsportname1/proxy/: tls baz (200; 24.365926ms)
Jan 23 22:44:30.031: INFO: (3) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:1080/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:1080/proxy/rewriteme">... (200; 11.096542ms)
Jan 23 22:44:30.038: INFO: (3) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:162/proxy/: bar (200; 17.917075ms)
Jan 23 22:44:30.039: INFO: (3) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:162/proxy/: bar (200; 16.909763ms)
Jan 23 22:44:30.039: INFO: (3) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:160/proxy/: foo (200; 17.885656ms)
Jan 23 22:44:30.040: INFO: (3) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:443/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:443/proxy/tlsrewritem... (200; 19.762942ms)
Jan 23 22:44:30.040: INFO: (3) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:460/proxy/: tls baz (200; 17.845182ms)
Jan 23 22:44:30.041: INFO: (3) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:160/proxy/: foo (200; 20.339975ms)
Jan 23 22:44:30.041: INFO: (3) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:1080/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:1080/proxy/rewriteme">test<... (200; 19.566892ms)
Jan 23 22:44:30.041: INFO: (3) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:462/proxy/: tls qux (200; 21.064941ms)
Jan 23 22:44:30.042: INFO: (3) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf/proxy/rewriteme">test</a> (200; 21.672707ms)
Jan 23 22:44:30.047: INFO: (3) /api/v1/namespaces/proxy-5749/services/proxy-service-ztsjp:portname1/proxy/: foo (200; 25.965082ms)
Jan 23 22:44:30.047: INFO: (3) /api/v1/namespaces/proxy-5749/services/https:proxy-service-ztsjp:tlsportname1/proxy/: tls baz (200; 26.671334ms)
Jan 23 22:44:30.048: INFO: (3) /api/v1/namespaces/proxy-5749/services/proxy-service-ztsjp:portname2/proxy/: bar (200; 25.630487ms)
Jan 23 22:44:30.048: INFO: (3) /api/v1/namespaces/proxy-5749/services/https:proxy-service-ztsjp:tlsportname2/proxy/: tls qux (200; 26.628206ms)
Jan 23 22:44:30.048: INFO: (3) /api/v1/namespaces/proxy-5749/services/http:proxy-service-ztsjp:portname2/proxy/: bar (200; 27.298731ms)
Jan 23 22:44:30.049: INFO: (3) /api/v1/namespaces/proxy-5749/services/http:proxy-service-ztsjp:portname1/proxy/: foo (200; 27.403759ms)
Jan 23 22:44:30.068: INFO: (4) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf/proxy/rewriteme">test</a> (200; 16.864464ms)
Jan 23 22:44:30.069: INFO: (4) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:162/proxy/: bar (200; 18.015364ms)
Jan 23 22:44:30.069: INFO: (4) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:160/proxy/: foo (200; 18.558567ms)
Jan 23 22:44:30.071: INFO: (4) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:1080/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:1080/proxy/rewriteme">test<... (200; 20.738171ms)
Jan 23 22:44:30.072: INFO: (4) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:1080/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:1080/proxy/rewriteme">... (200; 21.820959ms)
Jan 23 22:44:30.073: INFO: (4) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:162/proxy/: bar (200; 22.256679ms)
Jan 23 22:44:30.073: INFO: (4) /api/v1/namespaces/proxy-5749/services/http:proxy-service-ztsjp:portname1/proxy/: foo (200; 22.72079ms)
Jan 23 22:44:30.073: INFO: (4) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:460/proxy/: tls baz (200; 22.257963ms)
Jan 23 22:44:30.074: INFO: (4) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:160/proxy/: foo (200; 24.887031ms)
Jan 23 22:44:30.074: INFO: (4) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:462/proxy/: tls qux (200; 24.398025ms)
Jan 23 22:44:30.074: INFO: (4) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:443/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:443/proxy/tlsrewritem... (200; 24.791576ms)
Jan 23 22:44:30.074: INFO: (4) /api/v1/namespaces/proxy-5749/services/http:proxy-service-ztsjp:portname2/proxy/: bar (200; 24.126413ms)
Jan 23 22:44:30.077: INFO: (4) /api/v1/namespaces/proxy-5749/services/proxy-service-ztsjp:portname1/proxy/: foo (200; 27.029206ms)
Jan 23 22:44:30.079: INFO: (4) /api/v1/namespaces/proxy-5749/services/proxy-service-ztsjp:portname2/proxy/: bar (200; 29.130163ms)
Jan 23 22:44:30.079: INFO: (4) /api/v1/namespaces/proxy-5749/services/https:proxy-service-ztsjp:tlsportname1/proxy/: tls baz (200; 29.091744ms)
Jan 23 22:44:30.080: INFO: (4) /api/v1/namespaces/proxy-5749/services/https:proxy-service-ztsjp:tlsportname2/proxy/: tls qux (200; 29.941429ms)
Jan 23 22:44:30.092: INFO: (5) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:443/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:443/proxy/tlsrewritem... (200; 11.529003ms)
Jan 23 22:44:30.092: INFO: (5) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:1080/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:1080/proxy/rewriteme">... (200; 10.826789ms)
Jan 23 22:44:30.096: INFO: (5) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:460/proxy/: tls baz (200; 13.3193ms)
Jan 23 22:44:30.102: INFO: (5) /api/v1/namespaces/proxy-5749/services/http:proxy-service-ztsjp:portname1/proxy/: foo (200; 21.074743ms)
Jan 23 22:44:30.105: INFO: (5) /api/v1/namespaces/proxy-5749/services/proxy-service-ztsjp:portname2/proxy/: bar (200; 23.602534ms)
Jan 23 22:44:30.106: INFO: (5) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:1080/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:1080/proxy/rewriteme">test<... (200; 24.115633ms)
Jan 23 22:44:30.108: INFO: (5) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:162/proxy/: bar (200; 25.586783ms)
Jan 23 22:44:30.108: INFO: (5) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:162/proxy/: bar (200; 22.457475ms)
Jan 23 22:44:30.108: INFO: (5) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf/proxy/rewriteme">test</a> (200; 25.191864ms)
Jan 23 22:44:30.109: INFO: (5) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:160/proxy/: foo (200; 27.745154ms)
Jan 23 22:44:30.109: INFO: (5) /api/v1/namespaces/proxy-5749/services/https:proxy-service-ztsjp:tlsportname1/proxy/: tls baz (200; 28.557215ms)
Jan 23 22:44:30.109: INFO: (5) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:462/proxy/: tls qux (200; 26.497872ms)
Jan 23 22:44:30.109: INFO: (5) /api/v1/namespaces/proxy-5749/services/https:proxy-service-ztsjp:tlsportname2/proxy/: tls qux (200; 28.078473ms)
Jan 23 22:44:30.109: INFO: (5) /api/v1/namespaces/proxy-5749/services/proxy-service-ztsjp:portname1/proxy/: foo (200; 26.291804ms)
Jan 23 22:44:30.110: INFO: (5) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:160/proxy/: foo (200; 28.249124ms)
Jan 23 22:44:30.110: INFO: (5) /api/v1/namespaces/proxy-5749/services/http:proxy-service-ztsjp:portname2/proxy/: bar (200; 29.931836ms)
Jan 23 22:44:30.120: INFO: (6) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:1080/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:1080/proxy/rewriteme">test<... (200; 8.56226ms)
Jan 23 22:44:30.120: INFO: (6) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:160/proxy/: foo (200; 10.023643ms)
Jan 23 22:44:30.121: INFO: (6) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:460/proxy/: tls baz (200; 9.148667ms)
Jan 23 22:44:30.121: INFO: (6) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:162/proxy/: bar (200; 9.318282ms)
Jan 23 22:44:30.126: INFO: (6) /api/v1/namespaces/proxy-5749/services/http:proxy-service-ztsjp:portname2/proxy/: bar (200; 13.048915ms)
Jan 23 22:44:30.128: INFO: (6) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:162/proxy/: bar (200; 15.907825ms)
Jan 23 22:44:30.129: INFO: (6) /api/v1/namespaces/proxy-5749/services/proxy-service-ztsjp:portname2/proxy/: bar (200; 18.703908ms)
Jan 23 22:44:30.130: INFO: (6) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:1080/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:1080/proxy/rewriteme">... (200; 16.808937ms)
Jan 23 22:44:30.132: INFO: (6) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf/proxy/rewriteme">test</a> (200; 19.626607ms)
Jan 23 22:44:30.132: INFO: (6) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:160/proxy/: foo (200; 18.364905ms)
Jan 23 22:44:30.132: INFO: (6) /api/v1/namespaces/proxy-5749/services/proxy-service-ztsjp:portname1/proxy/: foo (200; 20.580405ms)
Jan 23 22:44:30.132: INFO: (6) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:462/proxy/: tls qux (200; 19.848754ms)
Jan 23 22:44:30.132: INFO: (6) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:443/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:443/proxy/tlsrewritem... (200; 19.861082ms)
Jan 23 22:44:30.135: INFO: (6) /api/v1/namespaces/proxy-5749/services/http:proxy-service-ztsjp:portname1/proxy/: foo (200; 22.044877ms)
Jan 23 22:44:30.136: INFO: (6) /api/v1/namespaces/proxy-5749/services/https:proxy-service-ztsjp:tlsportname1/proxy/: tls baz (200; 22.944642ms)
Jan 23 22:44:30.136: INFO: (6) /api/v1/namespaces/proxy-5749/services/https:proxy-service-ztsjp:tlsportname2/proxy/: tls qux (200; 22.804424ms)
Jan 23 22:44:30.147: INFO: (7) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:443/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:443/proxy/tlsrewritem... (200; 10.257507ms)
Jan 23 22:44:30.150: INFO: (7) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:462/proxy/: tls qux (200; 12.751913ms)
Jan 23 22:44:30.150: INFO: (7) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:1080/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:1080/proxy/rewriteme">... (200; 13.849122ms)
Jan 23 22:44:30.152: INFO: (7) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:162/proxy/: bar (200; 15.090469ms)
Jan 23 22:44:30.152: INFO: (7) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:160/proxy/: foo (200; 14.724399ms)
Jan 23 22:44:30.152: INFO: (7) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf/proxy/rewriteme">test</a> (200; 14.161704ms)
Jan 23 22:44:30.155: INFO: (7) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:160/proxy/: foo (200; 17.040444ms)
Jan 23 22:44:30.155: INFO: (7) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:162/proxy/: bar (200; 17.10695ms)
Jan 23 22:44:30.157: INFO: (7) /api/v1/namespaces/proxy-5749/services/proxy-service-ztsjp:portname2/proxy/: bar (200; 20.282172ms)
Jan 23 22:44:30.157: INFO: (7) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:1080/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:1080/proxy/rewriteme">test<... (200; 19.890573ms)
Jan 23 22:44:30.157: INFO: (7) /api/v1/namespaces/proxy-5749/services/https:proxy-service-ztsjp:tlsportname2/proxy/: tls qux (200; 19.99024ms)
Jan 23 22:44:30.157: INFO: (7) /api/v1/namespaces/proxy-5749/services/http:proxy-service-ztsjp:portname1/proxy/: foo (200; 20.553551ms)
Jan 23 22:44:30.157: INFO: (7) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:460/proxy/: tls baz (200; 20.226988ms)
Jan 23 22:44:30.157: INFO: (7) /api/v1/namespaces/proxy-5749/services/https:proxy-service-ztsjp:tlsportname1/proxy/: tls baz (200; 20.864673ms)
Jan 23 22:44:30.161: INFO: (7) /api/v1/namespaces/proxy-5749/services/proxy-service-ztsjp:portname1/proxy/: foo (200; 24.041713ms)
Jan 23 22:44:30.162: INFO: (7) /api/v1/namespaces/proxy-5749/services/http:proxy-service-ztsjp:portname2/proxy/: bar (200; 25.007542ms)
Jan 23 22:44:30.187: INFO: (8) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:160/proxy/: foo (200; 24.003608ms)
Jan 23 22:44:30.187: INFO: (8) /api/v1/namespaces/proxy-5749/services/http:proxy-service-ztsjp:portname1/proxy/: foo (200; 25.154259ms)
Jan 23 22:44:30.187: INFO: (8) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf/proxy/rewriteme">test</a> (200; 24.54962ms)
Jan 23 22:44:30.187: INFO: (8) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:460/proxy/: tls baz (200; 24.958445ms)
Jan 23 22:44:30.187: INFO: (8) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:162/proxy/: bar (200; 25.213773ms)
Jan 23 22:44:30.188: INFO: (8) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:1080/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:1080/proxy/rewriteme">test<... (200; 25.527324ms)
Jan 23 22:44:30.188: INFO: (8) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:160/proxy/: foo (200; 24.904986ms)
Jan 23 22:44:30.188: INFO: (8) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:462/proxy/: tls qux (200; 25.096795ms)
Jan 23 22:44:30.188: INFO: (8) /api/v1/namespaces/proxy-5749/services/https:proxy-service-ztsjp:tlsportname1/proxy/: tls baz (200; 26.261679ms)
Jan 23 22:44:30.188: INFO: (8) /api/v1/namespaces/proxy-5749/services/proxy-service-ztsjp:portname2/proxy/: bar (200; 25.964527ms)
Jan 23 22:44:30.189: INFO: (8) /api/v1/namespaces/proxy-5749/services/http:proxy-service-ztsjp:portname2/proxy/: bar (200; 26.318974ms)
Jan 23 22:44:30.189: INFO: (8) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:162/proxy/: bar (200; 25.993925ms)
Jan 23 22:44:30.189: INFO: (8) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:1080/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:1080/proxy/rewriteme">... (200; 26.756438ms)
Jan 23 22:44:30.189: INFO: (8) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:443/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:443/proxy/tlsrewritem... (200; 26.386984ms)
Jan 23 22:44:30.190: INFO: (8) /api/v1/namespaces/proxy-5749/services/proxy-service-ztsjp:portname1/proxy/: foo (200; 27.862678ms)
Jan 23 22:44:30.190: INFO: (8) /api/v1/namespaces/proxy-5749/services/https:proxy-service-ztsjp:tlsportname2/proxy/: tls qux (200; 27.797017ms)
Jan 23 22:44:30.205: INFO: (9) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:1080/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:1080/proxy/rewriteme">... (200; 14.749213ms)
Jan 23 22:44:30.205: INFO: (9) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:460/proxy/: tls baz (200; 14.44944ms)
Jan 23 22:44:30.213: INFO: (9) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:162/proxy/: bar (200; 21.395556ms)
Jan 23 22:44:30.214: INFO: (9) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:160/proxy/: foo (200; 22.095763ms)
Jan 23 22:44:30.218: INFO: (9) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:1080/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:1080/proxy/rewriteme">test<... (200; 25.958802ms)
Jan 23 22:44:30.225: INFO: (9) /api/v1/namespaces/proxy-5749/services/proxy-service-ztsjp:portname2/proxy/: bar (200; 32.693188ms)
Jan 23 22:44:30.227: INFO: (9) /api/v1/namespaces/proxy-5749/services/proxy-service-ztsjp:portname1/proxy/: foo (200; 36.533834ms)
Jan 23 22:44:30.228: INFO: (9) /api/v1/namespaces/proxy-5749/services/http:proxy-service-ztsjp:portname2/proxy/: bar (200; 36.835747ms)
Jan 23 22:44:30.229: INFO: (9) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:443/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:443/proxy/tlsrewritem... (200; 37.491885ms)
Jan 23 22:44:30.229: INFO: (9) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf/proxy/rewriteme">test</a> (200; 37.893322ms)
Jan 23 22:44:30.229: INFO: (9) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:160/proxy/: foo (200; 37.41487ms)
Jan 23 22:44:30.229: INFO: (9) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:162/proxy/: bar (200; 38.22539ms)
Jan 23 22:44:30.229: INFO: (9) /api/v1/namespaces/proxy-5749/services/https:proxy-service-ztsjp:tlsportname2/proxy/: tls qux (200; 37.974013ms)
Jan 23 22:44:30.229: INFO: (9) /api/v1/namespaces/proxy-5749/services/https:proxy-service-ztsjp:tlsportname1/proxy/: tls baz (200; 38.143767ms)
Jan 23 22:44:30.229: INFO: (9) /api/v1/namespaces/proxy-5749/services/http:proxy-service-ztsjp:portname1/proxy/: foo (200; 38.097788ms)
Jan 23 22:44:30.229: INFO: (9) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:462/proxy/: tls qux (200; 38.286375ms)
Jan 23 22:44:30.245: INFO: (10) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf/proxy/rewriteme">test</a> (200; 13.650058ms)
Jan 23 22:44:30.247: INFO: (10) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:162/proxy/: bar (200; 15.685148ms)
Jan 23 22:44:30.247: INFO: (10) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:1080/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:1080/proxy/rewriteme">... (200; 15.5401ms)
Jan 23 22:44:30.248: INFO: (10) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:460/proxy/: tls baz (200; 16.645581ms)
Jan 23 22:44:30.248: INFO: (10) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:160/proxy/: foo (200; 16.032644ms)
Jan 23 22:44:30.248: INFO: (10) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:162/proxy/: bar (200; 17.51384ms)
Jan 23 22:44:30.248: INFO: (10) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:1080/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:1080/proxy/rewriteme">test<... (200; 17.938666ms)
Jan 23 22:44:30.249: INFO: (10) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:160/proxy/: foo (200; 16.911388ms)
Jan 23 22:44:30.249: INFO: (10) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:443/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:443/proxy/tlsrewritem... (200; 19.483728ms)
Jan 23 22:44:30.250: INFO: (10) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:462/proxy/: tls qux (200; 17.855085ms)
Jan 23 22:44:30.250: INFO: (10) /api/v1/namespaces/proxy-5749/services/proxy-service-ztsjp:portname2/proxy/: bar (200; 20.087345ms)
Jan 23 22:44:30.251: INFO: (10) /api/v1/namespaces/proxy-5749/services/proxy-service-ztsjp:portname1/proxy/: foo (200; 19.840838ms)
Jan 23 22:44:30.255: INFO: (10) /api/v1/namespaces/proxy-5749/services/http:proxy-service-ztsjp:portname2/proxy/: bar (200; 22.675601ms)
Jan 23 22:44:30.256: INFO: (10) /api/v1/namespaces/proxy-5749/services/http:proxy-service-ztsjp:portname1/proxy/: foo (200; 23.598617ms)
Jan 23 22:44:30.256: INFO: (10) /api/v1/namespaces/proxy-5749/services/https:proxy-service-ztsjp:tlsportname2/proxy/: tls qux (200; 23.942243ms)
Jan 23 22:44:30.258: INFO: (10) /api/v1/namespaces/proxy-5749/services/https:proxy-service-ztsjp:tlsportname1/proxy/: tls baz (200; 25.832261ms)
Jan 23 22:44:30.279: INFO: (11) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:160/proxy/: foo (200; 18.733283ms)
Jan 23 22:44:30.281: INFO: (11) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:1080/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:1080/proxy/rewriteme">... (200; 20.504327ms)
Jan 23 22:44:30.281: INFO: (11) /api/v1/namespaces/proxy-5749/services/https:proxy-service-ztsjp:tlsportname1/proxy/: tls baz (200; 21.280079ms)
Jan 23 22:44:30.282: INFO: (11) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:460/proxy/: tls baz (200; 23.223709ms)
Jan 23 22:44:30.283: INFO: (11) /api/v1/namespaces/proxy-5749/services/http:proxy-service-ztsjp:portname1/proxy/: foo (200; 22.427593ms)
Jan 23 22:44:30.283: INFO: (11) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf/proxy/rewriteme">test</a> (200; 23.795399ms)
Jan 23 22:44:30.283: INFO: (11) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:162/proxy/: bar (200; 24.097822ms)
Jan 23 22:44:30.283: INFO: (11) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:1080/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:1080/proxy/rewriteme">test<... (200; 24.532956ms)
Jan 23 22:44:30.284: INFO: (11) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:462/proxy/: tls qux (200; 23.807955ms)
Jan 23 22:44:30.284: INFO: (11) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:443/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:443/proxy/tlsrewritem... (200; 24.132806ms)
Jan 23 22:44:30.290: INFO: (11) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:160/proxy/: foo (200; 30.325245ms)
Jan 23 22:44:30.290: INFO: (11) /api/v1/namespaces/proxy-5749/services/http:proxy-service-ztsjp:portname2/proxy/: bar (200; 30.269139ms)
Jan 23 22:44:30.290: INFO: (11) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:162/proxy/: bar (200; 30.899694ms)
Jan 23 22:44:30.293: INFO: (11) /api/v1/namespaces/proxy-5749/services/https:proxy-service-ztsjp:tlsportname2/proxy/: tls qux (200; 32.591611ms)
Jan 23 22:44:30.294: INFO: (11) /api/v1/namespaces/proxy-5749/services/proxy-service-ztsjp:portname1/proxy/: foo (200; 34.857847ms)
Jan 23 22:44:30.294: INFO: (11) /api/v1/namespaces/proxy-5749/services/proxy-service-ztsjp:portname2/proxy/: bar (200; 35.434651ms)
Jan 23 22:44:30.315: INFO: (12) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:460/proxy/: tls baz (200; 19.902141ms)
Jan 23 22:44:30.316: INFO: (12) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf/proxy/rewriteme">test</a> (200; 19.728291ms)
Jan 23 22:44:30.317: INFO: (12) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:160/proxy/: foo (200; 21.348399ms)
Jan 23 22:44:30.317: INFO: (12) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:462/proxy/: tls qux (200; 22.287ms)
Jan 23 22:44:30.318: INFO: (12) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:160/proxy/: foo (200; 23.328101ms)
Jan 23 22:44:30.319: INFO: (12) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:162/proxy/: bar (200; 24.446967ms)
Jan 23 22:44:30.319: INFO: (12) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:162/proxy/: bar (200; 23.621821ms)
Jan 23 22:44:30.319: INFO: (12) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:1080/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:1080/proxy/rewriteme">test<... (200; 24.029084ms)
Jan 23 22:44:30.320: INFO: (12) /api/v1/namespaces/proxy-5749/services/proxy-service-ztsjp:portname1/proxy/: foo (200; 23.644848ms)
Jan 23 22:44:30.320: INFO: (12) /api/v1/namespaces/proxy-5749/services/https:proxy-service-ztsjp:tlsportname1/proxy/: tls baz (200; 24.972317ms)
Jan 23 22:44:30.320: INFO: (12) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:443/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:443/proxy/tlsrewritem... (200; 25.149591ms)
Jan 23 22:44:30.320: INFO: (12) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:1080/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:1080/proxy/rewriteme">... (200; 24.523714ms)
Jan 23 22:44:30.326: INFO: (12) /api/v1/namespaces/proxy-5749/services/http:proxy-service-ztsjp:portname1/proxy/: foo (200; 30.40681ms)
Jan 23 22:44:30.326: INFO: (12) /api/v1/namespaces/proxy-5749/services/proxy-service-ztsjp:portname2/proxy/: bar (200; 30.76625ms)
Jan 23 22:44:30.326: INFO: (12) /api/v1/namespaces/proxy-5749/services/https:proxy-service-ztsjp:tlsportname2/proxy/: tls qux (200; 30.848036ms)
Jan 23 22:44:30.327: INFO: (12) /api/v1/namespaces/proxy-5749/services/http:proxy-service-ztsjp:portname2/proxy/: bar (200; 31.987262ms)
Jan 23 22:44:30.338: INFO: (13) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf/proxy/rewriteme">test</a> (200; 10.39904ms)
Jan 23 22:44:30.338: INFO: (13) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:162/proxy/: bar (200; 9.742176ms)
Jan 23 22:44:30.343: INFO: (13) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:162/proxy/: bar (200; 15.195733ms)
Jan 23 22:44:30.344: INFO: (13) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:462/proxy/: tls qux (200; 16.079528ms)
Jan 23 22:44:30.347: INFO: (13) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:1080/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:1080/proxy/rewriteme">test<... (200; 18.429694ms)
Jan 23 22:44:30.348: INFO: (13) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:1080/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:1080/proxy/rewriteme">... (200; 20.471893ms)
Jan 23 22:44:30.349: INFO: (13) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:443/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:443/proxy/tlsrewritem... (200; 20.457488ms)
Jan 23 22:44:30.349: INFO: (13) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:460/proxy/: tls baz (200; 22.07705ms)
Jan 23 22:44:30.350: INFO: (13) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:160/proxy/: foo (200; 21.985093ms)
Jan 23 22:44:30.350: INFO: (13) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:160/proxy/: foo (200; 22.009203ms)
Jan 23 22:44:30.350: INFO: (13) /api/v1/namespaces/proxy-5749/services/http:proxy-service-ztsjp:portname2/proxy/: bar (200; 22.126397ms)
Jan 23 22:44:30.350: INFO: (13) /api/v1/namespaces/proxy-5749/services/https:proxy-service-ztsjp:tlsportname1/proxy/: tls baz (200; 23.410883ms)
Jan 23 22:44:30.350: INFO: (13) /api/v1/namespaces/proxy-5749/services/proxy-service-ztsjp:portname2/proxy/: bar (200; 22.171535ms)
Jan 23 22:44:30.352: INFO: (13) /api/v1/namespaces/proxy-5749/services/proxy-service-ztsjp:portname1/proxy/: foo (200; 24.637099ms)
Jan 23 22:44:30.352: INFO: (13) /api/v1/namespaces/proxy-5749/services/http:proxy-service-ztsjp:portname1/proxy/: foo (200; 23.95308ms)
Jan 23 22:44:30.354: INFO: (13) /api/v1/namespaces/proxy-5749/services/https:proxy-service-ztsjp:tlsportname2/proxy/: tls qux (200; 26.190906ms)
Jan 23 22:44:30.365: INFO: (14) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf/proxy/rewriteme">test</a> (200; 9.99588ms)
Jan 23 22:44:30.370: INFO: (14) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:160/proxy/: foo (200; 14.182962ms)
Jan 23 22:44:30.370: INFO: (14) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:443/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:443/proxy/tlsrewritem... (200; 14.86654ms)
Jan 23 22:44:30.423: INFO: (14) /api/v1/namespaces/proxy-5749/services/http:proxy-service-ztsjp:portname1/proxy/: foo (200; 66.531716ms)
Jan 23 22:44:30.484: INFO: (14) /api/v1/namespaces/proxy-5749/services/https:proxy-service-ztsjp:tlsportname1/proxy/: tls baz (200; 128.484018ms)
Jan 23 22:44:30.484: INFO: (14) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:160/proxy/: foo (200; 128.75269ms)
Jan 23 22:44:30.501: INFO: (14) /api/v1/namespaces/proxy-5749/services/proxy-service-ztsjp:portname2/proxy/: bar (200; 146.670778ms)
Jan 23 22:44:30.502: INFO: (14) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:1080/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:1080/proxy/rewriteme">test<... (200; 145.914522ms)
Jan 23 22:44:30.505: INFO: (14) /api/v1/namespaces/proxy-5749/services/https:proxy-service-ztsjp:tlsportname2/proxy/: tls qux (200; 149.987675ms)
Jan 23 22:44:30.506: INFO: (14) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:462/proxy/: tls qux (200; 150.587441ms)
Jan 23 22:44:30.507: INFO: (14) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:460/proxy/: tls baz (200; 150.748475ms)
Jan 23 22:44:30.509: INFO: (14) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:162/proxy/: bar (200; 154.286793ms)
Jan 23 22:44:30.511: INFO: (14) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:1080/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:1080/proxy/rewriteme">... (200; 155.177071ms)
Jan 23 22:44:30.512: INFO: (14) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:162/proxy/: bar (200; 156.130334ms)
Jan 23 22:44:30.514: INFO: (14) /api/v1/namespaces/proxy-5749/services/proxy-service-ztsjp:portname1/proxy/: foo (200; 159.226582ms)
Jan 23 22:44:30.601: INFO: (14) /api/v1/namespaces/proxy-5749/services/http:proxy-service-ztsjp:portname2/proxy/: bar (200; 246.447906ms)
Jan 23 22:44:30.706: INFO: (15) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf/proxy/rewriteme">test</a> (200; 94.7273ms)
Jan 23 22:44:30.709: INFO: (15) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:162/proxy/: bar (200; 97.626037ms)
Jan 23 22:44:30.729: INFO: (15) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:160/proxy/: foo (200; 117.658913ms)
Jan 23 22:44:30.730: INFO: (15) /api/v1/namespaces/proxy-5749/services/proxy-service-ztsjp:portname2/proxy/: bar (200; 118.213864ms)
Jan 23 22:44:30.736: INFO: (15) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:1080/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:1080/proxy/rewriteme">test<... (200; 132.948946ms)
Jan 23 22:44:30.736: INFO: (15) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:443/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:443/proxy/tlsrewritem... (200; 125.567336ms)
Jan 23 22:44:30.738: INFO: (15) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:462/proxy/: tls qux (200; 127.100352ms)
Jan 23 22:44:30.738: INFO: (15) /api/v1/namespaces/proxy-5749/services/https:proxy-service-ztsjp:tlsportname2/proxy/: tls qux (200; 126.41078ms)
Jan 23 22:44:30.743: INFO: (15) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:160/proxy/: foo (200; 131.367148ms)
Jan 23 22:44:30.743: INFO: (15) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:162/proxy/: bar (200; 132.809868ms)
Jan 23 22:44:30.743: INFO: (15) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:460/proxy/: tls baz (200; 131.587707ms)
Jan 23 22:44:30.743: INFO: (15) /api/v1/namespaces/proxy-5749/services/http:proxy-service-ztsjp:portname1/proxy/: foo (200; 131.836435ms)
Jan 23 22:44:30.743: INFO: (15) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:1080/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:1080/proxy/rewriteme">... (200; 131.273529ms)
Jan 23 22:44:30.748: INFO: (15) /api/v1/namespaces/proxy-5749/services/https:proxy-service-ztsjp:tlsportname1/proxy/: tls baz (200; 144.206112ms)
Jan 23 22:44:30.753: INFO: (15) /api/v1/namespaces/proxy-5749/services/http:proxy-service-ztsjp:portname2/proxy/: bar (200; 140.32893ms)
Jan 23 22:44:30.753: INFO: (15) /api/v1/namespaces/proxy-5749/services/proxy-service-ztsjp:portname1/proxy/: foo (200; 141.800975ms)
Jan 23 22:44:30.774: INFO: (16) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:460/proxy/: tls baz (200; 19.974381ms)
Jan 23 22:44:30.779: INFO: (16) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:162/proxy/: bar (200; 23.33229ms)
Jan 23 22:44:30.780: INFO: (16) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:162/proxy/: bar (200; 24.699443ms)
Jan 23 22:44:30.789: INFO: (16) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:160/proxy/: foo (200; 34.462561ms)
Jan 23 22:44:30.789: INFO: (16) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf/proxy/rewriteme">test</a> (200; 33.790869ms)
Jan 23 22:44:30.789: INFO: (16) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:1080/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:1080/proxy/rewriteme">test<... (200; 34.623778ms)
Jan 23 22:44:30.789: INFO: (16) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:160/proxy/: foo (200; 34.772657ms)
Jan 23 22:44:30.791: INFO: (16) /api/v1/namespaces/proxy-5749/services/http:proxy-service-ztsjp:portname1/proxy/: foo (200; 37.223935ms)
Jan 23 22:44:30.793: INFO: (16) /api/v1/namespaces/proxy-5749/services/proxy-service-ztsjp:portname1/proxy/: foo (200; 38.503312ms)
Jan 23 22:44:30.794: INFO: (16) /api/v1/namespaces/proxy-5749/services/proxy-service-ztsjp:portname2/proxy/: bar (200; 39.230246ms)
Jan 23 22:44:30.794: INFO: (16) /api/v1/namespaces/proxy-5749/services/http:proxy-service-ztsjp:portname2/proxy/: bar (200; 38.56233ms)
Jan 23 22:44:30.796: INFO: (16) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:462/proxy/: tls qux (200; 40.039418ms)
Jan 23 22:44:30.796: INFO: (16) /api/v1/namespaces/proxy-5749/services/https:proxy-service-ztsjp:tlsportname2/proxy/: tls qux (200; 41.649752ms)
Jan 23 22:44:30.797: INFO: (16) /api/v1/namespaces/proxy-5749/services/https:proxy-service-ztsjp:tlsportname1/proxy/: tls baz (200; 41.17931ms)
Jan 23 22:44:30.803: INFO: (16) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:1080/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:1080/proxy/rewriteme">... (200; 49.328274ms)
Jan 23 22:44:30.805: INFO: (16) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:443/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:443/proxy/tlsrewritem... (200; 50.254684ms)
Jan 23 22:44:30.843: INFO: (17) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:162/proxy/: bar (200; 33.488035ms)
Jan 23 22:44:30.850: INFO: (17) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:1080/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:1080/proxy/rewriteme">test<... (200; 41.335795ms)
Jan 23 22:44:30.851: INFO: (17) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:443/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:443/proxy/tlsrewritem... (200; 42.348458ms)
Jan 23 22:44:30.851: INFO: (17) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:462/proxy/: tls qux (200; 41.952501ms)
Jan 23 22:44:30.852: INFO: (17) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:460/proxy/: tls baz (200; 46.379407ms)
Jan 23 22:44:30.855: INFO: (17) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf/proxy/rewriteme">test</a> (200; 43.721634ms)
Jan 23 22:44:30.858: INFO: (17) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:160/proxy/: foo (200; 51.425297ms)
Jan 23 22:44:30.858: INFO: (17) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:160/proxy/: foo (200; 50.606535ms)
Jan 23 22:44:30.858: INFO: (17) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:162/proxy/: bar (200; 50.0645ms)
Jan 23 22:44:30.858: INFO: (17) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:1080/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:1080/proxy/rewriteme">... (200; 48.878954ms)
Jan 23 22:44:30.864: INFO: (17) /api/v1/namespaces/proxy-5749/services/proxy-service-ztsjp:portname2/proxy/: bar (200; 55.528268ms)
Jan 23 22:44:30.864: INFO: (17) /api/v1/namespaces/proxy-5749/services/http:proxy-service-ztsjp:portname2/proxy/: bar (200; 52.152213ms)
Jan 23 22:44:30.865: INFO: (17) /api/v1/namespaces/proxy-5749/services/proxy-service-ztsjp:portname1/proxy/: foo (200; 54.115815ms)
Jan 23 22:44:30.865: INFO: (17) /api/v1/namespaces/proxy-5749/services/https:proxy-service-ztsjp:tlsportname2/proxy/: tls qux (200; 58.533424ms)
Jan 23 22:44:30.866: INFO: (17) /api/v1/namespaces/proxy-5749/services/http:proxy-service-ztsjp:portname1/proxy/: foo (200; 59.391903ms)
Jan 23 22:44:30.866: INFO: (17) /api/v1/namespaces/proxy-5749/services/https:proxy-service-ztsjp:tlsportname1/proxy/: tls baz (200; 54.470588ms)
Jan 23 22:44:30.890: INFO: (18) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:462/proxy/: tls qux (200; 21.062049ms)
Jan 23 22:44:30.891: INFO: (18) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:162/proxy/: bar (200; 22.844781ms)
Jan 23 22:44:30.891: INFO: (18) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:460/proxy/: tls baz (200; 23.322195ms)
Jan 23 22:44:30.891: INFO: (18) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:160/proxy/: foo (200; 24.490468ms)
Jan 23 22:44:30.893: INFO: (18) /api/v1/namespaces/proxy-5749/services/http:proxy-service-ztsjp:portname1/proxy/: foo (200; 25.59985ms)
Jan 23 22:44:30.894: INFO: (18) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:443/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:443/proxy/tlsrewritem... (200; 25.285493ms)
Jan 23 22:44:30.894: INFO: (18) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf/proxy/rewriteme">test</a> (200; 24.435548ms)
Jan 23 22:44:30.895: INFO: (18) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:162/proxy/: bar (200; 26.816971ms)
Jan 23 22:44:30.895: INFO: (18) /api/v1/namespaces/proxy-5749/services/http:proxy-service-ztsjp:portname2/proxy/: bar (200; 24.738736ms)
Jan 23 22:44:30.895: INFO: (18) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:160/proxy/: foo (200; 27.575364ms)
Jan 23 22:44:30.895: INFO: (18) /api/v1/namespaces/proxy-5749/services/https:proxy-service-ztsjp:tlsportname1/proxy/: tls baz (200; 28.929232ms)
Jan 23 22:44:30.895: INFO: (18) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:1080/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:1080/proxy/rewriteme">test<... (200; 28.83725ms)
Jan 23 22:44:30.895: INFO: (18) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:1080/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:1080/proxy/rewriteme">... (200; 26.07883ms)
Jan 23 22:44:30.899: INFO: (18) /api/v1/namespaces/proxy-5749/services/proxy-service-ztsjp:portname1/proxy/: foo (200; 30.680447ms)
Jan 23 22:44:30.901: INFO: (18) /api/v1/namespaces/proxy-5749/services/proxy-service-ztsjp:portname2/proxy/: bar (200; 32.881981ms)
Jan 23 22:44:30.902: INFO: (18) /api/v1/namespaces/proxy-5749/services/https:proxy-service-ztsjp:tlsportname2/proxy/: tls qux (200; 34.50353ms)
Jan 23 22:44:30.919: INFO: (19) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:160/proxy/: foo (200; 15.301141ms)
Jan 23 22:44:30.921: INFO: (19) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:460/proxy/: tls baz (200; 18.408568ms)
Jan 23 22:44:30.927: INFO: (19) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:162/proxy/: bar (200; 23.663881ms)
Jan 23 22:44:30.929: INFO: (19) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:160/proxy/: foo (200; 25.745357ms)
Jan 23 22:44:30.930: INFO: (19) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:462/proxy/: tls qux (200; 25.491366ms)
Jan 23 22:44:30.930: INFO: (19) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:1080/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:1080/proxy/rewriteme">... (200; 27.787018ms)
Jan 23 22:44:30.930: INFO: (19) /api/v1/namespaces/proxy-5749/services/http:proxy-service-ztsjp:portname1/proxy/: foo (200; 27.350588ms)
Jan 23 22:44:30.930: INFO: (19) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:1080/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf:1080/proxy/rewriteme">test<... (200; 26.783952ms)
Jan 23 22:44:30.932: INFO: (19) /api/v1/namespaces/proxy-5749/services/proxy-service-ztsjp:portname2/proxy/: bar (200; 28.654512ms)
Jan 23 22:44:30.932: INFO: (19) /api/v1/namespaces/proxy-5749/pods/http:proxy-service-ztsjp-4tqcf:162/proxy/: bar (200; 27.96763ms)
Jan 23 22:44:30.932: INFO: (19) /api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/proxy-service-ztsjp-4tqcf/proxy/rewriteme">test</a> (200; 27.495822ms)
Jan 23 22:44:30.933: INFO: (19) /api/v1/namespaces/proxy-5749/services/https:proxy-service-ztsjp:tlsportname2/proxy/: tls qux (200; 29.881838ms)
Jan 23 22:44:30.933: INFO: (19) /api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:443/proxy/: <a href="/api/v1/namespaces/proxy-5749/pods/https:proxy-service-ztsjp-4tqcf:443/proxy/tlsrewritem... (200; 28.541317ms)
Jan 23 22:44:30.939: INFO: (19) /api/v1/namespaces/proxy-5749/services/http:proxy-service-ztsjp:portname2/proxy/: bar (200; 34.173893ms)
Jan 23 22:44:30.939: INFO: (19) /api/v1/namespaces/proxy-5749/services/proxy-service-ztsjp:portname1/proxy/: foo (200; 34.646124ms)
Jan 23 22:44:30.939: INFO: (19) /api/v1/namespaces/proxy-5749/services/https:proxy-service-ztsjp:tlsportname1/proxy/: tls baz (200; 34.124357ms)
STEP: deleting ReplicationController proxy-service-ztsjp in namespace proxy-5749, will wait for the garbage collector to delete the pods
Jan 23 22:44:31.047: INFO: Deleting ReplicationController proxy-service-ztsjp took: 39.410707ms
Jan 23 22:44:31.655: INFO: Terminating ReplicationController proxy-service-ztsjp pods took: 608.512017ms
[AfterEach] version v1
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:44:36.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5749" for this suite.

• [SLOW TEST:14.169 seconds]
[sig-network] Proxy
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":280,"completed":118,"skipped":1892,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:44:36.848: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-27a979bf-05ef-4406-9370-efc60d28ae37
STEP: Creating a pod to test consume configMaps
Jan 23 22:44:37.011: INFO: Waiting up to 5m0s for pod "pod-configmaps-d5a05315-b610-4331-9304-e2717bd88ce4" in namespace "configmap-7524" to be "success or failure"
Jan 23 22:44:37.040: INFO: Pod "pod-configmaps-d5a05315-b610-4331-9304-e2717bd88ce4": Phase="Pending", Reason="", readiness=false. Elapsed: 28.538255ms
Jan 23 22:44:39.047: INFO: Pod "pod-configmaps-d5a05315-b610-4331-9304-e2717bd88ce4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.035631986s
STEP: Saw pod success
Jan 23 22:44:39.047: INFO: Pod "pod-configmaps-d5a05315-b610-4331-9304-e2717bd88ce4" satisfied condition "success or failure"
Jan 23 22:44:39.053: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod pod-configmaps-d5a05315-b610-4331-9304-e2717bd88ce4 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 23 22:44:39.151: INFO: Waiting for pod pod-configmaps-d5a05315-b610-4331-9304-e2717bd88ce4 to disappear
Jan 23 22:44:39.157: INFO: Pod pod-configmaps-d5a05315-b610-4331-9304-e2717bd88ce4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:44:39.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7524" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":119,"skipped":1895,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:44:39.181: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 23 22:44:39.648: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 23 22:44:43.325: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715416280, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715416280, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715416280, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715416280, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 23 22:44:46.354: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:44:46.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3708" for this suite.
STEP: Destroying namespace "webhook-3708-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.751 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":280,"completed":120,"skipped":1904,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:44:46.609: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Jan 23 22:44:46.725: INFO: Waiting up to 5m0s for pod "downward-api-41cb5405-03d5-4071-90b7-25107eb25099" in namespace "downward-api-3124" to be "success or failure"
Jan 23 22:44:46.737: INFO: Pod "downward-api-41cb5405-03d5-4071-90b7-25107eb25099": Phase="Pending", Reason="", readiness=false. Elapsed: 11.718215ms
Jan 23 22:44:48.742: INFO: Pod "downward-api-41cb5405-03d5-4071-90b7-25107eb25099": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016522292s
STEP: Saw pod success
Jan 23 22:44:48.742: INFO: Pod "downward-api-41cb5405-03d5-4071-90b7-25107eb25099" satisfied condition "success or failure"
Jan 23 22:44:48.745: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod downward-api-41cb5405-03d5-4071-90b7-25107eb25099 container dapi-container: <nil>
STEP: delete the pod
Jan 23 22:44:48.769: INFO: Waiting for pod downward-api-41cb5405-03d5-4071-90b7-25107eb25099 to disappear
Jan 23 22:44:48.773: INFO: Pod downward-api-41cb5405-03d5-4071-90b7-25107eb25099 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:44:48.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3124" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":280,"completed":121,"skipped":1917,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:44:48.793: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-downwardapi-mzl9
STEP: Creating a pod to test atomic-volume-subpath
Jan 23 22:44:48.861: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-mzl9" in namespace "subpath-8497" to be "success or failure"
Jan 23 22:44:48.869: INFO: Pod "pod-subpath-test-downwardapi-mzl9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.896657ms
Jan 23 22:44:50.876: INFO: Pod "pod-subpath-test-downwardapi-mzl9": Phase="Running", Reason="", readiness=true. Elapsed: 2.01513511s
Jan 23 22:44:52.882: INFO: Pod "pod-subpath-test-downwardapi-mzl9": Phase="Running", Reason="", readiness=true. Elapsed: 4.021434009s
Jan 23 22:44:54.890: INFO: Pod "pod-subpath-test-downwardapi-mzl9": Phase="Running", Reason="", readiness=true. Elapsed: 6.029300795s
Jan 23 22:44:56.897: INFO: Pod "pod-subpath-test-downwardapi-mzl9": Phase="Running", Reason="", readiness=true. Elapsed: 8.03584755s
Jan 23 22:44:58.909: INFO: Pod "pod-subpath-test-downwardapi-mzl9": Phase="Running", Reason="", readiness=true. Elapsed: 10.048147603s
Jan 23 22:45:00.927: INFO: Pod "pod-subpath-test-downwardapi-mzl9": Phase="Running", Reason="", readiness=true. Elapsed: 12.066377323s
Jan 23 22:45:02.934: INFO: Pod "pod-subpath-test-downwardapi-mzl9": Phase="Running", Reason="", readiness=true. Elapsed: 14.073406901s
Jan 23 22:45:04.939: INFO: Pod "pod-subpath-test-downwardapi-mzl9": Phase="Running", Reason="", readiness=true. Elapsed: 16.07848449s
Jan 23 22:45:06.957: INFO: Pod "pod-subpath-test-downwardapi-mzl9": Phase="Running", Reason="", readiness=true. Elapsed: 18.096593151s
Jan 23 22:45:08.963: INFO: Pod "pod-subpath-test-downwardapi-mzl9": Phase="Running", Reason="", readiness=true. Elapsed: 20.102268039s
Jan 23 22:45:10.969: INFO: Pod "pod-subpath-test-downwardapi-mzl9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.108094368s
STEP: Saw pod success
Jan 23 22:45:10.969: INFO: Pod "pod-subpath-test-downwardapi-mzl9" satisfied condition "success or failure"
Jan 23 22:45:10.973: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod pod-subpath-test-downwardapi-mzl9 container test-container-subpath-downwardapi-mzl9: <nil>
STEP: delete the pod
Jan 23 22:45:10.994: INFO: Waiting for pod pod-subpath-test-downwardapi-mzl9 to disappear
Jan 23 22:45:11.002: INFO: Pod pod-subpath-test-downwardapi-mzl9 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-mzl9
Jan 23 22:45:11.003: INFO: Deleting pod "pod-subpath-test-downwardapi-mzl9" in namespace "subpath-8497"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:45:11.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8497" for this suite.

• [SLOW TEST:22.233 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":280,"completed":122,"skipped":1939,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:45:11.030: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 23 22:45:11.107: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-8702c939-230c-4348-84ff-8a4b749961cf" in namespace "security-context-test-7392" to be "success or failure"
Jan 23 22:45:11.121: INFO: Pod "alpine-nnp-false-8702c939-230c-4348-84ff-8a4b749961cf": Phase="Pending", Reason="", readiness=false. Elapsed: 14.490281ms
Jan 23 22:45:13.127: INFO: Pod "alpine-nnp-false-8702c939-230c-4348-84ff-8a4b749961cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019802753s
Jan 23 22:45:13.127: INFO: Pod "alpine-nnp-false-8702c939-230c-4348-84ff-8a4b749961cf" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:45:13.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7392" for this suite.
•{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":123,"skipped":1949,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:45:13.158: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Jan 23 22:45:45.559: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0123 22:45:45.557787      21 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:45:45.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8680" for this suite.

• [SLOW TEST:30.742 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":280,"completed":124,"skipped":1953,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:45:45.587: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir volume type on node default medium
Jan 23 22:45:45.740: INFO: Waiting up to 5m0s for pod "pod-d24992e5-3615-4df6-b3b2-2df30695ce1b" in namespace "emptydir-5629" to be "success or failure"
Jan 23 22:45:45.778: INFO: Pod "pod-d24992e5-3615-4df6-b3b2-2df30695ce1b": Phase="Pending", Reason="", readiness=false. Elapsed: 37.393145ms
Jan 23 22:45:47.784: INFO: Pod "pod-d24992e5-3615-4df6-b3b2-2df30695ce1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044228534s
Jan 23 22:45:51.527: INFO: Pod "pod-d24992e5-3615-4df6-b3b2-2df30695ce1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050600006s
STEP: Saw pod success
Jan 23 22:45:51.527: INFO: Pod "pod-d24992e5-3615-4df6-b3b2-2df30695ce1b" satisfied condition "success or failure"
Jan 23 22:45:51.532: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod pod-d24992e5-3615-4df6-b3b2-2df30695ce1b container test-container: <nil>
STEP: delete the pod
Jan 23 22:45:51.579: INFO: Waiting for pod pod-d24992e5-3615-4df6-b3b2-2df30695ce1b to disappear
Jan 23 22:45:51.584: INFO: Pod pod-d24992e5-3615-4df6-b3b2-2df30695ce1b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:45:51.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5629" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":125,"skipped":1962,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:45:51.620: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jan 23 22:45:51.700: INFO: Waiting up to 5m0s for pod "pod-86f6c8ec-f4fb-410c-b8ba-50eac7624ab4" in namespace "emptydir-1652" to be "success or failure"
Jan 23 22:45:51.709: INFO: Pod "pod-86f6c8ec-f4fb-410c-b8ba-50eac7624ab4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.183384ms
Jan 23 22:45:53.715: INFO: Pod "pod-86f6c8ec-f4fb-410c-b8ba-50eac7624ab4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015566136s
STEP: Saw pod success
Jan 23 22:45:53.716: INFO: Pod "pod-86f6c8ec-f4fb-410c-b8ba-50eac7624ab4" satisfied condition "success or failure"
Jan 23 22:45:53.724: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod pod-86f6c8ec-f4fb-410c-b8ba-50eac7624ab4 container test-container: <nil>
STEP: delete the pod
Jan 23 22:45:53.762: INFO: Waiting for pod pod-86f6c8ec-f4fb-410c-b8ba-50eac7624ab4 to disappear
Jan 23 22:45:53.771: INFO: Pod pod-86f6c8ec-f4fb-410c-b8ba-50eac7624ab4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:45:53.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1652" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":126,"skipped":1975,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:45:53.874: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl run default
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1596
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jan 23 22:45:53.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-6062'
Jan 23 22:45:54.191: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan 23 22:45:54.191: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1602
Jan 23 22:45:56.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 delete deployment e2e-test-httpd-deployment --namespace=kubectl-6062'
Jan 23 22:45:56.356: INFO: stderr: ""
Jan 23 22:45:56.357: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:45:56.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6062" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run default should create an rc or deployment from an image  [Conformance]","total":280,"completed":127,"skipped":1994,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:45:56.398: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 23 22:45:57.125: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 23 22:46:00.271: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 23 22:46:00.278: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:46:02.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1941" for this suite.
STEP: Destroying namespace "webhook-1941-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.997 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":280,"completed":128,"skipped":1994,"failed":0}
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:46:02.397: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 23 22:46:02.468: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c09e51ef-9ad9-40f3-aebe-f5ca1caed231" in namespace "downward-api-9409" to be "success or failure"
Jan 23 22:46:02.480: INFO: Pod "downwardapi-volume-c09e51ef-9ad9-40f3-aebe-f5ca1caed231": Phase="Pending", Reason="", readiness=false. Elapsed: 12.657615ms
Jan 23 22:46:04.486: INFO: Pod "downwardapi-volume-c09e51ef-9ad9-40f3-aebe-f5ca1caed231": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018395226s
STEP: Saw pod success
Jan 23 22:46:04.486: INFO: Pod "downwardapi-volume-c09e51ef-9ad9-40f3-aebe-f5ca1caed231" satisfied condition "success or failure"
Jan 23 22:46:04.491: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod downwardapi-volume-c09e51ef-9ad9-40f3-aebe-f5ca1caed231 container client-container: <nil>
STEP: delete the pod
Jan 23 22:46:04.521: INFO: Waiting for pod downwardapi-volume-c09e51ef-9ad9-40f3-aebe-f5ca1caed231 to disappear
Jan 23 22:46:04.527: INFO: Pod downwardapi-volume-c09e51ef-9ad9-40f3-aebe-f5ca1caed231 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:46:04.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9409" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":280,"completed":129,"skipped":1994,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:46:04.547: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jan 23 22:46:04.609: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1444 /api/v1/namespaces/watch-1444/configmaps/e2e-watch-test-watch-closed fcef6fd5-eab3-45da-b6bd-3387343e3006 416117 0 2020-01-23 22:46:05 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 23 22:46:04.610: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1444 /api/v1/namespaces/watch-1444/configmaps/e2e-watch-test-watch-closed fcef6fd5-eab3-45da-b6bd-3387343e3006 416118 0 2020-01-23 22:46:05 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jan 23 22:46:04.635: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1444 /api/v1/namespaces/watch-1444/configmaps/e2e-watch-test-watch-closed fcef6fd5-eab3-45da-b6bd-3387343e3006 416119 0 2020-01-23 22:46:05 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 23 22:46:04.636: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1444 /api/v1/namespaces/watch-1444/configmaps/e2e-watch-test-watch-closed fcef6fd5-eab3-45da-b6bd-3387343e3006 416120 0 2020-01-23 22:46:05 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:46:04.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1444" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":280,"completed":130,"skipped":2015,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:46:04.662: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1692
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jan 23 22:46:04.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-3276'
Jan 23 22:46:04.834: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan 23 22:46:04.834: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
Jan 23 22:46:04.851: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Jan 23 22:46:04.861: INFO: scanned /root for discovery docs: <nil>
Jan 23 22:46:04.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-3276'
Jan 23 22:46:20.746: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jan 23 22:46:20.746: INFO: stdout: "Created e2e-test-httpd-rc-abacb10002243205da3047a597694bc7\nScaling up e2e-test-httpd-rc-abacb10002243205da3047a597694bc7 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-abacb10002243205da3047a597694bc7 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-abacb10002243205da3047a597694bc7 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Jan 23 22:46:20.746: INFO: stdout: "Created e2e-test-httpd-rc-abacb10002243205da3047a597694bc7\nScaling up e2e-test-httpd-rc-abacb10002243205da3047a597694bc7 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-abacb10002243205da3047a597694bc7 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-abacb10002243205da3047a597694bc7 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Jan 23 22:46:20.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-3276'
Jan 23 22:46:20.858: INFO: stderr: ""
Jan 23 22:46:20.858: INFO: stdout: "e2e-test-httpd-rc-abacb10002243205da3047a597694bc7-kmg9q "
Jan 23 22:46:20.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get pods e2e-test-httpd-rc-abacb10002243205da3047a597694bc7-kmg9q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3276'
Jan 23 22:46:20.936: INFO: stderr: ""
Jan 23 22:46:20.937: INFO: stdout: "true"
Jan 23 22:46:20.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get pods e2e-test-httpd-rc-abacb10002243205da3047a597694bc7-kmg9q -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3276'
Jan 23 22:46:21.018: INFO: stderr: ""
Jan 23 22:46:21.018: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Jan 23 22:46:21.019: INFO: e2e-test-httpd-rc-abacb10002243205da3047a597694bc7-kmg9q is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1698
Jan 23 22:46:21.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 delete rc e2e-test-httpd-rc --namespace=kubectl-3276'
Jan 23 22:46:21.110: INFO: stderr: ""
Jan 23 22:46:21.110: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:46:21.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3276" for this suite.

• [SLOW TEST:16.470 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1687
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl rolling-update should support rolling-update to same image  [Conformance]","total":280,"completed":131,"skipped":2031,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:46:21.145: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 23 22:46:22.049: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 23 22:46:26.843: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 23 22:46:26.849: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9551-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:46:28.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5067" for this suite.
STEP: Destroying namespace "webhook-5067-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.841 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":280,"completed":132,"skipped":2031,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:46:28.738: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 23 22:46:29.456: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 23 22:46:32.551: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:46:32.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1635" for this suite.
STEP: Destroying namespace "webhook-1635-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":280,"completed":133,"skipped":2035,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:46:32.818: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name s-test-opt-del-819484b6-30b7-44ca-830d-062bd02543c6
STEP: Creating secret with name s-test-opt-upd-5ef88062-0466-49d5-ab51-e7fb289258ab
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-819484b6-30b7-44ca-830d-062bd02543c6
STEP: Updating secret s-test-opt-upd-5ef88062-0466-49d5-ab51-e7fb289258ab
STEP: Creating secret with name s-test-opt-create-0314e661-c9fa-4190-bfde-c0ead3d666c7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:47:55.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4594" for this suite.

• [SLOW TEST:79.487 seconds]
[sig-storage] Secrets
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":134,"skipped":2047,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:47:55.936: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-29571830-5107-4e91-bf4b-cd6daeac5e98
STEP: Creating a pod to test consume configMaps
Jan 23 22:47:56.201: INFO: Waiting up to 5m0s for pod "pod-configmaps-db237e76-fa4c-4395-ba16-e0eb911d2678" in namespace "configmap-4120" to be "success or failure"
Jan 23 22:47:56.226: INFO: Pod "pod-configmaps-db237e76-fa4c-4395-ba16-e0eb911d2678": Phase="Pending", Reason="", readiness=false. Elapsed: 24.858013ms
Jan 23 22:47:58.234: INFO: Pod "pod-configmaps-db237e76-fa4c-4395-ba16-e0eb911d2678": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032487191s
STEP: Saw pod success
Jan 23 22:47:58.234: INFO: Pod "pod-configmaps-db237e76-fa4c-4395-ba16-e0eb911d2678" satisfied condition "success or failure"
Jan 23 22:47:58.249: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f pod pod-configmaps-db237e76-fa4c-4395-ba16-e0eb911d2678 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 23 22:47:58.317: INFO: Waiting for pod pod-configmaps-db237e76-fa4c-4395-ba16-e0eb911d2678 to disappear
Jan 23 22:47:58.328: INFO: Pod pod-configmaps-db237e76-fa4c-4395-ba16-e0eb911d2678 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:47:58.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4120" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":135,"skipped":2067,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:47:58.371: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-upd-cde18ec9-5761-4259-bb9d-11da141952d7
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:48:00.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-879" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":136,"skipped":2144,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:48:00.549: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jan 23 22:48:03.706: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:48:03.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7898" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":280,"completed":137,"skipped":2177,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:48:03.807: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Jan 23 22:48:03.885: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
Jan 23 22:48:10.016: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:48:24.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9036" for this suite.

• [SLOW TEST:19.795 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":280,"completed":138,"skipped":2180,"failed":0}
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:48:24.955: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-projected-all-test-volume-8325b048-5dce-4639-94ea-88910e12d4c8
STEP: Creating secret with name secret-projected-all-test-volume-a45440fd-f78e-40d1-bd85-93fad1732171
STEP: Creating a pod to test Check all projections for projected volume plugin
Jan 23 22:48:25.038: INFO: Waiting up to 5m0s for pod "projected-volume-fe225b31-a4e6-4da4-863b-4c9acc8b5cf3" in namespace "projected-4693" to be "success or failure"
Jan 23 22:48:25.047: INFO: Pod "projected-volume-fe225b31-a4e6-4da4-863b-4c9acc8b5cf3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.880999ms
Jan 23 22:48:27.052: INFO: Pod "projected-volume-fe225b31-a4e6-4da4-863b-4c9acc8b5cf3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014527319s
Jan 23 22:48:29.057: INFO: Pod "projected-volume-fe225b31-a4e6-4da4-863b-4c9acc8b5cf3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019745198s
STEP: Saw pod success
Jan 23 22:48:29.058: INFO: Pod "projected-volume-fe225b31-a4e6-4da4-863b-4c9acc8b5cf3" satisfied condition "success or failure"
Jan 23 22:48:29.063: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod projected-volume-fe225b31-a4e6-4da4-863b-4c9acc8b5cf3 container projected-all-volume-test: <nil>
STEP: delete the pod
Jan 23 22:48:29.102: INFO: Waiting for pod projected-volume-fe225b31-a4e6-4da4-863b-4c9acc8b5cf3 to disappear
Jan 23 22:48:29.111: INFO: Pod projected-volume-fe225b31-a4e6-4da4-863b-4c9acc8b5cf3 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:48:29.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4693" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":280,"completed":139,"skipped":2180,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:48:29.130: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8395.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8395.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 23 22:48:33.303: INFO: DNS probes using dns-8395/dns-test-aeec1415-5948-465d-97f6-26d02d3ef326 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:48:33.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8395" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":280,"completed":140,"skipped":2195,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:48:33.347: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-f4576611-5f78-4bc1-b50a-d6b041da1833
STEP: Creating a pod to test consume configMaps
Jan 23 22:48:33.426: INFO: Waiting up to 5m0s for pod "pod-configmaps-e61fc054-d303-4fdc-a14f-355b5882f6d6" in namespace "configmap-8020" to be "success or failure"
Jan 23 22:48:33.438: INFO: Pod "pod-configmaps-e61fc054-d303-4fdc-a14f-355b5882f6d6": Phase="Pending", Reason="", readiness=false. Elapsed: 12.062356ms
Jan 23 22:48:35.444: INFO: Pod "pod-configmaps-e61fc054-d303-4fdc-a14f-355b5882f6d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018006132s
STEP: Saw pod success
Jan 23 22:48:35.444: INFO: Pod "pod-configmaps-e61fc054-d303-4fdc-a14f-355b5882f6d6" satisfied condition "success or failure"
Jan 23 22:48:35.448: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod pod-configmaps-e61fc054-d303-4fdc-a14f-355b5882f6d6 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 23 22:48:35.477: INFO: Waiting for pod pod-configmaps-e61fc054-d303-4fdc-a14f-355b5882f6d6 to disappear
Jan 23 22:48:35.481: INFO: Pod pod-configmaps-e61fc054-d303-4fdc-a14f-355b5882f6d6 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:48:35.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8020" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":141,"skipped":2218,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:48:35.495: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 23 22:48:35.567: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jan 23 22:48:35.587: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:48:35.597: INFO: Number of nodes with available pods: 0
Jan 23 22:48:35.598: INFO: Node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz is running more than one daemon pod
Jan 23 22:48:36.605: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:48:36.611: INFO: Number of nodes with available pods: 0
Jan 23 22:48:36.611: INFO: Node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz is running more than one daemon pod
Jan 23 22:48:37.606: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:48:37.613: INFO: Number of nodes with available pods: 2
Jan 23 22:48:37.613: INFO: Node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f is running more than one daemon pod
Jan 23 22:48:39.999: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:48:40.005: INFO: Number of nodes with available pods: 3
Jan 23 22:48:40.006: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jan 23 22:48:40.081: INFO: Wrong image for pod: daemon-set-49vsr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:48:40.081: INFO: Wrong image for pod: daemon-set-9bmcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:48:40.081: INFO: Wrong image for pod: daemon-set-t8w6v. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:48:40.086: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:48:41.091: INFO: Wrong image for pod: daemon-set-49vsr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:48:41.091: INFO: Wrong image for pod: daemon-set-9bmcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:48:41.091: INFO: Wrong image for pod: daemon-set-t8w6v. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:48:41.096: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:48:42.106: INFO: Wrong image for pod: daemon-set-49vsr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:48:42.106: INFO: Pod daemon-set-49vsr is not available
Jan 23 22:48:42.106: INFO: Wrong image for pod: daemon-set-9bmcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:48:42.106: INFO: Wrong image for pod: daemon-set-t8w6v. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:48:42.117: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:48:43.095: INFO: Wrong image for pod: daemon-set-49vsr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:48:43.095: INFO: Pod daemon-set-49vsr is not available
Jan 23 22:48:43.095: INFO: Wrong image for pod: daemon-set-9bmcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:48:43.095: INFO: Wrong image for pod: daemon-set-t8w6v. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:48:43.101: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:48:44.093: INFO: Wrong image for pod: daemon-set-49vsr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:48:44.094: INFO: Pod daemon-set-49vsr is not available
Jan 23 22:48:44.094: INFO: Wrong image for pod: daemon-set-9bmcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:48:44.094: INFO: Wrong image for pod: daemon-set-t8w6v. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:48:44.103: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:48:45.093: INFO: Wrong image for pod: daemon-set-49vsr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:48:45.093: INFO: Pod daemon-set-49vsr is not available
Jan 23 22:48:45.093: INFO: Wrong image for pod: daemon-set-9bmcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:48:45.093: INFO: Wrong image for pod: daemon-set-t8w6v. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:48:45.099: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:48:46.095: INFO: Wrong image for pod: daemon-set-9bmcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:48:46.095: INFO: Wrong image for pod: daemon-set-t8w6v. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:48:46.095: INFO: Pod daemon-set-x79kc is not available
Jan 23 22:48:46.101: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:48:47.092: INFO: Wrong image for pod: daemon-set-9bmcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:48:47.092: INFO: Wrong image for pod: daemon-set-t8w6v. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:48:47.098: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:48:48.092: INFO: Wrong image for pod: daemon-set-9bmcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:48:48.093: INFO: Wrong image for pod: daemon-set-t8w6v. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:48:48.098: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:48:49.092: INFO: Wrong image for pod: daemon-set-9bmcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:48:49.093: INFO: Wrong image for pod: daemon-set-t8w6v. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:48:49.093: INFO: Pod daemon-set-t8w6v is not available
Jan 23 22:48:49.099: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:48:50.092: INFO: Wrong image for pod: daemon-set-9bmcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:48:50.092: INFO: Wrong image for pod: daemon-set-t8w6v. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:48:50.092: INFO: Pod daemon-set-t8w6v is not available
Jan 23 22:48:50.097: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:48:51.091: INFO: Wrong image for pod: daemon-set-9bmcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:48:51.092: INFO: Wrong image for pod: daemon-set-t8w6v. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:48:51.092: INFO: Pod daemon-set-t8w6v is not available
Jan 23 22:48:51.107: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:48:52.093: INFO: Wrong image for pod: daemon-set-9bmcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:48:52.093: INFO: Wrong image for pod: daemon-set-t8w6v. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:48:52.093: INFO: Pod daemon-set-t8w6v is not available
Jan 23 22:48:52.100: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:48:53.186: INFO: Wrong image for pod: daemon-set-9bmcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:48:53.193: INFO: Wrong image for pod: daemon-set-t8w6v. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:48:53.196: INFO: Pod daemon-set-t8w6v is not available
Jan 23 22:48:53.355: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:48:54.097: INFO: Wrong image for pod: daemon-set-9bmcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:48:54.097: INFO: Wrong image for pod: daemon-set-t8w6v. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:48:54.097: INFO: Pod daemon-set-t8w6v is not available
Jan 23 22:48:54.109: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:48:55.099: INFO: Wrong image for pod: daemon-set-9bmcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:48:55.099: INFO: Wrong image for pod: daemon-set-t8w6v. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:48:55.099: INFO: Pod daemon-set-t8w6v is not available
Jan 23 22:48:55.114: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:48:56.096: INFO: Wrong image for pod: daemon-set-9bmcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:48:56.096: INFO: Wrong image for pod: daemon-set-t8w6v. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:48:56.096: INFO: Pod daemon-set-t8w6v is not available
Jan 23 22:48:56.107: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:48:57.098: INFO: Wrong image for pod: daemon-set-9bmcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:48:57.098: INFO: Wrong image for pod: daemon-set-t8w6v. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:48:57.098: INFO: Pod daemon-set-t8w6v is not available
Jan 23 22:48:57.108: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:48:58.095: INFO: Wrong image for pod: daemon-set-9bmcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:48:58.095: INFO: Wrong image for pod: daemon-set-t8w6v. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:48:58.095: INFO: Pod daemon-set-t8w6v is not available
Jan 23 22:48:58.107: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:48:59.111: INFO: Wrong image for pod: daemon-set-9bmcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:48:59.112: INFO: Wrong image for pod: daemon-set-t8w6v. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:48:59.112: INFO: Pod daemon-set-t8w6v is not available
Jan 23 22:48:59.131: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:49:00.103: INFO: Wrong image for pod: daemon-set-9bmcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:49:00.104: INFO: Pod daemon-set-qsgv5 is not available
Jan 23 22:49:00.122: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:49:01.095: INFO: Wrong image for pod: daemon-set-9bmcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:49:01.095: INFO: Pod daemon-set-qsgv5 is not available
Jan 23 22:49:01.114: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:49:02.121: INFO: Wrong image for pod: daemon-set-9bmcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:49:02.160: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:49:03.095: INFO: Wrong image for pod: daemon-set-9bmcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:49:03.095: INFO: Pod daemon-set-9bmcm is not available
Jan 23 22:49:03.105: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:49:04.096: INFO: Wrong image for pod: daemon-set-9bmcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:49:04.096: INFO: Pod daemon-set-9bmcm is not available
Jan 23 22:49:04.108: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:49:05.094: INFO: Wrong image for pod: daemon-set-9bmcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:49:05.094: INFO: Pod daemon-set-9bmcm is not available
Jan 23 22:49:05.106: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:49:06.096: INFO: Wrong image for pod: daemon-set-9bmcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 23 22:49:06.096: INFO: Pod daemon-set-9bmcm is not available
Jan 23 22:49:06.105: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:49:07.102: INFO: Pod daemon-set-tpvkl is not available
Jan 23 22:49:07.113: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Jan 23 22:49:07.121: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:49:07.128: INFO: Number of nodes with available pods: 2
Jan 23 22:49:07.128: INFO: Node management-cluster-1-17-2-md-0-5f64bb5777-v477d is running more than one daemon pod
Jan 23 22:49:08.138: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:49:08.146: INFO: Number of nodes with available pods: 2
Jan 23 22:49:08.146: INFO: Node management-cluster-1-17-2-md-0-5f64bb5777-v477d is running more than one daemon pod
Jan 23 22:49:09.138: INFO: DaemonSet pods can't tolerate node management-cluster-1-17-2-controlplane-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 23 22:49:09.147: INFO: Number of nodes with available pods: 3
Jan 23 22:49:09.147: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3951, will wait for the garbage collector to delete the pods
Jan 23 22:49:09.249: INFO: Deleting DaemonSet.extensions daemon-set took: 11.993589ms
Jan 23 22:49:09.751: INFO: Terminating DaemonSet.extensions daemon-set pods took: 501.739354ms
Jan 23 22:49:20.996: INFO: Number of nodes with available pods: 0
Jan 23 22:49:20.996: INFO: Number of running nodes: 0, number of available pods: 0
Jan 23 22:49:21.003: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3951/daemonsets","resourceVersion":"417711"},"items":null}

Jan 23 22:49:21.009: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3951/pods","resourceVersion":"417711"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:49:21.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3951" for this suite.

• [SLOW TEST:42.637 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":280,"completed":142,"skipped":2227,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:49:21.078: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test env composition
Jan 23 22:49:21.199: INFO: Waiting up to 5m0s for pod "var-expansion-93d4559a-70e8-4bb5-92c1-396db402f562" in namespace "var-expansion-197" to be "success or failure"
Jan 23 22:49:21.211: INFO: Pod "var-expansion-93d4559a-70e8-4bb5-92c1-396db402f562": Phase="Pending", Reason="", readiness=false. Elapsed: 11.593273ms
Jan 23 22:49:23.217: INFO: Pod "var-expansion-93d4559a-70e8-4bb5-92c1-396db402f562": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017404262s
STEP: Saw pod success
Jan 23 22:49:23.217: INFO: Pod "var-expansion-93d4559a-70e8-4bb5-92c1-396db402f562" satisfied condition "success or failure"
Jan 23 22:49:23.221: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod var-expansion-93d4559a-70e8-4bb5-92c1-396db402f562 container dapi-container: <nil>
STEP: delete the pod
Jan 23 22:49:23.267: INFO: Waiting for pod var-expansion-93d4559a-70e8-4bb5-92c1-396db402f562 to disappear
Jan 23 22:49:23.271: INFO: Pod var-expansion-93d4559a-70e8-4bb5-92c1-396db402f562 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:49:23.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-197" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":280,"completed":143,"skipped":2235,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:49:23.295: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Jan 23 22:49:23.365: INFO: Waiting up to 5m0s for pod "downward-api-87b49ee4-e546-488c-a68e-a3a0f79ed7cc" in namespace "downward-api-2466" to be "success or failure"
Jan 23 22:49:23.372: INFO: Pod "downward-api-87b49ee4-e546-488c-a68e-a3a0f79ed7cc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.54206ms
Jan 23 22:49:25.377: INFO: Pod "downward-api-87b49ee4-e546-488c-a68e-a3a0f79ed7cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011526235s
STEP: Saw pod success
Jan 23 22:49:25.377: INFO: Pod "downward-api-87b49ee4-e546-488c-a68e-a3a0f79ed7cc" satisfied condition "success or failure"
Jan 23 22:49:25.380: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod downward-api-87b49ee4-e546-488c-a68e-a3a0f79ed7cc container dapi-container: <nil>
STEP: delete the pod
Jan 23 22:49:25.406: INFO: Waiting for pod downward-api-87b49ee4-e546-488c-a68e-a3a0f79ed7cc to disappear
Jan 23 22:49:25.411: INFO: Pod downward-api-87b49ee4-e546-488c-a68e-a3a0f79ed7cc no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:49:25.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2466" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":280,"completed":144,"skipped":2258,"failed":0}
SSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:49:25.424: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-c4c1ee1f-7a21-4e58-9b3d-ac32be2f2822
STEP: Creating a pod to test consume secrets
Jan 23 22:49:25.541: INFO: Waiting up to 5m0s for pod "pod-secrets-3e206c96-c6fe-44fc-8891-7fe216f855d8" in namespace "secrets-7947" to be "success or failure"
Jan 23 22:49:25.547: INFO: Pod "pod-secrets-3e206c96-c6fe-44fc-8891-7fe216f855d8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.06416ms
Jan 23 22:49:27.552: INFO: Pod "pod-secrets-3e206c96-c6fe-44fc-8891-7fe216f855d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01060447s
STEP: Saw pod success
Jan 23 22:49:27.552: INFO: Pod "pod-secrets-3e206c96-c6fe-44fc-8891-7fe216f855d8" satisfied condition "success or failure"
Jan 23 22:49:27.555: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod pod-secrets-3e206c96-c6fe-44fc-8891-7fe216f855d8 container secret-volume-test: <nil>
STEP: delete the pod
Jan 23 22:49:27.585: INFO: Waiting for pod pod-secrets-3e206c96-c6fe-44fc-8891-7fe216f855d8 to disappear
Jan 23 22:49:27.593: INFO: Pod pod-secrets-3e206c96-c6fe-44fc-8891-7fe216f855d8 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:49:27.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7947" for this suite.
STEP: Destroying namespace "secret-namespace-6474" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":280,"completed":145,"skipped":2263,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:49:27.638: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:49:31.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9258" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":280,"completed":146,"skipped":2284,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:49:31.745: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-aa8c514e-d0f3-403a-a025-699006660d79
STEP: Creating a pod to test consume configMaps
Jan 23 22:49:31.814: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bb5f348d-3b83-4958-98cf-e60894820775" in namespace "projected-4266" to be "success or failure"
Jan 23 22:49:31.827: INFO: Pod "pod-projected-configmaps-bb5f348d-3b83-4958-98cf-e60894820775": Phase="Pending", Reason="", readiness=false. Elapsed: 12.267374ms
Jan 23 22:49:33.834: INFO: Pod "pod-projected-configmaps-bb5f348d-3b83-4958-98cf-e60894820775": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01907327s
STEP: Saw pod success
Jan 23 22:49:33.834: INFO: Pod "pod-projected-configmaps-bb5f348d-3b83-4958-98cf-e60894820775" satisfied condition "success or failure"
Jan 23 22:49:33.839: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod pod-projected-configmaps-bb5f348d-3b83-4958-98cf-e60894820775 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 23 22:49:33.869: INFO: Waiting for pod pod-projected-configmaps-bb5f348d-3b83-4958-98cf-e60894820775 to disappear
Jan 23 22:49:33.877: INFO: Pod pod-projected-configmaps-bb5f348d-3b83-4958-98cf-e60894820775 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:49:33.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4266" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":147,"skipped":2300,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:49:33.903: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jan 23 22:49:36.536: INFO: Successfully updated pod "pod-update-f4fd3cd3-b49f-43fb-a786-73aacd364b0f"
STEP: verifying the updated pod is in kubernetes
Jan 23 22:49:36.545: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:49:36.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6392" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":280,"completed":148,"skipped":2313,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:49:36.560: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-3219e79c-fa0b-49f9-866d-954982ed4fc8
STEP: Creating a pod to test consume configMaps
Jan 23 22:49:36.622: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-363245e9-b8c4-4e06-8b30-7a856c8d067f" in namespace "projected-3301" to be "success or failure"
Jan 23 22:49:36.643: INFO: Pod "pod-projected-configmaps-363245e9-b8c4-4e06-8b30-7a856c8d067f": Phase="Pending", Reason="", readiness=false. Elapsed: 20.633012ms
Jan 23 22:49:38.649: INFO: Pod "pod-projected-configmaps-363245e9-b8c4-4e06-8b30-7a856c8d067f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026495425s
STEP: Saw pod success
Jan 23 22:49:38.649: INFO: Pod "pod-projected-configmaps-363245e9-b8c4-4e06-8b30-7a856c8d067f" satisfied condition "success or failure"
Jan 23 22:49:38.652: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f pod pod-projected-configmaps-363245e9-b8c4-4e06-8b30-7a856c8d067f container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 23 22:49:38.698: INFO: Waiting for pod pod-projected-configmaps-363245e9-b8c4-4e06-8b30-7a856c8d067f to disappear
Jan 23 22:49:38.703: INFO: Pod pod-projected-configmaps-363245e9-b8c4-4e06-8b30-7a856c8d067f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:49:38.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3301" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":149,"skipped":2321,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:49:38.725: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Jan 23 22:49:41.327: INFO: Successfully updated pod "annotationupdateacf5080e-6f73-421c-aaba-6667c895691d"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:49:46.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6298" for this suite.

• [SLOW TEST:6.639 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":280,"completed":150,"skipped":2351,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:49:46.908: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 23 22:49:46.943: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:49:47.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4379" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":280,"completed":151,"skipped":2358,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:49:48.011: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name projected-secret-test-1a5f9a56-986d-46f0-b766-066da7466470
STEP: Creating a pod to test consume secrets
Jan 23 22:49:48.309: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-94f309d3-c64e-4ab2-88c2-2175f32527e5" in namespace "projected-476" to be "success or failure"
Jan 23 22:49:48.343: INFO: Pod "pod-projected-secrets-94f309d3-c64e-4ab2-88c2-2175f32527e5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.984431ms
Jan 23 22:49:50.347: INFO: Pod "pod-projected-secrets-94f309d3-c64e-4ab2-88c2-2175f32527e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.037728424s
STEP: Saw pod success
Jan 23 22:49:50.347: INFO: Pod "pod-projected-secrets-94f309d3-c64e-4ab2-88c2-2175f32527e5" satisfied condition "success or failure"
Jan 23 22:49:50.351: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f pod pod-projected-secrets-94f309d3-c64e-4ab2-88c2-2175f32527e5 container secret-volume-test: <nil>
STEP: delete the pod
Jan 23 22:49:50.387: INFO: Waiting for pod pod-projected-secrets-94f309d3-c64e-4ab2-88c2-2175f32527e5 to disappear
Jan 23 22:49:50.400: INFO: Pod pod-projected-secrets-94f309d3-c64e-4ab2-88c2-2175f32527e5 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:49:50.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-476" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":280,"completed":152,"skipped":2375,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:49:50.422: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jan 23 22:49:50.550: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:50:02.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8942" for this suite.

• [SLOW TEST:12.154 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":280,"completed":153,"skipped":2390,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:50:02.583: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap that has name configmap-test-emptyKey-a5ef4089-3c84-42be-8d44-c806064387a3
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:50:02.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7463" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":280,"completed":154,"skipped":2427,"failed":0}

------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:50:02.654: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 23 22:50:02.699: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jan 23 22:50:06.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 --namespace=crd-publish-openapi-2272 create -f -'
Jan 23 22:50:07.298: INFO: stderr: ""
Jan 23 22:50:07.298: INFO: stdout: "e2e-test-crd-publish-openapi-761-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 23 22:50:07.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 --namespace=crd-publish-openapi-2272 delete e2e-test-crd-publish-openapi-761-crds test-cr'
Jan 23 22:50:07.407: INFO: stderr: ""
Jan 23 22:50:07.407: INFO: stdout: "e2e-test-crd-publish-openapi-761-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jan 23 22:50:07.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 --namespace=crd-publish-openapi-2272 apply -f -'
Jan 23 22:50:07.802: INFO: stderr: ""
Jan 23 22:50:07.802: INFO: stdout: "e2e-test-crd-publish-openapi-761-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 23 22:50:07.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 --namespace=crd-publish-openapi-2272 delete e2e-test-crd-publish-openapi-761-crds test-cr'
Jan 23 22:50:07.932: INFO: stderr: ""
Jan 23 22:50:07.932: INFO: stdout: "e2e-test-crd-publish-openapi-761-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jan 23 22:50:07.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 explain e2e-test-crd-publish-openapi-761-crds'
Jan 23 22:50:08.309: INFO: stderr: ""
Jan 23 22:50:08.309: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-761-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:50:11.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2272" for this suite.

• [SLOW TEST:9.309 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":280,"completed":155,"skipped":2427,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:50:11.965: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:50:26.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1635" for this suite.

• [SLOW TEST:13.230 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":280,"completed":156,"skipped":2433,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:50:26.758: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jan 23 22:50:30.908: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 23 22:50:30.911: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 23 22:50:32.912: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 23 22:50:32.916: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 23 22:50:34.912: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 23 22:50:36.699: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:50:36.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6404" for this suite.

• [SLOW TEST:10.015 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":280,"completed":157,"skipped":2467,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:50:36.777: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: set up a multi version CRD
Jan 23 22:50:36.827: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:51:00.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4430" for this suite.

• [SLOW TEST:22.125 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":280,"completed":158,"skipped":2473,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:51:00.495: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:51:07.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-404" for this suite.

• [SLOW TEST:7.173 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":280,"completed":159,"skipped":2477,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:51:07.670: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-secret-bz6x
STEP: Creating a pod to test atomic-volume-subpath
Jan 23 22:51:07.791: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-bz6x" in namespace "subpath-5207" to be "success or failure"
Jan 23 22:51:07.814: INFO: Pod "pod-subpath-test-secret-bz6x": Phase="Pending", Reason="", readiness=false. Elapsed: 22.794095ms
Jan 23 22:51:09.824: INFO: Pod "pod-subpath-test-secret-bz6x": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032686998s
Jan 23 22:51:11.828: INFO: Pod "pod-subpath-test-secret-bz6x": Phase="Running", Reason="", readiness=true. Elapsed: 4.036987997s
Jan 23 22:51:13.835: INFO: Pod "pod-subpath-test-secret-bz6x": Phase="Running", Reason="", readiness=true. Elapsed: 6.043717412s
Jan 23 22:51:15.839: INFO: Pod "pod-subpath-test-secret-bz6x": Phase="Running", Reason="", readiness=true. Elapsed: 8.047560688s
Jan 23 22:51:17.849: INFO: Pod "pod-subpath-test-secret-bz6x": Phase="Running", Reason="", readiness=true. Elapsed: 10.057387894s
Jan 23 22:51:19.854: INFO: Pod "pod-subpath-test-secret-bz6x": Phase="Running", Reason="", readiness=true. Elapsed: 12.062796189s
Jan 23 22:51:21.861: INFO: Pod "pod-subpath-test-secret-bz6x": Phase="Running", Reason="", readiness=true. Elapsed: 14.069659472s
Jan 23 22:51:23.868: INFO: Pod "pod-subpath-test-secret-bz6x": Phase="Running", Reason="", readiness=true. Elapsed: 16.07711202s
Jan 23 22:51:25.874: INFO: Pod "pod-subpath-test-secret-bz6x": Phase="Running", Reason="", readiness=true. Elapsed: 18.082850587s
Jan 23 22:51:29.500: INFO: Pod "pod-subpath-test-secret-bz6x": Phase="Running", Reason="", readiness=true. Elapsed: 20.087224234s
Jan 23 22:51:31.505: INFO: Pod "pod-subpath-test-secret-bz6x": Phase="Running", Reason="", readiness=true. Elapsed: 22.092793755s
Jan 23 22:51:33.512: INFO: Pod "pod-subpath-test-secret-bz6x": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.099601768s
STEP: Saw pod success
Jan 23 22:51:33.512: INFO: Pod "pod-subpath-test-secret-bz6x" satisfied condition "success or failure"
Jan 23 22:51:33.516: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod pod-subpath-test-secret-bz6x container test-container-subpath-secret-bz6x: <nil>
STEP: delete the pod
Jan 23 22:51:33.557: INFO: Waiting for pod pod-subpath-test-secret-bz6x to disappear
Jan 23 22:51:33.564: INFO: Pod pod-subpath-test-secret-bz6x no longer exists
STEP: Deleting pod pod-subpath-test-secret-bz6x
Jan 23 22:51:33.565: INFO: Deleting pod "pod-subpath-test-secret-bz6x" in namespace "subpath-5207"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:51:33.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5207" for this suite.

• [SLOW TEST:24.296 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":280,"completed":160,"skipped":2506,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:51:33.588: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-8652
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 23 22:51:33.646: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jan 23 22:51:55.841: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.117.222.238:8080/dial?request=hostname&protocol=udp&host=100.117.222.237&port=8081&tries=1'] Namespace:pod-network-test-8652 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 23 22:51:55.841: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
Jan 23 22:51:56.099: INFO: Waiting for responses: map[]
Jan 23 22:51:56.104: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.117.222.238:8080/dial?request=hostname&protocol=udp&host=100.120.200.33&port=8081&tries=1'] Namespace:pod-network-test-8652 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 23 22:51:56.104: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
Jan 23 22:51:56.366: INFO: Waiting for responses: map[]
Jan 23 22:51:56.375: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.117.222.238:8080/dial?request=hostname&protocol=udp&host=100.105.119.122&port=8081&tries=1'] Namespace:pod-network-test-8652 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 23 22:51:56.375: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
Jan 23 22:51:56.584: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:51:56.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8652" for this suite.

• [SLOW TEST:23.022 seconds]
[sig-network] Networking
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":161,"skipped":2514,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:51:56.617: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jan 23 22:51:59.399: INFO: Successfully updated pod "pod-update-activedeadlineseconds-a7997d99-c23e-42f8-960f-76f0a842ecd1"
Jan 23 22:51:59.400: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-a7997d99-c23e-42f8-960f-76f0a842ecd1" in namespace "pods-4717" to be "terminated due to deadline exceeded"
Jan 23 22:51:59.584: INFO: Pod "pod-update-activedeadlineseconds-a7997d99-c23e-42f8-960f-76f0a842ecd1": Phase="Running", Reason="", readiness=true. Elapsed: 183.226906ms
Jan 23 22:52:03.254: INFO: Pod "pod-update-activedeadlineseconds-a7997d99-c23e-42f8-960f-76f0a842ecd1": Phase="Running", Reason="", readiness=true. Elapsed: 2.193956017s
Jan 23 22:52:05.374: INFO: Pod "pod-update-activedeadlineseconds-a7997d99-c23e-42f8-960f-76f0a842ecd1": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.314499658s
Jan 23 22:52:05.377: INFO: Pod "pod-update-activedeadlineseconds-a7997d99-c23e-42f8-960f-76f0a842ecd1" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:52:05.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4717" for this suite.

• [SLOW TEST:7.189 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":280,"completed":162,"skipped":2527,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:52:05.510: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name s-test-opt-del-765db5bf-79a5-4809-bd39-10b3e29953df
STEP: Creating secret with name s-test-opt-upd-4ff14f57-ea2f-4c76-b0d7-96fa9ffa6239
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-765db5bf-79a5-4809-bd39-10b3e29953df
STEP: Updating secret s-test-opt-upd-4ff14f57-ea2f-4c76-b0d7-96fa9ffa6239
STEP: Creating secret with name s-test-opt-create-1a5ea5df-0842-4e83-9fe4-becba4668b38
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:53:24.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9358" for this suite.

• [SLOW TEST:75.210 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":163,"skipped":2558,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:53:24.137: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-961a0526-8b67-48cb-8f0d-09ef8d97c132
STEP: Creating a pod to test consume secrets
Jan 23 22:53:24.308: INFO: Waiting up to 5m0s for pod "pod-secrets-649a09cb-783c-429e-9fc2-d3e0cc25a452" in namespace "secrets-5964" to be "success or failure"
Jan 23 22:53:24.330: INFO: Pod "pod-secrets-649a09cb-783c-429e-9fc2-d3e0cc25a452": Phase="Pending", Reason="", readiness=false. Elapsed: 22.270737ms
Jan 23 22:53:26.407: INFO: Pod "pod-secrets-649a09cb-783c-429e-9fc2-d3e0cc25a452": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.099509035s
STEP: Saw pod success
Jan 23 22:53:26.409: INFO: Pod "pod-secrets-649a09cb-783c-429e-9fc2-d3e0cc25a452" satisfied condition "success or failure"
Jan 23 22:53:26.420: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f pod pod-secrets-649a09cb-783c-429e-9fc2-d3e0cc25a452 container secret-volume-test: <nil>
STEP: delete the pod
Jan 23 22:53:26.553: INFO: Waiting for pod pod-secrets-649a09cb-783c-429e-9fc2-d3e0cc25a452 to disappear
Jan 23 22:53:26.563: INFO: Pod pod-secrets-649a09cb-783c-429e-9fc2-d3e0cc25a452 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:53:26.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5964" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":164,"skipped":2573,"failed":0}
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:53:26.611: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-9076
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating statefulset ss in namespace statefulset-9076
Jan 23 22:53:26.961: INFO: Found 0 stateful pods, waiting for 1
Jan 23 22:53:36.969: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jan 23 22:53:37.012: INFO: Deleting all statefulset in ns statefulset-9076
Jan 23 22:53:37.057: INFO: Scaling statefulset ss to 0
Jan 23 22:53:58.828: INFO: Waiting for statefulset status.replicas updated to 0
Jan 23 22:53:58.832: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:53:58.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9076" for this suite.

• [SLOW TEST:30.509 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":280,"completed":165,"skipped":2579,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:53:58.873: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Jan 23 22:53:58.919: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 23 22:53:58.942: INFO: Waiting for terminating namespaces to be deleted...
Jan 23 22:53:58.945: INFO: 
Logging pods the kubelet thinks is on node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz before test
Jan 23 22:53:58.974: INFO: sonobuoy-systemd-logs-daemon-set-82eb3c000b9d48f1-m9c8t from sonobuoy started at 2020-01-23 22:01:22 +0000 UTC (2 container statuses recorded)
Jan 23 22:53:58.974: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 23 22:53:58.974: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 23 22:53:58.974: INFO: vsphere-csi-node-4knv2 from kube-system started at 2020-01-23 22:13:07 +0000 UTC (3 container statuses recorded)
Jan 23 22:53:58.974: INFO: 	Container liveness-probe ready: true, restart count 0
Jan 23 22:53:58.974: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 23 22:53:58.974: INFO: 	Container vsphere-csi-node ready: true, restart count 0
Jan 23 22:53:58.974: INFO: kube-proxy-qfmln from kube-system started at 2020-01-22 22:09:02 +0000 UTC (1 container statuses recorded)
Jan 23 22:53:58.974: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 23 22:53:58.974: INFO: calico-node-zvhsz from kube-system started at 2020-01-22 22:09:02 +0000 UTC (1 container statuses recorded)
Jan 23 22:53:58.974: INFO: 	Container calico-node ready: true, restart count 0
Jan 23 22:53:58.974: INFO: 
Logging pods the kubelet thinks is on node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f before test
Jan 23 22:53:58.985: INFO: kube-proxy-n9fnt from kube-system started at 2020-01-22 22:09:05 +0000 UTC (1 container statuses recorded)
Jan 23 22:53:58.985: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 23 22:53:58.985: INFO: sonobuoy-systemd-logs-daemon-set-82eb3c000b9d48f1-mkq6m from sonobuoy started at 2020-01-23 22:01:23 +0000 UTC (2 container statuses recorded)
Jan 23 22:53:58.985: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 23 22:53:58.992: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 23 22:53:58.992: INFO: calico-node-27bq2 from kube-system started at 2020-01-22 22:09:05 +0000 UTC (1 container statuses recorded)
Jan 23 22:53:58.992: INFO: 	Container calico-node ready: true, restart count 0
Jan 23 22:53:58.992: INFO: vsphere-csi-node-rd47x from kube-system started at 2020-01-22 22:09:27 +0000 UTC (3 container statuses recorded)
Jan 23 22:53:58.992: INFO: 	Container liveness-probe ready: true, restart count 0
Jan 23 22:53:58.992: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 23 22:53:58.992: INFO: 	Container vsphere-csi-node ready: true, restart count 0
Jan 23 22:53:58.992: INFO: sonobuoy from sonobuoy started at 2020-01-23 22:01:11 +0000 UTC (1 container statuses recorded)
Jan 23 22:53:58.992: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 23 22:53:58.992: INFO: 
Logging pods the kubelet thinks is on node management-cluster-1-17-2-md-0-5f64bb5777-v477d before test
Jan 23 22:53:59.023: INFO: vsphere-csi-node-h9rth from kube-system started at 2020-01-22 22:09:25 +0000 UTC (3 container statuses recorded)
Jan 23 22:53:59.023: INFO: 	Container liveness-probe ready: true, restart count 0
Jan 23 22:53:59.023: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 23 22:53:59.023: INFO: 	Container vsphere-csi-node ready: true, restart count 0
Jan 23 22:53:59.023: INFO: calico-node-lbpft from kube-system started at 2020-01-22 22:09:03 +0000 UTC (1 container statuses recorded)
Jan 23 22:53:59.023: INFO: 	Container calico-node ready: true, restart count 0
Jan 23 22:53:59.023: INFO: sonobuoy-systemd-logs-daemon-set-82eb3c000b9d48f1-fwpld from sonobuoy started at 2020-01-23 22:01:22 +0000 UTC (2 container statuses recorded)
Jan 23 22:53:59.023: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 23 22:53:59.023: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 23 22:53:59.023: INFO: sonobuoy-e2e-job-61735b81990a421a from sonobuoy started at 2020-01-23 22:01:22 +0000 UTC (2 container statuses recorded)
Jan 23 22:53:59.023: INFO: 	Container e2e ready: true, restart count 0
Jan 23 22:53:59.023: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 23 22:53:59.023: INFO: kube-proxy-7w2h9 from kube-system started at 2020-01-22 22:09:03 +0000 UTC (1 container statuses recorded)
Jan 23 22:53:59.023: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: verifying the node has the label node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz
STEP: verifying the node has the label node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f
STEP: verifying the node has the label node management-cluster-1-17-2-md-0-5f64bb5777-v477d
Jan 23 22:53:59.205: INFO: Pod calico-node-27bq2 requesting resource cpu=250m on Node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f
Jan 23 22:53:59.205: INFO: Pod calico-node-lbpft requesting resource cpu=250m on Node management-cluster-1-17-2-md-0-5f64bb5777-v477d
Jan 23 22:53:59.205: INFO: Pod calico-node-zvhsz requesting resource cpu=250m on Node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz
Jan 23 22:53:59.205: INFO: Pod kube-proxy-7w2h9 requesting resource cpu=0m on Node management-cluster-1-17-2-md-0-5f64bb5777-v477d
Jan 23 22:53:59.205: INFO: Pod kube-proxy-n9fnt requesting resource cpu=0m on Node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f
Jan 23 22:53:59.205: INFO: Pod kube-proxy-qfmln requesting resource cpu=0m on Node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz
Jan 23 22:53:59.205: INFO: Pod vsphere-csi-node-4knv2 requesting resource cpu=0m on Node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz
Jan 23 22:53:59.205: INFO: Pod vsphere-csi-node-h9rth requesting resource cpu=0m on Node management-cluster-1-17-2-md-0-5f64bb5777-v477d
Jan 23 22:53:59.205: INFO: Pod vsphere-csi-node-rd47x requesting resource cpu=0m on Node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f
Jan 23 22:53:59.205: INFO: Pod sonobuoy requesting resource cpu=0m on Node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f
Jan 23 22:53:59.205: INFO: Pod sonobuoy-e2e-job-61735b81990a421a requesting resource cpu=0m on Node management-cluster-1-17-2-md-0-5f64bb5777-v477d
Jan 23 22:53:59.205: INFO: Pod sonobuoy-systemd-logs-daemon-set-82eb3c000b9d48f1-fwpld requesting resource cpu=0m on Node management-cluster-1-17-2-md-0-5f64bb5777-v477d
Jan 23 22:53:59.205: INFO: Pod sonobuoy-systemd-logs-daemon-set-82eb3c000b9d48f1-m9c8t requesting resource cpu=0m on Node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz
Jan 23 22:53:59.205: INFO: Pod sonobuoy-systemd-logs-daemon-set-82eb3c000b9d48f1-mkq6m requesting resource cpu=0m on Node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f
STEP: Starting Pods to consume most of the cluster CPU.
Jan 23 22:53:59.205: INFO: Creating a pod which consumes cpu=1225m on Node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f
Jan 23 22:53:59.220: INFO: Creating a pod which consumes cpu=1225m on Node management-cluster-1-17-2-md-0-5f64bb5777-v477d
Jan 23 22:53:59.227: INFO: Creating a pod which consumes cpu=1225m on Node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-34665a6d-ef76-44b1-8403-e4449c83fdcd.15eca5f4c74bbdcc], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1621/filler-pod-34665a6d-ef76-44b1-8403-e4449c83fdcd to management-cluster-1-17-2-md-0-5f64bb5777-v477d]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-34665a6d-ef76-44b1-8403-e4449c83fdcd.15eca5f4d02b68ec], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-34665a6d-ef76-44b1-8403-e4449c83fdcd.15eca5f4d61aaf5a], Reason = [Created], Message = [Created container filler-pod-34665a6d-ef76-44b1-8403-e4449c83fdcd]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-34665a6d-ef76-44b1-8403-e4449c83fdcd.15eca5f4e1573206], Reason = [Started], Message = [Started container filler-pod-34665a6d-ef76-44b1-8403-e4449c83fdcd]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d938c646-f44e-4735-8c9d-9e58b2b2a8cc.15eca5f4aa901208], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d938c646-f44e-4735-8c9d-9e58b2b2a8cc.15eca5f4af6c04bc], Reason = [Created], Message = [Created container filler-pod-d938c646-f44e-4735-8c9d-9e58b2b2a8cc]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d938c646-f44e-4735-8c9d-9e58b2b2a8cc.15eca5f4ba341ba5], Reason = [Started], Message = [Started container filler-pod-d938c646-f44e-4735-8c9d-9e58b2b2a8cc]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d938c646-f44e-4735-8c9d-9e58b2b2a8cc.15eca5f4c858517a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1621/filler-pod-d938c646-f44e-4735-8c9d-9e58b2b2a8cc to management-cluster-1-17-2-md-0-5f64bb5777-dlwcz]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e226e017-6aa9-4a4a-8485-5a031271be59.15eca5f4c6a89fcc], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1621/filler-pod-e226e017-6aa9-4a4a-8485-5a031271be59 to management-cluster-1-17-2-md-0-5f64bb5777-fsd6f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e226e017-6aa9-4a4a-8485-5a031271be59.15eca5f4d3ba23db], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e226e017-6aa9-4a4a-8485-5a031271be59.15eca5f4d89ed9c1], Reason = [Created], Message = [Created container filler-pod-e226e017-6aa9-4a4a-8485-5a031271be59]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e226e017-6aa9-4a4a-8485-5a031271be59.15eca5f4e213af6c], Reason = [Started], Message = [Started container filler-pod-e226e017-6aa9-4a4a-8485-5a031271be59]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15eca5f541a72398], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 Insufficient cpu.]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15eca5f5433655bb], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 Insufficient cpu.]
STEP: removing the label node off the node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node management-cluster-1-17-2-md-0-5f64bb5777-v477d
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:54:02.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1621" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":280,"completed":166,"skipped":2592,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:54:02.424: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jan 23 22:54:02.501: INFO: Pod name pod-release: Found 0 pods out of 1
Jan 23 22:54:07.512: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:54:07.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4822" for this suite.

• [SLOW TEST:5.199 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":280,"completed":167,"skipped":2600,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:54:07.632: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 23 22:54:07.743: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6c37eb28-9f42-41ef-bed7-9c9bc4f41eaa" in namespace "projected-1061" to be "success or failure"
Jan 23 22:54:07.751: INFO: Pod "downwardapi-volume-6c37eb28-9f42-41ef-bed7-9c9bc4f41eaa": Phase="Pending", Reason="", readiness=false. Elapsed: 7.480266ms
Jan 23 22:54:09.757: INFO: Pod "downwardapi-volume-6c37eb28-9f42-41ef-bed7-9c9bc4f41eaa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01411711s
Jan 23 22:54:11.763: INFO: Pod "downwardapi-volume-6c37eb28-9f42-41ef-bed7-9c9bc4f41eaa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019810757s
STEP: Saw pod success
Jan 23 22:54:11.763: INFO: Pod "downwardapi-volume-6c37eb28-9f42-41ef-bed7-9c9bc4f41eaa" satisfied condition "success or failure"
Jan 23 22:54:11.767: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod downwardapi-volume-6c37eb28-9f42-41ef-bed7-9c9bc4f41eaa container client-container: <nil>
STEP: delete the pod
Jan 23 22:54:11.801: INFO: Waiting for pod downwardapi-volume-6c37eb28-9f42-41ef-bed7-9c9bc4f41eaa to disappear
Jan 23 22:54:11.805: INFO: Pod downwardapi-volume-6c37eb28-9f42-41ef-bed7-9c9bc4f41eaa no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:54:11.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1061" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":280,"completed":168,"skipped":2643,"failed":0}
S
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:54:11.825: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Jan 23 22:54:11.905: INFO: Waiting up to 5m0s for pod "downward-api-53acf738-5a3f-48ba-a1c4-fa9b09bcc087" in namespace "downward-api-3870" to be "success or failure"
Jan 23 22:54:11.919: INFO: Pod "downward-api-53acf738-5a3f-48ba-a1c4-fa9b09bcc087": Phase="Pending", Reason="", readiness=false. Elapsed: 14.129687ms
Jan 23 22:54:13.926: INFO: Pod "downward-api-53acf738-5a3f-48ba-a1c4-fa9b09bcc087": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020917639s
Jan 23 22:54:15.932: INFO: Pod "downward-api-53acf738-5a3f-48ba-a1c4-fa9b09bcc087": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026743983s
STEP: Saw pod success
Jan 23 22:54:15.932: INFO: Pod "downward-api-53acf738-5a3f-48ba-a1c4-fa9b09bcc087" satisfied condition "success or failure"
Jan 23 22:54:15.937: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod downward-api-53acf738-5a3f-48ba-a1c4-fa9b09bcc087 container dapi-container: <nil>
STEP: delete the pod
Jan 23 22:54:15.977: INFO: Waiting for pod downward-api-53acf738-5a3f-48ba-a1c4-fa9b09bcc087 to disappear
Jan 23 22:54:15.986: INFO: Pod downward-api-53acf738-5a3f-48ba-a1c4-fa9b09bcc087 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:54:15.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3870" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":280,"completed":169,"skipped":2644,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:54:16.017: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-ff0157e4-a3a9-490e-b470-3db3e5a29d5d
STEP: Creating a pod to test consume configMaps
Jan 23 22:54:16.141: INFO: Waiting up to 5m0s for pod "pod-configmaps-375f43a1-9efd-4191-b2c1-ff70e41b4496" in namespace "configmap-5568" to be "success or failure"
Jan 23 22:54:16.149: INFO: Pod "pod-configmaps-375f43a1-9efd-4191-b2c1-ff70e41b4496": Phase="Pending", Reason="", readiness=false. Elapsed: 7.365561ms
Jan 23 22:54:19.972: INFO: Pod "pod-configmaps-375f43a1-9efd-4191-b2c1-ff70e41b4496": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020072741s
STEP: Saw pod success
Jan 23 22:54:19.973: INFO: Pod "pod-configmaps-375f43a1-9efd-4191-b2c1-ff70e41b4496" satisfied condition "success or failure"
Jan 23 22:54:19.979: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod pod-configmaps-375f43a1-9efd-4191-b2c1-ff70e41b4496 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 23 22:54:20.012: INFO: Waiting for pod pod-configmaps-375f43a1-9efd-4191-b2c1-ff70e41b4496 to disappear
Jan 23 22:54:20.017: INFO: Pod pod-configmaps-375f43a1-9efd-4191-b2c1-ff70e41b4496 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:54:20.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5568" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":170,"skipped":2682,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:54:20.210: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: validating cluster-info
Jan 23 22:54:20.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 cluster-info'
Jan 23 22:54:21.049: INFO: stderr: ""
Jan 23 22:54:21.049: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://100.64.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://100.64.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:54:21.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8806" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":280,"completed":171,"skipped":2682,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:54:21.067: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 23 22:54:21.170: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:54:23.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8001" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":280,"completed":172,"skipped":2695,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:54:23.259: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:54:34.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2417" for this suite.

• [SLOW TEST:11.157 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":280,"completed":173,"skipped":2724,"failed":0}
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:54:34.417: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-1743
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating stateful set ss in namespace statefulset-1743
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1743
Jan 23 22:54:34.485: INFO: Found 0 stateful pods, waiting for 1
Jan 23 22:54:44.491: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jan 23 22:54:44.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 exec --namespace=statefulset-1743 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 23 22:54:44.827: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 23 22:54:44.827: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 23 22:54:44.827: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 23 22:54:44.831: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 23 22:54:56.677: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 23 22:54:56.679: INFO: Waiting for statefulset status.replicas updated to 0
Jan 23 22:54:56.781: INFO: POD   NODE                                             PHASE    GRACE  CONDITIONS
Jan 23 22:54:56.782: INFO: ss-0  management-cluster-1-17-2-md-0-5f64bb5777-dlwcz  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:54:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:54:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:54:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:54:35 +0000 UTC  }]
Jan 23 22:54:56.782: INFO: 
Jan 23 22:54:56.782: INFO: StatefulSet ss has not reached scale 3, at 1
Jan 23 22:54:57.795: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.966065507s
Jan 23 22:54:58.803: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.953330191s
Jan 23 22:54:59.812: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.945468306s
Jan 23 22:55:00.822: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.936379701s
Jan 23 22:55:01.830: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.926640129s
Jan 23 22:55:02.865: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.91794753s
Jan 23 22:55:03.874: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.883196445s
Jan 23 22:55:04.882: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.874640848s
Jan 23 22:55:05.890: INFO: Verifying statefulset ss doesn't scale past 3 for another 866.7515ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1743
Jan 23 22:55:06.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 exec --namespace=statefulset-1743 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 23 22:55:07.326: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 23 22:55:07.326: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 23 22:55:07.326: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 23 22:55:07.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 exec --namespace=statefulset-1743 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 23 22:55:07.621: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 23 22:55:07.621: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 23 22:55:07.621: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 23 22:55:07.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 exec --namespace=statefulset-1743 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 23 22:55:07.960: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 23 22:55:07.960: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 23 22:55:07.960: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 23 22:55:07.967: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Jan 23 22:55:17.980: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 23 22:55:17.981: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 23 22:55:17.981: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jan 23 22:55:17.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 exec --namespace=statefulset-1743 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 23 22:55:18.306: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 23 22:55:18.306: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 23 22:55:18.306: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 23 22:55:18.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 exec --namespace=statefulset-1743 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 23 22:55:18.591: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 23 22:55:18.591: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 23 22:55:18.591: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 23 22:55:18.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 exec --namespace=statefulset-1743 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 23 22:55:18.900: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 23 22:55:18.900: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 23 22:55:18.900: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 23 22:55:18.900: INFO: Waiting for statefulset status.replicas updated to 0
Jan 23 22:55:18.904: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jan 23 22:55:30.266: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 23 22:55:30.266: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 23 22:55:30.266: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 23 22:55:30.284: INFO: POD   NODE                                             PHASE    GRACE  CONDITIONS
Jan 23 22:55:30.284: INFO: ss-0  management-cluster-1-17-2-md-0-5f64bb5777-dlwcz  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:54:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:55:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:55:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:54:35 +0000 UTC  }]
Jan 23 22:55:30.284: INFO: ss-1  management-cluster-1-17-2-md-0-5f64bb5777-fsd6f  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:54:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:55:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:55:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:54:55 +0000 UTC  }]
Jan 23 22:55:30.284: INFO: ss-2  management-cluster-1-17-2-md-0-5f64bb5777-v477d  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:54:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:55:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:55:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:54:55 +0000 UTC  }]
Jan 23 22:55:30.284: INFO: 
Jan 23 22:55:30.284: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 23 22:55:31.290: INFO: POD   NODE                                             PHASE    GRACE  CONDITIONS
Jan 23 22:55:31.291: INFO: ss-0  management-cluster-1-17-2-md-0-5f64bb5777-dlwcz  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:54:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:55:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:55:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:54:35 +0000 UTC  }]
Jan 23 22:55:31.291: INFO: ss-1  management-cluster-1-17-2-md-0-5f64bb5777-fsd6f  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:54:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:55:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:55:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:54:55 +0000 UTC  }]
Jan 23 22:55:31.291: INFO: ss-2  management-cluster-1-17-2-md-0-5f64bb5777-v477d  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:54:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:55:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:55:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:54:55 +0000 UTC  }]
Jan 23 22:55:31.291: INFO: 
Jan 23 22:55:31.291: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 23 22:55:32.300: INFO: POD   NODE                                             PHASE    GRACE  CONDITIONS
Jan 23 22:55:32.300: INFO: ss-0  management-cluster-1-17-2-md-0-5f64bb5777-dlwcz  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:54:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:55:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:55:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:54:35 +0000 UTC  }]
Jan 23 22:55:32.300: INFO: ss-1  management-cluster-1-17-2-md-0-5f64bb5777-fsd6f  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:54:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:55:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:55:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:54:55 +0000 UTC  }]
Jan 23 22:55:32.300: INFO: ss-2  management-cluster-1-17-2-md-0-5f64bb5777-v477d  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:54:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:55:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:55:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:54:55 +0000 UTC  }]
Jan 23 22:55:32.300: INFO: 
Jan 23 22:55:32.300: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 23 22:55:33.305: INFO: POD   NODE                                             PHASE    GRACE  CONDITIONS
Jan 23 22:55:33.305: INFO: ss-0  management-cluster-1-17-2-md-0-5f64bb5777-dlwcz  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:54:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:55:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:55:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:54:35 +0000 UTC  }]
Jan 23 22:55:33.305: INFO: 
Jan 23 22:55:33.305: INFO: StatefulSet ss has not reached scale 0, at 1
Jan 23 22:55:34.331: INFO: POD   NODE                                             PHASE    GRACE  CONDITIONS
Jan 23 22:55:34.331: INFO: ss-0  management-cluster-1-17-2-md-0-5f64bb5777-dlwcz  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:54:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:55:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:55:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-23 22:54:35 +0000 UTC  }]
Jan 23 22:55:34.331: INFO: 
Jan 23 22:55:34.331: INFO: StatefulSet ss has not reached scale 0, at 1
Jan 23 22:55:35.336: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.944475694s
Jan 23 22:55:36.341: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.939552429s
Jan 23 22:55:37.346: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.93441663s
Jan 23 22:55:38.350: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.929828592s
Jan 23 22:55:39.355: INFO: Verifying statefulset ss doesn't scale past 0 for another 925.373775ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1743
Jan 23 22:55:40.360: INFO: Scaling statefulset ss to 0
Jan 23 22:55:40.371: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jan 23 22:55:40.376: INFO: Deleting all statefulset in ns statefulset-1743
Jan 23 22:55:40.379: INFO: Scaling statefulset ss to 0
Jan 23 22:55:40.391: INFO: Waiting for statefulset status.replicas updated to 0
Jan 23 22:55:40.395: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:55:40.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1743" for this suite.

• [SLOW TEST:62.898 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":280,"completed":174,"skipped":2724,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:55:40.455: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jan 23 22:55:40.536: INFO: Waiting up to 5m0s for pod "pod-52e5beea-5768-4b04-a943-488d7b6f9d2e" in namespace "emptydir-5018" to be "success or failure"
Jan 23 22:55:40.543: INFO: Pod "pod-52e5beea-5768-4b04-a943-488d7b6f9d2e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.653997ms
Jan 23 22:55:42.551: INFO: Pod "pod-52e5beea-5768-4b04-a943-488d7b6f9d2e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014217799s
STEP: Saw pod success
Jan 23 22:55:42.551: INFO: Pod "pod-52e5beea-5768-4b04-a943-488d7b6f9d2e" satisfied condition "success or failure"
Jan 23 22:55:42.560: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod pod-52e5beea-5768-4b04-a943-488d7b6f9d2e container test-container: <nil>
STEP: delete the pod
Jan 23 22:55:42.603: INFO: Waiting for pod pod-52e5beea-5768-4b04-a943-488d7b6f9d2e to disappear
Jan 23 22:55:42.607: INFO: Pod pod-52e5beea-5768-4b04-a943-488d7b6f9d2e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:55:42.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5018" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":175,"skipped":2735,"failed":0}
SSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:55:42.624: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod busybox-9c7ac3bb-36d2-4f0d-9882-4bd492a3ef49 in namespace container-probe-630
Jan 23 22:55:45.020: INFO: Started pod busybox-9c7ac3bb-36d2-4f0d-9882-4bd492a3ef49 in namespace container-probe-630
STEP: checking the pod's current state and verifying that restartCount is present
Jan 23 22:55:45.074: INFO: Initial restart count of pod busybox-9c7ac3bb-36d2-4f0d-9882-4bd492a3ef49 is 0
Jan 23 22:56:34.178: INFO: Restart count of pod container-probe-630/busybox-9c7ac3bb-36d2-4f0d-9882-4bd492a3ef49 is now 1 (46.175700095s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:56:34.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-630" for this suite.

• [SLOW TEST:48.669 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":280,"completed":176,"skipped":2741,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:56:34.237: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 23 22:56:34.315: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dddbd211-2eea-4f95-87c5-3bb2921f1d7e" in namespace "projected-4783" to be "success or failure"
Jan 23 22:56:34.326: INFO: Pod "downwardapi-volume-dddbd211-2eea-4f95-87c5-3bb2921f1d7e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.885056ms
Jan 23 22:56:36.333: INFO: Pod "downwardapi-volume-dddbd211-2eea-4f95-87c5-3bb2921f1d7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01783756s
STEP: Saw pod success
Jan 23 22:56:36.333: INFO: Pod "downwardapi-volume-dddbd211-2eea-4f95-87c5-3bb2921f1d7e" satisfied condition "success or failure"
Jan 23 22:56:36.338: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod downwardapi-volume-dddbd211-2eea-4f95-87c5-3bb2921f1d7e container client-container: <nil>
STEP: delete the pod
Jan 23 22:56:36.395: INFO: Waiting for pod downwardapi-volume-dddbd211-2eea-4f95-87c5-3bb2921f1d7e to disappear
Jan 23 22:56:36.400: INFO: Pod downwardapi-volume-dddbd211-2eea-4f95-87c5-3bb2921f1d7e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:56:36.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4783" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":280,"completed":177,"skipped":2792,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:56:36.434: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service multi-endpoint-test in namespace services-8441
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8441 to expose endpoints map[]
Jan 23 22:56:36.512: INFO: Get endpoints failed (10.216505ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Jan 23 22:56:37.518: INFO: successfully validated that service multi-endpoint-test in namespace services-8441 exposes endpoints map[] (1.015893157s elapsed)
STEP: Creating pod pod1 in namespace services-8441
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8441 to expose endpoints map[pod1:[100]]
Jan 23 22:56:40.567: INFO: successfully validated that service multi-endpoint-test in namespace services-8441 exposes endpoints map[pod1:[100]] (3.036346174s elapsed)
STEP: Creating pod pod2 in namespace services-8441
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8441 to expose endpoints map[pod1:[100] pod2:[101]]
Jan 23 22:56:42.619: INFO: successfully validated that service multi-endpoint-test in namespace services-8441 exposes endpoints map[pod1:[100] pod2:[101]] (2.047395017s elapsed)
STEP: Deleting pod pod1 in namespace services-8441
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8441 to expose endpoints map[pod2:[101]]
Jan 23 22:56:43.660: INFO: successfully validated that service multi-endpoint-test in namespace services-8441 exposes endpoints map[pod2:[101]] (1.035084106s elapsed)
STEP: Deleting pod pod2 in namespace services-8441
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8441 to expose endpoints map[]
Jan 23 22:56:44.684: INFO: successfully validated that service multi-endpoint-test in namespace services-8441 exposes endpoints map[] (1.017677697s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:56:44.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8441" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:8.337 seconds]
[sig-network] Services
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":280,"completed":178,"skipped":2811,"failed":0}
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:56:44.771: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jan 23 22:56:44.881: INFO: Waiting up to 5m0s for pod "pod-c3f06fef-eeba-4e75-b0e0-a6fd85a6244b" in namespace "emptydir-6753" to be "success or failure"
Jan 23 22:56:44.903: INFO: Pod "pod-c3f06fef-eeba-4e75-b0e0-a6fd85a6244b": Phase="Pending", Reason="", readiness=false. Elapsed: 21.941307ms
Jan 23 22:56:46.908: INFO: Pod "pod-c3f06fef-eeba-4e75-b0e0-a6fd85a6244b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026797751s
Jan 23 22:56:48.913: INFO: Pod "pod-c3f06fef-eeba-4e75-b0e0-a6fd85a6244b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031814342s
STEP: Saw pod success
Jan 23 22:56:48.913: INFO: Pod "pod-c3f06fef-eeba-4e75-b0e0-a6fd85a6244b" satisfied condition "success or failure"
Jan 23 22:56:48.916: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod pod-c3f06fef-eeba-4e75-b0e0-a6fd85a6244b container test-container: <nil>
STEP: delete the pod
Jan 23 22:56:48.935: INFO: Waiting for pod pod-c3f06fef-eeba-4e75-b0e0-a6fd85a6244b to disappear
Jan 23 22:56:48.940: INFO: Pod pod-c3f06fef-eeba-4e75-b0e0-a6fd85a6244b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:56:48.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6753" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":179,"skipped":2811,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:56:48.966: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-5813
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-5813
STEP: Creating statefulset with conflicting port in namespace statefulset-5813
STEP: Waiting until pod test-pod will start running in namespace statefulset-5813
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5813
Jan 23 22:56:53.062: INFO: Observed stateful pod in namespace: statefulset-5813, name: ss-0, uid: 06b44202-976e-4bb1-bb66-874451bc3bf9, status phase: Pending. Waiting for statefulset controller to delete.
Jan 23 22:56:53.251: INFO: Observed stateful pod in namespace: statefulset-5813, name: ss-0, uid: 06b44202-976e-4bb1-bb66-874451bc3bf9, status phase: Failed. Waiting for statefulset controller to delete.
Jan 23 22:56:53.260: INFO: Observed stateful pod in namespace: statefulset-5813, name: ss-0, uid: 06b44202-976e-4bb1-bb66-874451bc3bf9, status phase: Failed. Waiting for statefulset controller to delete.
Jan 23 22:56:53.268: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5813
STEP: Removing pod with conflicting port in namespace statefulset-5813
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5813 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jan 23 22:56:57.334: INFO: Deleting all statefulset in ns statefulset-5813
Jan 23 22:56:57.338: INFO: Scaling statefulset ss to 0
Jan 23 22:57:08.996: INFO: Waiting for statefulset status.replicas updated to 0
Jan 23 22:57:09.023: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:57:09.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5813" for this suite.

• [SLOW TEST:18.763 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":280,"completed":180,"skipped":2821,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:57:09.386: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-e02e669f-b699-49bf-8053-27fd6d650cd2
STEP: Creating a pod to test consume secrets
Jan 23 22:57:09.668: INFO: Waiting up to 5m0s for pod "pod-secrets-5ac0fd16-92e4-43ef-ae90-4ef4e2c83c85" in namespace "secrets-2092" to be "success or failure"
Jan 23 22:57:09.753: INFO: Pod "pod-secrets-5ac0fd16-92e4-43ef-ae90-4ef4e2c83c85": Phase="Pending", Reason="", readiness=false. Elapsed: 85.218867ms
Jan 23 22:57:11.760: INFO: Pod "pod-secrets-5ac0fd16-92e4-43ef-ae90-4ef4e2c83c85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.092630294s
Jan 23 22:57:13.768: INFO: Pod "pod-secrets-5ac0fd16-92e4-43ef-ae90-4ef4e2c83c85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.100588337s
STEP: Saw pod success
Jan 23 22:57:13.768: INFO: Pod "pod-secrets-5ac0fd16-92e4-43ef-ae90-4ef4e2c83c85" satisfied condition "success or failure"
Jan 23 22:57:13.774: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod pod-secrets-5ac0fd16-92e4-43ef-ae90-4ef4e2c83c85 container secret-volume-test: <nil>
STEP: delete the pod
Jan 23 22:57:13.821: INFO: Waiting for pod pod-secrets-5ac0fd16-92e4-43ef-ae90-4ef4e2c83c85 to disappear
Jan 23 22:57:13.832: INFO: Pod pod-secrets-5ac0fd16-92e4-43ef-ae90-4ef4e2c83c85 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:57:13.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2092" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":181,"skipped":2841,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:57:13.857: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 23 22:57:13.924: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a13c69d3-80b5-4c25-baba-513e08180908" in namespace "downward-api-8487" to be "success or failure"
Jan 23 22:57:13.934: INFO: Pod "downwardapi-volume-a13c69d3-80b5-4c25-baba-513e08180908": Phase="Pending", Reason="", readiness=false. Elapsed: 10.532869ms
Jan 23 22:57:15.939: INFO: Pod "downwardapi-volume-a13c69d3-80b5-4c25-baba-513e08180908": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014737444s
STEP: Saw pod success
Jan 23 22:57:15.939: INFO: Pod "downwardapi-volume-a13c69d3-80b5-4c25-baba-513e08180908" satisfied condition "success or failure"
Jan 23 22:57:15.943: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod downwardapi-volume-a13c69d3-80b5-4c25-baba-513e08180908 container client-container: <nil>
STEP: delete the pod
Jan 23 22:57:15.969: INFO: Waiting for pod downwardapi-volume-a13c69d3-80b5-4c25-baba-513e08180908 to disappear
Jan 23 22:57:15.976: INFO: Pod downwardapi-volume-a13c69d3-80b5-4c25-baba-513e08180908 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:57:15.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8487" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":280,"completed":182,"skipped":2869,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:57:16.002: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:57:19.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7655" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":280,"completed":183,"skipped":2912,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:57:19.127: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7041
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-7041
I0123 22:57:19.245040      21 runners.go:189] Created replication controller with name: externalname-service, namespace: services-7041, replica count: 2
Jan 23 22:57:22.305: INFO: Creating new exec pod
I0123 22:57:22.305156      21 runners.go:189] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 23 22:57:25.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 exec --namespace=services-7041 execpod569jr -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jan 23 22:57:25.853: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 23 22:57:25.853: INFO: stdout: ""
Jan 23 22:57:25.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 exec --namespace=services-7041 execpod569jr -- /bin/sh -x -c nc -zv -t -w 2 100.69.150.121 80'
Jan 23 22:57:26.140: INFO: stderr: "+ nc -zv -t -w 2 100.69.150.121 80\nConnection to 100.69.150.121 80 port [tcp/http] succeeded!\n"
Jan 23 22:57:26.141: INFO: stdout: ""
Jan 23 22:57:26.141: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:57:26.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7041" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:7.131 seconds]
[sig-network] Services
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":280,"completed":184,"skipped":2916,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:57:26.260: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Jan 23 22:57:26.340: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:57:30.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8918" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":280,"completed":185,"skipped":2934,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:57:30.218: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 23 22:57:30.717: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 23 22:57:33.469: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715417051, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715417051, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715417051, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715417051, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 23 22:57:36.598: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:57:36.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9123" for this suite.
STEP: Destroying namespace "webhook-9123-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.161 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":280,"completed":186,"skipped":2940,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:57:37.390: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-cd4edc5e-65ed-43fd-b4e5-c66de4238ceb
STEP: Creating a pod to test consume configMaps
Jan 23 22:57:37.587: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5ec89f10-6c16-4b2c-b8c2-056218e68189" in namespace "projected-5620" to be "success or failure"
Jan 23 22:57:37.599: INFO: Pod "pod-projected-configmaps-5ec89f10-6c16-4b2c-b8c2-056218e68189": Phase="Pending", Reason="", readiness=false. Elapsed: 11.522965ms
Jan 23 22:57:41.166: INFO: Pod "pod-projected-configmaps-5ec89f10-6c16-4b2c-b8c2-056218e68189": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018315182s
STEP: Saw pod success
Jan 23 22:57:41.166: INFO: Pod "pod-projected-configmaps-5ec89f10-6c16-4b2c-b8c2-056218e68189" satisfied condition "success or failure"
Jan 23 22:57:41.170: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod pod-projected-configmaps-5ec89f10-6c16-4b2c-b8c2-056218e68189 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 23 22:57:41.215: INFO: Waiting for pod pod-projected-configmaps-5ec89f10-6c16-4b2c-b8c2-056218e68189 to disappear
Jan 23 22:57:41.221: INFO: Pod pod-projected-configmaps-5ec89f10-6c16-4b2c-b8c2-056218e68189 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:57:41.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5620" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":280,"completed":187,"skipped":2978,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:57:41.248: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:57:41.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2696" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":280,"completed":188,"skipped":2990,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:57:41.329: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 23 22:57:42.332: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 23 22:57:44.352: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715417061, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715417061, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715417061, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715417061, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 23 22:57:47.414: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
Jan 23 22:57:48.496: INFO: Waiting for webhook configuration to be ready...
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:57:59.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2555" for this suite.
STEP: Destroying namespace "webhook-2555-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:18.546 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":280,"completed":189,"skipped":3032,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:57:59.888: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 23 22:58:00.065: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"70d0bf29-0b39-4371-82c2-be89906a4e7e", Controller:(*bool)(0xc005f57e2a), BlockOwnerDeletion:(*bool)(0xc005f57e2b)}}
Jan 23 22:58:00.102: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"c6d7e4d0-89bf-42f5-b179-29cc8463596c", Controller:(*bool)(0xc005f7f772), BlockOwnerDeletion:(*bool)(0xc005f7f773)}}
Jan 23 22:58:00.123: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"c296934c-2b4a-4cd2-bd01-9045824da60e", Controller:(*bool)(0xc005f57ff2), BlockOwnerDeletion:(*bool)(0xc005f57ff3)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:58:05.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8467" for this suite.

• [SLOW TEST:5.271 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":280,"completed":190,"skipped":3042,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:58:05.164: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:58:07.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5619" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":280,"completed":191,"skipped":3050,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:58:07.289: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:58:09.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-622" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":192,"skipped":3091,"failed":0}
SSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:58:09.414: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 23 22:58:09.459: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jan 23 22:58:11.517: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:58:12.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6760" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":280,"completed":193,"skipped":3096,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:58:12.804: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 23 22:58:15.240: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 23 22:58:17.251: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715417094, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715417094, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715417094, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715417094, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 23 22:58:20.271: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 23 22:58:20.281: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8249-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:58:21.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4675" for this suite.
STEP: Destroying namespace "webhook-4675-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.575 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":280,"completed":194,"skipped":3122,"failed":0}
S
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:58:21.974: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:46
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 22:58:22.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-5343" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":280,"completed":195,"skipped":3123,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 22:58:22.163: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-2328
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-2328
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2328
Jan 23 22:58:22.527: INFO: Found 0 stateful pods, waiting for 1
Jan 23 22:58:32.532: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jan 23 22:58:32.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 exec --namespace=statefulset-2328 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 23 22:58:33.733: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 23 22:58:33.735: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 23 22:58:33.735: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 23 22:58:33.785: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 23 22:58:43.792: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 23 22:58:43.792: INFO: Waiting for statefulset status.replicas updated to 0
Jan 23 22:58:43.817: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999334s
Jan 23 22:58:44.823: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.99360119s
Jan 23 22:58:45.829: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.9882546s
Jan 23 22:58:46.835: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.981370963s
Jan 23 22:58:49.461: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.975559033s
Jan 23 22:58:50.466: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.97034488s
Jan 23 22:58:51.470: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.965495885s
Jan 23 22:58:52.475: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.960875863s
Jan 23 22:58:53.481: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.955966327s
Jan 23 22:58:54.487: INFO: Verifying statefulset ss doesn't scale past 1 for another 950.421583ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2328
Jan 23 22:58:55.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 exec --namespace=statefulset-2328 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 23 22:58:55.848: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 23 22:58:55.848: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 23 22:58:55.848: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 23 22:58:55.853: INFO: Found 1 stateful pods, waiting for 3
Jan 23 22:59:05.866: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 23 22:59:05.866: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 23 22:59:05.866: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jan 23 22:59:05.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 exec --namespace=statefulset-2328 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 23 22:59:06.162: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 23 22:59:06.162: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 23 22:59:06.162: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 23 22:59:06.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 exec --namespace=statefulset-2328 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 23 22:59:06.431: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 23 22:59:06.431: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 23 22:59:06.431: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 23 22:59:06.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 exec --namespace=statefulset-2328 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 23 22:59:06.787: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 23 22:59:06.787: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 23 22:59:06.787: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 23 22:59:06.787: INFO: Waiting for statefulset status.replicas updated to 0
Jan 23 22:59:06.791: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jan 23 22:59:16.801: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 23 22:59:16.801: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 23 22:59:16.801: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 23 22:59:16.818: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999357s
Jan 23 22:59:17.825: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.991802568s
Jan 23 22:59:18.836: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.98511784s
Jan 23 22:59:19.842: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.973772246s
Jan 23 22:59:22.509: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.967662014s
Jan 23 22:59:23.515: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.961031214s
Jan 23 22:59:24.520: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.955676384s
Jan 23 22:59:25.527: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.950519177s
Jan 23 22:59:26.533: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.943423858s
Jan 23 22:59:27.547: INFO: Verifying statefulset ss doesn't scale past 3 for another 937.115367ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2328
Jan 23 22:59:28.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 exec --namespace=statefulset-2328 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 23 22:59:29.044: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 23 22:59:29.044: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 23 22:59:29.044: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 23 22:59:29.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 exec --namespace=statefulset-2328 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 23 22:59:29.339: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 23 22:59:29.339: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 23 22:59:29.339: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 23 22:59:29.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 exec --namespace=statefulset-2328 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 23 22:59:29.636: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 23 22:59:29.636: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 23 22:59:29.636: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 23 22:59:29.636: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jan 23 23:00:01.349: INFO: Deleting all statefulset in ns statefulset-2328
Jan 23 23:00:01.357: INFO: Scaling statefulset ss to 0
Jan 23 23:00:01.376: INFO: Waiting for statefulset status.replicas updated to 0
Jan 23 23:00:01.380: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:00:01.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2328" for this suite.

• [SLOW TEST:94.324 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":280,"completed":196,"skipped":3154,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:00:01.467: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jan 23 23:00:07.668: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 23 23:00:07.674: INFO: Pod pod-with-poststart-http-hook still exists
Jan 23 23:00:09.674: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 23 23:00:09.680: INFO: Pod pod-with-poststart-http-hook still exists
Jan 23 23:00:11.674: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 23 23:00:11.680: INFO: Pod pod-with-poststart-http-hook still exists
Jan 23 23:00:13.674: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 23 23:00:13.680: INFO: Pod pod-with-poststart-http-hook still exists
Jan 23 23:00:15.674: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 23 23:00:15.679: INFO: Pod pod-with-poststart-http-hook still exists
Jan 23 23:00:17.674: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 23 23:00:17.679: INFO: Pod pod-with-poststart-http-hook still exists
Jan 23 23:00:19.674: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 23 23:00:19.679: INFO: Pod pod-with-poststart-http-hook still exists
Jan 23 23:00:21.675: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 23 23:00:21.684: INFO: Pod pod-with-poststart-http-hook still exists
Jan 23 23:00:23.674: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 23 23:00:23.679: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:00:23.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5830" for this suite.

• [SLOW TEST:22.227 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":280,"completed":197,"skipped":3197,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:00:23.699: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1861
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jan 23 23:00:23.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-226'
Jan 23 23:00:24.223: INFO: stderr: ""
Jan 23 23:00:24.223: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1866
Jan 23 23:00:24.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 delete pods e2e-test-httpd-pod --namespace=kubectl-226'
Jan 23 23:00:39.646: INFO: stderr: ""
Jan 23 23:00:39.646: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:00:39.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-226" for this suite.

• [SLOW TEST:14.233 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1857
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":280,"completed":198,"skipped":3205,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:00:39.674: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir volume type on tmpfs
Jan 23 23:00:39.734: INFO: Waiting up to 5m0s for pod "pod-6a03fe81-ebd7-4d4f-85e5-93dd697cc571" in namespace "emptydir-2256" to be "success or failure"
Jan 23 23:00:39.741: INFO: Pod "pod-6a03fe81-ebd7-4d4f-85e5-93dd697cc571": Phase="Pending", Reason="", readiness=false. Elapsed: 6.58484ms
Jan 23 23:00:41.746: INFO: Pod "pod-6a03fe81-ebd7-4d4f-85e5-93dd697cc571": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012260764s
STEP: Saw pod success
Jan 23 23:00:41.746: INFO: Pod "pod-6a03fe81-ebd7-4d4f-85e5-93dd697cc571" satisfied condition "success or failure"
Jan 23 23:00:41.750: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod pod-6a03fe81-ebd7-4d4f-85e5-93dd697cc571 container test-container: <nil>
STEP: delete the pod
Jan 23 23:00:41.776: INFO: Waiting for pod pod-6a03fe81-ebd7-4d4f-85e5-93dd697cc571 to disappear
Jan 23 23:00:41.782: INFO: Pod pod-6a03fe81-ebd7-4d4f-85e5-93dd697cc571 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:00:41.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2256" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":199,"skipped":3226,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:00:41.801: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name secret-emptykey-test-e42c2742-5bd0-42fe-982f-c2ba7c2d53a5
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:00:41.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7548" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":280,"completed":200,"skipped":3251,"failed":0}

------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:00:41.869: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Jan 23 23:00:41.932: INFO: Waiting up to 5m0s for pod "downward-api-98aa1554-968d-49e3-a558-62d9eba19812" in namespace "downward-api-7100" to be "success or failure"
Jan 23 23:00:41.947: INFO: Pod "downward-api-98aa1554-968d-49e3-a558-62d9eba19812": Phase="Pending", Reason="", readiness=false. Elapsed: 13.963423ms
Jan 23 23:00:43.952: INFO: Pod "downward-api-98aa1554-968d-49e3-a558-62d9eba19812": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018988638s
STEP: Saw pod success
Jan 23 23:00:43.952: INFO: Pod "downward-api-98aa1554-968d-49e3-a558-62d9eba19812" satisfied condition "success or failure"
Jan 23 23:00:43.957: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod downward-api-98aa1554-968d-49e3-a558-62d9eba19812 container dapi-container: <nil>
STEP: delete the pod
Jan 23 23:00:43.983: INFO: Waiting for pod downward-api-98aa1554-968d-49e3-a558-62d9eba19812 to disappear
Jan 23 23:00:43.989: INFO: Pod downward-api-98aa1554-968d-49e3-a558-62d9eba19812 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:00:43.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7100" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":280,"completed":201,"skipped":3251,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:00:44.008: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 23 23:00:44.053: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Jan 23 23:00:47.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 --namespace=crd-publish-openapi-6830 create -f -'
Jan 23 23:00:48.471: INFO: stderr: ""
Jan 23 23:00:48.471: INFO: stdout: "e2e-test-crd-publish-openapi-8241-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 23 23:00:48.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 --namespace=crd-publish-openapi-6830 delete e2e-test-crd-publish-openapi-8241-crds test-foo'
Jan 23 23:00:48.592: INFO: stderr: ""
Jan 23 23:00:48.592: INFO: stdout: "e2e-test-crd-publish-openapi-8241-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jan 23 23:00:48.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 --namespace=crd-publish-openapi-6830 apply -f -'
Jan 23 23:00:48.947: INFO: stderr: ""
Jan 23 23:00:48.947: INFO: stdout: "e2e-test-crd-publish-openapi-8241-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 23 23:00:48.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 --namespace=crd-publish-openapi-6830 delete e2e-test-crd-publish-openapi-8241-crds test-foo'
Jan 23 23:00:49.043: INFO: stderr: ""
Jan 23 23:00:49.043: INFO: stdout: "e2e-test-crd-publish-openapi-8241-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Jan 23 23:00:49.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 --namespace=crd-publish-openapi-6830 create -f -'
Jan 23 23:00:49.226: INFO: rc: 1
Jan 23 23:00:49.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 --namespace=crd-publish-openapi-6830 apply -f -'
Jan 23 23:00:49.400: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Jan 23 23:00:49.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 --namespace=crd-publish-openapi-6830 create -f -'
Jan 23 23:00:49.514: INFO: rc: 1
Jan 23 23:00:49.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 --namespace=crd-publish-openapi-6830 apply -f -'
Jan 23 23:00:49.643: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Jan 23 23:00:49.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 explain e2e-test-crd-publish-openapi-8241-crds'
Jan 23 23:00:49.753: INFO: stderr: ""
Jan 23 23:00:49.753: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8241-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Jan 23 23:00:49.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 explain e2e-test-crd-publish-openapi-8241-crds.metadata'
Jan 23 23:00:49.856: INFO: stderr: ""
Jan 23 23:00:49.857: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8241-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jan 23 23:00:49.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 explain e2e-test-crd-publish-openapi-8241-crds.spec'
Jan 23 23:00:49.966: INFO: stderr: ""
Jan 23 23:00:49.966: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8241-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jan 23 23:00:49.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 explain e2e-test-crd-publish-openapi-8241-crds.spec.bars'
Jan 23 23:00:50.144: INFO: stderr: ""
Jan 23 23:00:50.144: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8241-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Jan 23 23:00:50.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 explain e2e-test-crd-publish-openapi-8241-crds.spec.bars2'
Jan 23 23:00:50.301: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:00:53.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6830" for this suite.

• [SLOW TEST:9.344 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":280,"completed":202,"skipped":3262,"failed":0}
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:00:53.351: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Jan 23 23:00:53.383: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 23 23:00:53.404: INFO: Waiting for terminating namespaces to be deleted...
Jan 23 23:00:53.407: INFO: 
Logging pods the kubelet thinks is on node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz before test
Jan 23 23:00:53.419: INFO: kube-proxy-qfmln from kube-system started at 2020-01-22 22:09:02 +0000 UTC (1 container statuses recorded)
Jan 23 23:00:53.419: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 23 23:00:53.419: INFO: sonobuoy-systemd-logs-daemon-set-82eb3c000b9d48f1-m9c8t from sonobuoy started at 2020-01-23 22:01:22 +0000 UTC (2 container statuses recorded)
Jan 23 23:00:53.419: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 23 23:00:53.419: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 23 23:00:53.419: INFO: vsphere-csi-node-4knv2 from kube-system started at 2020-01-23 22:13:07 +0000 UTC (3 container statuses recorded)
Jan 23 23:00:53.419: INFO: 	Container liveness-probe ready: true, restart count 0
Jan 23 23:00:53.419: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 23 23:00:53.419: INFO: 	Container vsphere-csi-node ready: true, restart count 0
Jan 23 23:00:53.419: INFO: calico-node-zvhsz from kube-system started at 2020-01-22 22:09:02 +0000 UTC (1 container statuses recorded)
Jan 23 23:00:53.419: INFO: 	Container calico-node ready: true, restart count 0
Jan 23 23:00:53.419: INFO: 
Logging pods the kubelet thinks is on node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f before test
Jan 23 23:00:53.449: INFO: kube-proxy-n9fnt from kube-system started at 2020-01-22 22:09:05 +0000 UTC (1 container statuses recorded)
Jan 23 23:00:53.449: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 23 23:00:53.449: INFO: sonobuoy-systemd-logs-daemon-set-82eb3c000b9d48f1-mkq6m from sonobuoy started at 2020-01-23 22:01:23 +0000 UTC (2 container statuses recorded)
Jan 23 23:00:53.449: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 23 23:00:53.449: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 23 23:00:53.449: INFO: calico-node-27bq2 from kube-system started at 2020-01-22 22:09:05 +0000 UTC (1 container statuses recorded)
Jan 23 23:00:53.449: INFO: 	Container calico-node ready: true, restart count 0
Jan 23 23:00:53.449: INFO: vsphere-csi-node-rd47x from kube-system started at 2020-01-22 22:09:27 +0000 UTC (3 container statuses recorded)
Jan 23 23:00:53.449: INFO: 	Container liveness-probe ready: true, restart count 0
Jan 23 23:00:53.449: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 23 23:00:53.449: INFO: 	Container vsphere-csi-node ready: true, restart count 0
Jan 23 23:00:53.449: INFO: sonobuoy from sonobuoy started at 2020-01-23 22:01:11 +0000 UTC (1 container statuses recorded)
Jan 23 23:00:53.449: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 23 23:00:53.449: INFO: 
Logging pods the kubelet thinks is on node management-cluster-1-17-2-md-0-5f64bb5777-v477d before test
Jan 23 23:00:53.482: INFO: calico-node-lbpft from kube-system started at 2020-01-22 22:09:03 +0000 UTC (1 container statuses recorded)
Jan 23 23:00:53.482: INFO: 	Container calico-node ready: true, restart count 0
Jan 23 23:00:53.482: INFO: sonobuoy-systemd-logs-daemon-set-82eb3c000b9d48f1-fwpld from sonobuoy started at 2020-01-23 22:01:22 +0000 UTC (2 container statuses recorded)
Jan 23 23:00:53.482: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 23 23:00:53.482: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 23 23:00:53.482: INFO: sonobuoy-e2e-job-61735b81990a421a from sonobuoy started at 2020-01-23 22:01:22 +0000 UTC (2 container statuses recorded)
Jan 23 23:00:53.482: INFO: 	Container e2e ready: true, restart count 0
Jan 23 23:00:53.482: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 23 23:00:53.482: INFO: kube-proxy-7w2h9 from kube-system started at 2020-01-22 22:09:03 +0000 UTC (1 container statuses recorded)
Jan 23 23:00:53.482: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 23 23:00:53.482: INFO: vsphere-csi-node-h9rth from kube-system started at 2020-01-22 22:09:25 +0000 UTC (3 container statuses recorded)
Jan 23 23:00:53.482: INFO: 	Container liveness-probe ready: true, restart count 0
Jan 23 23:00:53.482: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 23 23:00:53.482: INFO: 	Container vsphere-csi-node ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15eca6553a29dcb5], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match node selector.]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15eca6553c82cd77], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:00:55.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3353" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":280,"completed":203,"skipped":3267,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:00:56.091: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-c18adc1c-f178-4168-889b-df99360b1c51
STEP: Creating a pod to test consume secrets
Jan 23 23:00:56.166: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-abbada9a-6e05-4385-b651-2587a7d360b0" in namespace "projected-7402" to be "success or failure"
Jan 23 23:00:56.178: INFO: Pod "pod-projected-secrets-abbada9a-6e05-4385-b651-2587a7d360b0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.116762ms
Jan 23 23:00:58.204: INFO: Pod "pod-projected-secrets-abbada9a-6e05-4385-b651-2587a7d360b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.03773934s
STEP: Saw pod success
Jan 23 23:00:58.204: INFO: Pod "pod-projected-secrets-abbada9a-6e05-4385-b651-2587a7d360b0" satisfied condition "success or failure"
Jan 23 23:00:58.212: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod pod-projected-secrets-abbada9a-6e05-4385-b651-2587a7d360b0 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 23 23:00:58.248: INFO: Waiting for pod pod-projected-secrets-abbada9a-6e05-4385-b651-2587a7d360b0 to disappear
Jan 23 23:00:58.256: INFO: Pod pod-projected-secrets-abbada9a-6e05-4385-b651-2587a7d360b0 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:00:58.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7402" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":204,"skipped":3275,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:00:58.267: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:01:02.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2900" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":205,"skipped":3292,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:01:02.429: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:01:15.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9720" for this suite.

• [SLOW TEST:11.229 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":280,"completed":206,"skipped":3303,"failed":0}
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:01:15.410: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-projected-b5w8
STEP: Creating a pod to test atomic-volume-subpath
Jan 23 23:01:15.548: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-b5w8" in namespace "subpath-367" to be "success or failure"
Jan 23 23:01:15.591: INFO: Pod "pod-subpath-test-projected-b5w8": Phase="Pending", Reason="", readiness=false. Elapsed: 43.101394ms
Jan 23 23:01:17.595: INFO: Pod "pod-subpath-test-projected-b5w8": Phase="Running", Reason="", readiness=true. Elapsed: 2.047658545s
Jan 23 23:01:19.600: INFO: Pod "pod-subpath-test-projected-b5w8": Phase="Running", Reason="", readiness=true. Elapsed: 4.052099645s
Jan 23 23:01:21.608: INFO: Pod "pod-subpath-test-projected-b5w8": Phase="Running", Reason="", readiness=true. Elapsed: 6.060048446s
Jan 23 23:01:23.612: INFO: Pod "pod-subpath-test-projected-b5w8": Phase="Running", Reason="", readiness=true. Elapsed: 8.064355696s
Jan 23 23:01:25.618: INFO: Pod "pod-subpath-test-projected-b5w8": Phase="Running", Reason="", readiness=true. Elapsed: 10.069936632s
Jan 23 23:01:27.624: INFO: Pod "pod-subpath-test-projected-b5w8": Phase="Running", Reason="", readiness=true. Elapsed: 12.075993278s
Jan 23 23:01:29.629: INFO: Pod "pod-subpath-test-projected-b5w8": Phase="Running", Reason="", readiness=true. Elapsed: 14.081185497s
Jan 23 23:01:31.633: INFO: Pod "pod-subpath-test-projected-b5w8": Phase="Running", Reason="", readiness=true. Elapsed: 16.085692581s
Jan 23 23:01:33.639: INFO: Pod "pod-subpath-test-projected-b5w8": Phase="Running", Reason="", readiness=true. Elapsed: 18.091140203s
Jan 23 23:01:35.644: INFO: Pod "pod-subpath-test-projected-b5w8": Phase="Running", Reason="", readiness=true. Elapsed: 20.09595262s
Jan 23 23:01:39.464: INFO: Pod "pod-subpath-test-projected-b5w8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.105878468s
STEP: Saw pod success
Jan 23 23:01:39.464: INFO: Pod "pod-subpath-test-projected-b5w8" satisfied condition "success or failure"
Jan 23 23:01:39.470: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f pod pod-subpath-test-projected-b5w8 container test-container-subpath-projected-b5w8: <nil>
STEP: delete the pod
Jan 23 23:01:39.517: INFO: Waiting for pod pod-subpath-test-projected-b5w8 to disappear
Jan 23 23:01:39.529: INFO: Pod pod-subpath-test-projected-b5w8 no longer exists
STEP: Deleting pod pod-subpath-test-projected-b5w8
Jan 23 23:01:39.530: INFO: Deleting pod "pod-subpath-test-projected-b5w8" in namespace "subpath-367"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:01:39.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-367" for this suite.

• [SLOW TEST:22.340 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":280,"completed":207,"skipped":3308,"failed":0}
SSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:01:39.563: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test hostPath mode
Jan 23 23:01:39.697: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-4033" to be "success or failure"
Jan 23 23:01:39.724: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 26.346071ms
Jan 23 23:01:41.729: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03143229s
Jan 23 23:01:43.733: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035998063s
STEP: Saw pod success
Jan 23 23:01:43.733: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Jan 23 23:01:43.737: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Jan 23 23:01:43.783: INFO: Waiting for pod pod-host-path-test to disappear
Jan 23 23:01:43.788: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:01:43.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-4033" for this suite.
•{"msg":"PASSED [sig-storage] HostPath should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":208,"skipped":3311,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:01:43.816: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Update Demo
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:330
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a replication controller
Jan 23 23:01:43.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 create -f - --namespace=kubectl-9046'
Jan 23 23:01:44.204: INFO: stderr: ""
Jan 23 23:01:44.204: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 23 23:01:44.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9046'
Jan 23 23:01:44.297: INFO: stderr: ""
Jan 23 23:01:44.297: INFO: stdout: "update-demo-nautilus-9jvth update-demo-nautilus-hrp4z "
Jan 23 23:01:44.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get pods update-demo-nautilus-9jvth -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9046'
Jan 23 23:01:44.376: INFO: stderr: ""
Jan 23 23:01:44.376: INFO: stdout: ""
Jan 23 23:01:44.376: INFO: update-demo-nautilus-9jvth is created but not running
Jan 23 23:01:49.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9046'
Jan 23 23:01:49.500: INFO: stderr: ""
Jan 23 23:01:49.500: INFO: stdout: "update-demo-nautilus-9jvth update-demo-nautilus-hrp4z "
Jan 23 23:01:49.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get pods update-demo-nautilus-9jvth -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9046'
Jan 23 23:01:49.667: INFO: stderr: ""
Jan 23 23:01:49.667: INFO: stdout: "true"
Jan 23 23:01:49.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get pods update-demo-nautilus-9jvth -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9046'
Jan 23 23:01:49.883: INFO: stderr: ""
Jan 23 23:01:49.883: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 23 23:01:49.883: INFO: validating pod update-demo-nautilus-9jvth
Jan 23 23:01:49.929: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 23 23:01:49.929: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 23 23:01:49.930: INFO: update-demo-nautilus-9jvth is verified up and running
Jan 23 23:01:49.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get pods update-demo-nautilus-hrp4z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9046'
Jan 23 23:01:50.025: INFO: stderr: ""
Jan 23 23:01:50.025: INFO: stdout: "true"
Jan 23 23:01:50.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get pods update-demo-nautilus-hrp4z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9046'
Jan 23 23:01:50.104: INFO: stderr: ""
Jan 23 23:01:50.104: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 23 23:01:50.104: INFO: validating pod update-demo-nautilus-hrp4z
Jan 23 23:01:50.114: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 23 23:01:50.115: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 23 23:01:50.115: INFO: update-demo-nautilus-hrp4z is verified up and running
STEP: scaling down the replication controller
Jan 23 23:01:50.121: INFO: scanned /root for discovery docs: <nil>
Jan 23 23:01:50.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-9046'
Jan 23 23:01:51.224: INFO: stderr: ""
Jan 23 23:01:51.224: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 23 23:01:51.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9046'
Jan 23 23:01:51.308: INFO: stderr: ""
Jan 23 23:01:51.308: INFO: stdout: "update-demo-nautilus-9jvth update-demo-nautilus-hrp4z "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jan 23 23:01:56.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9046'
Jan 23 23:01:56.396: INFO: stderr: ""
Jan 23 23:01:56.396: INFO: stdout: "update-demo-nautilus-9jvth update-demo-nautilus-hrp4z "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jan 23 23:02:01.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9046'
Jan 23 23:02:01.476: INFO: stderr: ""
Jan 23 23:02:01.476: INFO: stdout: "update-demo-nautilus-9jvth "
Jan 23 23:02:01.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get pods update-demo-nautilus-9jvth -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9046'
Jan 23 23:02:01.551: INFO: stderr: ""
Jan 23 23:02:01.551: INFO: stdout: "true"
Jan 23 23:02:01.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get pods update-demo-nautilus-9jvth -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9046'
Jan 23 23:02:01.623: INFO: stderr: ""
Jan 23 23:02:01.623: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 23 23:02:01.623: INFO: validating pod update-demo-nautilus-9jvth
Jan 23 23:02:01.628: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 23 23:02:01.628: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 23 23:02:01.628: INFO: update-demo-nautilus-9jvth is verified up and running
STEP: scaling up the replication controller
Jan 23 23:02:01.634: INFO: scanned /root for discovery docs: <nil>
Jan 23 23:02:01.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-9046'
Jan 23 23:02:02.746: INFO: stderr: ""
Jan 23 23:02:02.746: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 23 23:02:02.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9046'
Jan 23 23:02:02.840: INFO: stderr: ""
Jan 23 23:02:02.840: INFO: stdout: "update-demo-nautilus-9jvth update-demo-nautilus-dtmp4 "
Jan 23 23:02:02.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get pods update-demo-nautilus-9jvth -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9046'
Jan 23 23:02:02.932: INFO: stderr: ""
Jan 23 23:02:02.932: INFO: stdout: "true"
Jan 23 23:02:02.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get pods update-demo-nautilus-9jvth -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9046'
Jan 23 23:02:03.007: INFO: stderr: ""
Jan 23 23:02:03.007: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 23 23:02:03.007: INFO: validating pod update-demo-nautilus-9jvth
Jan 23 23:02:03.012: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 23 23:02:03.012: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 23 23:02:03.012: INFO: update-demo-nautilus-9jvth is verified up and running
Jan 23 23:02:03.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get pods update-demo-nautilus-dtmp4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9046'
Jan 23 23:02:03.092: INFO: stderr: ""
Jan 23 23:02:03.093: INFO: stdout: "true"
Jan 23 23:02:03.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get pods update-demo-nautilus-dtmp4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9046'
Jan 23 23:02:03.176: INFO: stderr: ""
Jan 23 23:02:03.176: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 23 23:02:03.176: INFO: validating pod update-demo-nautilus-dtmp4
Jan 23 23:02:03.183: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 23 23:02:03.183: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 23 23:02:03.183: INFO: update-demo-nautilus-dtmp4 is verified up and running
STEP: using delete to clean up resources
Jan 23 23:02:03.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 delete --grace-period=0 --force -f - --namespace=kubectl-9046'
Jan 23 23:02:03.281: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 23 23:02:03.281: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 23 23:02:03.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9046'
Jan 23 23:02:03.381: INFO: stderr: "No resources found in kubectl-9046 namespace.\n"
Jan 23 23:02:03.381: INFO: stdout: ""
Jan 23 23:02:03.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get pods -l name=update-demo --namespace=kubectl-9046 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 23 23:02:03.468: INFO: stderr: ""
Jan 23 23:02:03.468: INFO: stdout: "update-demo-nautilus-9jvth\nupdate-demo-nautilus-dtmp4\n"
Jan 23 23:02:03.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9046'
Jan 23 23:02:04.060: INFO: stderr: "No resources found in kubectl-9046 namespace.\n"
Jan 23 23:02:04.060: INFO: stdout: ""
Jan 23 23:02:04.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get pods -l name=update-demo --namespace=kubectl-9046 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 23 23:02:04.154: INFO: stderr: ""
Jan 23 23:02:04.154: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:02:04.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9046" for this suite.

• [SLOW TEST:20.353 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:328
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":280,"completed":209,"skipped":3326,"failed":0}
SSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:02:04.170: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jan 23 23:02:04.244: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9960 /api/v1/namespaces/watch-9960/configmaps/e2e-watch-test-label-changed 5d44d4dc-e3a4-41b5-baf0-29e4683a5b55 424068 0 2020-01-23 23:02:05 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 23 23:02:04.246: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9960 /api/v1/namespaces/watch-9960/configmaps/e2e-watch-test-label-changed 5d44d4dc-e3a4-41b5-baf0-29e4683a5b55 424069 0 2020-01-23 23:02:05 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jan 23 23:02:04.246: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9960 /api/v1/namespaces/watch-9960/configmaps/e2e-watch-test-label-changed 5d44d4dc-e3a4-41b5-baf0-29e4683a5b55 424070 0 2020-01-23 23:02:05 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jan 23 23:02:16.070: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9960 /api/v1/namespaces/watch-9960/configmaps/e2e-watch-test-label-changed 5d44d4dc-e3a4-41b5-baf0-29e4683a5b55 424148 0 2020-01-23 23:02:05 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 23 23:02:16.072: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9960 /api/v1/namespaces/watch-9960/configmaps/e2e-watch-test-label-changed 5d44d4dc-e3a4-41b5-baf0-29e4683a5b55 424149 0 2020-01-23 23:02:05 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Jan 23 23:02:16.072: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9960 /api/v1/namespaces/watch-9960/configmaps/e2e-watch-test-label-changed 5d44d4dc-e3a4-41b5-baf0-29e4683a5b55 424150 0 2020-01-23 23:02:05 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:02:16.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9960" for this suite.

• [SLOW TEST:10.130 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":280,"completed":210,"skipped":3329,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:02:16.095: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Jan 23 23:02:57.562: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:02:57.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0123 23:02:57.561432      21 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-2140" for this suite.

• [SLOW TEST:40.132 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":280,"completed":211,"skipped":3338,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:02:57.587: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:03:02.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5586" for this suite.

• [SLOW TEST:5.164 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":280,"completed":212,"skipped":3364,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Lease
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:03:02.757: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Lease
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:03:02.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-3583" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":280,"completed":213,"skipped":3376,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:03:02.932: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl run job
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1788
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jan 23 23:03:03.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-9166'
Jan 23 23:03:03.194: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan 23 23:03:03.194: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1793
Jan 23 23:03:03.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 delete jobs e2e-test-httpd-job --namespace=kubectl-9166'
Jan 23 23:03:03.364: INFO: stderr: ""
Jan 23 23:03:03.365: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:03:03.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9166" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run job should create a job from an image when restart is OnFailure  [Conformance]","total":280,"completed":214,"skipped":3452,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:03:03.404: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:03:36.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4715" for this suite.
STEP: Destroying namespace "nsdeletetest-1266" for this suite.
Jan 23 23:03:36.082: INFO: Namespace nsdeletetest-1266 was already deleted
STEP: Destroying namespace "nsdeletetest-2257" for this suite.

• [SLOW TEST:31.297 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":280,"completed":215,"skipped":3467,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:03:36.094: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 23 23:03:36.175: INFO: (0) /api/v1/nodes/management-cluster-1-17-2-md-0-5f64bb5777-v477d/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init-output.log">cloud-init-output.log</a>
<a href="... (200; 36.385388ms)
Jan 23 23:03:36.180: INFO: (1) /api/v1/nodes/management-cluster-1-17-2-md-0-5f64bb5777-v477d/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init-output.log">cloud-init-output.log</a>
<a href="... (200; 4.729489ms)
Jan 23 23:03:36.187: INFO: (2) /api/v1/nodes/management-cluster-1-17-2-md-0-5f64bb5777-v477d/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init-output.log">cloud-init-output.log</a>
<a href="... (200; 7.444558ms)
Jan 23 23:03:36.193: INFO: (3) /api/v1/nodes/management-cluster-1-17-2-md-0-5f64bb5777-v477d/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init-output.log">cloud-init-output.log</a>
<a href="... (200; 6.060251ms)
Jan 23 23:03:36.200: INFO: (4) /api/v1/nodes/management-cluster-1-17-2-md-0-5f64bb5777-v477d/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init-output.log">cloud-init-output.log</a>
<a href="... (200; 6.787729ms)
Jan 23 23:03:36.206: INFO: (5) /api/v1/nodes/management-cluster-1-17-2-md-0-5f64bb5777-v477d/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init-output.log">cloud-init-output.log</a>
<a href="... (200; 5.576356ms)
Jan 23 23:03:36.212: INFO: (6) /api/v1/nodes/management-cluster-1-17-2-md-0-5f64bb5777-v477d/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init-output.log">cloud-init-output.log</a>
<a href="... (200; 6.176226ms)
Jan 23 23:03:36.217: INFO: (7) /api/v1/nodes/management-cluster-1-17-2-md-0-5f64bb5777-v477d/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init-output.log">cloud-init-output.log</a>
<a href="... (200; 4.776631ms)
Jan 23 23:03:36.222: INFO: (8) /api/v1/nodes/management-cluster-1-17-2-md-0-5f64bb5777-v477d/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init-output.log">cloud-init-output.log</a>
<a href="... (200; 5.583175ms)
Jan 23 23:03:36.228: INFO: (9) /api/v1/nodes/management-cluster-1-17-2-md-0-5f64bb5777-v477d/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init-output.log">cloud-init-output.log</a>
<a href="... (200; 5.757093ms)
Jan 23 23:03:36.233: INFO: (10) /api/v1/nodes/management-cluster-1-17-2-md-0-5f64bb5777-v477d/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init-output.log">cloud-init-output.log</a>
<a href="... (200; 4.218083ms)
Jan 23 23:03:36.237: INFO: (11) /api/v1/nodes/management-cluster-1-17-2-md-0-5f64bb5777-v477d/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init-output.log">cloud-init-output.log</a>
<a href="... (200; 4.5605ms)
Jan 23 23:03:36.243: INFO: (12) /api/v1/nodes/management-cluster-1-17-2-md-0-5f64bb5777-v477d/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init-output.log">cloud-init-output.log</a>
<a href="... (200; 5.231187ms)
Jan 23 23:03:36.248: INFO: (13) /api/v1/nodes/management-cluster-1-17-2-md-0-5f64bb5777-v477d/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init-output.log">cloud-init-output.log</a>
<a href="... (200; 5.138799ms)
Jan 23 23:03:36.253: INFO: (14) /api/v1/nodes/management-cluster-1-17-2-md-0-5f64bb5777-v477d/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init-output.log">cloud-init-output.log</a>
<a href="... (200; 4.775377ms)
Jan 23 23:03:36.258: INFO: (15) /api/v1/nodes/management-cluster-1-17-2-md-0-5f64bb5777-v477d/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init-output.log">cloud-init-output.log</a>
<a href="... (200; 4.709539ms)
Jan 23 23:03:36.262: INFO: (16) /api/v1/nodes/management-cluster-1-17-2-md-0-5f64bb5777-v477d/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init-output.log">cloud-init-output.log</a>
<a href="... (200; 4.881018ms)
Jan 23 23:03:36.268: INFO: (17) /api/v1/nodes/management-cluster-1-17-2-md-0-5f64bb5777-v477d/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init-output.log">cloud-init-output.log</a>
<a href="... (200; 5.159719ms)
Jan 23 23:03:36.272: INFO: (18) /api/v1/nodes/management-cluster-1-17-2-md-0-5f64bb5777-v477d/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init-output.log">cloud-init-output.log</a>
<a href="... (200; 4.652687ms)
Jan 23 23:03:36.278: INFO: (19) /api/v1/nodes/management-cluster-1-17-2-md-0-5f64bb5777-v477d/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init-output.log">cloud-init-output.log</a>
<a href="... (200; 5.010717ms)
[AfterEach] version v1
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:03:36.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2526" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node using proxy subresource  [Conformance]","total":280,"completed":216,"skipped":3480,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:03:36.291: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jan 23 23:03:36.341: INFO: Waiting up to 5m0s for pod "pod-f0733045-91f9-4f25-bb65-6c898ed2f0ce" in namespace "emptydir-9851" to be "success or failure"
Jan 23 23:03:36.351: INFO: Pod "pod-f0733045-91f9-4f25-bb65-6c898ed2f0ce": Phase="Pending", Reason="", readiness=false. Elapsed: 9.004093ms
Jan 23 23:03:38.356: INFO: Pod "pod-f0733045-91f9-4f25-bb65-6c898ed2f0ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013790369s
STEP: Saw pod success
Jan 23 23:03:38.356: INFO: Pod "pod-f0733045-91f9-4f25-bb65-6c898ed2f0ce" satisfied condition "success or failure"
Jan 23 23:03:38.360: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod pod-f0733045-91f9-4f25-bb65-6c898ed2f0ce container test-container: <nil>
STEP: delete the pod
Jan 23 23:03:38.446: INFO: Waiting for pod pod-f0733045-91f9-4f25-bb65-6c898ed2f0ce to disappear
Jan 23 23:03:38.451: INFO: Pod pod-f0733045-91f9-4f25-bb65-6c898ed2f0ce no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:03:38.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9851" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":217,"skipped":3506,"failed":0}
SS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:03:38.464: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:46
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Jan 23 23:03:40.539: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-837620349 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Jan 23 23:03:45.658: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:03:45.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9950" for this suite.

• [SLOW TEST:7.215 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should be submitted and removed [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period should be submitted and removed [Conformance]","total":280,"completed":218,"skipped":3508,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:03:45.681: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod liveness-44c19f00-648e-48be-930c-520d4f538bbd in namespace container-probe-1553
Jan 23 23:03:47.770: INFO: Started pod liveness-44c19f00-648e-48be-930c-520d4f538bbd in namespace container-probe-1553
STEP: checking the pod's current state and verifying that restartCount is present
Jan 23 23:03:47.779: INFO: Initial restart count of pod liveness-44c19f00-648e-48be-930c-520d4f538bbd is 0
Jan 23 23:04:07.366: INFO: Restart count of pod container-probe-1553/liveness-44c19f00-648e-48be-930c-520d4f538bbd is now 1 (18.045620032s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:04:07.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1553" for this suite.

• [SLOW TEST:20.180 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":280,"completed":219,"skipped":3523,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:04:07.406: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-6701
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 23 23:04:07.450: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jan 23 23:04:29.156: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.117.222.224 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6701 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 23 23:04:29.156: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
Jan 23 23:04:30.410: INFO: Found all expected endpoints: [netserver-0]
Jan 23 23:04:30.414: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.120.200.54 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6701 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 23 23:04:30.414: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
Jan 23 23:04:31.577: INFO: Found all expected endpoints: [netserver-1]
Jan 23 23:04:31.580: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.105.119.79 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6701 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 23 23:04:31.580: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
Jan 23 23:04:32.751: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:04:32.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6701" for this suite.

• [SLOW TEST:23.813 seconds]
[sig-network] Networking
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":220,"skipped":3537,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:04:32.774: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 23 23:04:32.835: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a9b7e11f-215a-4a95-af5b-4ecf34c8ab62" in namespace "downward-api-2016" to be "success or failure"
Jan 23 23:04:32.842: INFO: Pod "downwardapi-volume-a9b7e11f-215a-4a95-af5b-4ecf34c8ab62": Phase="Pending", Reason="", readiness=false. Elapsed: 6.272656ms
Jan 23 23:04:34.923: INFO: Pod "downwardapi-volume-a9b7e11f-215a-4a95-af5b-4ecf34c8ab62": Phase="Pending", Reason="", readiness=false. Elapsed: 2.087899057s
Jan 23 23:04:36.942: INFO: Pod "downwardapi-volume-a9b7e11f-215a-4a95-af5b-4ecf34c8ab62": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.107049742s
STEP: Saw pod success
Jan 23 23:04:36.943: INFO: Pod "downwardapi-volume-a9b7e11f-215a-4a95-af5b-4ecf34c8ab62" satisfied condition "success or failure"
Jan 23 23:04:36.948: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod downwardapi-volume-a9b7e11f-215a-4a95-af5b-4ecf34c8ab62 container client-container: <nil>
STEP: delete the pod
Jan 23 23:04:37.039: INFO: Waiting for pod downwardapi-volume-a9b7e11f-215a-4a95-af5b-4ecf34c8ab62 to disappear
Jan 23 23:04:37.049: INFO: Pod downwardapi-volume-a9b7e11f-215a-4a95-af5b-4ecf34c8ab62 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:04:37.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2016" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":221,"skipped":3564,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:04:37.125: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Jan 23 23:04:47.278: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
W0123 23:04:47.277453      21 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jan 23 23:04:47.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7" for this suite.

• [SLOW TEST:10.169 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":280,"completed":222,"skipped":3575,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:04:47.299: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-c3346847-1301-4767-b6f4-10535746131e
STEP: Creating a pod to test consume secrets
Jan 23 23:04:47.400: INFO: Waiting up to 5m0s for pod "pod-secrets-19f80c7c-ab73-405c-a56d-2d7fc0864ddc" in namespace "secrets-6220" to be "success or failure"
Jan 23 23:04:47.410: INFO: Pod "pod-secrets-19f80c7c-ab73-405c-a56d-2d7fc0864ddc": Phase="Pending", Reason="", readiness=false. Elapsed: 10.602031ms
Jan 23 23:04:49.420: INFO: Pod "pod-secrets-19f80c7c-ab73-405c-a56d-2d7fc0864ddc": Phase="Running", Reason="", readiness=true. Elapsed: 2.019953335s
Jan 23 23:04:51.426: INFO: Pod "pod-secrets-19f80c7c-ab73-405c-a56d-2d7fc0864ddc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02644239s
STEP: Saw pod success
Jan 23 23:04:51.426: INFO: Pod "pod-secrets-19f80c7c-ab73-405c-a56d-2d7fc0864ddc" satisfied condition "success or failure"
Jan 23 23:04:51.431: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod pod-secrets-19f80c7c-ab73-405c-a56d-2d7fc0864ddc container secret-env-test: <nil>
STEP: delete the pod
Jan 23 23:04:51.486: INFO: Waiting for pod pod-secrets-19f80c7c-ab73-405c-a56d-2d7fc0864ddc to disappear
Jan 23 23:04:51.492: INFO: Pod pod-secrets-19f80c7c-ab73-405c-a56d-2d7fc0864ddc no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:04:51.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6220" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":280,"completed":223,"skipped":3582,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:04:51.530: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Jan 23 23:04:51.596: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the sample API server.
Jan 23 23:04:52.546: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Jan 23 23:05:01.550: INFO: Waited 5.257531205s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:05:02.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-8000" for this suite.

• [SLOW TEST:9.305 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]","total":280,"completed":224,"skipped":3612,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:05:02.403: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jan 23 23:05:04.599: INFO: &Pod{ObjectMeta:{send-events-7dd26426-cea6-4919-8660-949c7d16825c  events-6462 /api/v1/namespaces/events-6462/pods/send-events-7dd26426-cea6-4919-8660-949c7d16825c 3fb41e75-671f-4924-86b6-594f05e7ba52 425805 0 2020-01-23 23:05:01 +0000 UTC <nil> <nil> map[name:foo time:559095160] map[cni.projectcalico.org/podIP:100.117.222.227/32] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5bjng,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5bjng,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5bjng,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:management-cluster-1-17-2-md-0-5f64bb5777-dlwcz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 23:05:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 23:05:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 23:05:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 23:05:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.220.23,PodIP:100.117.222.227,StartTime:2020-01-23 23:05:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-23 23:05:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:containerd://24a9dee00c051f9c8f1fe07ce69374e9d001b10e97385ff3c0136618593fa3f8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.117.222.227,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Jan 23 23:05:06.614: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jan 23 23:05:08.619: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:05:08.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6462" for this suite.

• [SLOW TEST:6.240 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":280,"completed":225,"skipped":3620,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:05:08.645: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 23 23:05:08.691: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jan 23 23:05:13.708: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 23 23:05:13.708: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Jan 23 23:05:13.741: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-8038 /apis/apps/v1/namespaces/deployment-8038/deployments/test-cleanup-deployment aac257be-582b-44fb-aaae-a40f63dae848 425899 1 2020-01-23 23:05:12 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00663d568 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Jan 23 23:05:13.764: INFO: New ReplicaSet "test-cleanup-deployment-55ffc6b7b6" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-55ffc6b7b6  deployment-8038 /apis/apps/v1/namespaces/deployment-8038/replicasets/test-cleanup-deployment-55ffc6b7b6 70e0315c-2272-4800-9c38-20dda01804ba 425901 1 2020-01-23 23:05:12 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:55ffc6b7b6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment aac257be-582b-44fb-aaae-a40f63dae848 0xc00663db47 0xc00663db48}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55ffc6b7b6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:55ffc6b7b6] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00663dbb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 23 23:05:13.764: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jan 23 23:05:13.764: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-8038 /apis/apps/v1/namespaces/deployment-8038/replicasets/test-cleanup-controller bbf061ef-d455-4406-905e-bc27ac404cf0 425900 1 2020-01-23 23:05:07 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment aac257be-582b-44fb-aaae-a40f63dae848 0xc00663da77 0xc00663da78}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00663dad8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 23 23:05:13.777: INFO: Pod "test-cleanup-controller-dvcwv" is available:
&Pod{ObjectMeta:{test-cleanup-controller-dvcwv test-cleanup-controller- deployment-8038 /api/v1/namespaces/deployment-8038/pods/test-cleanup-controller-dvcwv 07d9cc04-e3bd-45ad-a7be-31f4f985ac59 425875 0 2020-01-23 23:05:07 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/podIP:100.117.222.232/32] [{apps/v1 ReplicaSet test-cleanup-controller bbf061ef-d455-4406-905e-bc27ac404cf0 0xc006634277 0xc006634278}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4pd7n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4pd7n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4pd7n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:management-cluster-1-17-2-md-0-5f64bb5777-dlwcz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 23:05:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 23:05:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 23:05:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 23:05:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.220.23,PodIP:100.117.222.232,StartTime:2020-01-23 23:05:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-23 23:05:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://f0a4dca6c2b9e7f3191511fdccd4fad875895d69aea57ae2ebf520a677976f3b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.117.222.232,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 23 23:05:13.777: INFO: Pod "test-cleanup-deployment-55ffc6b7b6-t8q7v" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-55ffc6b7b6-t8q7v test-cleanup-deployment-55ffc6b7b6- deployment-8038 /api/v1/namespaces/deployment-8038/pods/test-cleanup-deployment-55ffc6b7b6-t8q7v f48735b6-9ae6-4eb2-a04a-2d14ed40b453 425905 0 2020-01-23 23:05:12 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:55ffc6b7b6] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-55ffc6b7b6 70e0315c-2272-4800-9c38-20dda01804ba 0xc0066343f7 0xc0066343f8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4pd7n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4pd7n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4pd7n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:management-cluster-1-17-2-md-0-5f64bb5777-fsd6f,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-23 23:05:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:05:13.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8038" for this suite.

• [SLOW TEST:5.175 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":280,"completed":226,"skipped":3629,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:05:13.825: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating pod
Jan 23 23:05:15.924: INFO: Pod pod-hostip-cbc6074a-d30d-4e33-a96a-2f7bcec0b232 has hostIP: 10.78.220.23
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:05:15.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6495" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":280,"completed":227,"skipped":3644,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:05:15.943: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:125
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jan 23 23:05:16.310: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jan 23 23:05:18.326: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715417517, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715417517, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715417517, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715417516, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 23 23:05:21.346: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 23 23:05:21.353: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:05:22.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5314" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:136

• [SLOW TEST:7.084 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":280,"completed":228,"skipped":3693,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:05:23.067: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 23 23:05:23.150: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d5f06d6d-3ee6-416e-9850-48c2d93ba01a" in namespace "projected-7800" to be "success or failure"
Jan 23 23:05:23.177: INFO: Pod "downwardapi-volume-d5f06d6d-3ee6-416e-9850-48c2d93ba01a": Phase="Pending", Reason="", readiness=false. Elapsed: 26.50491ms
Jan 23 23:05:25.190: INFO: Pod "downwardapi-volume-d5f06d6d-3ee6-416e-9850-48c2d93ba01a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.039784094s
STEP: Saw pod success
Jan 23 23:05:25.190: INFO: Pod "downwardapi-volume-d5f06d6d-3ee6-416e-9850-48c2d93ba01a" satisfied condition "success or failure"
Jan 23 23:05:25.209: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f pod downwardapi-volume-d5f06d6d-3ee6-416e-9850-48c2d93ba01a container client-container: <nil>
STEP: delete the pod
Jan 23 23:05:25.416: INFO: Waiting for pod downwardapi-volume-d5f06d6d-3ee6-416e-9850-48c2d93ba01a to disappear
Jan 23 23:05:25.508: INFO: Pod downwardapi-volume-d5f06d6d-3ee6-416e-9850-48c2d93ba01a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:05:25.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7800" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":280,"completed":229,"skipped":3715,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:05:25.564: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-e4acdaa9-a3d3-409d-8dee-cb76e8a1655e
STEP: Creating a pod to test consume configMaps
Jan 23 23:05:25.677: INFO: Waiting up to 5m0s for pod "pod-configmaps-4a57014f-75a0-4777-bbe2-96b6f4d2fc89" in namespace "configmap-8764" to be "success or failure"
Jan 23 23:05:25.699: INFO: Pod "pod-configmaps-4a57014f-75a0-4777-bbe2-96b6f4d2fc89": Phase="Pending", Reason="", readiness=false. Elapsed: 21.901696ms
Jan 23 23:05:27.704: INFO: Pod "pod-configmaps-4a57014f-75a0-4777-bbe2-96b6f4d2fc89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026475341s
STEP: Saw pod success
Jan 23 23:05:27.704: INFO: Pod "pod-configmaps-4a57014f-75a0-4777-bbe2-96b6f4d2fc89" satisfied condition "success or failure"
Jan 23 23:05:27.710: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f pod pod-configmaps-4a57014f-75a0-4777-bbe2-96b6f4d2fc89 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 23 23:05:27.742: INFO: Waiting for pod pod-configmaps-4a57014f-75a0-4777-bbe2-96b6f4d2fc89 to disappear
Jan 23 23:05:27.766: INFO: Pod pod-configmaps-4a57014f-75a0-4777-bbe2-96b6f4d2fc89 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:05:27.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8764" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":230,"skipped":3739,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:05:27.808: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:163
Jan 23 23:05:27.889: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 23 23:06:31.145: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 23 23:06:31.156: INFO: Starting informer...
STEP: Starting pod...
Jan 23 23:06:31.394: INFO: Pod is running on management-cluster-1-17-2-md-0-5f64bb5777-dlwcz. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Jan 23 23:06:31.448: INFO: Pod wasn't evicted. Proceeding
Jan 23 23:06:31.448: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Jan 23 23:07:51.689: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:07:51.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-8028" for this suite.

• [SLOW TEST:135.625 seconds]
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":280,"completed":231,"skipped":3759,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:07:51.733: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Jan 23 23:07:51.808: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Jan 23 23:08:07.851: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
Jan 23 23:08:11.542: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:08:27.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3331" for this suite.

• [SLOW TEST:34.017 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":280,"completed":232,"skipped":3785,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:08:27.510: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 23 23:08:27.586: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b8fab9be-cc04-4b2d-9872-dd1c4f1e52b6" in namespace "downward-api-3985" to be "success or failure"
Jan 23 23:08:27.607: INFO: Pod "downwardapi-volume-b8fab9be-cc04-4b2d-9872-dd1c4f1e52b6": Phase="Pending", Reason="", readiness=false. Elapsed: 19.851522ms
Jan 23 23:08:29.613: INFO: Pod "downwardapi-volume-b8fab9be-cc04-4b2d-9872-dd1c4f1e52b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02649008s
STEP: Saw pod success
Jan 23 23:08:29.613: INFO: Pod "downwardapi-volume-b8fab9be-cc04-4b2d-9872-dd1c4f1e52b6" satisfied condition "success or failure"
Jan 23 23:08:29.617: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod downwardapi-volume-b8fab9be-cc04-4b2d-9872-dd1c4f1e52b6 container client-container: <nil>
STEP: delete the pod
Jan 23 23:08:29.706: INFO: Waiting for pod downwardapi-volume-b8fab9be-cc04-4b2d-9872-dd1c4f1e52b6 to disappear
Jan 23 23:08:29.709: INFO: Pod downwardapi-volume-b8fab9be-cc04-4b2d-9872-dd1c4f1e52b6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:08:29.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3985" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":280,"completed":233,"skipped":3822,"failed":0}
S
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:08:29.721: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:08:29.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3741" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":280,"completed":234,"skipped":3823,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:08:29.812: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating replication controller my-hostname-basic-75ac5963-0ad2-4846-acd1-a33717d20f43
Jan 23 23:08:29.864: INFO: Pod name my-hostname-basic-75ac5963-0ad2-4846-acd1-a33717d20f43: Found 0 pods out of 1
Jan 23 23:08:34.870: INFO: Pod name my-hostname-basic-75ac5963-0ad2-4846-acd1-a33717d20f43: Found 1 pods out of 1
Jan 23 23:08:34.871: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-75ac5963-0ad2-4846-acd1-a33717d20f43" are running
Jan 23 23:08:34.884: INFO: Pod "my-hostname-basic-75ac5963-0ad2-4846-acd1-a33717d20f43-c5xpp" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-01-23 23:08:29 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-01-23 23:08:30 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-01-23 23:08:30 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-01-23 23:08:28 +0000 UTC Reason: Message:}])
Jan 23 23:08:34.885: INFO: Trying to dial the pod
Jan 23 23:08:39.902: INFO: Controller my-hostname-basic-75ac5963-0ad2-4846-acd1-a33717d20f43: Got expected result from replica 1 [my-hostname-basic-75ac5963-0ad2-4846-acd1-a33717d20f43-c5xpp]: "my-hostname-basic-75ac5963-0ad2-4846-acd1-a33717d20f43-c5xpp", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:08:39.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8007" for this suite.

• [SLOW TEST:10.103 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":280,"completed":235,"skipped":3835,"failed":0}
S
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:08:39.916: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating secret secrets-8659/secret-test-bd76369f-eaed-4853-871d-35c3059af218
STEP: Creating a pod to test consume secrets
Jan 23 23:08:39.966: INFO: Waiting up to 5m0s for pod "pod-configmaps-07cdd76e-1883-4dfc-9176-32e0f9676aca" in namespace "secrets-8659" to be "success or failure"
Jan 23 23:08:39.970: INFO: Pod "pod-configmaps-07cdd76e-1883-4dfc-9176-32e0f9676aca": Phase="Pending", Reason="", readiness=false. Elapsed: 3.960254ms
Jan 23 23:08:41.974: INFO: Pod "pod-configmaps-07cdd76e-1883-4dfc-9176-32e0f9676aca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008048291s
Jan 23 23:08:43.978: INFO: Pod "pod-configmaps-07cdd76e-1883-4dfc-9176-32e0f9676aca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012231862s
STEP: Saw pod success
Jan 23 23:08:43.978: INFO: Pod "pod-configmaps-07cdd76e-1883-4dfc-9176-32e0f9676aca" satisfied condition "success or failure"
Jan 23 23:08:43.981: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f pod pod-configmaps-07cdd76e-1883-4dfc-9176-32e0f9676aca container env-test: <nil>
STEP: delete the pod
Jan 23 23:08:44.034: INFO: Waiting for pod pod-configmaps-07cdd76e-1883-4dfc-9176-32e0f9676aca to disappear
Jan 23 23:08:44.038: INFO: Pod pod-configmaps-07cdd76e-1883-4dfc-9176-32e0f9676aca no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:08:44.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8659" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":280,"completed":236,"skipped":3836,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:08:44.054: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Jan 23 23:08:46.638: INFO: Successfully updated pod "labelsupdate6180b9a8-11ec-4ed0-99c6-a015256c21e3"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:08:48.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-466" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":280,"completed":237,"skipped":3866,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:08:48.669: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on node default medium
Jan 23 23:08:48.720: INFO: Waiting up to 5m0s for pod "pod-d821a91c-9376-4bc2-a190-8bca91bbeaa0" in namespace "emptydir-4705" to be "success or failure"
Jan 23 23:08:48.733: INFO: Pod "pod-d821a91c-9376-4bc2-a190-8bca91bbeaa0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.262296ms
Jan 23 23:08:50.737: INFO: Pod "pod-d821a91c-9376-4bc2-a190-8bca91bbeaa0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016951323s
STEP: Saw pod success
Jan 23 23:08:50.737: INFO: Pod "pod-d821a91c-9376-4bc2-a190-8bca91bbeaa0" satisfied condition "success or failure"
Jan 23 23:08:50.740: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod pod-d821a91c-9376-4bc2-a190-8bca91bbeaa0 container test-container: <nil>
STEP: delete the pod
Jan 23 23:08:50.768: INFO: Waiting for pod pod-d821a91c-9376-4bc2-a190-8bca91bbeaa0 to disappear
Jan 23 23:08:50.772: INFO: Pod pod-d821a91c-9376-4bc2-a190-8bca91bbeaa0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:08:50.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4705" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":238,"skipped":3869,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:08:50.793: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jan 23 23:08:50.838: INFO: Waiting up to 5m0s for pod "pod-1fe72077-d53f-4186-9cb5-8d979551dbf0" in namespace "emptydir-4044" to be "success or failure"
Jan 23 23:08:50.963: INFO: Pod "pod-1fe72077-d53f-4186-9cb5-8d979551dbf0": Phase="Pending", Reason="", readiness=false. Elapsed: 123.704638ms
Jan 23 23:08:52.970: INFO: Pod "pod-1fe72077-d53f-4186-9cb5-8d979551dbf0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.130704145s
STEP: Saw pod success
Jan 23 23:08:52.970: INFO: Pod "pod-1fe72077-d53f-4186-9cb5-8d979551dbf0" satisfied condition "success or failure"
Jan 23 23:08:52.977: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod pod-1fe72077-d53f-4186-9cb5-8d979551dbf0 container test-container: <nil>
STEP: delete the pod
Jan 23 23:08:53.010: INFO: Waiting for pod pod-1fe72077-d53f-4186-9cb5-8d979551dbf0 to disappear
Jan 23 23:08:53.015: INFO: Pod pod-1fe72077-d53f-4186-9cb5-8d979551dbf0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:08:53.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4044" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":239,"skipped":3896,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:08:53.035: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1632
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jan 23 23:08:53.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-8084'
Jan 23 23:08:53.374: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan 23 23:08:53.374: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Jan 23 23:08:53.404: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-5f4rh]
Jan 23 23:08:53.405: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-5f4rh" in namespace "kubectl-8084" to be "running and ready"
Jan 23 23:08:53.413: INFO: Pod "e2e-test-httpd-rc-5f4rh": Phase="Pending", Reason="", readiness=false. Elapsed: 7.702784ms
Jan 23 23:08:55.418: INFO: Pod "e2e-test-httpd-rc-5f4rh": Phase="Running", Reason="", readiness=true. Elapsed: 2.013339244s
Jan 23 23:08:55.418: INFO: Pod "e2e-test-httpd-rc-5f4rh" satisfied condition "running and ready"
Jan 23 23:08:55.418: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-5f4rh]
Jan 23 23:08:55.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 logs rc/e2e-test-httpd-rc --namespace=kubectl-8084'
Jan 23 23:08:55.567: INFO: stderr: ""
Jan 23 23:08:55.567: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 100.117.222.236. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 100.117.222.236. Set the 'ServerName' directive globally to suppress this message\n[Thu Jan 23 23:08:55.823909 2020] [mpm_event:notice] [pid 1:tid 139778258463592] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Thu Jan 23 23:08:55.824059 2020] [core:notice] [pid 1:tid 139778258463592] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1637
Jan 23 23:08:55.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 delete rc e2e-test-httpd-rc --namespace=kubectl-8084'
Jan 23 23:08:55.676: INFO: stderr: ""
Jan 23 23:08:55.676: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:08:55.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8084" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run rc should create an rc from an image  [Conformance]","total":280,"completed":240,"skipped":3906,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:08:55.696: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating all guestbook components
Jan 23 23:08:55.742: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-slave
  labels:
    app: agnhost
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: slave
    tier: backend

Jan 23 23:08:55.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 create -f - --namespace=kubectl-8668'
Jan 23 23:08:56.135: INFO: stderr: ""
Jan 23 23:08:56.135: INFO: stdout: "service/agnhost-slave created\n"
Jan 23 23:08:56.135: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-master
  labels:
    app: agnhost
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: master
    tier: backend

Jan 23 23:08:56.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 create -f - --namespace=kubectl-8668'
Jan 23 23:08:56.418: INFO: stderr: ""
Jan 23 23:08:56.418: INFO: stdout: "service/agnhost-master created\n"
Jan 23 23:08:56.418: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jan 23 23:08:56.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 create -f - --namespace=kubectl-8668'
Jan 23 23:08:56.603: INFO: stderr: ""
Jan 23 23:08:56.603: INFO: stdout: "service/frontend created\n"
Jan 23 23:08:56.604: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jan 23 23:08:56.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 create -f - --namespace=kubectl-8668'
Jan 23 23:08:56.783: INFO: stderr: ""
Jan 23 23:08:56.783: INFO: stdout: "deployment.apps/frontend created\n"
Jan 23 23:08:56.784: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 23 23:08:56.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 create -f - --namespace=kubectl-8668'
Jan 23 23:08:58.840: INFO: stderr: ""
Jan 23 23:08:58.840: INFO: stdout: "deployment.apps/agnhost-master created\n"
Jan 23 23:08:58.841: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--slaveof", "agnhost-master", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 23 23:08:58.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 create -f - --namespace=kubectl-8668'
Jan 23 23:08:59.077: INFO: stderr: ""
Jan 23 23:08:59.077: INFO: stdout: "deployment.apps/agnhost-slave created\n"
STEP: validating guestbook app
Jan 23 23:08:59.077: INFO: Waiting for all frontend pods to be Running.
Jan 23 23:09:04.129: INFO: Waiting for frontend to serve content.
Jan 23 23:09:04.164: INFO: Trying to add a new entry to the guestbook.
Jan 23 23:09:04.189: INFO: Verifying that added entry can be retrieved.
Jan 23 23:09:04.221: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources
Jan 23 23:09:09.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 delete --grace-period=0 --force -f - --namespace=kubectl-8668'
Jan 23 23:09:09.591: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 23 23:09:09.591: INFO: stdout: "service \"agnhost-slave\" force deleted\n"
STEP: using delete to clean up resources
Jan 23 23:09:09.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 delete --grace-period=0 --force -f - --namespace=kubectl-8668'
Jan 23 23:09:09.833: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 23 23:09:09.833: INFO: stdout: "service \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Jan 23 23:09:09.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 delete --grace-period=0 --force -f - --namespace=kubectl-8668'
Jan 23 23:09:10.170: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 23 23:09:10.170: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jan 23 23:09:10.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 delete --grace-period=0 --force -f - --namespace=kubectl-8668'
Jan 23 23:09:10.356: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 23 23:09:10.356: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jan 23 23:09:10.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 delete --grace-period=0 --force -f - --namespace=kubectl-8668'
Jan 23 23:09:10.474: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 23 23:09:10.474: INFO: stdout: "deployment.apps \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Jan 23 23:09:10.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 delete --grace-period=0 --force -f - --namespace=kubectl-8668'
Jan 23 23:09:10.609: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 23 23:09:10.609: INFO: stdout: "deployment.apps \"agnhost-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:09:10.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8668" for this suite.

• [SLOW TEST:13.198 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:386
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":280,"completed":241,"skipped":3926,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:09:10.704: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-upd-0f13ace1-abce-49a1-bbf2-3c0ba8a6a986
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-0f13ace1-abce-49a1-bbf2-3c0ba8a6a986
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:09:15.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9199" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":242,"skipped":3939,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:09:15.348: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1733
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jan 23 23:09:15.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-6997'
Jan 23 23:09:15.572: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan 23 23:09:15.572: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1738
Jan 23 23:09:17.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 delete deployment e2e-test-httpd-deployment --namespace=kubectl-6997'
Jan 23 23:09:17.728: INFO: stderr: ""
Jan 23 23:09:17.729: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:09:17.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6997" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run deployment should create a deployment from an image  [Conformance]","total":280,"completed":243,"skipped":3947,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:09:17.762: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8686.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8686.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8686.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8686.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 23 23:09:22.344: INFO: DNS probes using dns-test-4b7161e1-899d-428f-ab9d-ccfa47d418b6 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8686.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8686.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8686.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8686.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 23 23:09:26.848: INFO: File wheezy_udp@dns-test-service-3.dns-8686.svc.cluster.local from pod  dns-8686/dns-test-e1b9a7ad-6563-45dc-ac7f-25acd756d449 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 23 23:09:26.857: INFO: File jessie_udp@dns-test-service-3.dns-8686.svc.cluster.local from pod  dns-8686/dns-test-e1b9a7ad-6563-45dc-ac7f-25acd756d449 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 23 23:09:26.857: INFO: Lookups using dns-8686/dns-test-e1b9a7ad-6563-45dc-ac7f-25acd756d449 failed for: [wheezy_udp@dns-test-service-3.dns-8686.svc.cluster.local jessie_udp@dns-test-service-3.dns-8686.svc.cluster.local]

Jan 23 23:09:33.675: INFO: File wheezy_udp@dns-test-service-3.dns-8686.svc.cluster.local from pod  dns-8686/dns-test-e1b9a7ad-6563-45dc-ac7f-25acd756d449 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 23 23:09:33.680: INFO: File jessie_udp@dns-test-service-3.dns-8686.svc.cluster.local from pod  dns-8686/dns-test-e1b9a7ad-6563-45dc-ac7f-25acd756d449 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 23 23:09:33.680: INFO: Lookups using dns-8686/dns-test-e1b9a7ad-6563-45dc-ac7f-25acd756d449 failed for: [wheezy_udp@dns-test-service-3.dns-8686.svc.cluster.local jessie_udp@dns-test-service-3.dns-8686.svc.cluster.local]

Jan 23 23:09:38.674: INFO: File wheezy_udp@dns-test-service-3.dns-8686.svc.cluster.local from pod  dns-8686/dns-test-e1b9a7ad-6563-45dc-ac7f-25acd756d449 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 23 23:09:38.680: INFO: File jessie_udp@dns-test-service-3.dns-8686.svc.cluster.local from pod  dns-8686/dns-test-e1b9a7ad-6563-45dc-ac7f-25acd756d449 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 23 23:09:38.680: INFO: Lookups using dns-8686/dns-test-e1b9a7ad-6563-45dc-ac7f-25acd756d449 failed for: [wheezy_udp@dns-test-service-3.dns-8686.svc.cluster.local jessie_udp@dns-test-service-3.dns-8686.svc.cluster.local]

Jan 23 23:09:43.675: INFO: File wheezy_udp@dns-test-service-3.dns-8686.svc.cluster.local from pod  dns-8686/dns-test-e1b9a7ad-6563-45dc-ac7f-25acd756d449 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 23 23:09:43.683: INFO: File jessie_udp@dns-test-service-3.dns-8686.svc.cluster.local from pod  dns-8686/dns-test-e1b9a7ad-6563-45dc-ac7f-25acd756d449 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 23 23:09:43.683: INFO: Lookups using dns-8686/dns-test-e1b9a7ad-6563-45dc-ac7f-25acd756d449 failed for: [wheezy_udp@dns-test-service-3.dns-8686.svc.cluster.local jessie_udp@dns-test-service-3.dns-8686.svc.cluster.local]

Jan 23 23:09:48.673: INFO: File wheezy_udp@dns-test-service-3.dns-8686.svc.cluster.local from pod  dns-8686/dns-test-e1b9a7ad-6563-45dc-ac7f-25acd756d449 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 23 23:09:48.681: INFO: File jessie_udp@dns-test-service-3.dns-8686.svc.cluster.local from pod  dns-8686/dns-test-e1b9a7ad-6563-45dc-ac7f-25acd756d449 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 23 23:09:48.681: INFO: Lookups using dns-8686/dns-test-e1b9a7ad-6563-45dc-ac7f-25acd756d449 failed for: [wheezy_udp@dns-test-service-3.dns-8686.svc.cluster.local jessie_udp@dns-test-service-3.dns-8686.svc.cluster.local]

Jan 23 23:09:53.682: INFO: DNS probes using dns-test-e1b9a7ad-6563-45dc-ac7f-25acd756d449 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8686.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-8686.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8686.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-8686.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 23 23:09:57.851: INFO: DNS probes using dns-test-91562e88-465b-42bb-a053-139c638a5ccd succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:09:57.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8686" for this suite.

• [SLOW TEST:38.394 seconds]
[sig-network] DNS
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":280,"completed":244,"skipped":3995,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:09:57.980: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on node default medium
Jan 23 23:09:58.095: INFO: Waiting up to 5m0s for pod "pod-5ce4d24d-38e8-4869-9879-04300895c39d" in namespace "emptydir-2815" to be "success or failure"
Jan 23 23:09:58.107: INFO: Pod "pod-5ce4d24d-38e8-4869-9879-04300895c39d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.561045ms
Jan 23 23:10:00.405: INFO: Pod "pod-5ce4d24d-38e8-4869-9879-04300895c39d": Phase="Running", Reason="", readiness=true. Elapsed: 2.309022692s
Jan 23 23:10:02.440: INFO: Pod "pod-5ce4d24d-38e8-4869-9879-04300895c39d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.343999278s
STEP: Saw pod success
Jan 23 23:10:02.440: INFO: Pod "pod-5ce4d24d-38e8-4869-9879-04300895c39d" satisfied condition "success or failure"
Jan 23 23:10:02.451: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod pod-5ce4d24d-38e8-4869-9879-04300895c39d container test-container: <nil>
STEP: delete the pod
Jan 23 23:10:02.540: INFO: Waiting for pod pod-5ce4d24d-38e8-4869-9879-04300895c39d to disappear
Jan 23 23:10:02.561: INFO: Pod pod-5ce4d24d-38e8-4869-9879-04300895c39d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:10:02.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2815" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":245,"skipped":4000,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:10:02.693: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on node default medium
Jan 23 23:10:02.959: INFO: Waiting up to 5m0s for pod "pod-ba1a6057-054f-4eea-9dc8-a5c6f4ac8ae3" in namespace "emptydir-4977" to be "success or failure"
Jan 23 23:10:03.003: INFO: Pod "pod-ba1a6057-054f-4eea-9dc8-a5c6f4ac8ae3": Phase="Pending", Reason="", readiness=false. Elapsed: 43.593301ms
Jan 23 23:10:06.359: INFO: Pod "pod-ba1a6057-054f-4eea-9dc8-a5c6f4ac8ae3": Phase="Running", Reason="", readiness=true. Elapsed: 2.050216869s
Jan 23 23:10:08.367: INFO: Pod "pod-ba1a6057-054f-4eea-9dc8-a5c6f4ac8ae3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.058020718s
STEP: Saw pod success
Jan 23 23:10:08.367: INFO: Pod "pod-ba1a6057-054f-4eea-9dc8-a5c6f4ac8ae3" satisfied condition "success or failure"
Jan 23 23:10:08.373: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod pod-ba1a6057-054f-4eea-9dc8-a5c6f4ac8ae3 container test-container: <nil>
STEP: delete the pod
Jan 23 23:10:08.408: INFO: Waiting for pod pod-ba1a6057-054f-4eea-9dc8-a5c6f4ac8ae3 to disappear
Jan 23 23:10:08.413: INFO: Pod pod-ba1a6057-054f-4eea-9dc8-a5c6f4ac8ae3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:10:08.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4977" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":246,"skipped":4003,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:10:08.439: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-3677, will wait for the garbage collector to delete the pods
Jan 23 23:10:12.603: INFO: Deleting Job.batch foo took: 11.969878ms
Jan 23 23:10:13.105: INFO: Terminating Job.batch foo pods took: 501.285458ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:10:47.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3677" for this suite.

• [SLOW TEST:37.718 seconds]
[sig-apps] Job
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":280,"completed":247,"skipped":4010,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:10:47.554: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Jan 23 23:10:47.647: INFO: PodSpec: initContainers in spec.initContainers
Jan 23 23:11:32.055: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-abe89bf8-21f5-4db8-a273-6f4381822656", GenerateName:"", Namespace:"init-container-4416", SelfLink:"/api/v1/namespaces/init-container-4416/pods/pod-init-abe89bf8-21f5-4db8-a273-6f4381822656", UID:"5308eb41-5ae6-4bb0-a2e9-04cac6b203c8", ResourceVersion:"428719", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63715417846, loc:(*time.Location)(0x7db4bc0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"647096690"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"100.117.222.245/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-8rrkt", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc007376040), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-8rrkt", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-8rrkt", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-8rrkt", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0061dc068), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"management-cluster-1-17-2-md-0-5f64bb5777-dlwcz", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0040dc000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0061dc0e0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0061dc100)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0061dc108), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0061dc10c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715417847, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715417847, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715417847, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715417846, loc:(*time.Location)(0x7db4bc0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.78.220.23", PodIP:"100.117.222.245", PodIPs:[]v1.PodIP{v1.PodIP{IP:"100.117.222.245"}}, StartTime:(*v1.Time)(0xc00418e040), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001ebc070)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001ebc0e0)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"containerd://ac4828a61ff7d1f684b4c4ed4749ca2091d24a3817d63ebfe121977cf65052c0", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00418e080), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00418e060), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc0061dc18f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:11:32.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4416" for this suite.

• [SLOW TEST:42.990 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":280,"completed":248,"skipped":4035,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:11:32.089: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:11:38.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8116" for this suite.
STEP: Destroying namespace "nsdeletetest-4747" for this suite.
Jan 23 23:11:38.394: INFO: Namespace nsdeletetest-4747 was already deleted
STEP: Destroying namespace "nsdeletetest-9127" for this suite.

• [SLOW TEST:6.314 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":280,"completed":249,"skipped":4039,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:11:38.405: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-5661
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-5661
STEP: creating replication controller externalsvc in namespace services-5661
I0123 23:11:38.563549      21 runners.go:189] Created replication controller with name: externalsvc, namespace: services-5661, replica count: 2
I0123 23:11:41.615912      21 runners.go:189] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Jan 23 23:11:41.646: INFO: Creating new exec pod
Jan 23 23:11:43.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 exec --namespace=services-5661 execpodszbm8 -- /bin/sh -x -c nslookup clusterip-service'
Jan 23 23:11:45.104: INFO: stderr: "+ nslookup clusterip-service\n"
Jan 23 23:11:45.104: INFO: stdout: "Server:\t\t100.64.0.10\nAddress:\t100.64.0.10#53\n\nclusterip-service.services-5661.svc.cluster.local\tcanonical name = externalsvc.services-5661.svc.cluster.local.\nName:\texternalsvc.services-5661.svc.cluster.local\nAddress: 100.66.71.137\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-5661, will wait for the garbage collector to delete the pods
Jan 23 23:11:45.167: INFO: Deleting ReplicationController externalsvc took: 8.856246ms
Jan 23 23:11:45.669: INFO: Terminating ReplicationController externalsvc pods took: 501.907303ms
Jan 23 23:12:01.850: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:12:01.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5661" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:21.947 seconds]
[sig-network] Services
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":280,"completed":250,"skipped":4048,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:12:01.901: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: executing a command with run --rm and attach with stdin
Jan 23 23:12:01.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 --namespace=kubectl-391 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Jan 23 23:12:03.757: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Jan 23 23:12:03.757: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:12:05.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-391" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run --rm job should create a job from an image, then delete the job  [Conformance]","total":280,"completed":251,"skipped":4049,"failed":0}
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:12:05.791: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1192.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1192.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1192.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1192.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1192.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1192.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1192.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1192.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1192.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1192.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1192.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 74.98.70.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.70.98.74_udp@PTR;check="$$(dig +tcp +noall +answer +search 74.98.70.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.70.98.74_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1192.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1192.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1192.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1192.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1192.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1192.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1192.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1192.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1192.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1192.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1192.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 74.98.70.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.70.98.74_udp@PTR;check="$$(dig +tcp +noall +answer +search 74.98.70.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.70.98.74_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 23 23:12:09.930: INFO: Unable to read wheezy_udp@dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:09.942: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:09.947: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:09.952: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:09.996: INFO: Unable to read jessie_udp@dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:10.001: INFO: Unable to read jessie_tcp@dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:10.005: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:10.010: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:10.042: INFO: Lookups using dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1 failed for: [wheezy_udp@dns-test-service.dns-1192.svc.cluster.local wheezy_tcp@dns-test-service.dns-1192.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local jessie_udp@dns-test-service.dns-1192.svc.cluster.local jessie_tcp@dns-test-service.dns-1192.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local]

Jan 23 23:12:15.048: INFO: Unable to read wheezy_udp@dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:15.054: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:15.061: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:15.065: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:15.109: INFO: Unable to read jessie_udp@dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:15.114: INFO: Unable to read jessie_tcp@dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:15.119: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:15.123: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:15.161: INFO: Lookups using dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1 failed for: [wheezy_udp@dns-test-service.dns-1192.svc.cluster.local wheezy_tcp@dns-test-service.dns-1192.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local jessie_udp@dns-test-service.dns-1192.svc.cluster.local jessie_tcp@dns-test-service.dns-1192.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local]

Jan 23 23:12:21.612: INFO: Unable to read wheezy_udp@dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:21.617: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:21.622: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:21.628: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:21.672: INFO: Unable to read jessie_udp@dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:21.677: INFO: Unable to read jessie_tcp@dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:21.680: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:21.685: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:21.723: INFO: Lookups using dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1 failed for: [wheezy_udp@dns-test-service.dns-1192.svc.cluster.local wheezy_tcp@dns-test-service.dns-1192.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local jessie_udp@dns-test-service.dns-1192.svc.cluster.local jessie_tcp@dns-test-service.dns-1192.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local]

Jan 23 23:12:26.613: INFO: Unable to read wheezy_udp@dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:26.619: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:26.624: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:26.629: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:26.670: INFO: Unable to read jessie_udp@dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:26.675: INFO: Unable to read jessie_tcp@dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:26.680: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:26.686: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:26.713: INFO: Lookups using dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1 failed for: [wheezy_udp@dns-test-service.dns-1192.svc.cluster.local wheezy_tcp@dns-test-service.dns-1192.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local jessie_udp@dns-test-service.dns-1192.svc.cluster.local jessie_tcp@dns-test-service.dns-1192.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local]

Jan 23 23:12:31.612: INFO: Unable to read wheezy_udp@dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:31.619: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:31.624: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:31.628: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:31.673: INFO: Unable to read jessie_udp@dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:31.683: INFO: Unable to read jessie_tcp@dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:31.689: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:31.695: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:31.722: INFO: Lookups using dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1 failed for: [wheezy_udp@dns-test-service.dns-1192.svc.cluster.local wheezy_tcp@dns-test-service.dns-1192.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local jessie_udp@dns-test-service.dns-1192.svc.cluster.local jessie_tcp@dns-test-service.dns-1192.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local]

Jan 23 23:12:36.611: INFO: Unable to read wheezy_udp@dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:36.615: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:36.621: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:36.626: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:36.671: INFO: Unable to read jessie_udp@dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:36.675: INFO: Unable to read jessie_tcp@dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:36.679: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:36.684: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local from pod dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1: the server could not find the requested resource (get pods dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1)
Jan 23 23:12:36.712: INFO: Lookups using dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1 failed for: [wheezy_udp@dns-test-service.dns-1192.svc.cluster.local wheezy_tcp@dns-test-service.dns-1192.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local jessie_udp@dns-test-service.dns-1192.svc.cluster.local jessie_tcp@dns-test-service.dns-1192.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1192.svc.cluster.local]

Jan 23 23:12:41.713: INFO: DNS probes using dns-1192/dns-test-4c658bf5-5b0a-4da5-a059-63ba4ed795f1 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:12:41.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1192" for this suite.

• [SLOW TEST:34.539 seconds]
[sig-network] DNS
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":280,"completed":252,"skipped":4052,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:12:41.921: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 23 23:12:41.999: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jan 23 23:12:46.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 --namespace=crd-publish-openapi-335 create -f -'
Jan 23 23:12:46.872: INFO: stderr: ""
Jan 23 23:12:46.872: INFO: stdout: "e2e-test-crd-publish-openapi-4439-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 23 23:12:46.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 --namespace=crd-publish-openapi-335 delete e2e-test-crd-publish-openapi-4439-crds test-cr'
Jan 23 23:12:46.982: INFO: stderr: ""
Jan 23 23:12:46.982: INFO: stdout: "e2e-test-crd-publish-openapi-4439-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jan 23 23:12:46.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 --namespace=crd-publish-openapi-335 apply -f -'
Jan 23 23:12:47.195: INFO: stderr: ""
Jan 23 23:12:47.195: INFO: stdout: "e2e-test-crd-publish-openapi-4439-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 23 23:12:47.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 --namespace=crd-publish-openapi-335 delete e2e-test-crd-publish-openapi-4439-crds test-cr'
Jan 23 23:12:47.285: INFO: stderr: ""
Jan 23 23:12:47.285: INFO: stdout: "e2e-test-crd-publish-openapi-4439-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jan 23 23:12:47.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 explain e2e-test-crd-publish-openapi-4439-crds'
Jan 23 23:12:47.546: INFO: stderr: ""
Jan 23 23:12:47.546: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4439-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:12:50.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-335" for this suite.

• [SLOW TEST:9.054 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":280,"completed":253,"skipped":4112,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:12:51.010: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Jan 23 23:12:57.456: INFO: Successfully updated pod "labelsupdatef465e5e1-ca11-47e9-8dbc-3e77debb9589"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:12:59.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1407" for this suite.

• [SLOW TEST:6.899 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":280,"completed":254,"skipped":4121,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:12:59.503: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 23 23:12:59.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 version'
Jan 23 23:12:59.724: INFO: stderr: ""
Jan 23 23:12:59.724: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.2\", GitCommit:\"59603c6e503c87169aea6106f57b9f242f64df89\", GitTreeState:\"clean\", BuildDate:\"2020-01-18T23:30:10Z\", GoVersion:\"go1.13.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.2+vmware.1\", GitCommit:\"9eddeb14cfe1c0523acd65432343d5ebf22e9b57\", GitTreeState:\"clean\", BuildDate:\"2020-01-22T04:25:06Z\", GoVersion:\"go1.13.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:12:59.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5505" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":280,"completed":255,"skipped":4160,"failed":0}
S
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:12:59.742: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 23 23:12:59.875: INFO: (0) /api/v1/nodes/management-cluster-1-17-2-md-0-5f64bb5777-dlwcz:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init-output.log">cloud-init-output.log</a>
<a href="... (200; 18.229947ms)
Jan 23 23:12:59.882: INFO: (1) /api/v1/nodes/management-cluster-1-17-2-md-0-5f64bb5777-dlwcz:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init-output.log">cloud-init-output.log</a>
<a href="... (200; 6.488589ms)
Jan 23 23:12:59.887: INFO: (2) /api/v1/nodes/management-cluster-1-17-2-md-0-5f64bb5777-dlwcz:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init-output.log">cloud-init-output.log</a>
<a href="... (200; 5.350106ms)
Jan 23 23:12:59.893: INFO: (3) /api/v1/nodes/management-cluster-1-17-2-md-0-5f64bb5777-dlwcz:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init-output.log">cloud-init-output.log</a>
<a href="... (200; 5.336714ms)
Jan 23 23:12:59.899: INFO: (4) /api/v1/nodes/management-cluster-1-17-2-md-0-5f64bb5777-dlwcz:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init-output.log">cloud-init-output.log</a>
<a href="... (200; 6.408381ms)
Jan 23 23:12:59.905: INFO: (5) /api/v1/nodes/management-cluster-1-17-2-md-0-5f64bb5777-dlwcz:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init-output.log">cloud-init-output.log</a>
<a href="... (200; 5.900856ms)
Jan 23 23:12:59.912: INFO: (6) /api/v1/nodes/management-cluster-1-17-2-md-0-5f64bb5777-dlwcz:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init-output.log">cloud-init-output.log</a>
<a href="... (200; 6.157453ms)
Jan 23 23:12:59.917: INFO: (7) /api/v1/nodes/management-cluster-1-17-2-md-0-5f64bb5777-dlwcz:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init-output.log">cloud-init-output.log</a>
<a href="... (200; 5.003741ms)
Jan 23 23:12:59.922: INFO: (8) /api/v1/nodes/management-cluster-1-17-2-md-0-5f64bb5777-dlwcz:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init-output.log">cloud-init-output.log</a>
<a href="... (200; 4.847825ms)
Jan 23 23:12:59.929: INFO: (9) /api/v1/nodes/management-cluster-1-17-2-md-0-5f64bb5777-dlwcz:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init-output.log">cloud-init-output.log</a>
<a href="... (200; 6.976776ms)
Jan 23 23:12:59.946: INFO: (10) /api/v1/nodes/management-cluster-1-17-2-md-0-5f64bb5777-dlwcz:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init-output.log">cloud-init-output.log</a>
<a href="... (200; 17.098334ms)
Jan 23 23:12:59.954: INFO: (11) /api/v1/nodes/management-cluster-1-17-2-md-0-5f64bb5777-dlwcz:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init-output.log">cloud-init-output.log</a>
<a href="... (200; 7.245962ms)
Jan 23 23:12:59.961: INFO: (12) /api/v1/nodes/management-cluster-1-17-2-md-0-5f64bb5777-dlwcz:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init-output.log">cloud-init-output.log</a>
<a href="... (200; 7.192172ms)
Jan 23 23:12:59.967: INFO: (13) /api/v1/nodes/management-cluster-1-17-2-md-0-5f64bb5777-dlwcz:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init-output.log">cloud-init-output.log</a>
<a href="... (200; 5.593516ms)
Jan 23 23:12:59.973: INFO: (14) /api/v1/nodes/management-cluster-1-17-2-md-0-5f64bb5777-dlwcz:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init-output.log">cloud-init-output.log</a>
<a href="... (200; 5.842036ms)
Jan 23 23:12:59.979: INFO: (15) /api/v1/nodes/management-cluster-1-17-2-md-0-5f64bb5777-dlwcz:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init-output.log">cloud-init-output.log</a>
<a href="... (200; 5.696952ms)
Jan 23 23:12:59.987: INFO: (16) /api/v1/nodes/management-cluster-1-17-2-md-0-5f64bb5777-dlwcz:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init-output.log">cloud-init-output.log</a>
<a href="... (200; 7.88476ms)
Jan 23 23:12:59.997: INFO: (17) /api/v1/nodes/management-cluster-1-17-2-md-0-5f64bb5777-dlwcz:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init-output.log">cloud-init-output.log</a>
<a href="... (200; 10.609226ms)
Jan 23 23:13:00.005: INFO: (18) /api/v1/nodes/management-cluster-1-17-2-md-0-5f64bb5777-dlwcz:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init-output.log">cloud-init-output.log</a>
<a href="... (200; 7.078865ms)
Jan 23 23:13:00.012: INFO: (19) /api/v1/nodes/management-cluster-1-17-2-md-0-5f64bb5777-dlwcz:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init-output.log">cloud-init-output.log</a>
<a href="... (200; 7.157354ms)
[AfterEach] version v1
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:13:00.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-886" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]","total":280,"completed":256,"skipped":4161,"failed":0}
S
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:13:00.031: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jan 23 23:13:00.089: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8727 /api/v1/namespaces/watch-8727/configmaps/e2e-watch-test-configmap-a 96493f59-1981-46f3-bd73-58548baef9b0 429412 0 2020-01-23 23:12:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 23 23:13:00.091: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8727 /api/v1/namespaces/watch-8727/configmaps/e2e-watch-test-configmap-a 96493f59-1981-46f3-bd73-58548baef9b0 429412 0 2020-01-23 23:12:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jan 23 23:13:10.100: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8727 /api/v1/namespaces/watch-8727/configmaps/e2e-watch-test-configmap-a 96493f59-1981-46f3-bd73-58548baef9b0 429484 0 2020-01-23 23:12:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jan 23 23:13:10.100: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8727 /api/v1/namespaces/watch-8727/configmaps/e2e-watch-test-configmap-a 96493f59-1981-46f3-bd73-58548baef9b0 429484 0 2020-01-23 23:12:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jan 23 23:13:20.116: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8727 /api/v1/namespaces/watch-8727/configmaps/e2e-watch-test-configmap-a 96493f59-1981-46f3-bd73-58548baef9b0 429530 0 2020-01-23 23:12:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 23 23:13:20.117: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8727 /api/v1/namespaces/watch-8727/configmaps/e2e-watch-test-configmap-a 96493f59-1981-46f3-bd73-58548baef9b0 429530 0 2020-01-23 23:12:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jan 23 23:13:31.748: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8727 /api/v1/namespaces/watch-8727/configmaps/e2e-watch-test-configmap-a 96493f59-1981-46f3-bd73-58548baef9b0 429572 0 2020-01-23 23:12:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 23 23:13:31.749: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8727 /api/v1/namespaces/watch-8727/configmaps/e2e-watch-test-configmap-a 96493f59-1981-46f3-bd73-58548baef9b0 429572 0 2020-01-23 23:12:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jan 23 23:13:41.759: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8727 /api/v1/namespaces/watch-8727/configmaps/e2e-watch-test-configmap-b e17d9eca-9c70-4776-bd12-48dab8b2a490 429618 0 2020-01-23 23:13:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 23 23:13:41.759: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8727 /api/v1/namespaces/watch-8727/configmaps/e2e-watch-test-configmap-b e17d9eca-9c70-4776-bd12-48dab8b2a490 429618 0 2020-01-23 23:13:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jan 23 23:13:51.784: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8727 /api/v1/namespaces/watch-8727/configmaps/e2e-watch-test-configmap-b e17d9eca-9c70-4776-bd12-48dab8b2a490 429665 0 2020-01-23 23:13:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 23 23:13:51.786: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8727 /api/v1/namespaces/watch-8727/configmaps/e2e-watch-test-configmap-b e17d9eca-9c70-4776-bd12-48dab8b2a490 429665 0 2020-01-23 23:13:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:14:03.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8727" for this suite.

• [SLOW TEST:60.152 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":280,"completed":257,"skipped":4162,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:14:03.486: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9792.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9792.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9792.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9792.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9792.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9792.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 23 23:14:07.657: INFO: DNS probes using dns-9792/dns-test-62ad98ae-2ff3-4c8c-9a4a-561a99e1c329 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:14:07.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9792" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":280,"completed":258,"skipped":4184,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:14:07.706: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 23 23:14:08.303: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 23 23:14:11.347: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:14:11.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8739" for this suite.
STEP: Destroying namespace "webhook-8739-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":280,"completed":259,"skipped":4214,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:14:11.547: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 23 23:14:11.626: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jan 23 23:14:11.642: INFO: Number of nodes with available pods: 0
Jan 23 23:14:11.643: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jan 23 23:14:11.747: INFO: Number of nodes with available pods: 0
Jan 23 23:14:11.747: INFO: Node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz is running more than one daemon pod
Jan 23 23:14:12.750: INFO: Number of nodes with available pods: 0
Jan 23 23:14:12.750: INFO: Node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz is running more than one daemon pod
Jan 23 23:14:13.751: INFO: Number of nodes with available pods: 1
Jan 23 23:14:13.751: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jan 23 23:14:13.793: INFO: Number of nodes with available pods: 1
Jan 23 23:14:13.793: INFO: Number of running nodes: 0, number of available pods: 1
Jan 23 23:14:14.798: INFO: Number of nodes with available pods: 0
Jan 23 23:14:14.798: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jan 23 23:14:14.815: INFO: Number of nodes with available pods: 0
Jan 23 23:14:14.815: INFO: Node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz is running more than one daemon pod
Jan 23 23:14:15.819: INFO: Number of nodes with available pods: 0
Jan 23 23:14:15.819: INFO: Node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz is running more than one daemon pod
Jan 23 23:14:16.822: INFO: Number of nodes with available pods: 0
Jan 23 23:14:16.822: INFO: Node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz is running more than one daemon pod
Jan 23 23:14:17.822: INFO: Number of nodes with available pods: 0
Jan 23 23:14:17.822: INFO: Node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz is running more than one daemon pod
Jan 23 23:14:18.825: INFO: Number of nodes with available pods: 0
Jan 23 23:14:18.825: INFO: Node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz is running more than one daemon pod
Jan 23 23:14:19.820: INFO: Number of nodes with available pods: 0
Jan 23 23:14:19.820: INFO: Node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz is running more than one daemon pod
Jan 23 23:14:20.821: INFO: Number of nodes with available pods: 0
Jan 23 23:14:20.821: INFO: Node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz is running more than one daemon pod
Jan 23 23:14:21.819: INFO: Number of nodes with available pods: 0
Jan 23 23:14:21.819: INFO: Node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz is running more than one daemon pod
Jan 23 23:14:22.819: INFO: Number of nodes with available pods: 0
Jan 23 23:14:22.819: INFO: Node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz is running more than one daemon pod
Jan 23 23:14:23.819: INFO: Number of nodes with available pods: 0
Jan 23 23:14:23.819: INFO: Node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz is running more than one daemon pod
Jan 23 23:14:24.822: INFO: Number of nodes with available pods: 0
Jan 23 23:14:24.822: INFO: Node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz is running more than one daemon pod
Jan 23 23:14:25.822: INFO: Number of nodes with available pods: 0
Jan 23 23:14:25.822: INFO: Node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz is running more than one daemon pod
Jan 23 23:14:26.820: INFO: Number of nodes with available pods: 0
Jan 23 23:14:26.820: INFO: Node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz is running more than one daemon pod
Jan 23 23:14:27.821: INFO: Number of nodes with available pods: 0
Jan 23 23:14:27.821: INFO: Node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz is running more than one daemon pod
Jan 23 23:14:28.819: INFO: Number of nodes with available pods: 0
Jan 23 23:14:28.819: INFO: Node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz is running more than one daemon pod
Jan 23 23:14:29.820: INFO: Number of nodes with available pods: 1
Jan 23 23:14:29.820: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5728, will wait for the garbage collector to delete the pods
Jan 23 23:14:29.889: INFO: Deleting DaemonSet.extensions daemon-set took: 7.047362ms
Jan 23 23:14:30.390: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.648452ms
Jan 23 23:14:39.879: INFO: Number of nodes with available pods: 0
Jan 23 23:14:39.879: INFO: Number of running nodes: 0, number of available pods: 0
Jan 23 23:14:39.884: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5728/daemonsets","resourceVersion":"430061"},"items":null}

Jan 23 23:14:39.888: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5728/pods","resourceVersion":"430061"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:14:39.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5728" for this suite.

• [SLOW TEST:26.723 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":280,"completed":260,"skipped":4230,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:14:39.965: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-8301
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 23 23:14:40.015: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jan 23 23:15:00.152: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.117.222.250:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8301 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 23 23:15:00.152: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
Jan 23 23:15:00.398: INFO: Found all expected endpoints: [netserver-0]
Jan 23 23:15:00.404: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.120.200.8:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8301 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 23 23:15:00.404: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
Jan 23 23:15:00.615: INFO: Found all expected endpoints: [netserver-1]
Jan 23 23:15:00.618: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.105.119.83:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8301 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 23 23:15:00.618: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
Jan 23 23:15:00.812: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:15:00.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8301" for this suite.

• [SLOW TEST:20.861 seconds]
[sig-network] Networking
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":261,"skipped":4263,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:15:00.832: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-5acd42bf-038c-4a39-96cb-d6426a59bd8d
STEP: Creating a pod to test consume configMaps
Jan 23 23:15:00.883: INFO: Waiting up to 5m0s for pod "pod-configmaps-55862326-5c4e-444b-a40c-2df063add3ff" in namespace "configmap-6253" to be "success or failure"
Jan 23 23:15:00.896: INFO: Pod "pod-configmaps-55862326-5c4e-444b-a40c-2df063add3ff": Phase="Pending", Reason="", readiness=false. Elapsed: 13.174933ms
Jan 23 23:15:02.901: INFO: Pod "pod-configmaps-55862326-5c4e-444b-a40c-2df063add3ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017799488s
STEP: Saw pod success
Jan 23 23:15:02.901: INFO: Pod "pod-configmaps-55862326-5c4e-444b-a40c-2df063add3ff" satisfied condition "success or failure"
Jan 23 23:15:02.904: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f pod pod-configmaps-55862326-5c4e-444b-a40c-2df063add3ff container configmap-volume-test: <nil>
STEP: delete the pod
Jan 23 23:15:02.965: INFO: Waiting for pod pod-configmaps-55862326-5c4e-444b-a40c-2df063add3ff to disappear
Jan 23 23:15:02.969: INFO: Pod pod-configmaps-55862326-5c4e-444b-a40c-2df063add3ff no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:15:02.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6253" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":280,"completed":262,"skipped":4333,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:15:02.989: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service nodeport-test with type=NodePort in namespace services-4210
STEP: creating replication controller nodeport-test in namespace services-4210
I0123 23:15:03.069518      21 runners.go:189] Created replication controller with name: nodeport-test, namespace: services-4210, replica count: 2
Jan 23 23:15:06.120: INFO: Creating new exec pod
I0123 23:15:06.120188      21 runners.go:189] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 23 23:15:10.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 exec --namespace=services-4210 execpod5d8ff -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Jan 23 23:15:11.177: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 23 23:15:11.177: INFO: stdout: ""
Jan 23 23:15:11.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 exec --namespace=services-4210 execpod5d8ff -- /bin/sh -x -c nc -zv -t -w 2 100.67.194.49 80'
Jan 23 23:15:11.450: INFO: stderr: "+ nc -zv -t -w 2 100.67.194.49 80\nConnection to 100.67.194.49 80 port [tcp/http] succeeded!\n"
Jan 23 23:15:11.450: INFO: stdout: ""
Jan 23 23:15:11.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 exec --namespace=services-4210 execpod5d8ff -- /bin/sh -x -c nc -zv -t -w 2 10.78.220.23 31355'
Jan 23 23:15:11.722: INFO: stderr: "+ nc -zv -t -w 2 10.78.220.23 31355\nConnection to 10.78.220.23 31355 port [tcp/31355] succeeded!\n"
Jan 23 23:15:11.722: INFO: stdout: ""
Jan 23 23:15:11.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 exec --namespace=services-4210 execpod5d8ff -- /bin/sh -x -c nc -zv -t -w 2 10.78.219.138 31355'
Jan 23 23:15:12.015: INFO: stderr: "+ nc -zv -t -w 2 10.78.219.138 31355\nConnection to 10.78.219.138 31355 port [tcp/31355] succeeded!\n"
Jan 23 23:15:12.016: INFO: stdout: ""
Jan 23 23:15:12.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 exec --namespace=services-4210 execpod5d8ff -- /bin/sh -x -c nc -zv -t -w 2 10.78.220.23 31355'
Jan 23 23:15:12.282: INFO: stderr: "+ nc -zv -t -w 2 10.78.220.23 31355\nConnection to 10.78.220.23 31355 port [tcp/31355] succeeded!\n"
Jan 23 23:15:12.282: INFO: stdout: ""
Jan 23 23:15:12.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 exec --namespace=services-4210 execpod5d8ff -- /bin/sh -x -c nc -zv -t -w 2 10.78.219.138 31355'
Jan 23 23:15:12.556: INFO: stderr: "+ nc -zv -t -w 2 10.78.219.138 31355\nConnection to 10.78.219.138 31355 port [tcp/31355] succeeded!\n"
Jan 23 23:15:12.556: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:15:12.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4210" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:7.853 seconds]
[sig-network] Services
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":280,"completed":263,"skipped":4341,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:15:12.571: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 23 23:15:12.881: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 23 23:15:15.914: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:15:15.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7226" for this suite.
STEP: Destroying namespace "webhook-7226-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":280,"completed":264,"skipped":4343,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:15:16.105: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 23 23:15:16.177: INFO: Waiting up to 5m0s for pod "downwardapi-volume-95a0dacc-1158-49c6-8f47-3348a33362ca" in namespace "projected-5323" to be "success or failure"
Jan 23 23:15:16.193: INFO: Pod "downwardapi-volume-95a0dacc-1158-49c6-8f47-3348a33362ca": Phase="Pending", Reason="", readiness=false. Elapsed: 16.027212ms
Jan 23 23:15:18.198: INFO: Pod "downwardapi-volume-95a0dacc-1158-49c6-8f47-3348a33362ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021139065s
STEP: Saw pod success
Jan 23 23:15:18.198: INFO: Pod "downwardapi-volume-95a0dacc-1158-49c6-8f47-3348a33362ca" satisfied condition "success or failure"
Jan 23 23:15:18.202: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f pod downwardapi-volume-95a0dacc-1158-49c6-8f47-3348a33362ca container client-container: <nil>
STEP: delete the pod
Jan 23 23:15:18.231: INFO: Waiting for pod downwardapi-volume-95a0dacc-1158-49c6-8f47-3348a33362ca to disappear
Jan 23 23:15:18.236: INFO: Pod downwardapi-volume-95a0dacc-1158-49c6-8f47-3348a33362ca no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:15:18.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5323" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":265,"skipped":4353,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:15:18.247: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override command
Jan 23 23:15:18.303: INFO: Waiting up to 5m0s for pod "client-containers-9ab64ad3-7a7a-441d-a060-ccd198de3e20" in namespace "containers-6486" to be "success or failure"
Jan 23 23:15:18.309: INFO: Pod "client-containers-9ab64ad3-7a7a-441d-a060-ccd198de3e20": Phase="Pending", Reason="", readiness=false. Elapsed: 5.49948ms
Jan 23 23:15:20.318: INFO: Pod "client-containers-9ab64ad3-7a7a-441d-a060-ccd198de3e20": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014828164s
STEP: Saw pod success
Jan 23 23:15:20.318: INFO: Pod "client-containers-9ab64ad3-7a7a-441d-a060-ccd198de3e20" satisfied condition "success or failure"
Jan 23 23:15:20.326: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f pod client-containers-9ab64ad3-7a7a-441d-a060-ccd198de3e20 container test-container: <nil>
STEP: delete the pod
Jan 23 23:15:20.364: INFO: Waiting for pod client-containers-9ab64ad3-7a7a-441d-a060-ccd198de3e20 to disappear
Jan 23 23:15:20.370: INFO: Pod client-containers-9ab64ad3-7a7a-441d-a060-ccd198de3e20 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:15:20.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6486" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":280,"completed":266,"skipped":4360,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:15:20.391: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Update Demo
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:330
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a replication controller
Jan 23 23:15:20.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 create -f - --namespace=kubectl-117'
Jan 23 23:15:20.873: INFO: stderr: ""
Jan 23 23:15:20.873: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 23 23:15:20.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-117'
Jan 23 23:15:20.996: INFO: stderr: ""
Jan 23 23:15:20.997: INFO: stdout: "update-demo-nautilus-fj9gz update-demo-nautilus-v5j9n "
Jan 23 23:15:20.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get pods update-demo-nautilus-fj9gz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-117'
Jan 23 23:15:21.089: INFO: stderr: ""
Jan 23 23:15:21.089: INFO: stdout: ""
Jan 23 23:15:21.089: INFO: update-demo-nautilus-fj9gz is created but not running
Jan 23 23:15:26.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-117'
Jan 23 23:15:26.195: INFO: stderr: ""
Jan 23 23:15:26.195: INFO: stdout: "update-demo-nautilus-fj9gz update-demo-nautilus-v5j9n "
Jan 23 23:15:26.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get pods update-demo-nautilus-fj9gz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-117'
Jan 23 23:15:26.279: INFO: stderr: ""
Jan 23 23:15:26.279: INFO: stdout: "true"
Jan 23 23:15:26.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get pods update-demo-nautilus-fj9gz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-117'
Jan 23 23:15:26.363: INFO: stderr: ""
Jan 23 23:15:26.364: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 23 23:15:26.364: INFO: validating pod update-demo-nautilus-fj9gz
Jan 23 23:15:26.379: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 23 23:15:26.379: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 23 23:15:26.379: INFO: update-demo-nautilus-fj9gz is verified up and running
Jan 23 23:15:26.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get pods update-demo-nautilus-v5j9n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-117'
Jan 23 23:15:26.457: INFO: stderr: ""
Jan 23 23:15:26.457: INFO: stdout: "true"
Jan 23 23:15:26.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get pods update-demo-nautilus-v5j9n -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-117'
Jan 23 23:15:26.540: INFO: stderr: ""
Jan 23 23:15:26.540: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 23 23:15:26.540: INFO: validating pod update-demo-nautilus-v5j9n
Jan 23 23:15:26.557: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 23 23:15:26.557: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 23 23:15:26.557: INFO: update-demo-nautilus-v5j9n is verified up and running
STEP: using delete to clean up resources
Jan 23 23:15:26.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 delete --grace-period=0 --force -f - --namespace=kubectl-117'
Jan 23 23:15:26.655: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 23 23:15:26.655: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 23 23:15:26.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-117'
Jan 23 23:15:26.738: INFO: stderr: "No resources found in kubectl-117 namespace.\n"
Jan 23 23:15:26.738: INFO: stdout: ""
Jan 23 23:15:26.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get pods -l name=update-demo --namespace=kubectl-117 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 23 23:15:26.822: INFO: stderr: ""
Jan 23 23:15:26.822: INFO: stdout: "update-demo-nautilus-fj9gz\nupdate-demo-nautilus-v5j9n\n"
Jan 23 23:15:27.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-117'
Jan 23 23:15:27.403: INFO: stderr: "No resources found in kubectl-117 namespace.\n"
Jan 23 23:15:27.403: INFO: stdout: ""
Jan 23 23:15:27.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 get pods -l name=update-demo --namespace=kubectl-117 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 23 23:15:27.491: INFO: stderr: ""
Jan 23 23:15:27.491: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:15:27.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-117" for this suite.

• [SLOW TEST:7.115 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:328
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":280,"completed":267,"skipped":4367,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:15:27.508: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name cm-test-opt-del-b5966b9d-2342-4f53-98c4-92d3ed6e9723
STEP: Creating configMap with name cm-test-opt-upd-ff85bf6f-81e7-475a-a4e4-afc5848bbdf3
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-b5966b9d-2342-4f53-98c4-92d3ed6e9723
STEP: Updating configmap cm-test-opt-upd-ff85bf6f-81e7-475a-a4e4-afc5848bbdf3
STEP: Creating configMap with name cm-test-opt-create-5fa760c6-20c9-4be6-8def-236d03cc11b5
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:15:31.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4739" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":268,"skipped":4397,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:15:31.728: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 23 23:15:32.380: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 23 23:15:36.182: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 23 23:15:36.196: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7849-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:15:37.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9079" for this suite.
STEP: Destroying namespace "webhook-9079-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.241 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":280,"completed":269,"skipped":4399,"failed":0}
S
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:15:40.009: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 23 23:15:58.069: INFO: Container started at 2020-01-23 23:15:41 +0000 UTC, pod became ready at 2020-01-23 23:15:57 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:15:58.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4180" for this suite.

• [SLOW TEST:16.330 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":280,"completed":270,"skipped":4400,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:15:58.089: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating Agnhost RC
Jan 23 23:15:58.130: INFO: namespace kubectl-1153
Jan 23 23:15:58.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 create -f - --namespace=kubectl-1153'
Jan 23 23:15:58.466: INFO: stderr: ""
Jan 23 23:15:58.466: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Jan 23 23:15:59.470: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 23 23:15:59.470: INFO: Found 0 / 1
Jan 23 23:16:00.471: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 23 23:16:00.471: INFO: Found 1 / 1
Jan 23 23:16:00.471: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 23 23:16:00.476: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 23 23:16:00.476: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 23 23:16:00.476: INFO: wait on agnhost-master startup in kubectl-1153 
Jan 23 23:16:00.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 logs agnhost-master-l4xw7 agnhost-master --namespace=kubectl-1153'
Jan 23 23:16:00.582: INFO: stderr: ""
Jan 23 23:16:00.583: INFO: stdout: "Paused\n"
STEP: exposing RC
Jan 23 23:16:00.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 expose rc agnhost-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-1153'
Jan 23 23:16:00.697: INFO: stderr: ""
Jan 23 23:16:00.697: INFO: stdout: "service/rm2 exposed\n"
Jan 23 23:16:00.708: INFO: Service rm2 in namespace kubectl-1153 found.
STEP: exposing service
Jan 23 23:16:02.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837620349 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-1153'
Jan 23 23:16:02.824: INFO: stderr: ""
Jan 23 23:16:02.824: INFO: stdout: "service/rm3 exposed\n"
Jan 23 23:16:02.836: INFO: Service rm3 in namespace kubectl-1153 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:16:04.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1153" for this suite.

• [SLOW TEST:6.768 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1295
    should create services for rc  [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":280,"completed":271,"skipped":4408,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:16:04.861: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap configmap-5287/configmap-test-45a5e46f-9c86-4cb0-a9a3-14e88b2605c4
STEP: Creating a pod to test consume configMaps
Jan 23 23:16:04.910: INFO: Waiting up to 5m0s for pod "pod-configmaps-a4b400c8-ac59-40a9-86a3-70250e62f2db" in namespace "configmap-5287" to be "success or failure"
Jan 23 23:16:04.916: INFO: Pod "pod-configmaps-a4b400c8-ac59-40a9-86a3-70250e62f2db": Phase="Pending", Reason="", readiness=false. Elapsed: 5.416768ms
Jan 23 23:16:06.919: INFO: Pod "pod-configmaps-a4b400c8-ac59-40a9-86a3-70250e62f2db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009285219s
STEP: Saw pod success
Jan 23 23:16:06.920: INFO: Pod "pod-configmaps-a4b400c8-ac59-40a9-86a3-70250e62f2db" satisfied condition "success or failure"
Jan 23 23:16:06.924: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod pod-configmaps-a4b400c8-ac59-40a9-86a3-70250e62f2db container env-test: <nil>
STEP: delete the pod
Jan 23 23:16:06.949: INFO: Waiting for pod pod-configmaps-a4b400c8-ac59-40a9-86a3-70250e62f2db to disappear
Jan 23 23:16:06.953: INFO: Pod pod-configmaps-a4b400c8-ac59-40a9-86a3-70250e62f2db no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:16:06.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5287" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":280,"completed":272,"skipped":4430,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:16:06.975: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:16:15.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-203" for this suite.

• [SLOW TEST:8.063 seconds]
[sig-apps] Job
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":280,"completed":273,"skipped":4455,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:16:15.040: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 23 23:16:15.073: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:16:15.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-877" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":280,"completed":274,"skipped":4456,"failed":0}
SSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:16:15.799: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 23 23:16:19.719: INFO: Waiting up to 5m0s for pod "client-envvars-fb67f2d2-9806-42ab-8ffc-189dc3640716" in namespace "pods-8446" to be "success or failure"
Jan 23 23:16:19.727: INFO: Pod "client-envvars-fb67f2d2-9806-42ab-8ffc-189dc3640716": Phase="Pending", Reason="", readiness=false. Elapsed: 8.831032ms
Jan 23 23:16:21.733: INFO: Pod "client-envvars-fb67f2d2-9806-42ab-8ffc-189dc3640716": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013901736s
STEP: Saw pod success
Jan 23 23:16:21.733: INFO: Pod "client-envvars-fb67f2d2-9806-42ab-8ffc-189dc3640716" satisfied condition "success or failure"
Jan 23 23:16:21.736: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-fsd6f pod client-envvars-fb67f2d2-9806-42ab-8ffc-189dc3640716 container env3cont: <nil>
STEP: delete the pod
Jan 23 23:16:21.772: INFO: Waiting for pod client-envvars-fb67f2d2-9806-42ab-8ffc-189dc3640716 to disappear
Jan 23 23:16:21.784: INFO: Pod client-envvars-fb67f2d2-9806-42ab-8ffc-189dc3640716 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:16:21.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8446" for this suite.
•{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":280,"completed":275,"skipped":4460,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:16:21.820: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jan 23 23:16:22.120: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-356 /api/v1/namespaces/watch-356/configmaps/e2e-watch-test-resource-version 667cdc53-aed1-4520-aa69-7ab3db3e9f87 431368 0 2020-01-23 23:16:21 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 23 23:16:22.121: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-356 /api/v1/namespaces/watch-356/configmaps/e2e-watch-test-resource-version 667cdc53-aed1-4520-aa69-7ab3db3e9f87 431370 0 2020-01-23 23:16:21 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:16:22.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-356" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":280,"completed":276,"skipped":4489,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:16:22.157: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 23 23:16:22.778: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 23 23:16:25.804: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:16:35.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7547" for this suite.
STEP: Destroying namespace "webhook-7547-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:13.949 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":280,"completed":277,"skipped":4490,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:16:36.113: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:17:03.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8505" for this suite.

• [SLOW TEST:25.857 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  blackbox test
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":280,"completed":278,"skipped":4501,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:17:03.801: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on node default medium
Jan 23 23:17:03.910: INFO: Waiting up to 5m0s for pod "pod-1ac2dcd7-2636-476a-84e9-e25aa72026a6" in namespace "emptydir-4011" to be "success or failure"
Jan 23 23:17:03.926: INFO: Pod "pod-1ac2dcd7-2636-476a-84e9-e25aa72026a6": Phase="Pending", Reason="", readiness=false. Elapsed: 15.970558ms
Jan 23 23:17:05.931: INFO: Pod "pod-1ac2dcd7-2636-476a-84e9-e25aa72026a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020736978s
Jan 23 23:17:07.935: INFO: Pod "pod-1ac2dcd7-2636-476a-84e9-e25aa72026a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024959737s
STEP: Saw pod success
Jan 23 23:17:07.935: INFO: Pod "pod-1ac2dcd7-2636-476a-84e9-e25aa72026a6" satisfied condition "success or failure"
Jan 23 23:17:07.939: INFO: Trying to get logs from node management-cluster-1-17-2-md-0-5f64bb5777-dlwcz pod pod-1ac2dcd7-2636-476a-84e9-e25aa72026a6 container test-container: <nil>
STEP: delete the pod
Jan 23 23:17:08.269: INFO: Waiting for pod pod-1ac2dcd7-2636-476a-84e9-e25aa72026a6 to disappear
Jan 23 23:17:08.274: INFO: Pod pod-1ac2dcd7-2636-476a-84e9-e25aa72026a6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:17:08.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4011" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":279,"skipped":4546,"failed":0}
SSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 23 23:17:08.299: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 23 23:17:08.343: INFO: >>> kubeConfig: /tmp/kubeconfig-837620349
STEP: creating replication controller svc-latency-rc in namespace svc-latency-7614
I0123 23:17:08.386336      21 runners.go:189] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7614, replica count: 1
I0123 23:17:09.438968      21 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 23 23:17:09.559: INFO: Created: latency-svc-xv5xn
Jan 23 23:17:09.599: INFO: Got endpoints: latency-svc-xv5xn [59.697651ms]
Jan 23 23:17:09.634: INFO: Created: latency-svc-8v2z6
Jan 23 23:17:09.653: INFO: Got endpoints: latency-svc-8v2z6 [51.51288ms]
Jan 23 23:17:09.661: INFO: Created: latency-svc-vndjw
Jan 23 23:17:09.670: INFO: Got endpoints: latency-svc-vndjw [67.49305ms]
Jan 23 23:17:09.685: INFO: Created: latency-svc-mf2t9
Jan 23 23:17:09.695: INFO: Got endpoints: latency-svc-mf2t9 [91.446406ms]
Jan 23 23:17:09.753: INFO: Created: latency-svc-wnq2w
Jan 23 23:17:09.756: INFO: Got endpoints: latency-svc-wnq2w [152.913973ms]
Jan 23 23:17:09.762: INFO: Created: latency-svc-rcbbr
Jan 23 23:17:09.780: INFO: Got endpoints: latency-svc-rcbbr [176.361372ms]
Jan 23 23:17:09.788: INFO: Created: latency-svc-xjzg5
Jan 23 23:17:09.795: INFO: Got endpoints: latency-svc-xjzg5 [191.479745ms]
Jan 23 23:17:09.826: INFO: Created: latency-svc-wv5pw
Jan 23 23:17:09.845: INFO: Got endpoints: latency-svc-wv5pw [241.642834ms]
Jan 23 23:17:09.874: INFO: Created: latency-svc-mm5sd
Jan 23 23:17:09.874: INFO: Got endpoints: latency-svc-mm5sd [269.866631ms]
Jan 23 23:17:09.920: INFO: Created: latency-svc-hgwfr
Jan 23 23:17:09.921: INFO: Got endpoints: latency-svc-hgwfr [316.198547ms]
Jan 23 23:17:09.953: INFO: Created: latency-svc-fzv4b
Jan 23 23:17:09.957: INFO: Got endpoints: latency-svc-fzv4b [352.907178ms]
Jan 23 23:17:09.969: INFO: Created: latency-svc-dmg45
Jan 23 23:17:09.978: INFO: Got endpoints: latency-svc-dmg45 [373.418756ms]
Jan 23 23:17:10.036: INFO: Created: latency-svc-q87p2
Jan 23 23:17:10.037: INFO: Got endpoints: latency-svc-q87p2 [432.593292ms]
Jan 23 23:17:10.052: INFO: Created: latency-svc-n27j6
Jan 23 23:17:10.058: INFO: Got endpoints: latency-svc-n27j6 [453.013405ms]
Jan 23 23:17:10.104: INFO: Created: latency-svc-sb8pc
Jan 23 23:17:10.112: INFO: Got endpoints: latency-svc-sb8pc [507.210761ms]
Jan 23 23:17:10.131: INFO: Created: latency-svc-dbsvd
Jan 23 23:17:10.136: INFO: Got endpoints: latency-svc-dbsvd [531.674863ms]
Jan 23 23:17:10.191: INFO: Created: latency-svc-gtjrx
Jan 23 23:17:10.198: INFO: Created: latency-svc-w85ng
Jan 23 23:17:10.206: INFO: Got endpoints: latency-svc-w85ng [535.872245ms]
Jan 23 23:17:10.207: INFO: Got endpoints: latency-svc-gtjrx [553.239222ms]
Jan 23 23:17:10.241: INFO: Created: latency-svc-dsp6s
Jan 23 23:17:10.247: INFO: Got endpoints: latency-svc-dsp6s [551.785448ms]
Jan 23 23:17:10.275: INFO: Created: latency-svc-8xm2m
Jan 23 23:17:10.276: INFO: Got endpoints: latency-svc-8xm2m [519.768445ms]
Jan 23 23:17:10.304: INFO: Created: latency-svc-mfbwf
Jan 23 23:17:10.354: INFO: Got endpoints: latency-svc-mfbwf [574.553759ms]
Jan 23 23:17:10.376: INFO: Created: latency-svc-h6zbf
Jan 23 23:17:10.379: INFO: Got endpoints: latency-svc-h6zbf [582.727029ms]
Jan 23 23:17:10.407: INFO: Created: latency-svc-57x9c
Jan 23 23:17:10.412: INFO: Got endpoints: latency-svc-57x9c [565.964123ms]
Jan 23 23:17:10.427: INFO: Created: latency-svc-8h8dx
Jan 23 23:17:10.438: INFO: Got endpoints: latency-svc-8h8dx [563.987443ms]
Jan 23 23:17:10.442: INFO: Created: latency-svc-8qjcs
Jan 23 23:17:10.472: INFO: Created: latency-svc-9j2bd
Jan 23 23:17:10.474: INFO: Got endpoints: latency-svc-8qjcs [553.704017ms]
Jan 23 23:17:10.485: INFO: Got endpoints: latency-svc-9j2bd [527.861862ms]
Jan 23 23:17:10.499: INFO: Created: latency-svc-jttc8
Jan 23 23:17:10.516: INFO: Got endpoints: latency-svc-jttc8 [537.831038ms]
Jan 23 23:17:10.525: INFO: Created: latency-svc-tfswb
Jan 23 23:17:10.540: INFO: Got endpoints: latency-svc-tfswb [503.586894ms]
Jan 23 23:17:10.548: INFO: Created: latency-svc-pp4hb
Jan 23 23:17:10.567: INFO: Created: latency-svc-xxsvq
Jan 23 23:17:10.568: INFO: Got endpoints: latency-svc-pp4hb [509.57996ms]
Jan 23 23:17:10.577: INFO: Got endpoints: latency-svc-xxsvq [465.469633ms]
Jan 23 23:17:10.597: INFO: Created: latency-svc-n9xpf
Jan 23 23:17:10.602: INFO: Got endpoints: latency-svc-n9xpf [465.257811ms]
Jan 23 23:17:10.623: INFO: Created: latency-svc-gbd58
Jan 23 23:17:10.625: INFO: Got endpoints: latency-svc-gbd58 [417.33139ms]
Jan 23 23:17:10.645: INFO: Created: latency-svc-hcx5t
Jan 23 23:17:10.649: INFO: Got endpoints: latency-svc-hcx5t [442.814917ms]
Jan 23 23:17:10.662: INFO: Created: latency-svc-n728n
Jan 23 23:17:10.664: INFO: Got endpoints: latency-svc-n728n [416.890388ms]
Jan 23 23:17:10.679: INFO: Created: latency-svc-49n5m
Jan 23 23:17:10.698: INFO: Got endpoints: latency-svc-49n5m [421.627447ms]
Jan 23 23:17:10.704: INFO: Created: latency-svc-9h2rr
Jan 23 23:17:10.725: INFO: Got endpoints: latency-svc-9h2rr [370.43781ms]
Jan 23 23:17:10.742: INFO: Created: latency-svc-rf97n
Jan 23 23:17:10.754: INFO: Got endpoints: latency-svc-rf97n [374.415876ms]
Jan 23 23:17:10.762: INFO: Created: latency-svc-r55wl
Jan 23 23:17:10.806: INFO: Got endpoints: latency-svc-r55wl [393.862247ms]
Jan 23 23:17:10.832: INFO: Created: latency-svc-7dfck
Jan 23 23:17:10.849: INFO: Got endpoints: latency-svc-7dfck [410.329702ms]
Jan 23 23:17:10.850: INFO: Created: latency-svc-vrgbp
Jan 23 23:17:10.858: INFO: Got endpoints: latency-svc-vrgbp [383.724279ms]
Jan 23 23:17:10.896: INFO: Created: latency-svc-kgp4c
Jan 23 23:17:10.904: INFO: Got endpoints: latency-svc-kgp4c [417.590301ms]
Jan 23 23:17:10.935: INFO: Created: latency-svc-p8crd
Jan 23 23:17:10.959: INFO: Got endpoints: latency-svc-p8crd [443.341958ms]
Jan 23 23:17:10.969: INFO: Created: latency-svc-kcvr4
Jan 23 23:17:10.982: INFO: Got endpoints: latency-svc-kcvr4 [441.200757ms]
Jan 23 23:17:11.002: INFO: Created: latency-svc-7479m
Jan 23 23:17:11.020: INFO: Got endpoints: latency-svc-7479m [452.179246ms]
Jan 23 23:17:11.044: INFO: Created: latency-svc-mtn8v
Jan 23 23:17:11.045: INFO: Got endpoints: latency-svc-mtn8v [466.812767ms]
Jan 23 23:17:11.067: INFO: Created: latency-svc-4gcj4
Jan 23 23:17:11.076: INFO: Got endpoints: latency-svc-4gcj4 [474.495493ms]
Jan 23 23:17:11.090: INFO: Created: latency-svc-cwt65
Jan 23 23:17:11.104: INFO: Got endpoints: latency-svc-cwt65 [478.762202ms]
Jan 23 23:17:11.120: INFO: Created: latency-svc-xpfmv
Jan 23 23:17:11.134: INFO: Got endpoints: latency-svc-xpfmv [483.806987ms]
Jan 23 23:17:11.134: INFO: Created: latency-svc-4j5m4
Jan 23 23:17:11.150: INFO: Got endpoints: latency-svc-4j5m4 [485.0652ms]
Jan 23 23:17:11.162: INFO: Created: latency-svc-79cjt
Jan 23 23:17:11.164: INFO: Got endpoints: latency-svc-79cjt [466.435128ms]
Jan 23 23:17:11.202: INFO: Created: latency-svc-jkqhb
Jan 23 23:17:11.208: INFO: Got endpoints: latency-svc-jkqhb [483.312111ms]
Jan 23 23:17:11.238: INFO: Created: latency-svc-4zb29
Jan 23 23:17:11.238: INFO: Got endpoints: latency-svc-4zb29 [484.365523ms]
Jan 23 23:17:11.267: INFO: Created: latency-svc-btq9j
Jan 23 23:17:11.269: INFO: Got endpoints: latency-svc-btq9j [463.084439ms]
Jan 23 23:17:11.282: INFO: Created: latency-svc-vjqh4
Jan 23 23:17:11.283: INFO: Got endpoints: latency-svc-vjqh4 [434.073078ms]
Jan 23 23:17:11.310: INFO: Created: latency-svc-8rhnz
Jan 23 23:17:11.332: INFO: Got endpoints: latency-svc-8rhnz [473.146809ms]
Jan 23 23:17:11.343: INFO: Created: latency-svc-rv7jz
Jan 23 23:17:11.356: INFO: Got endpoints: latency-svc-rv7jz [451.636794ms]
Jan 23 23:17:11.360: INFO: Created: latency-svc-j9k5j
Jan 23 23:17:11.380: INFO: Got endpoints: latency-svc-j9k5j [419.970247ms]
Jan 23 23:17:11.400: INFO: Created: latency-svc-fb4px
Jan 23 23:17:11.401: INFO: Got endpoints: latency-svc-fb4px [418.182677ms]
Jan 23 23:17:11.414: INFO: Created: latency-svc-hnkxt
Jan 23 23:17:11.424: INFO: Got endpoints: latency-svc-hnkxt [403.610897ms]
Jan 23 23:17:11.441: INFO: Created: latency-svc-cgck9
Jan 23 23:17:11.444: INFO: Got endpoints: latency-svc-cgck9 [398.861727ms]
Jan 23 23:17:11.478: INFO: Created: latency-svc-5t66j
Jan 23 23:17:11.508: INFO: Got endpoints: latency-svc-5t66j [431.721826ms]
Jan 23 23:17:11.517: INFO: Created: latency-svc-hn8qz
Jan 23 23:17:11.532: INFO: Got endpoints: latency-svc-hn8qz [428.084594ms]
Jan 23 23:17:11.567: INFO: Created: latency-svc-bq7jn
Jan 23 23:17:11.587: INFO: Got endpoints: latency-svc-bq7jn [453.035299ms]
Jan 23 23:17:11.611: INFO: Created: latency-svc-6hjbv
Jan 23 23:17:11.624: INFO: Got endpoints: latency-svc-6hjbv [473.832046ms]
Jan 23 23:17:11.636: INFO: Created: latency-svc-67kpc
Jan 23 23:17:11.651: INFO: Got endpoints: latency-svc-67kpc [486.426218ms]
Jan 23 23:17:11.663: INFO: Created: latency-svc-6gtxr
Jan 23 23:17:11.669: INFO: Got endpoints: latency-svc-6gtxr [460.93381ms]
Jan 23 23:17:11.690: INFO: Created: latency-svc-kq5qr
Jan 23 23:17:11.704: INFO: Got endpoints: latency-svc-kq5qr [464.884004ms]
Jan 23 23:17:11.714: INFO: Created: latency-svc-fmtds
Jan 23 23:17:11.721: INFO: Got endpoints: latency-svc-fmtds [450.201985ms]
Jan 23 23:17:11.766: INFO: Created: latency-svc-ndwbg
Jan 23 23:17:11.768: INFO: Got endpoints: latency-svc-ndwbg [483.998309ms]
Jan 23 23:17:11.796: INFO: Created: latency-svc-ttdtn
Jan 23 23:17:11.799: INFO: Got endpoints: latency-svc-ttdtn [465.646319ms]
Jan 23 23:17:11.827: INFO: Created: latency-svc-qbb2d
Jan 23 23:17:11.832: INFO: Got endpoints: latency-svc-qbb2d [476.326703ms]
Jan 23 23:17:11.848: INFO: Created: latency-svc-m77ch
Jan 23 23:17:11.858: INFO: Got endpoints: latency-svc-m77ch [476.98801ms]
Jan 23 23:17:11.873: INFO: Created: latency-svc-8tc28
Jan 23 23:17:11.883: INFO: Got endpoints: latency-svc-8tc28 [481.503411ms]
Jan 23 23:17:11.910: INFO: Created: latency-svc-fg955
Jan 23 23:17:11.922: INFO: Got endpoints: latency-svc-fg955 [497.728116ms]
Jan 23 23:17:11.946: INFO: Created: latency-svc-hfpgv
Jan 23 23:17:11.951: INFO: Got endpoints: latency-svc-hfpgv [504.684274ms]
Jan 23 23:17:11.967: INFO: Created: latency-svc-xxhlv
Jan 23 23:17:11.983: INFO: Got endpoints: latency-svc-xxhlv [473.594623ms]
Jan 23 23:17:12.012: INFO: Created: latency-svc-b5cft
Jan 23 23:17:12.020: INFO: Got endpoints: latency-svc-b5cft [487.977192ms]
Jan 23 23:17:12.072: INFO: Created: latency-svc-rbt66
Jan 23 23:17:12.085: INFO: Got endpoints: latency-svc-rbt66 [497.056643ms]
Jan 23 23:17:12.100: INFO: Created: latency-svc-np86f
Jan 23 23:17:12.107: INFO: Got endpoints: latency-svc-np86f [482.744986ms]
Jan 23 23:17:12.128: INFO: Created: latency-svc-9df97
Jan 23 23:17:12.135: INFO: Got endpoints: latency-svc-9df97 [483.190153ms]
Jan 23 23:17:12.157: INFO: Created: latency-svc-9bvf9
Jan 23 23:17:12.164: INFO: Got endpoints: latency-svc-9bvf9 [492.464312ms]
Jan 23 23:17:12.203: INFO: Created: latency-svc-mltkq
Jan 23 23:17:12.209: INFO: Got endpoints: latency-svc-mltkq [504.408735ms]
Jan 23 23:17:12.238: INFO: Created: latency-svc-lr66t
Jan 23 23:17:12.243: INFO: Got endpoints: latency-svc-lr66t [522.411136ms]
Jan 23 23:17:12.255: INFO: Created: latency-svc-nts4f
Jan 23 23:17:12.291: INFO: Created: latency-svc-9jpjw
Jan 23 23:17:12.325: INFO: Got endpoints: latency-svc-nts4f [557.646949ms]
Jan 23 23:17:12.340: INFO: Got endpoints: latency-svc-9jpjw [541.42443ms]
Jan 23 23:17:12.345: INFO: Created: latency-svc-z4k2t
Jan 23 23:17:12.372: INFO: Created: latency-svc-7p486
Jan 23 23:17:12.399: INFO: Got endpoints: latency-svc-z4k2t [566.961113ms]
Jan 23 23:17:12.401: INFO: Created: latency-svc-h5l2g
Jan 23 23:17:12.431: INFO: Created: latency-svc-djzqt
Jan 23 23:17:12.441: INFO: Got endpoints: latency-svc-7p486 [583.342154ms]
Jan 23 23:17:12.474: INFO: Created: latency-svc-6w9sc
Jan 23 23:17:12.479: INFO: Created: latency-svc-psgsf
Jan 23 23:17:12.497: INFO: Got endpoints: latency-svc-h5l2g [614.337017ms]
Jan 23 23:17:12.501: INFO: Created: latency-svc-tmj6r
Jan 23 23:17:12.520: INFO: Created: latency-svc-c6zl9
Jan 23 23:17:12.535: INFO: Created: latency-svc-4zc8f
Jan 23 23:17:12.544: INFO: Got endpoints: latency-svc-djzqt [622.022555ms]
Jan 23 23:17:12.561: INFO: Created: latency-svc-f9mkz
Jan 23 23:17:12.579: INFO: Created: latency-svc-48ltt
Jan 23 23:17:12.589: INFO: Created: latency-svc-hckkk
Jan 23 23:17:12.599: INFO: Got endpoints: latency-svc-6w9sc [647.616142ms]
Jan 23 23:17:12.620: INFO: Created: latency-svc-gs2c7
Jan 23 23:17:12.636: INFO: Created: latency-svc-lxlbq
Jan 23 23:17:12.653: INFO: Got endpoints: latency-svc-psgsf [669.616324ms]
Jan 23 23:17:12.665: INFO: Created: latency-svc-gcjkg
Jan 23 23:17:12.668: INFO: Created: latency-svc-tgftk
Jan 23 23:17:12.696: INFO: Got endpoints: latency-svc-tmj6r [675.26736ms]
Jan 23 23:17:12.706: INFO: Created: latency-svc-gg4zx
Jan 23 23:17:12.720: INFO: Created: latency-svc-wf8lk
Jan 23 23:17:12.742: INFO: Created: latency-svc-4vrjw
Jan 23 23:17:12.747: INFO: Got endpoints: latency-svc-c6zl9 [662.185444ms]
Jan 23 23:17:12.764: INFO: Created: latency-svc-j9pw9
Jan 23 23:17:12.791: INFO: Created: latency-svc-jcrtg
Jan 23 23:17:12.801: INFO: Got endpoints: latency-svc-4zc8f [694.087369ms]
Jan 23 23:17:12.833: INFO: Created: latency-svc-5b6r8
Jan 23 23:17:12.834: INFO: Created: latency-svc-ct9kt
Jan 23 23:17:12.840: INFO: Got endpoints: latency-svc-f9mkz [700.746934ms]
Jan 23 23:17:12.900: INFO: Created: latency-svc-c7s98
Jan 23 23:17:12.907: INFO: Got endpoints: latency-svc-48ltt [742.788512ms]
Jan 23 23:17:12.948: INFO: Created: latency-svc-lq96m
Jan 23 23:17:12.956: INFO: Got endpoints: latency-svc-hckkk [746.653935ms]
Jan 23 23:17:12.963: INFO: Created: latency-svc-4gng9
Jan 23 23:17:12.992: INFO: Created: latency-svc-dtq2d
Jan 23 23:17:13.000: INFO: Got endpoints: latency-svc-gs2c7 [756.08332ms]
Jan 23 23:17:13.019: INFO: Created: latency-svc-6h9q8
Jan 23 23:17:13.039: INFO: Got endpoints: latency-svc-lxlbq [713.55639ms]
Jan 23 23:17:13.074: INFO: Created: latency-svc-r79gx
Jan 23 23:17:13.095: INFO: Got endpoints: latency-svc-gcjkg [755.173255ms]
Jan 23 23:17:13.144: INFO: Created: latency-svc-5gdbq
Jan 23 23:17:13.145: INFO: Got endpoints: latency-svc-tgftk [745.641469ms]
Jan 23 23:17:13.178: INFO: Created: latency-svc-qh5c4
Jan 23 23:17:13.193: INFO: Got endpoints: latency-svc-gg4zx [751.561481ms]
Jan 23 23:17:13.246: INFO: Created: latency-svc-zz82m
Jan 23 23:17:13.249: INFO: Got endpoints: latency-svc-wf8lk [752.268584ms]
Jan 23 23:17:13.291: INFO: Created: latency-svc-k5zrx
Jan 23 23:17:13.295: INFO: Got endpoints: latency-svc-4vrjw [750.466883ms]
Jan 23 23:17:13.377: INFO: Created: latency-svc-jnz4q
Jan 23 23:17:13.389: INFO: Got endpoints: latency-svc-j9pw9 [789.619402ms]
Jan 23 23:17:13.431: INFO: Got endpoints: latency-svc-jcrtg [777.534872ms]
Jan 23 23:17:13.439: INFO: Created: latency-svc-22v7j
Jan 23 23:17:13.450: INFO: Got endpoints: latency-svc-ct9kt [753.498637ms]
Jan 23 23:17:13.498: INFO: Got endpoints: latency-svc-5b6r8 [749.926498ms]
Jan 23 23:17:13.550: INFO: Created: latency-svc-nflqx
Jan 23 23:17:13.550: INFO: Got endpoints: latency-svc-c7s98 [748.384063ms]
Jan 23 23:17:13.589: INFO: Created: latency-svc-2r84w
Jan 23 23:17:13.622: INFO: Got endpoints: latency-svc-lq96m [781.71529ms]
Jan 23 23:17:13.649: INFO: Got endpoints: latency-svc-4gng9 [741.566024ms]
Jan 23 23:17:13.655: INFO: Created: latency-svc-c2f2k
Jan 23 23:17:13.703: INFO: Got endpoints: latency-svc-dtq2d [747.109451ms]
Jan 23 23:17:13.710: INFO: Created: latency-svc-7v96d
Jan 23 23:17:13.746: INFO: Created: latency-svc-ck4mh
Jan 23 23:17:13.749: INFO: Got endpoints: latency-svc-6h9q8 [748.161095ms]
Jan 23 23:17:13.780: INFO: Created: latency-svc-ck6lg
Jan 23 23:17:13.792: INFO: Got endpoints: latency-svc-r79gx [751.66702ms]
Jan 23 23:17:13.802: INFO: Created: latency-svc-jqzgt
Jan 23 23:17:13.823: INFO: Created: latency-svc-wj827
Jan 23 23:17:13.843: INFO: Got endpoints: latency-svc-5gdbq [747.227808ms]
Jan 23 23:17:13.861: INFO: Created: latency-svc-6fdgp
Jan 23 23:17:13.899: INFO: Created: latency-svc-gbn9h
Jan 23 23:17:13.902: INFO: Got endpoints: latency-svc-qh5c4 [756.737621ms]
Jan 23 23:17:13.941: INFO: Got endpoints: latency-svc-zz82m [747.205312ms]
Jan 23 23:17:13.999: INFO: Created: latency-svc-mvh9n
Jan 23 23:17:14.059: INFO: Got endpoints: latency-svc-k5zrx [809.15882ms]
Jan 23 23:17:14.102: INFO: Created: latency-svc-drz6v
Jan 23 23:17:14.114: INFO: Got endpoints: latency-svc-jnz4q [819.323437ms]
Jan 23 23:17:14.118: INFO: Got endpoints: latency-svc-22v7j [728.275902ms]
Jan 23 23:17:14.164: INFO: Got endpoints: latency-svc-nflqx [731.73658ms]
Jan 23 23:17:14.205: INFO: Created: latency-svc-rm7j2
Jan 23 23:17:14.209: INFO: Got endpoints: latency-svc-2r84w [758.287219ms]
Jan 23 23:17:14.244: INFO: Got endpoints: latency-svc-c2f2k [746.173637ms]
Jan 23 23:17:14.305: INFO: Got endpoints: latency-svc-7v96d [754.999214ms]
Jan 23 23:17:14.323: INFO: Created: latency-svc-fnz8l
Jan 23 23:17:14.353: INFO: Got endpoints: latency-svc-ck4mh [730.9397ms]
Jan 23 23:17:14.395: INFO: Created: latency-svc-z5hbm
Jan 23 23:17:14.426: INFO: Got endpoints: latency-svc-ck6lg [776.879179ms]
Jan 23 23:17:14.441: INFO: Created: latency-svc-vvd4z
Jan 23 23:17:14.454: INFO: Got endpoints: latency-svc-jqzgt [750.579848ms]
Jan 23 23:17:14.482: INFO: Created: latency-svc-xxbt2
Jan 23 23:17:14.491: INFO: Got endpoints: latency-svc-wj827 [741.867355ms]
Jan 23 23:17:14.507: INFO: Created: latency-svc-qvfnr
Jan 23 23:17:14.537: INFO: Created: latency-svc-n674c
Jan 23 23:17:14.662: INFO: Got endpoints: latency-svc-6fdgp [870.867129ms]
Jan 23 23:17:14.742: INFO: Created: latency-svc-fn8t4
Jan 23 23:17:14.745: INFO: Got endpoints: latency-svc-mvh9n [842.689686ms]
Jan 23 23:17:14.746: INFO: Got endpoints: latency-svc-gbn9h [902.509001ms]
Jan 23 23:17:14.746: INFO: Got endpoints: latency-svc-drz6v [804.818266ms]
Jan 23 23:17:14.840: INFO: Got endpoints: latency-svc-rm7j2 [780.285276ms]
Jan 23 23:17:14.850: INFO: Got endpoints: latency-svc-fnz8l [735.575235ms]
Jan 23 23:17:14.866: INFO: Created: latency-svc-w6lgr
Jan 23 23:17:14.870: INFO: Got endpoints: latency-svc-z5hbm [751.390896ms]
Jan 23 23:17:15.097: INFO: Got endpoints: latency-svc-vvd4z [931.947436ms]
Jan 23 23:17:15.102: INFO: Got endpoints: latency-svc-xxbt2 [892.625752ms]
Jan 23 23:17:15.109: INFO: Got endpoints: latency-svc-qvfnr [864.208631ms]
Jan 23 23:17:15.109: INFO: Got endpoints: latency-svc-n674c [802.879067ms]
Jan 23 23:17:15.171: INFO: Got endpoints: latency-svc-fn8t4 [817.336115ms]
Jan 23 23:17:15.196: INFO: Got endpoints: latency-svc-w6lgr [768.316017ms]
Jan 23 23:17:15.197: INFO: Created: latency-svc-lc7jt
Jan 23 23:17:15.317: INFO: Got endpoints: latency-svc-lc7jt [862.766198ms]
Jan 23 23:17:15.341: INFO: Created: latency-svc-snvcm
Jan 23 23:17:15.371: INFO: Got endpoints: latency-svc-snvcm [879.54212ms]
Jan 23 23:17:15.454: INFO: Created: latency-svc-lrfvk
Jan 23 23:17:15.460: INFO: Got endpoints: latency-svc-lrfvk [797.051226ms]
Jan 23 23:17:15.481: INFO: Created: latency-svc-tprtc
Jan 23 23:17:15.493: INFO: Got endpoints: latency-svc-tprtc [746.511489ms]
Jan 23 23:17:15.507: INFO: Created: latency-svc-m4rbf
Jan 23 23:17:15.514: INFO: Got endpoints: latency-svc-m4rbf [767.716109ms]
Jan 23 23:17:15.530: INFO: Created: latency-svc-r44dl
Jan 23 23:17:15.540: INFO: Got endpoints: latency-svc-r44dl [790.980187ms]
Jan 23 23:17:15.562: INFO: Created: latency-svc-zlqzg
Jan 23 23:17:15.562: INFO: Got endpoints: latency-svc-zlqzg [721.307831ms]
Jan 23 23:17:15.586: INFO: Created: latency-svc-ntzdv
Jan 23 23:17:15.594: INFO: Got endpoints: latency-svc-ntzdv [743.837312ms]
Jan 23 23:17:15.609: INFO: Created: latency-svc-zr5rw
Jan 23 23:17:15.613: INFO: Got endpoints: latency-svc-zr5rw [742.492757ms]
Jan 23 23:17:15.631: INFO: Created: latency-svc-24gmm
Jan 23 23:17:15.641: INFO: Got endpoints: latency-svc-24gmm [543.950027ms]
Jan 23 23:17:15.673: INFO: Created: latency-svc-q5fhz
Jan 23 23:17:15.692: INFO: Created: latency-svc-xwrg5
Jan 23 23:17:15.695: INFO: Got endpoints: latency-svc-q5fhz [592.74923ms]
Jan 23 23:17:15.721: INFO: Created: latency-svc-xbzjb
Jan 23 23:17:15.747: INFO: Created: latency-svc-2fms5
Jan 23 23:17:15.752: INFO: Got endpoints: latency-svc-xwrg5 [643.039165ms]
Jan 23 23:17:15.772: INFO: Created: latency-svc-jvd9r
Jan 23 23:17:15.799: INFO: Created: latency-svc-cctpb
Jan 23 23:17:15.819: INFO: Created: latency-svc-8gspk
Jan 23 23:17:15.820: INFO: Got endpoints: latency-svc-xbzjb [710.740275ms]
Jan 23 23:17:15.848: INFO: Got endpoints: latency-svc-2fms5 [676.551433ms]
Jan 23 23:17:15.855: INFO: Created: latency-svc-585c2
Jan 23 23:17:15.874: INFO: Created: latency-svc-7kfb4
Jan 23 23:17:15.898: INFO: Got endpoints: latency-svc-jvd9r [701.895141ms]
Jan 23 23:17:15.900: INFO: Created: latency-svc-m8wj6
Jan 23 23:17:15.929: INFO: Created: latency-svc-jmbtn
Jan 23 23:17:15.936: INFO: Created: latency-svc-hp54s
Jan 23 23:17:15.947: INFO: Got endpoints: latency-svc-cctpb [629.585655ms]
Jan 23 23:17:15.956: INFO: Created: latency-svc-lx4bg
Jan 23 23:17:15.979: INFO: Created: latency-svc-fwl88
Jan 23 23:17:15.994: INFO: Created: latency-svc-2xnls
Jan 23 23:17:16.002: INFO: Got endpoints: latency-svc-8gspk [631.575937ms]
Jan 23 23:17:16.024: INFO: Created: latency-svc-ghz6c
Jan 23 23:17:16.046: INFO: Created: latency-svc-285zk
Jan 23 23:17:16.046: INFO: Got endpoints: latency-svc-585c2 [584.921872ms]
Jan 23 23:17:16.068: INFO: Created: latency-svc-6ps9p
Jan 23 23:17:16.088: INFO: Created: latency-svc-l4f5x
Jan 23 23:17:16.096: INFO: Got endpoints: latency-svc-7kfb4 [602.454506ms]
Jan 23 23:17:16.099: INFO: Created: latency-svc-ktln9
Jan 23 23:17:16.120: INFO: Created: latency-svc-472jj
Jan 23 23:17:16.133: INFO: Created: latency-svc-hqs54
Jan 23 23:17:16.144: INFO: Got endpoints: latency-svc-m8wj6 [629.293105ms]
Jan 23 23:17:16.152: INFO: Created: latency-svc-mv85k
Jan 23 23:17:16.173: INFO: Created: latency-svc-vcm9g
Jan 23 23:17:16.185: INFO: Created: latency-svc-68nqd
Jan 23 23:17:16.195: INFO: Got endpoints: latency-svc-jmbtn [654.698188ms]
Jan 23 23:17:16.216: INFO: Created: latency-svc-lvzmm
Jan 23 23:17:16.240: INFO: Got endpoints: latency-svc-hp54s [677.510992ms]
Jan 23 23:17:16.274: INFO: Created: latency-svc-j5z9f
Jan 23 23:17:16.311: INFO: Got endpoints: latency-svc-lx4bg [716.116503ms]
Jan 23 23:17:16.356: INFO: Got endpoints: latency-svc-fwl88 [742.085011ms]
Jan 23 23:17:16.360: INFO: Created: latency-svc-r7ll7
Jan 23 23:17:16.398: INFO: Created: latency-svc-mtp9f
Jan 23 23:17:16.404: INFO: Got endpoints: latency-svc-2xnls [761.906229ms]
Jan 23 23:17:16.436: INFO: Created: latency-svc-qxxn6
Jan 23 23:17:16.442: INFO: Got endpoints: latency-svc-ghz6c [747.190497ms]
Jan 23 23:17:16.458: INFO: Created: latency-svc-kdqxr
Jan 23 23:17:16.491: INFO: Got endpoints: latency-svc-285zk [738.280561ms]
Jan 23 23:17:16.510: INFO: Created: latency-svc-8bc8t
Jan 23 23:17:16.543: INFO: Got endpoints: latency-svc-6ps9p [722.784463ms]
Jan 23 23:17:16.564: INFO: Created: latency-svc-chr5k
Jan 23 23:17:16.591: INFO: Got endpoints: latency-svc-l4f5x [743.175859ms]
Jan 23 23:17:16.617: INFO: Created: latency-svc-r9mtr
Jan 23 23:17:16.641: INFO: Got endpoints: latency-svc-ktln9 [741.950411ms]
Jan 23 23:17:16.665: INFO: Created: latency-svc-lwcp5
Jan 23 23:17:16.692: INFO: Got endpoints: latency-svc-472jj [744.693958ms]
Jan 23 23:17:16.719: INFO: Created: latency-svc-strst
Jan 23 23:17:16.739: INFO: Got endpoints: latency-svc-hqs54 [736.236459ms]
Jan 23 23:17:16.756: INFO: Created: latency-svc-2pn29
Jan 23 23:17:16.796: INFO: Got endpoints: latency-svc-mv85k [749.694354ms]
Jan 23 23:17:16.814: INFO: Created: latency-svc-k49zl
Jan 23 23:17:16.843: INFO: Got endpoints: latency-svc-vcm9g [746.300006ms]
Jan 23 23:17:16.860: INFO: Created: latency-svc-rnt9f
Jan 23 23:17:16.893: INFO: Got endpoints: latency-svc-68nqd [748.414833ms]
Jan 23 23:17:16.930: INFO: Created: latency-svc-kzdw5
Jan 23 23:17:16.940: INFO: Got endpoints: latency-svc-lvzmm [744.296159ms]
Jan 23 23:17:16.959: INFO: Created: latency-svc-b5xg8
Jan 23 23:17:16.994: INFO: Got endpoints: latency-svc-j5z9f [753.497009ms]
Jan 23 23:17:17.014: INFO: Created: latency-svc-ghcrb
Jan 23 23:17:17.041: INFO: Got endpoints: latency-svc-r7ll7 [728.396918ms]
Jan 23 23:17:17.064: INFO: Created: latency-svc-kcxw9
Jan 23 23:17:17.091: INFO: Got endpoints: latency-svc-mtp9f [735.124722ms]
Jan 23 23:17:17.123: INFO: Created: latency-svc-6j2nl
Jan 23 23:17:17.143: INFO: Got endpoints: latency-svc-qxxn6 [738.401822ms]
Jan 23 23:17:17.166: INFO: Created: latency-svc-jwzv2
Jan 23 23:17:17.194: INFO: Got endpoints: latency-svc-kdqxr [751.221674ms]
Jan 23 23:17:17.215: INFO: Created: latency-svc-nwbf5
Jan 23 23:17:17.243: INFO: Got endpoints: latency-svc-8bc8t [751.999257ms]
Jan 23 23:17:17.268: INFO: Created: latency-svc-wkdrh
Jan 23 23:17:17.296: INFO: Got endpoints: latency-svc-chr5k [752.028864ms]
Jan 23 23:17:17.315: INFO: Created: latency-svc-rnk6x
Jan 23 23:17:17.341: INFO: Got endpoints: latency-svc-r9mtr [749.966366ms]
Jan 23 23:17:17.363: INFO: Created: latency-svc-lpkt8
Jan 23 23:17:17.390: INFO: Got endpoints: latency-svc-lwcp5 [746.451948ms]
Jan 23 23:17:17.414: INFO: Created: latency-svc-9x5gs
Jan 23 23:17:17.445: INFO: Got endpoints: latency-svc-strst [753.093708ms]
Jan 23 23:17:17.493: INFO: Got endpoints: latency-svc-2pn29 [753.167972ms]
Jan 23 23:17:17.543: INFO: Got endpoints: latency-svc-k49zl [747.592517ms]
Jan 23 23:17:17.821: INFO: Got endpoints: latency-svc-rnt9f [977.612224ms]
Jan 23 23:17:17.821: INFO: Got endpoints: latency-svc-kzdw5 [927.549235ms]
Jan 23 23:17:17.823: INFO: Got endpoints: latency-svc-ghcrb [828.395531ms]
Jan 23 23:17:17.823: INFO: Got endpoints: latency-svc-b5xg8 [883.244449ms]
Jan 23 23:17:17.824: INFO: Got endpoints: latency-svc-kcxw9 [782.595569ms]
Jan 23 23:17:17.840: INFO: Got endpoints: latency-svc-6j2nl [748.548157ms]
Jan 23 23:17:17.890: INFO: Got endpoints: latency-svc-jwzv2 [747.014246ms]
Jan 23 23:17:17.948: INFO: Got endpoints: latency-svc-nwbf5 [753.613252ms]
Jan 23 23:17:17.991: INFO: Got endpoints: latency-svc-wkdrh [746.44326ms]
Jan 23 23:17:18.044: INFO: Got endpoints: latency-svc-rnk6x [748.228392ms]
Jan 23 23:17:18.090: INFO: Got endpoints: latency-svc-lpkt8 [748.138158ms]
Jan 23 23:17:18.148: INFO: Got endpoints: latency-svc-9x5gs [758.168729ms]
Jan 23 23:17:18.148: INFO: Latencies: [51.51288ms 67.49305ms 91.446406ms 152.913973ms 176.361372ms 191.479745ms 241.642834ms 269.866631ms 316.198547ms 352.907178ms 370.43781ms 373.418756ms 374.415876ms 383.724279ms 393.862247ms 398.861727ms 403.610897ms 410.329702ms 416.890388ms 417.33139ms 417.590301ms 418.182677ms 419.970247ms 421.627447ms 428.084594ms 431.721826ms 432.593292ms 434.073078ms 441.200757ms 442.814917ms 443.341958ms 450.201985ms 451.636794ms 452.179246ms 453.013405ms 453.035299ms 460.93381ms 463.084439ms 464.884004ms 465.257811ms 465.469633ms 465.646319ms 466.435128ms 466.812767ms 473.146809ms 473.594623ms 473.832046ms 474.495493ms 476.326703ms 476.98801ms 478.762202ms 481.503411ms 482.744986ms 483.190153ms 483.312111ms 483.806987ms 483.998309ms 484.365523ms 485.0652ms 486.426218ms 487.977192ms 492.464312ms 497.056643ms 497.728116ms 503.586894ms 504.408735ms 504.684274ms 507.210761ms 509.57996ms 519.768445ms 522.411136ms 527.861862ms 531.674863ms 535.872245ms 537.831038ms 541.42443ms 543.950027ms 551.785448ms 553.239222ms 553.704017ms 557.646949ms 563.987443ms 565.964123ms 566.961113ms 574.553759ms 582.727029ms 583.342154ms 584.921872ms 592.74923ms 602.454506ms 614.337017ms 622.022555ms 629.293105ms 629.585655ms 631.575937ms 643.039165ms 647.616142ms 654.698188ms 662.185444ms 669.616324ms 675.26736ms 676.551433ms 677.510992ms 694.087369ms 700.746934ms 701.895141ms 710.740275ms 713.55639ms 716.116503ms 721.307831ms 722.784463ms 728.275902ms 728.396918ms 730.9397ms 731.73658ms 735.124722ms 735.575235ms 736.236459ms 738.280561ms 738.401822ms 741.566024ms 741.867355ms 741.950411ms 742.085011ms 742.492757ms 742.788512ms 743.175859ms 743.837312ms 744.296159ms 744.693958ms 745.641469ms 746.173637ms 746.300006ms 746.44326ms 746.451948ms 746.511489ms 746.653935ms 747.014246ms 747.109451ms 747.190497ms 747.205312ms 747.227808ms 747.592517ms 748.138158ms 748.161095ms 748.228392ms 748.384063ms 748.414833ms 748.548157ms 749.694354ms 749.926498ms 749.966366ms 750.466883ms 750.579848ms 751.221674ms 751.390896ms 751.561481ms 751.66702ms 751.999257ms 752.028864ms 752.268584ms 753.093708ms 753.167972ms 753.497009ms 753.498637ms 753.613252ms 754.999214ms 755.173255ms 756.08332ms 756.737621ms 758.168729ms 758.287219ms 761.906229ms 767.716109ms 768.316017ms 776.879179ms 777.534872ms 780.285276ms 781.71529ms 782.595569ms 789.619402ms 790.980187ms 797.051226ms 802.879067ms 804.818266ms 809.15882ms 817.336115ms 819.323437ms 828.395531ms 842.689686ms 862.766198ms 864.208631ms 870.867129ms 879.54212ms 883.244449ms 892.625752ms 902.509001ms 927.549235ms 931.947436ms 977.612224ms]
Jan 23 23:17:18.149: INFO: 50 %ile: 675.26736ms
Jan 23 23:17:18.149: INFO: 90 %ile: 789.619402ms
Jan 23 23:17:18.149: INFO: 99 %ile: 931.947436ms
Jan 23 23:17:18.149: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 23 23:17:18.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-7614" for this suite.

• [SLOW TEST:9.897 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":280,"completed":280,"skipped":4552,"failed":0}
SSSSSSSSSSSJan 23 23:17:18.203: INFO: Running AfterSuite actions on all nodes
Jan 23 23:17:18.204: INFO: Running AfterSuite actions on node 1
Jan 23 23:17:18.204: INFO: Skipping dumping logs from cluster
{"msg":"Test Suite completed","total":280,"completed":280,"skipped":4563,"failed":0}

Ran 280 of 4843 Specs in 4275.604 seconds
SUCCESS! -- 280 Passed | 0 Failed | 0 Pending | 4563 Skipped
PASS

Ginkgo ran 1 suite in 1h11m18.05590654s
Test Suite Passed
