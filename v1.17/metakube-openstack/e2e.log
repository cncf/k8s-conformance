I0129 13:30:10.005882      22 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-836921002
I0129 13:30:10.006076      22 test_context.go:419] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0129 13:30:10.006574      22 e2e.go:109] Starting e2e run "9d5c9a6d-a307-4091-8121-22facd703954" on Ginkgo node 1
{"msg":"Test Suite starting","total":276,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1580304607 - Will randomize all specs
Will run 276 of 4843 specs

Jan 29 13:30:10.110: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
Jan 29 13:30:10.116: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jan 29 13:30:10.155: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jan 29 13:30:10.230: INFO: 17 / 17 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jan 29 13:30:10.230: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
Jan 29 13:30:10.230: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jan 29 13:30:10.246: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'canal' (0 seconds elapsed)
Jan 29 13:30:10.247: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jan 29 13:30:10.247: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Jan 29 13:30:10.247: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
Jan 29 13:30:10.247: INFO: e2e test version: v1.17.2
Jan 29 13:30:10.251: INFO: kube-apiserver version: v1.17.2
Jan 29 13:30:10.251: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
Jan 29 13:30:10.263: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:30:10.264: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename downward-api
Jan 29 13:30:10.351: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Jan 29 13:30:10.376: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2672
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 29 13:30:10.521: INFO: Waiting up to 5m0s for pod "downwardapi-volume-216ce9dd-c0a5-48f0-b2ec-ac1096d87a22" in namespace "downward-api-2672" to be "success or failure"
Jan 29 13:30:10.535: INFO: Pod "downwardapi-volume-216ce9dd-c0a5-48f0-b2ec-ac1096d87a22": Phase="Pending", Reason="", readiness=false. Elapsed: 13.186197ms
Jan 29 13:30:12.542: INFO: Pod "downwardapi-volume-216ce9dd-c0a5-48f0-b2ec-ac1096d87a22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019963032s
Jan 29 13:30:14.549: INFO: Pod "downwardapi-volume-216ce9dd-c0a5-48f0-b2ec-ac1096d87a22": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027857787s
Jan 29 13:30:16.558: INFO: Pod "downwardapi-volume-216ce9dd-c0a5-48f0-b2ec-ac1096d87a22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036449856s
STEP: Saw pod success
Jan 29 13:30:16.559: INFO: Pod "downwardapi-volume-216ce9dd-c0a5-48f0-b2ec-ac1096d87a22" satisfied condition "success or failure"
Jan 29 13:30:16.564: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod downwardapi-volume-216ce9dd-c0a5-48f0-b2ec-ac1096d87a22 container client-container: <nil>
STEP: delete the pod
Jan 29 13:30:16.667: INFO: Waiting for pod downwardapi-volume-216ce9dd-c0a5-48f0-b2ec-ac1096d87a22 to disappear
Jan 29 13:30:16.673: INFO: Pod downwardapi-volume-216ce9dd-c0a5-48f0-b2ec-ac1096d87a22 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:30:16.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2672" for this suite.

• [SLOW TEST:6.613 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":276,"completed":1,"skipped":18,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:30:16.877: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4066
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Update Demo
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:330
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a replication controller
Jan 29 13:30:17.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 create -f - --namespace=kubectl-4066'
Jan 29 13:30:19.573: INFO: stderr: ""
Jan 29 13:30:19.574: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 29 13:30:19.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4066'
Jan 29 13:30:19.723: INFO: stderr: ""
Jan 29 13:30:19.723: INFO: stdout: "update-demo-nautilus-mpzgj "
STEP: Replicas for name=update-demo: expected=2 actual=1
Jan 29 13:30:24.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4066'
Jan 29 13:30:24.914: INFO: stderr: ""
Jan 29 13:30:24.914: INFO: stdout: "update-demo-nautilus-lpxk4 update-demo-nautilus-mpzgj "
Jan 29 13:30:24.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods update-demo-nautilus-lpxk4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4066'
Jan 29 13:30:25.058: INFO: stderr: ""
Jan 29 13:30:25.058: INFO: stdout: "true"
Jan 29 13:30:25.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods update-demo-nautilus-lpxk4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4066'
Jan 29 13:30:25.188: INFO: stderr: ""
Jan 29 13:30:25.188: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 29 13:30:25.188: INFO: validating pod update-demo-nautilus-lpxk4
Jan 29 13:30:25.303: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 29 13:30:25.303: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 29 13:30:25.303: INFO: update-demo-nautilus-lpxk4 is verified up and running
Jan 29 13:30:25.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods update-demo-nautilus-mpzgj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4066'
Jan 29 13:30:25.437: INFO: stderr: ""
Jan 29 13:30:25.437: INFO: stdout: "true"
Jan 29 13:30:25.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods update-demo-nautilus-mpzgj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4066'
Jan 29 13:30:25.588: INFO: stderr: ""
Jan 29 13:30:25.588: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 29 13:30:25.588: INFO: validating pod update-demo-nautilus-mpzgj
Jan 29 13:30:25.710: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 29 13:30:25.710: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 29 13:30:25.710: INFO: update-demo-nautilus-mpzgj is verified up and running
STEP: scaling down the replication controller
Jan 29 13:30:25.716: INFO: scanned /root for discovery docs: <nil>
Jan 29 13:30:25.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-4066'
Jan 29 13:30:26.892: INFO: stderr: ""
Jan 29 13:30:26.892: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 29 13:30:26.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4066'
Jan 29 13:30:27.024: INFO: stderr: ""
Jan 29 13:30:27.024: INFO: stdout: "update-demo-nautilus-lpxk4 update-demo-nautilus-mpzgj "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jan 29 13:30:32.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4066'
Jan 29 13:30:32.171: INFO: stderr: ""
Jan 29 13:30:32.171: INFO: stdout: "update-demo-nautilus-lpxk4 update-demo-nautilus-mpzgj "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jan 29 13:30:37.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4066'
Jan 29 13:30:37.299: INFO: stderr: ""
Jan 29 13:30:37.299: INFO: stdout: "update-demo-nautilus-lpxk4 "
Jan 29 13:30:37.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods update-demo-nautilus-lpxk4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4066'
Jan 29 13:30:37.434: INFO: stderr: ""
Jan 29 13:30:37.434: INFO: stdout: "true"
Jan 29 13:30:37.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods update-demo-nautilus-lpxk4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4066'
Jan 29 13:30:37.566: INFO: stderr: ""
Jan 29 13:30:37.566: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 29 13:30:37.566: INFO: validating pod update-demo-nautilus-lpxk4
Jan 29 13:30:37.582: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 29 13:30:37.582: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 29 13:30:37.582: INFO: update-demo-nautilus-lpxk4 is verified up and running
STEP: scaling up the replication controller
Jan 29 13:30:37.585: INFO: scanned /root for discovery docs: <nil>
Jan 29 13:30:37.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-4066'
Jan 29 13:30:38.782: INFO: stderr: ""
Jan 29 13:30:38.782: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 29 13:30:38.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4066'
Jan 29 13:30:38.941: INFO: stderr: ""
Jan 29 13:30:38.941: INFO: stdout: "update-demo-nautilus-bmdnt update-demo-nautilus-lpxk4 "
Jan 29 13:30:38.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods update-demo-nautilus-bmdnt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4066'
Jan 29 13:30:39.094: INFO: stderr: ""
Jan 29 13:30:39.094: INFO: stdout: ""
Jan 29 13:30:39.095: INFO: update-demo-nautilus-bmdnt is created but not running
Jan 29 13:30:44.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4066'
Jan 29 13:30:44.232: INFO: stderr: ""
Jan 29 13:30:44.232: INFO: stdout: "update-demo-nautilus-bmdnt update-demo-nautilus-lpxk4 "
Jan 29 13:30:44.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods update-demo-nautilus-bmdnt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4066'
Jan 29 13:30:44.355: INFO: stderr: ""
Jan 29 13:30:44.355: INFO: stdout: "true"
Jan 29 13:30:44.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods update-demo-nautilus-bmdnt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4066'
Jan 29 13:30:44.621: INFO: stderr: ""
Jan 29 13:30:44.621: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 29 13:30:44.621: INFO: validating pod update-demo-nautilus-bmdnt
Jan 29 13:30:44.742: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 29 13:30:44.742: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 29 13:30:44.743: INFO: update-demo-nautilus-bmdnt is verified up and running
Jan 29 13:30:44.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods update-demo-nautilus-lpxk4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4066'
Jan 29 13:30:44.899: INFO: stderr: ""
Jan 29 13:30:44.899: INFO: stdout: "true"
Jan 29 13:30:44.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods update-demo-nautilus-lpxk4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4066'
Jan 29 13:30:45.031: INFO: stderr: ""
Jan 29 13:30:45.032: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 29 13:30:45.032: INFO: validating pod update-demo-nautilus-lpxk4
Jan 29 13:30:46.394: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 29 13:30:46.394: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 29 13:30:46.394: INFO: update-demo-nautilus-lpxk4 is verified up and running
STEP: using delete to clean up resources
Jan 29 13:30:46.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 delete --grace-period=0 --force -f - --namespace=kubectl-4066'
Jan 29 13:30:48.431: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 29 13:30:48.431: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 29 13:30:48.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4066'
Jan 29 13:30:49.430: INFO: stderr: "No resources found in kubectl-4066 namespace.\n"
Jan 29 13:30:49.431: INFO: stdout: ""
Jan 29 13:30:49.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods -l name=update-demo --namespace=kubectl-4066 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 29 13:30:49.622: INFO: stderr: ""
Jan 29 13:30:49.622: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:30:49.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4066" for this suite.

• [SLOW TEST:32.814 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:328
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":276,"completed":2,"skipped":26,"failed":0}
S
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:30:49.692: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6712
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service nodeport-service with the type=NodePort in namespace services-6712
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-6712
STEP: creating replication controller externalsvc in namespace services-6712
I0129 13:30:50.075659      22 runners.go:189] Created replication controller with name: externalsvc, namespace: services-6712, replica count: 2
I0129 13:30:53.126430      22 runners.go:189] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Jan 29 13:30:53.194: INFO: Creating new exec pod
Jan 29 13:30:57.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=services-6712 execpodljq6b -- /bin/sh -x -c nslookup nodeport-service'
Jan 29 13:30:58.859: INFO: stderr: "+ nslookup nodeport-service\n"
Jan 29 13:30:58.859: INFO: stdout: "Server:\t\t169.254.20.10\nAddress:\t169.254.20.10#53\n\nnodeport-service.services-6712.svc.cluster.local\tcanonical name = externalsvc.services-6712.svc.cluster.local.\nName:\texternalsvc.services-6712.svc.cluster.local\nAddress: 10.240.27.193\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6712, will wait for the garbage collector to delete the pods
Jan 29 13:30:58.986: INFO: Deleting ReplicationController externalsvc took: 69.275983ms
Jan 29 13:30:59.487: INFO: Terminating ReplicationController externalsvc pods took: 500.434002ms
Jan 29 13:31:13.239: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:31:13.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6712" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:23.620 seconds]
[sig-network] Services
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":276,"completed":3,"skipped":27,"failed":0}
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:31:13.312: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3233
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jan 29 13:31:23.765: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0129 13:31:23.764664      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:31:23.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3233" for this suite.

• [SLOW TEST:10.480 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":276,"completed":4,"skipped":29,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:31:23.795: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-361
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 29 13:31:24.034: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b1643b84-6130-45ef-b311-f375511b2b46" in namespace "projected-361" to be "success or failure"
Jan 29 13:31:24.041: INFO: Pod "downwardapi-volume-b1643b84-6130-45ef-b311-f375511b2b46": Phase="Pending", Reason="", readiness=false. Elapsed: 6.881305ms
Jan 29 13:31:26.052: INFO: Pod "downwardapi-volume-b1643b84-6130-45ef-b311-f375511b2b46": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017433956s
Jan 29 13:31:28.060: INFO: Pod "downwardapi-volume-b1643b84-6130-45ef-b311-f375511b2b46": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025183142s
Jan 29 13:31:30.093: INFO: Pod "downwardapi-volume-b1643b84-6130-45ef-b311-f375511b2b46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.058548147s
STEP: Saw pod success
Jan 29 13:31:30.093: INFO: Pod "downwardapi-volume-b1643b84-6130-45ef-b311-f375511b2b46" satisfied condition "success or failure"
Jan 29 13:31:30.100: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod downwardapi-volume-b1643b84-6130-45ef-b311-f375511b2b46 container client-container: <nil>
STEP: delete the pod
Jan 29 13:31:30.214: INFO: Waiting for pod downwardapi-volume-b1643b84-6130-45ef-b311-f375511b2b46 to disappear
Jan 29 13:31:30.226: INFO: Pod downwardapi-volume-b1643b84-6130-45ef-b311-f375511b2b46 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:31:30.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-361" for this suite.

• [SLOW TEST:6.474 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":276,"completed":5,"skipped":46,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:31:30.270: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-2069
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:31:34.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2069" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":276,"completed":6,"skipped":79,"failed":0}
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:31:34.641: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5789
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Jan 29 13:31:34.929: INFO: Waiting up to 5m0s for pod "downward-api-b52f0e15-382a-424a-bdd7-e671071c89c9" in namespace "downward-api-5789" to be "success or failure"
Jan 29 13:31:34.939: INFO: Pod "downward-api-b52f0e15-382a-424a-bdd7-e671071c89c9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.491212ms
Jan 29 13:31:36.948: INFO: Pod "downward-api-b52f0e15-382a-424a-bdd7-e671071c89c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018965597s
Jan 29 13:31:38.956: INFO: Pod "downward-api-b52f0e15-382a-424a-bdd7-e671071c89c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027135888s
STEP: Saw pod success
Jan 29 13:31:38.956: INFO: Pod "downward-api-b52f0e15-382a-424a-bdd7-e671071c89c9" satisfied condition "success or failure"
Jan 29 13:31:38.964: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod downward-api-b52f0e15-382a-424a-bdd7-e671071c89c9 container dapi-container: <nil>
STEP: delete the pod
Jan 29 13:31:39.071: INFO: Waiting for pod downward-api-b52f0e15-382a-424a-bdd7-e671071c89c9 to disappear
Jan 29 13:31:39.078: INFO: Pod downward-api-b52f0e15-382a-424a-bdd7-e671071c89c9 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:31:39.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5789" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":276,"completed":7,"skipped":86,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:31:39.114: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-1503
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 29 13:31:39.374: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-6f672999-a681-4d54-ae3a-9073bef445f5" in namespace "security-context-test-1503" to be "success or failure"
Jan 29 13:31:39.382: INFO: Pod "alpine-nnp-false-6f672999-a681-4d54-ae3a-9073bef445f5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.470124ms
Jan 29 13:31:41.392: INFO: Pod "alpine-nnp-false-6f672999-a681-4d54-ae3a-9073bef445f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018195725s
Jan 29 13:31:43.398: INFO: Pod "alpine-nnp-false-6f672999-a681-4d54-ae3a-9073bef445f5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024248628s
Jan 29 13:31:45.405: INFO: Pod "alpine-nnp-false-6f672999-a681-4d54-ae3a-9073bef445f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031462488s
Jan 29 13:31:45.405: INFO: Pod "alpine-nnp-false-6f672999-a681-4d54-ae3a-9073bef445f5" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:31:45.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1503" for this suite.

• [SLOW TEST:6.355 seconds]
[k8s.io] Security Context
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:289
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":276,"completed":8,"skipped":97,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:31:45.470: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1728
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 29 13:31:46.639: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 29 13:31:48.664: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715901506, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715901506, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715901506, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715901506, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 13:31:50.689: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715901506, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715901506, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715901506, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715901506, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 13:31:52.672: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715901506, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715901506, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715901506, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715901506, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 29 13:31:55.737: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:31:56.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1728" for this suite.
STEP: Destroying namespace "webhook-1728-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:10.807 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":276,"completed":9,"skipped":97,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:31:56.278: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3507
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-88f082f8-e242-4536-a22d-ec3eb2155c5c
STEP: Creating a pod to test consume configMaps
Jan 29 13:31:56.562: INFO: Waiting up to 5m0s for pod "pod-configmaps-a0e8c005-e26f-4a60-8a97-aeb71f383670" in namespace "configmap-3507" to be "success or failure"
Jan 29 13:31:56.592: INFO: Pod "pod-configmaps-a0e8c005-e26f-4a60-8a97-aeb71f383670": Phase="Pending", Reason="", readiness=false. Elapsed: 29.871801ms
Jan 29 13:31:58.617: INFO: Pod "pod-configmaps-a0e8c005-e26f-4a60-8a97-aeb71f383670": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055175158s
Jan 29 13:32:00.626: INFO: Pod "pod-configmaps-a0e8c005-e26f-4a60-8a97-aeb71f383670": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.064336654s
STEP: Saw pod success
Jan 29 13:32:00.626: INFO: Pod "pod-configmaps-a0e8c005-e26f-4a60-8a97-aeb71f383670" satisfied condition "success or failure"
Jan 29 13:32:00.633: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-configmaps-a0e8c005-e26f-4a60-8a97-aeb71f383670 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 29 13:32:00.743: INFO: Waiting for pod pod-configmaps-a0e8c005-e26f-4a60-8a97-aeb71f383670 to disappear
Jan 29 13:32:00.750: INFO: Pod pod-configmaps-a0e8c005-e26f-4a60-8a97-aeb71f383670 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:32:00.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3507" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":276,"completed":10,"skipped":104,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:32:00.785: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-6012
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 29 13:32:01.001: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:32:01.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6012" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":276,"completed":11,"skipped":126,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:32:01.759: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7016
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name cm-test-opt-del-984b2145-ef99-46ce-8c70-f96174b2c48c
STEP: Creating configMap with name cm-test-opt-upd-7a330b59-a828-46ba-a698-7929b8beda4c
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-984b2145-ef99-46ce-8c70-f96174b2c48c
STEP: Updating configmap cm-test-opt-upd-7a330b59-a828-46ba-a698-7929b8beda4c
STEP: Creating configMap with name cm-test-opt-create-8deaba75-26ac-4591-978c-737966df0d83
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:32:10.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7016" for this suite.

• [SLOW TEST:9.104 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":276,"completed":12,"skipped":139,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:32:10.873: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2893
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:32:27.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2893" for this suite.

• [SLOW TEST:16.614 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":276,"completed":13,"skipped":214,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:32:27.495: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-2135
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test env composition
Jan 29 13:32:27.747: INFO: Waiting up to 5m0s for pod "var-expansion-b4dcf9fa-a4ca-4fb9-bd96-1cb572e67439" in namespace "var-expansion-2135" to be "success or failure"
Jan 29 13:32:27.755: INFO: Pod "var-expansion-b4dcf9fa-a4ca-4fb9-bd96-1cb572e67439": Phase="Pending", Reason="", readiness=false. Elapsed: 8.131308ms
Jan 29 13:32:29.763: INFO: Pod "var-expansion-b4dcf9fa-a4ca-4fb9-bd96-1cb572e67439": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016357461s
Jan 29 13:32:31.771: INFO: Pod "var-expansion-b4dcf9fa-a4ca-4fb9-bd96-1cb572e67439": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024104792s
Jan 29 13:32:33.779: INFO: Pod "var-expansion-b4dcf9fa-a4ca-4fb9-bd96-1cb572e67439": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031585299s
STEP: Saw pod success
Jan 29 13:32:33.779: INFO: Pod "var-expansion-b4dcf9fa-a4ca-4fb9-bd96-1cb572e67439" satisfied condition "success or failure"
Jan 29 13:32:33.787: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod var-expansion-b4dcf9fa-a4ca-4fb9-bd96-1cb572e67439 container dapi-container: <nil>
STEP: delete the pod
Jan 29 13:32:33.906: INFO: Waiting for pod var-expansion-b4dcf9fa-a4ca-4fb9-bd96-1cb572e67439 to disappear
Jan 29 13:32:33.913: INFO: Pod var-expansion-b4dcf9fa-a4ca-4fb9-bd96-1cb572e67439 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:32:33.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2135" for this suite.

• [SLOW TEST:6.452 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":276,"completed":14,"skipped":234,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:32:33.953: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3909
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 29 13:32:34.227: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8b0d4606-a183-4803-b65c-8e7931fc7a39" in namespace "downward-api-3909" to be "success or failure"
Jan 29 13:32:34.234: INFO: Pod "downwardapi-volume-8b0d4606-a183-4803-b65c-8e7931fc7a39": Phase="Pending", Reason="", readiness=false. Elapsed: 6.50877ms
Jan 29 13:32:36.241: INFO: Pod "downwardapi-volume-8b0d4606-a183-4803-b65c-8e7931fc7a39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014100112s
Jan 29 13:32:38.262: INFO: Pod "downwardapi-volume-8b0d4606-a183-4803-b65c-8e7931fc7a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034934697s
STEP: Saw pod success
Jan 29 13:32:38.262: INFO: Pod "downwardapi-volume-8b0d4606-a183-4803-b65c-8e7931fc7a39" satisfied condition "success or failure"
Jan 29 13:32:38.269: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod downwardapi-volume-8b0d4606-a183-4803-b65c-8e7931fc7a39 container client-container: <nil>
STEP: delete the pod
Jan 29 13:32:38.345: INFO: Waiting for pod downwardapi-volume-8b0d4606-a183-4803-b65c-8e7931fc7a39 to disappear
Jan 29 13:32:38.353: INFO: Pod downwardapi-volume-8b0d4606-a183-4803-b65c-8e7931fc7a39 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:32:38.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3909" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":276,"completed":15,"skipped":252,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:32:38.390: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7322
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 29 13:32:38.699: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jan 29 13:32:38.722: INFO: Number of nodes with available pods: 0
Jan 29 13:32:38.722: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jan 29 13:32:38.783: INFO: Number of nodes with available pods: 0
Jan 29 13:32:38.783: INFO: Node metakube-worker-cmccl-6d88bd94fc-znv5g is running more than one daemon pod
Jan 29 13:32:39.791: INFO: Number of nodes with available pods: 0
Jan 29 13:32:39.791: INFO: Node metakube-worker-cmccl-6d88bd94fc-znv5g is running more than one daemon pod
Jan 29 13:32:40.796: INFO: Number of nodes with available pods: 0
Jan 29 13:32:40.796: INFO: Node metakube-worker-cmccl-6d88bd94fc-znv5g is running more than one daemon pod
Jan 29 13:32:41.791: INFO: Number of nodes with available pods: 1
Jan 29 13:32:41.791: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jan 29 13:32:41.834: INFO: Number of nodes with available pods: 1
Jan 29 13:32:41.834: INFO: Number of running nodes: 0, number of available pods: 1
Jan 29 13:32:42.843: INFO: Number of nodes with available pods: 0
Jan 29 13:32:42.843: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jan 29 13:32:42.870: INFO: Number of nodes with available pods: 0
Jan 29 13:32:42.871: INFO: Node metakube-worker-cmccl-6d88bd94fc-znv5g is running more than one daemon pod
Jan 29 13:32:43.888: INFO: Number of nodes with available pods: 0
Jan 29 13:32:43.888: INFO: Node metakube-worker-cmccl-6d88bd94fc-znv5g is running more than one daemon pod
Jan 29 13:32:44.878: INFO: Number of nodes with available pods: 0
Jan 29 13:32:44.878: INFO: Node metakube-worker-cmccl-6d88bd94fc-znv5g is running more than one daemon pod
Jan 29 13:32:45.878: INFO: Number of nodes with available pods: 0
Jan 29 13:32:45.878: INFO: Node metakube-worker-cmccl-6d88bd94fc-znv5g is running more than one daemon pod
Jan 29 13:32:46.879: INFO: Number of nodes with available pods: 0
Jan 29 13:32:46.879: INFO: Node metakube-worker-cmccl-6d88bd94fc-znv5g is running more than one daemon pod
Jan 29 13:32:47.879: INFO: Number of nodes with available pods: 0
Jan 29 13:32:47.879: INFO: Node metakube-worker-cmccl-6d88bd94fc-znv5g is running more than one daemon pod
Jan 29 13:32:48.879: INFO: Number of nodes with available pods: 0
Jan 29 13:32:48.879: INFO: Node metakube-worker-cmccl-6d88bd94fc-znv5g is running more than one daemon pod
Jan 29 13:32:49.879: INFO: Number of nodes with available pods: 0
Jan 29 13:32:49.879: INFO: Node metakube-worker-cmccl-6d88bd94fc-znv5g is running more than one daemon pod
Jan 29 13:32:50.886: INFO: Number of nodes with available pods: 0
Jan 29 13:32:50.886: INFO: Node metakube-worker-cmccl-6d88bd94fc-znv5g is running more than one daemon pod
Jan 29 13:32:51.878: INFO: Number of nodes with available pods: 0
Jan 29 13:32:51.878: INFO: Node metakube-worker-cmccl-6d88bd94fc-znv5g is running more than one daemon pod
Jan 29 13:32:52.878: INFO: Number of nodes with available pods: 0
Jan 29 13:32:52.878: INFO: Node metakube-worker-cmccl-6d88bd94fc-znv5g is running more than one daemon pod
Jan 29 13:32:53.879: INFO: Number of nodes with available pods: 0
Jan 29 13:32:53.880: INFO: Node metakube-worker-cmccl-6d88bd94fc-znv5g is running more than one daemon pod
Jan 29 13:32:54.879: INFO: Number of nodes with available pods: 0
Jan 29 13:32:54.879: INFO: Node metakube-worker-cmccl-6d88bd94fc-znv5g is running more than one daemon pod
Jan 29 13:32:55.879: INFO: Number of nodes with available pods: 0
Jan 29 13:32:55.879: INFO: Node metakube-worker-cmccl-6d88bd94fc-znv5g is running more than one daemon pod
Jan 29 13:32:56.879: INFO: Number of nodes with available pods: 0
Jan 29 13:32:56.879: INFO: Node metakube-worker-cmccl-6d88bd94fc-znv5g is running more than one daemon pod
Jan 29 13:32:57.877: INFO: Number of nodes with available pods: 0
Jan 29 13:32:57.877: INFO: Node metakube-worker-cmccl-6d88bd94fc-znv5g is running more than one daemon pod
Jan 29 13:32:58.880: INFO: Number of nodes with available pods: 0
Jan 29 13:32:58.880: INFO: Node metakube-worker-cmccl-6d88bd94fc-znv5g is running more than one daemon pod
Jan 29 13:32:59.879: INFO: Number of nodes with available pods: 0
Jan 29 13:32:59.879: INFO: Node metakube-worker-cmccl-6d88bd94fc-znv5g is running more than one daemon pod
Jan 29 13:33:00.902: INFO: Number of nodes with available pods: 0
Jan 29 13:33:00.903: INFO: Node metakube-worker-cmccl-6d88bd94fc-znv5g is running more than one daemon pod
Jan 29 13:33:01.879: INFO: Number of nodes with available pods: 0
Jan 29 13:33:01.879: INFO: Node metakube-worker-cmccl-6d88bd94fc-znv5g is running more than one daemon pod
Jan 29 13:33:02.877: INFO: Number of nodes with available pods: 0
Jan 29 13:33:02.877: INFO: Node metakube-worker-cmccl-6d88bd94fc-znv5g is running more than one daemon pod
Jan 29 13:33:03.879: INFO: Number of nodes with available pods: 0
Jan 29 13:33:03.879: INFO: Node metakube-worker-cmccl-6d88bd94fc-znv5g is running more than one daemon pod
Jan 29 13:33:04.884: INFO: Number of nodes with available pods: 1
Jan 29 13:33:04.885: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7322, will wait for the garbage collector to delete the pods
Jan 29 13:33:04.973: INFO: Deleting DaemonSet.extensions daemon-set took: 17.232213ms
Jan 29 13:33:05.574: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.345898ms
Jan 29 13:33:20.981: INFO: Number of nodes with available pods: 0
Jan 29 13:33:20.981: INFO: Number of running nodes: 0, number of available pods: 0
Jan 29 13:33:20.996: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7322/daemonsets","resourceVersion":"62466"},"items":null}

Jan 29 13:33:21.012: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7322/pods","resourceVersion":"62467"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:33:21.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7322" for this suite.

• [SLOW TEST:42.695 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":276,"completed":16,"skipped":264,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:33:21.090: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4771
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-918ed89e-2e34-4baa-885e-c374756494b3
STEP: Creating a pod to test consume configMaps
Jan 29 13:33:21.397: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-94579ec7-eee9-40a2-b5e1-ecf83932be60" in namespace "projected-4771" to be "success or failure"
Jan 29 13:33:21.403: INFO: Pod "pod-projected-configmaps-94579ec7-eee9-40a2-b5e1-ecf83932be60": Phase="Pending", Reason="", readiness=false. Elapsed: 6.107108ms
Jan 29 13:33:23.415: INFO: Pod "pod-projected-configmaps-94579ec7-eee9-40a2-b5e1-ecf83932be60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017730111s
Jan 29 13:33:25.421: INFO: Pod "pod-projected-configmaps-94579ec7-eee9-40a2-b5e1-ecf83932be60": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023638156s
Jan 29 13:33:27.427: INFO: Pod "pod-projected-configmaps-94579ec7-eee9-40a2-b5e1-ecf83932be60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030266321s
STEP: Saw pod success
Jan 29 13:33:27.428: INFO: Pod "pod-projected-configmaps-94579ec7-eee9-40a2-b5e1-ecf83932be60" satisfied condition "success or failure"
Jan 29 13:33:27.435: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-projected-configmaps-94579ec7-eee9-40a2-b5e1-ecf83932be60 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 29 13:33:27.506: INFO: Waiting for pod pod-projected-configmaps-94579ec7-eee9-40a2-b5e1-ecf83932be60 to disappear
Jan 29 13:33:27.512: INFO: Pod pod-projected-configmaps-94579ec7-eee9-40a2-b5e1-ecf83932be60 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:33:27.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4771" for this suite.

• [SLOW TEST:6.449 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":276,"completed":17,"skipped":271,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:33:27.543: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-7557
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 29 13:33:27.801: INFO: (0) /api/v1/nodes/metakube-worker-cmccl-6d88bd94fc-znv5g:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 25.922658ms)
Jan 29 13:33:27.866: INFO: (1) /api/v1/nodes/metakube-worker-cmccl-6d88bd94fc-znv5g:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 65.135716ms)
Jan 29 13:33:27.891: INFO: (2) /api/v1/nodes/metakube-worker-cmccl-6d88bd94fc-znv5g:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 24.948165ms)
Jan 29 13:33:27.909: INFO: (3) /api/v1/nodes/metakube-worker-cmccl-6d88bd94fc-znv5g:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 18.154851ms)
Jan 29 13:33:27.936: INFO: (4) /api/v1/nodes/metakube-worker-cmccl-6d88bd94fc-znv5g:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 26.786036ms)
Jan 29 13:33:27.958: INFO: (5) /api/v1/nodes/metakube-worker-cmccl-6d88bd94fc-znv5g:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 21.991774ms)
Jan 29 13:33:27.981: INFO: (6) /api/v1/nodes/metakube-worker-cmccl-6d88bd94fc-znv5g:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 21.90999ms)
Jan 29 13:33:28.006: INFO: (7) /api/v1/nodes/metakube-worker-cmccl-6d88bd94fc-znv5g:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 25.315162ms)
Jan 29 13:33:28.035: INFO: (8) /api/v1/nodes/metakube-worker-cmccl-6d88bd94fc-znv5g:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 28.465796ms)
Jan 29 13:33:28.061: INFO: (9) /api/v1/nodes/metakube-worker-cmccl-6d88bd94fc-znv5g:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 25.898912ms)
Jan 29 13:33:28.087: INFO: (10) /api/v1/nodes/metakube-worker-cmccl-6d88bd94fc-znv5g:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 25.407057ms)
Jan 29 13:33:28.117: INFO: (11) /api/v1/nodes/metakube-worker-cmccl-6d88bd94fc-znv5g:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 29.20854ms)
Jan 29 13:33:28.220: INFO: (12) /api/v1/nodes/metakube-worker-cmccl-6d88bd94fc-znv5g:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 103.398655ms)
Jan 29 13:33:28.271: INFO: (13) /api/v1/nodes/metakube-worker-cmccl-6d88bd94fc-znv5g:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 50.286164ms)
Jan 29 13:33:28.297: INFO: (14) /api/v1/nodes/metakube-worker-cmccl-6d88bd94fc-znv5g:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 26.861241ms)
Jan 29 13:33:28.312: INFO: (15) /api/v1/nodes/metakube-worker-cmccl-6d88bd94fc-znv5g:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 13.575209ms)
Jan 29 13:33:28.329: INFO: (16) /api/v1/nodes/metakube-worker-cmccl-6d88bd94fc-znv5g:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 16.875771ms)
Jan 29 13:33:28.342: INFO: (17) /api/v1/nodes/metakube-worker-cmccl-6d88bd94fc-znv5g:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 12.886054ms)
Jan 29 13:33:28.356: INFO: (18) /api/v1/nodes/metakube-worker-cmccl-6d88bd94fc-znv5g:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 14.229384ms)
Jan 29 13:33:28.371: INFO: (19) /api/v1/nodes/metakube-worker-cmccl-6d88bd94fc-znv5g:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 14.882086ms)
[AfterEach] version v1
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:33:28.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7557" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]","total":276,"completed":18,"skipped":298,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:33:28.406: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1193
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting the proxy server
Jan 29 13:33:28.624: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-836921002 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:33:28.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1193" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":276,"completed":19,"skipped":303,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:33:28.796: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3981
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: validating api versions
Jan 29 13:33:29.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 api-versions'
Jan 29 13:33:29.295: INFO: stderr: ""
Jan 29 13:33:29.295: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncluster.k8s.io/v1alpha1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetakube.syseleven.de/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:33:29.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3981" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":276,"completed":20,"skipped":320,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:33:29.330: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3132
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-f13547fb-19f7-4211-8c64-e4d37d332577
STEP: Creating a pod to test consume secrets
Jan 29 13:33:29.609: INFO: Waiting up to 5m0s for pod "pod-secrets-f7917bb6-12e5-42a4-8e73-4e368e5abc40" in namespace "secrets-3132" to be "success or failure"
Jan 29 13:33:29.620: INFO: Pod "pod-secrets-f7917bb6-12e5-42a4-8e73-4e368e5abc40": Phase="Pending", Reason="", readiness=false. Elapsed: 10.811781ms
Jan 29 13:33:31.626: INFO: Pod "pod-secrets-f7917bb6-12e5-42a4-8e73-4e368e5abc40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017696415s
Jan 29 13:33:33.634: INFO: Pod "pod-secrets-f7917bb6-12e5-42a4-8e73-4e368e5abc40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025213715s
STEP: Saw pod success
Jan 29 13:33:33.634: INFO: Pod "pod-secrets-f7917bb6-12e5-42a4-8e73-4e368e5abc40" satisfied condition "success or failure"
Jan 29 13:33:33.641: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-secrets-f7917bb6-12e5-42a4-8e73-4e368e5abc40 container secret-volume-test: <nil>
STEP: delete the pod
Jan 29 13:33:33.717: INFO: Waiting for pod pod-secrets-f7917bb6-12e5-42a4-8e73-4e368e5abc40 to disappear
Jan 29 13:33:33.725: INFO: Pod pod-secrets-f7917bb6-12e5-42a4-8e73-4e368e5abc40 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:33:33.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3132" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":276,"completed":21,"skipped":345,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:33:33.757: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9502
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 29 13:33:34.004: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:33:42.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9502" for this suite.

• [SLOW TEST:8.642 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":276,"completed":22,"skipped":399,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:33:42.401: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-9851
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jan 29 13:33:55.633: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 29 13:33:55.641: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 29 13:33:57.642: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 29 13:33:57.650: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 29 13:33:59.642: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 29 13:33:59.652: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 29 13:34:01.642: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 29 13:34:01.651: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:34:01.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9851" for this suite.

• [SLOW TEST:19.277 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":276,"completed":23,"skipped":432,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:34:01.687: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-9868
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:46
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:34:01.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-9868" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":276,"completed":24,"skipped":442,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:34:01.996: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8340
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating all guestbook components
Jan 29 13:34:02.239: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-slave
  labels:
    app: agnhost
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: slave
    tier: backend

Jan 29 13:34:02.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 create -f - --namespace=kubectl-8340'
Jan 29 13:34:02.773: INFO: stderr: ""
Jan 29 13:34:02.773: INFO: stdout: "service/agnhost-slave created\n"
Jan 29 13:34:02.774: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-master
  labels:
    app: agnhost
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: master
    tier: backend

Jan 29 13:34:02.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 create -f - --namespace=kubectl-8340'
Jan 29 13:34:03.237: INFO: stderr: ""
Jan 29 13:34:03.237: INFO: stdout: "service/agnhost-master created\n"
Jan 29 13:34:03.238: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jan 29 13:34:03.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 create -f - --namespace=kubectl-8340'
Jan 29 13:34:03.635: INFO: stderr: ""
Jan 29 13:34:03.636: INFO: stdout: "service/frontend created\n"
Jan 29 13:34:03.636: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jan 29 13:34:03.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 create -f - --namespace=kubectl-8340'
Jan 29 13:34:04.102: INFO: stderr: ""
Jan 29 13:34:04.102: INFO: stdout: "deployment.apps/frontend created\n"
Jan 29 13:34:04.102: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 29 13:34:04.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 create -f - --namespace=kubectl-8340'
Jan 29 13:34:04.761: INFO: stderr: ""
Jan 29 13:34:04.761: INFO: stdout: "deployment.apps/agnhost-master created\n"
Jan 29 13:34:04.761: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--slaveof", "agnhost-master", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 29 13:34:04.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 create -f - --namespace=kubectl-8340'
Jan 29 13:34:05.353: INFO: stderr: ""
Jan 29 13:34:05.353: INFO: stdout: "deployment.apps/agnhost-slave created\n"
STEP: validating guestbook app
Jan 29 13:34:05.353: INFO: Waiting for all frontend pods to be Running.
Jan 29 13:34:10.404: INFO: Waiting for frontend to serve content.
Jan 29 13:34:10.511: INFO: Trying to add a new entry to the guestbook.
Jan 29 13:34:10.659: INFO: Verifying that added entry can be retrieved.
Jan 29 13:34:10.729: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources
Jan 29 13:34:15.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 delete --grace-period=0 --force -f - --namespace=kubectl-8340'
Jan 29 13:34:15.940: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 29 13:34:15.940: INFO: stdout: "service \"agnhost-slave\" force deleted\n"
STEP: using delete to clean up resources
Jan 29 13:34:15.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 delete --grace-period=0 --force -f - --namespace=kubectl-8340'
Jan 29 13:34:16.142: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 29 13:34:16.142: INFO: stdout: "service \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Jan 29 13:34:16.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 delete --grace-period=0 --force -f - --namespace=kubectl-8340'
Jan 29 13:34:16.343: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 29 13:34:16.343: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jan 29 13:34:16.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 delete --grace-period=0 --force -f - --namespace=kubectl-8340'
Jan 29 13:34:16.499: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 29 13:34:16.499: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jan 29 13:34:16.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 delete --grace-period=0 --force -f - --namespace=kubectl-8340'
Jan 29 13:34:16.657: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 29 13:34:16.657: INFO: stdout: "deployment.apps \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Jan 29 13:34:16.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 delete --grace-period=0 --force -f - --namespace=kubectl-8340'
Jan 29 13:34:16.793: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 29 13:34:16.793: INFO: stdout: "deployment.apps \"agnhost-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:34:16.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8340" for this suite.

• [SLOW TEST:14.840 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:386
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":276,"completed":25,"skipped":452,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:34:16.837: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5257
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 29 13:34:17.060: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jan 29 13:34:21.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 --namespace=crd-publish-openapi-5257 create -f -'
Jan 29 13:34:23.208: INFO: stderr: ""
Jan 29 13:34:23.208: INFO: stdout: "e2e-test-crd-publish-openapi-6094-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 29 13:34:23.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 --namespace=crd-publish-openapi-5257 delete e2e-test-crd-publish-openapi-6094-crds test-cr'
Jan 29 13:34:23.367: INFO: stderr: ""
Jan 29 13:34:23.367: INFO: stdout: "e2e-test-crd-publish-openapi-6094-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jan 29 13:34:23.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 --namespace=crd-publish-openapi-5257 apply -f -'
Jan 29 13:34:24.073: INFO: stderr: ""
Jan 29 13:34:24.073: INFO: stdout: "e2e-test-crd-publish-openapi-6094-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 29 13:34:24.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 --namespace=crd-publish-openapi-5257 delete e2e-test-crd-publish-openapi-6094-crds test-cr'
Jan 29 13:34:24.218: INFO: stderr: ""
Jan 29 13:34:24.218: INFO: stdout: "e2e-test-crd-publish-openapi-6094-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jan 29 13:34:24.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 explain e2e-test-crd-publish-openapi-6094-crds'
Jan 29 13:34:24.588: INFO: stderr: ""
Jan 29 13:34:24.588: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6094-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:34:28.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5257" for this suite.

• [SLOW TEST:11.801 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":276,"completed":26,"skipped":460,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:34:28.639: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4484
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Jan 29 13:34:29.313: INFO: Waiting up to 5m0s for pod "downward-api-438830fd-f3b6-41d8-b167-274a59fe17aa" in namespace "downward-api-4484" to be "success or failure"
Jan 29 13:34:29.357: INFO: Pod "downward-api-438830fd-f3b6-41d8-b167-274a59fe17aa": Phase="Pending", Reason="", readiness=false. Elapsed: 43.895138ms
Jan 29 13:34:31.364: INFO: Pod "downward-api-438830fd-f3b6-41d8-b167-274a59fe17aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050789368s
Jan 29 13:34:33.372: INFO: Pod "downward-api-438830fd-f3b6-41d8-b167-274a59fe17aa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05856542s
Jan 29 13:34:35.384: INFO: Pod "downward-api-438830fd-f3b6-41d8-b167-274a59fe17aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.070377507s
STEP: Saw pod success
Jan 29 13:34:35.384: INFO: Pod "downward-api-438830fd-f3b6-41d8-b167-274a59fe17aa" satisfied condition "success or failure"
Jan 29 13:34:35.392: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod downward-api-438830fd-f3b6-41d8-b167-274a59fe17aa container dapi-container: <nil>
STEP: delete the pod
Jan 29 13:34:35.486: INFO: Waiting for pod downward-api-438830fd-f3b6-41d8-b167-274a59fe17aa to disappear
Jan 29 13:34:35.501: INFO: Pod downward-api-438830fd-f3b6-41d8-b167-274a59fe17aa no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:34:35.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4484" for this suite.

• [SLOW TEST:6.907 seconds]
[sig-node] Downward API
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:33
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":276,"completed":27,"skipped":471,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:34:35.549: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9779
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-310e79ed-6985-40bc-8dd4-57781535cef5
STEP: Creating a pod to test consume secrets
Jan 29 13:34:35.858: INFO: Waiting up to 5m0s for pod "pod-secrets-159c77ea-cc93-4b0f-8ee6-0e6ec572de37" in namespace "secrets-9779" to be "success or failure"
Jan 29 13:34:35.866: INFO: Pod "pod-secrets-159c77ea-cc93-4b0f-8ee6-0e6ec572de37": Phase="Pending", Reason="", readiness=false. Elapsed: 7.709969ms
Jan 29 13:34:37.873: INFO: Pod "pod-secrets-159c77ea-cc93-4b0f-8ee6-0e6ec572de37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015652443s
Jan 29 13:34:39.912: INFO: Pod "pod-secrets-159c77ea-cc93-4b0f-8ee6-0e6ec572de37": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054022641s
Jan 29 13:34:41.921: INFO: Pod "pod-secrets-159c77ea-cc93-4b0f-8ee6-0e6ec572de37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.062803785s
STEP: Saw pod success
Jan 29 13:34:41.921: INFO: Pod "pod-secrets-159c77ea-cc93-4b0f-8ee6-0e6ec572de37" satisfied condition "success or failure"
Jan 29 13:34:41.929: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-secrets-159c77ea-cc93-4b0f-8ee6-0e6ec572de37 container secret-volume-test: <nil>
STEP: delete the pod
Jan 29 13:34:42.763: INFO: Waiting for pod pod-secrets-159c77ea-cc93-4b0f-8ee6-0e6ec572de37 to disappear
Jan 29 13:34:43.060: INFO: Pod pod-secrets-159c77ea-cc93-4b0f-8ee6-0e6ec572de37 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:34:43.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9779" for this suite.

• [SLOW TEST:7.545 seconds]
[sig-storage] Secrets
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":276,"completed":28,"skipped":483,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:34:43.097: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7129
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 29 13:34:44.682: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a7e353d5-03b7-4ec1-bafa-f4e83903df11" in namespace "downward-api-7129" to be "success or failure"
Jan 29 13:34:44.776: INFO: Pod "downwardapi-volume-a7e353d5-03b7-4ec1-bafa-f4e83903df11": Phase="Pending", Reason="", readiness=false. Elapsed: 93.748914ms
Jan 29 13:34:46.786: INFO: Pod "downwardapi-volume-a7e353d5-03b7-4ec1-bafa-f4e83903df11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.103531342s
Jan 29 13:34:48.794: INFO: Pod "downwardapi-volume-a7e353d5-03b7-4ec1-bafa-f4e83903df11": Phase="Pending", Reason="", readiness=false. Elapsed: 4.111616009s
Jan 29 13:34:50.804: INFO: Pod "downwardapi-volume-a7e353d5-03b7-4ec1-bafa-f4e83903df11": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.12185844s
STEP: Saw pod success
Jan 29 13:34:50.804: INFO: Pod "downwardapi-volume-a7e353d5-03b7-4ec1-bafa-f4e83903df11" satisfied condition "success or failure"
Jan 29 13:34:50.810: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod downwardapi-volume-a7e353d5-03b7-4ec1-bafa-f4e83903df11 container client-container: <nil>
STEP: delete the pod
Jan 29 13:34:50.910: INFO: Waiting for pod downwardapi-volume-a7e353d5-03b7-4ec1-bafa-f4e83903df11 to disappear
Jan 29 13:34:50.917: INFO: Pod downwardapi-volume-a7e353d5-03b7-4ec1-bafa-f4e83903df11 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:34:50.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7129" for this suite.

• [SLOW TEST:7.852 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":276,"completed":29,"skipped":512,"failed":0}
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:34:50.951: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4220
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4220.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4220.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 29 13:34:57.844: INFO: DNS probes using dns-4220/dns-test-bc70af11-5cf1-4b7a-afd3-569ef146dab8 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:34:57.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4220" for this suite.

• [SLOW TEST:6.976 seconds]
[sig-network] DNS
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":276,"completed":30,"skipped":515,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:34:57.928: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6673
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:34:58.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6673" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":276,"completed":31,"skipped":530,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:34:58.172: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9960
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Starting the proxy
Jan 29 13:34:58.406: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-836921002 proxy --unix-socket=/tmp/kubectl-proxy-unix729105605/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:34:58.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9960" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":276,"completed":32,"skipped":540,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:34:58.539: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3898
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-f4a4d84d-c8c3-4016-82aa-5035d3f47404
STEP: Creating a pod to test consume configMaps
Jan 29 13:34:58.796: INFO: Waiting up to 5m0s for pod "pod-configmaps-c4dbf9ce-e283-4550-a1b2-c30116796710" in namespace "configmap-3898" to be "success or failure"
Jan 29 13:34:58.802: INFO: Pod "pod-configmaps-c4dbf9ce-e283-4550-a1b2-c30116796710": Phase="Pending", Reason="", readiness=false. Elapsed: 5.899089ms
Jan 29 13:35:00.813: INFO: Pod "pod-configmaps-c4dbf9ce-e283-4550-a1b2-c30116796710": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016545972s
Jan 29 13:35:02.827: INFO: Pod "pod-configmaps-c4dbf9ce-e283-4550-a1b2-c30116796710": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030500372s
Jan 29 13:35:04.835: INFO: Pod "pod-configmaps-c4dbf9ce-e283-4550-a1b2-c30116796710": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038852217s
STEP: Saw pod success
Jan 29 13:35:04.835: INFO: Pod "pod-configmaps-c4dbf9ce-e283-4550-a1b2-c30116796710" satisfied condition "success or failure"
Jan 29 13:35:04.841: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-configmaps-c4dbf9ce-e283-4550-a1b2-c30116796710 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 29 13:35:04.901: INFO: Waiting for pod pod-configmaps-c4dbf9ce-e283-4550-a1b2-c30116796710 to disappear
Jan 29 13:35:04.907: INFO: Pod pod-configmaps-c4dbf9ce-e283-4550-a1b2-c30116796710 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:35:04.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3898" for this suite.

• [SLOW TEST:6.418 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":276,"completed":33,"skipped":569,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:35:04.965: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8609
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 29 13:35:05.220: INFO: Waiting up to 5m0s for pod "downwardapi-volume-149ba438-06e9-4ffa-8462-bac2e0170907" in namespace "projected-8609" to be "success or failure"
Jan 29 13:35:05.239: INFO: Pod "downwardapi-volume-149ba438-06e9-4ffa-8462-bac2e0170907": Phase="Pending", Reason="", readiness=false. Elapsed: 18.95041ms
Jan 29 13:35:07.246: INFO: Pod "downwardapi-volume-149ba438-06e9-4ffa-8462-bac2e0170907": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025914476s
Jan 29 13:35:09.255: INFO: Pod "downwardapi-volume-149ba438-06e9-4ffa-8462-bac2e0170907": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035090265s
STEP: Saw pod success
Jan 29 13:35:09.255: INFO: Pod "downwardapi-volume-149ba438-06e9-4ffa-8462-bac2e0170907" satisfied condition "success or failure"
Jan 29 13:35:09.263: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod downwardapi-volume-149ba438-06e9-4ffa-8462-bac2e0170907 container client-container: <nil>
STEP: delete the pod
Jan 29 13:35:09.373: INFO: Waiting for pod downwardapi-volume-149ba438-06e9-4ffa-8462-bac2e0170907 to disappear
Jan 29 13:35:09.391: INFO: Pod downwardapi-volume-149ba438-06e9-4ffa-8462-bac2e0170907 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:35:09.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8609" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":276,"completed":34,"skipped":579,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:35:09.434: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9644
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service multi-endpoint-test in namespace services-9644
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9644 to expose endpoints map[]
Jan 29 13:35:09.800: INFO: Get endpoints failed (46.861274ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Jan 29 13:35:10.809: INFO: successfully validated that service multi-endpoint-test in namespace services-9644 exposes endpoints map[] (1.055886222s elapsed)
STEP: Creating pod pod1 in namespace services-9644
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9644 to expose endpoints map[pod1:[100]]
Jan 29 13:35:13.902: INFO: successfully validated that service multi-endpoint-test in namespace services-9644 exposes endpoints map[pod1:[100]] (3.06344175s elapsed)
STEP: Creating pod pod2 in namespace services-9644
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9644 to expose endpoints map[pod1:[100] pod2:[101]]
Jan 29 13:35:17.017: INFO: successfully validated that service multi-endpoint-test in namespace services-9644 exposes endpoints map[pod1:[100] pod2:[101]] (3.104028647s elapsed)
STEP: Deleting pod pod1 in namespace services-9644
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9644 to expose endpoints map[pod2:[101]]
Jan 29 13:35:18.067: INFO: successfully validated that service multi-endpoint-test in namespace services-9644 exposes endpoints map[pod2:[101]] (1.03303037s elapsed)
STEP: Deleting pod pod2 in namespace services-9644
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9644 to expose endpoints map[]
Jan 29 13:35:19.104: INFO: successfully validated that service multi-endpoint-test in namespace services-9644 exposes endpoints map[] (1.020297114s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:35:19.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9644" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:9.780 seconds]
[sig-network] Services
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":276,"completed":35,"skipped":594,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:35:19.220: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7588
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-091bf0e4-f4b7-4fdf-8765-6ac150146f14
STEP: Creating a pod to test consume configMaps
Jan 29 13:35:19.487: INFO: Waiting up to 5m0s for pod "pod-configmaps-142b2a09-fc53-4895-a6da-3bb495e3f155" in namespace "configmap-7588" to be "success or failure"
Jan 29 13:35:19.507: INFO: Pod "pod-configmaps-142b2a09-fc53-4895-a6da-3bb495e3f155": Phase="Pending", Reason="", readiness=false. Elapsed: 19.867567ms
Jan 29 13:35:21.515: INFO: Pod "pod-configmaps-142b2a09-fc53-4895-a6da-3bb495e3f155": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027972279s
Jan 29 13:35:23.522: INFO: Pod "pod-configmaps-142b2a09-fc53-4895-a6da-3bb495e3f155": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035303064s
STEP: Saw pod success
Jan 29 13:35:23.522: INFO: Pod "pod-configmaps-142b2a09-fc53-4895-a6da-3bb495e3f155" satisfied condition "success or failure"
Jan 29 13:35:23.530: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-configmaps-142b2a09-fc53-4895-a6da-3bb495e3f155 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 29 13:35:23.600: INFO: Waiting for pod pod-configmaps-142b2a09-fc53-4895-a6da-3bb495e3f155 to disappear
Jan 29 13:35:23.606: INFO: Pod pod-configmaps-142b2a09-fc53-4895-a6da-3bb495e3f155 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:35:23.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7588" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]","total":276,"completed":36,"skipped":610,"failed":0}
SS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:35:23.635: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-381
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:35:29.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-381" for this suite.

• [SLOW TEST:5.982 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":276,"completed":37,"skipped":612,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:35:29.620: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3648
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: validating cluster-info
Jan 29 13:35:29.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 cluster-info'
Jan 29 13:35:29.977: INFO: stderr: ""
Jan 29 13:35:29.977: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.240.16.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.240.16.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:35:29.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3648" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":276,"completed":38,"skipped":636,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:35:30.021: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1299
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:35:43.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1299" for this suite.

• [SLOW TEST:13.999 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":276,"completed":39,"skipped":643,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:35:44.032: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3725
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name projected-secret-test-ff91b8c7-ec03-4d35-a999-4703bcec7c91
STEP: Creating a pod to test consume secrets
Jan 29 13:35:44.337: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ae4e6a5d-e014-4b32-bf1e-01b40cb9adee" in namespace "projected-3725" to be "success or failure"
Jan 29 13:35:44.369: INFO: Pod "pod-projected-secrets-ae4e6a5d-e014-4b32-bf1e-01b40cb9adee": Phase="Pending", Reason="", readiness=false. Elapsed: 31.932356ms
Jan 29 13:35:46.375: INFO: Pod "pod-projected-secrets-ae4e6a5d-e014-4b32-bf1e-01b40cb9adee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038036103s
Jan 29 13:35:48.382: INFO: Pod "pod-projected-secrets-ae4e6a5d-e014-4b32-bf1e-01b40cb9adee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04475353s
STEP: Saw pod success
Jan 29 13:35:48.382: INFO: Pod "pod-projected-secrets-ae4e6a5d-e014-4b32-bf1e-01b40cb9adee" satisfied condition "success or failure"
Jan 29 13:35:48.389: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-projected-secrets-ae4e6a5d-e014-4b32-bf1e-01b40cb9adee container secret-volume-test: <nil>
STEP: delete the pod
Jan 29 13:35:48.480: INFO: Waiting for pod pod-projected-secrets-ae4e6a5d-e014-4b32-bf1e-01b40cb9adee to disappear
Jan 29 13:35:48.487: INFO: Pod pod-projected-secrets-ae4e6a5d-e014-4b32-bf1e-01b40cb9adee no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:35:48.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3725" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":276,"completed":40,"skipped":673,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:35:48.513: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8390
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on node default medium
Jan 29 13:35:48.803: INFO: Waiting up to 5m0s for pod "pod-2d1de66e-3c36-4ea5-923f-3041409d4198" in namespace "emptydir-8390" to be "success or failure"
Jan 29 13:35:48.809: INFO: Pod "pod-2d1de66e-3c36-4ea5-923f-3041409d4198": Phase="Pending", Reason="", readiness=false. Elapsed: 5.623263ms
Jan 29 13:35:50.817: INFO: Pod "pod-2d1de66e-3c36-4ea5-923f-3041409d4198": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01320661s
Jan 29 13:35:52.825: INFO: Pod "pod-2d1de66e-3c36-4ea5-923f-3041409d4198": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02146639s
STEP: Saw pod success
Jan 29 13:35:52.826: INFO: Pod "pod-2d1de66e-3c36-4ea5-923f-3041409d4198" satisfied condition "success or failure"
Jan 29 13:35:52.834: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-2d1de66e-3c36-4ea5-923f-3041409d4198 container test-container: <nil>
STEP: delete the pod
Jan 29 13:35:52.888: INFO: Waiting for pod pod-2d1de66e-3c36-4ea5-923f-3041409d4198 to disappear
Jan 29 13:35:52.894: INFO: Pod pod-2d1de66e-3c36-4ea5-923f-3041409d4198 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:35:52.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8390" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":276,"completed":41,"skipped":685,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:35:52.928: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7615
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on node default medium
Jan 29 13:35:53.181: INFO: Waiting up to 5m0s for pod "pod-fb19491d-39b9-4abc-836f-3fc3c62cb7fa" in namespace "emptydir-7615" to be "success or failure"
Jan 29 13:35:53.196: INFO: Pod "pod-fb19491d-39b9-4abc-836f-3fc3c62cb7fa": Phase="Pending", Reason="", readiness=false. Elapsed: 14.583848ms
Jan 29 13:35:55.205: INFO: Pod "pod-fb19491d-39b9-4abc-836f-3fc3c62cb7fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023556894s
Jan 29 13:35:57.213: INFO: Pod "pod-fb19491d-39b9-4abc-836f-3fc3c62cb7fa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031837429s
Jan 29 13:35:59.222: INFO: Pod "pod-fb19491d-39b9-4abc-836f-3fc3c62cb7fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041112998s
STEP: Saw pod success
Jan 29 13:35:59.223: INFO: Pod "pod-fb19491d-39b9-4abc-836f-3fc3c62cb7fa" satisfied condition "success or failure"
Jan 29 13:35:59.230: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-fb19491d-39b9-4abc-836f-3fc3c62cb7fa container test-container: <nil>
STEP: delete the pod
Jan 29 13:36:00.398: INFO: Waiting for pod pod-fb19491d-39b9-4abc-836f-3fc3c62cb7fa to disappear
Jan 29 13:36:00.409: INFO: Pod pod-fb19491d-39b9-4abc-836f-3fc3c62cb7fa no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:36:00.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7615" for this suite.

• [SLOW TEST:7.511 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":276,"completed":42,"skipped":689,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Lease
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:36:00.443: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-3596
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Lease
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:36:04.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-3596" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":276,"completed":43,"skipped":725,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:36:04.219: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8037
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8037.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-8037.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8037.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8037.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-8037.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8037.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 29 13:36:12.689: INFO: DNS probes using dns-8037/dns-test-e887a423-8caa-4c6f-acdc-dcd04edd5128 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:36:13.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8037" for this suite.

• [SLOW TEST:9.767 seconds]
[sig-network] DNS
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":276,"completed":44,"skipped":738,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:36:13.989: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6479
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 29 13:36:14.455: INFO: Create a RollingUpdate DaemonSet
Jan 29 13:36:14.478: INFO: Check that daemon pods launch on every node of the cluster
Jan 29 13:36:14.496: INFO: Number of nodes with available pods: 0
Jan 29 13:36:14.496: INFO: Node metakube-worker-cmccl-6d88bd94fc-87n7l is running more than one daemon pod
Jan 29 13:36:15.545: INFO: Number of nodes with available pods: 0
Jan 29 13:36:15.545: INFO: Node metakube-worker-cmccl-6d88bd94fc-87n7l is running more than one daemon pod
Jan 29 13:36:16.525: INFO: Number of nodes with available pods: 0
Jan 29 13:36:16.525: INFO: Node metakube-worker-cmccl-6d88bd94fc-87n7l is running more than one daemon pod
Jan 29 13:36:17.535: INFO: Number of nodes with available pods: 2
Jan 29 13:36:17.535: INFO: Node metakube-worker-cmccl-6d88bd94fc-lqfxz is running more than one daemon pod
Jan 29 13:36:18.516: INFO: Number of nodes with available pods: 2
Jan 29 13:36:18.517: INFO: Node metakube-worker-cmccl-6d88bd94fc-lqfxz is running more than one daemon pod
Jan 29 13:36:19.627: INFO: Number of nodes with available pods: 3
Jan 29 13:36:19.627: INFO: Number of running nodes: 3, number of available pods: 3
Jan 29 13:36:19.627: INFO: Update the DaemonSet to trigger a rollout
Jan 29 13:36:19.957: INFO: Updating DaemonSet daemon-set
Jan 29 13:36:25.171: INFO: Roll back the DaemonSet before rollout is complete
Jan 29 13:36:25.197: INFO: Updating DaemonSet daemon-set
Jan 29 13:36:25.197: INFO: Make sure DaemonSet rollback is complete
Jan 29 13:36:25.208: INFO: Wrong image for pod: daemon-set-95sv8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jan 29 13:36:25.208: INFO: Pod daemon-set-95sv8 is not available
Jan 29 13:36:26.270: INFO: Wrong image for pod: daemon-set-95sv8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jan 29 13:36:26.270: INFO: Pod daemon-set-95sv8 is not available
Jan 29 13:36:27.233: INFO: Wrong image for pod: daemon-set-95sv8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jan 29 13:36:27.233: INFO: Pod daemon-set-95sv8 is not available
Jan 29 13:36:28.234: INFO: Pod daemon-set-hqs45 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6479, will wait for the garbage collector to delete the pods
Jan 29 13:36:28.361: INFO: Deleting DaemonSet.extensions daemon-set took: 38.015043ms
Jan 29 13:36:28.962: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.491372ms
Jan 29 13:36:40.969: INFO: Number of nodes with available pods: 0
Jan 29 13:36:40.969: INFO: Number of running nodes: 0, number of available pods: 0
Jan 29 13:36:40.975: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6479/daemonsets","resourceVersion":"64397"},"items":null}

Jan 29 13:36:40.983: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6479/pods","resourceVersion":"64397"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:36:41.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6479" for this suite.

• [SLOW TEST:27.051 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":276,"completed":45,"skipped":769,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:36:41.043: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7730
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Jan 29 13:36:41.275: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 29 13:36:41.301: INFO: Waiting for terminating namespaces to be deleted...
Jan 29 13:36:41.309: INFO: 
Logging pods the kubelet thinks is on node metakube-worker-cmccl-6d88bd94fc-87n7l before test
Jan 29 13:36:41.341: INFO: coredns-6f745f6d74-kr8n2 from kube-system started at 2020-01-29 10:31:44 +0000 UTC (1 container statuses recorded)
Jan 29 13:36:41.341: INFO: 	Container coredns ready: true, restart count 0
Jan 29 13:36:41.341: INFO: sonobuoy-systemd-logs-daemon-set-6c9811082b824a7c-sbw8r from sonobuoy started at 2020-01-29 13:30:03 +0000 UTC (2 container statuses recorded)
Jan 29 13:36:41.341: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 29 13:36:41.341: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 29 13:36:41.341: INFO: kube-proxy-7jg5r from kube-system started at 2020-01-29 10:31:06 +0000 UTC (1 container statuses recorded)
Jan 29 13:36:41.341: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 29 13:36:41.341: INFO: node-local-dns-nhzkw from kube-system started at 2020-01-29 10:31:06 +0000 UTC (1 container statuses recorded)
Jan 29 13:36:41.341: INFO: 	Container node-cache ready: true, restart count 0
Jan 29 13:36:41.341: INFO: cluster-autoscaler-8c65c7d54-z9ddg from kube-system started at 2020-01-29 10:31:35 +0000 UTC (1 container statuses recorded)
Jan 29 13:36:41.341: INFO: 	Container cluster-autoscaler ready: true, restart count 1
Jan 29 13:36:41.341: INFO: tiller-deploy-78f78bb476-hz2w6 from kube-system started at 2020-01-29 10:31:35 +0000 UTC (1 container statuses recorded)
Jan 29 13:36:41.341: INFO: 	Container tiller ready: true, restart count 0
Jan 29 13:36:41.341: INFO: coredns-6f745f6d74-8q86x from kube-system started at 2020-01-29 10:31:43 +0000 UTC (1 container statuses recorded)
Jan 29 13:36:41.341: INFO: 	Container coredns ready: true, restart count 0
Jan 29 13:36:41.341: INFO: canal-lpkd9 from kube-system started at 2020-01-29 10:31:07 +0000 UTC (2 container statuses recorded)
Jan 29 13:36:41.341: INFO: 	Container calico-node ready: true, restart count 0
Jan 29 13:36:41.341: INFO: 	Container kube-flannel ready: true, restart count 0
Jan 29 13:36:41.341: INFO: node-exporter-4wc5c from kube-system started at 2020-01-29 10:31:07 +0000 UTC (1 container statuses recorded)
Jan 29 13:36:41.341: INFO: 	Container node-exporter ready: true, restart count 0
Jan 29 13:36:41.341: INFO: openvpn-client-64df8b95c9-5fqpn from kube-system started at 2020-01-29 10:31:44 +0000 UTC (2 container statuses recorded)
Jan 29 13:36:41.341: INFO: 	Container dnat-controller ready: true, restart count 0
Jan 29 13:36:41.341: INFO: 	Container openvpn-client ready: true, restart count 1
Jan 29 13:36:41.341: INFO: 
Logging pods the kubelet thinks is on node metakube-worker-cmccl-6d88bd94fc-lqfxz before test
Jan 29 13:36:41.401: INFO: kube-proxy-rr5gp from kube-system started at 2020-01-29 10:32:02 +0000 UTC (1 container statuses recorded)
Jan 29 13:36:41.401: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 29 13:36:41.401: INFO: node-exporter-99p82 from kube-system started at 2020-01-29 10:32:02 +0000 UTC (1 container statuses recorded)
Jan 29 13:36:41.401: INFO: 	Container node-exporter ready: true, restart count 0
Jan 29 13:36:41.401: INFO: node-local-dns-72k8g from kube-system started at 2020-01-29 10:32:02 +0000 UTC (1 container statuses recorded)
Jan 29 13:36:41.401: INFO: 	Container node-cache ready: true, restart count 0
Jan 29 13:36:41.401: INFO: sonobuoy-systemd-logs-daemon-set-6c9811082b824a7c-9g4cq from sonobuoy started at 2020-01-29 13:30:03 +0000 UTC (2 container statuses recorded)
Jan 29 13:36:41.401: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 29 13:36:41.402: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 29 13:36:41.402: INFO: canal-4zhl8 from kube-system started at 2020-01-29 10:32:02 +0000 UTC (2 container statuses recorded)
Jan 29 13:36:41.402: INFO: 	Container calico-node ready: true, restart count 0
Jan 29 13:36:41.402: INFO: 	Container kube-flannel ready: true, restart count 0
Jan 29 13:36:41.402: INFO: 
Logging pods the kubelet thinks is on node metakube-worker-cmccl-6d88bd94fc-znv5g before test
Jan 29 13:36:41.466: INFO: node-exporter-xblbq from kube-system started at 2020-01-29 10:31:23 +0000 UTC (1 container statuses recorded)
Jan 29 13:36:41.466: INFO: 	Container node-exporter ready: true, restart count 0
Jan 29 13:36:41.466: INFO: sonobuoy-e2e-job-bb288164d824466f from sonobuoy started at 2020-01-29 13:30:03 +0000 UTC (2 container statuses recorded)
Jan 29 13:36:41.466: INFO: 	Container e2e ready: true, restart count 0
Jan 29 13:36:41.466: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 29 13:36:41.466: INFO: node-local-dns-2tvcw from kube-system started at 2020-01-29 10:31:23 +0000 UTC (1 container statuses recorded)
Jan 29 13:36:41.466: INFO: 	Container node-cache ready: true, restart count 0
Jan 29 13:36:41.466: INFO: canal-d5wpn from kube-system started at 2020-01-29 10:31:23 +0000 UTC (2 container statuses recorded)
Jan 29 13:36:41.466: INFO: 	Container calico-node ready: true, restart count 0
Jan 29 13:36:41.466: INFO: 	Container kube-flannel ready: true, restart count 0
Jan 29 13:36:41.466: INFO: sonobuoy-systemd-logs-daemon-set-6c9811082b824a7c-mxxcq from sonobuoy started at 2020-01-29 13:30:03 +0000 UTC (2 container statuses recorded)
Jan 29 13:36:41.466: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 29 13:36:41.466: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 29 13:36:41.466: INFO: kube-proxy-h6b8h from kube-system started at 2020-01-29 10:31:22 +0000 UTC (1 container statuses recorded)
Jan 29 13:36:41.466: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 29 13:36:41.466: INFO: sonobuoy from sonobuoy started at 2020-01-29 13:30:00 +0000 UTC (1 container statuses recorded)
Jan 29 13:36:41.466: INFO: 	Container kube-sonobuoy ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-45326a19-6074-49f7-a383-c4ec65fc64be 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-45326a19-6074-49f7-a383-c4ec65fc64be off the node metakube-worker-cmccl-6d88bd94fc-lqfxz
STEP: verifying the node doesn't have the label kubernetes.io/e2e-45326a19-6074-49f7-a383-c4ec65fc64be
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:37:05.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7730" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:24.904 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":276,"completed":46,"skipped":788,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:37:05.950: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-96
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:37:06.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-96" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":276,"completed":47,"skipped":805,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:37:06.432: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5545
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 29 13:37:06.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 version'
Jan 29 13:37:06.897: INFO: stderr: ""
Jan 29 13:37:06.897: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.2\", GitCommit:\"59603c6e503c87169aea6106f57b9f242f64df89\", GitTreeState:\"clean\", BuildDate:\"2020-01-18T23:30:10Z\", GoVersion:\"go1.13.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.2\", GitCommit:\"59603c6e503c87169aea6106f57b9f242f64df89\", GitTreeState:\"clean\", BuildDate:\"2020-01-18T23:22:30Z\", GoVersion:\"go1.13.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:37:06.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5545" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":276,"completed":48,"skipped":832,"failed":0}
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:37:06.923: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2227
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-6bebf995-053a-442f-8170-358740fc3985
STEP: Creating a pod to test consume secrets
Jan 29 13:37:07.173: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1e24d047-84ce-478b-b29d-d4fcc302ac23" in namespace "projected-2227" to be "success or failure"
Jan 29 13:37:07.190: INFO: Pod "pod-projected-secrets-1e24d047-84ce-478b-b29d-d4fcc302ac23": Phase="Pending", Reason="", readiness=false. Elapsed: 16.998501ms
Jan 29 13:37:09.201: INFO: Pod "pod-projected-secrets-1e24d047-84ce-478b-b29d-d4fcc302ac23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027427377s
Jan 29 13:37:11.216: INFO: Pod "pod-projected-secrets-1e24d047-84ce-478b-b29d-d4fcc302ac23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042293287s
STEP: Saw pod success
Jan 29 13:37:11.216: INFO: Pod "pod-projected-secrets-1e24d047-84ce-478b-b29d-d4fcc302ac23" satisfied condition "success or failure"
Jan 29 13:37:11.224: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-projected-secrets-1e24d047-84ce-478b-b29d-d4fcc302ac23 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 29 13:37:11.297: INFO: Waiting for pod pod-projected-secrets-1e24d047-84ce-478b-b29d-d4fcc302ac23 to disappear
Jan 29 13:37:11.335: INFO: Pod pod-projected-secrets-1e24d047-84ce-478b-b29d-d4fcc302ac23 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:37:11.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2227" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":276,"completed":49,"skipped":834,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:37:11.358: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4884
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1733
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jan 29 13:37:11.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-4884'
Jan 29 13:37:11.741: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan 29 13:37:11.741: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1738
Jan 29 13:37:15.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 delete deployment e2e-test-httpd-deployment --namespace=kubectl-4884'
Jan 29 13:37:15.985: INFO: stderr: ""
Jan 29 13:37:15.985: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:37:15.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4884" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run deployment should create a deployment from an image  [Conformance]","total":276,"completed":50,"skipped":840,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:37:16.006: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-9829
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-9829
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 29 13:37:16.241: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jan 29 13:37:46.697: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.0.66:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9829 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 13:37:46.698: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
Jan 29 13:37:47.304: INFO: Found all expected endpoints: [netserver-0]
Jan 29 13:37:47.312: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.2.39:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9829 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 13:37:47.312: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
Jan 29 13:37:47.949: INFO: Found all expected endpoints: [netserver-1]
Jan 29 13:37:47.959: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.1.140:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9829 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 13:37:47.959: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
Jan 29 13:37:48.484: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:37:48.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9829" for this suite.

• [SLOW TEST:32.528 seconds]
[sig-network] Networking
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":276,"completed":51,"skipped":859,"failed":0}
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:37:48.536: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6458
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 29 13:37:48.912: INFO: Number of nodes with available pods: 0
Jan 29 13:37:48.912: INFO: Node metakube-worker-cmccl-6d88bd94fc-87n7l is running more than one daemon pod
Jan 29 13:37:49.933: INFO: Number of nodes with available pods: 0
Jan 29 13:37:49.933: INFO: Node metakube-worker-cmccl-6d88bd94fc-87n7l is running more than one daemon pod
Jan 29 13:37:50.929: INFO: Number of nodes with available pods: 0
Jan 29 13:37:50.929: INFO: Node metakube-worker-cmccl-6d88bd94fc-87n7l is running more than one daemon pod
Jan 29 13:37:51.934: INFO: Number of nodes with available pods: 0
Jan 29 13:37:51.934: INFO: Node metakube-worker-cmccl-6d88bd94fc-87n7l is running more than one daemon pod
Jan 29 13:37:52.940: INFO: Number of nodes with available pods: 1
Jan 29 13:37:52.941: INFO: Node metakube-worker-cmccl-6d88bd94fc-87n7l is running more than one daemon pod
Jan 29 13:37:53.930: INFO: Number of nodes with available pods: 1
Jan 29 13:37:53.930: INFO: Node metakube-worker-cmccl-6d88bd94fc-87n7l is running more than one daemon pod
Jan 29 13:37:54.932: INFO: Number of nodes with available pods: 1
Jan 29 13:37:54.932: INFO: Node metakube-worker-cmccl-6d88bd94fc-87n7l is running more than one daemon pod
Jan 29 13:37:55.942: INFO: Number of nodes with available pods: 1
Jan 29 13:37:55.942: INFO: Node metakube-worker-cmccl-6d88bd94fc-87n7l is running more than one daemon pod
Jan 29 13:37:56.933: INFO: Number of nodes with available pods: 1
Jan 29 13:37:56.933: INFO: Node metakube-worker-cmccl-6d88bd94fc-87n7l is running more than one daemon pod
Jan 29 13:37:57.931: INFO: Number of nodes with available pods: 2
Jan 29 13:37:57.931: INFO: Node metakube-worker-cmccl-6d88bd94fc-lqfxz is running more than one daemon pod
Jan 29 13:37:58.933: INFO: Number of nodes with available pods: 2
Jan 29 13:37:58.933: INFO: Node metakube-worker-cmccl-6d88bd94fc-lqfxz is running more than one daemon pod
Jan 29 13:37:59.940: INFO: Number of nodes with available pods: 2
Jan 29 13:37:59.941: INFO: Node metakube-worker-cmccl-6d88bd94fc-lqfxz is running more than one daemon pod
Jan 29 13:38:00.932: INFO: Number of nodes with available pods: 2
Jan 29 13:38:00.932: INFO: Node metakube-worker-cmccl-6d88bd94fc-lqfxz is running more than one daemon pod
Jan 29 13:38:01.927: INFO: Number of nodes with available pods: 3
Jan 29 13:38:01.927: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jan 29 13:38:01.991: INFO: Number of nodes with available pods: 3
Jan 29 13:38:01.991: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6458, will wait for the garbage collector to delete the pods
Jan 29 13:38:08.144: INFO: Deleting DaemonSet.extensions daemon-set took: 1.546836247s
Jan 29 13:38:08.745: INFO: Terminating DaemonSet.extensions daemon-set pods took: 601.267966ms
Jan 29 13:38:13.153: INFO: Number of nodes with available pods: 0
Jan 29 13:38:13.153: INFO: Number of running nodes: 0, number of available pods: 0
Jan 29 13:38:13.163: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6458/daemonsets","resourceVersion":"65130"},"items":null}

Jan 29 13:38:13.169: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6458/pods","resourceVersion":"65130"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:38:13.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6458" for this suite.

• [SLOW TEST:24.718 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":276,"completed":52,"skipped":864,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:38:13.265: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3669
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-3a5a11e2-3974-49f2-a1ce-f7c4d5352606
STEP: Creating a pod to test consume configMaps
Jan 29 13:38:13.931: INFO: Waiting up to 5m0s for pod "pod-configmaps-616817be-8a72-4788-9456-cdd398da098e" in namespace "configmap-3669" to be "success or failure"
Jan 29 13:38:13.939: INFO: Pod "pod-configmaps-616817be-8a72-4788-9456-cdd398da098e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.929134ms
Jan 29 13:38:15.948: INFO: Pod "pod-configmaps-616817be-8a72-4788-9456-cdd398da098e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016461019s
Jan 29 13:38:17.962: INFO: Pod "pod-configmaps-616817be-8a72-4788-9456-cdd398da098e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030421613s
STEP: Saw pod success
Jan 29 13:38:17.962: INFO: Pod "pod-configmaps-616817be-8a72-4788-9456-cdd398da098e" satisfied condition "success or failure"
Jan 29 13:38:17.977: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-configmaps-616817be-8a72-4788-9456-cdd398da098e container configmap-volume-test: <nil>
STEP: delete the pod
Jan 29 13:38:18.155: INFO: Waiting for pod pod-configmaps-616817be-8a72-4788-9456-cdd398da098e to disappear
Jan 29 13:38:18.589: INFO: Pod pod-configmaps-616817be-8a72-4788-9456-cdd398da098e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:38:18.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3669" for this suite.

• [SLOW TEST:5.358 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":276,"completed":53,"skipped":886,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:38:18.623: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3052
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 29 13:38:23.512: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715901900, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715901900, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715901902, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715901900, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 13:38:25.531: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715901900, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715901900, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715901902, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715901900, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 29 13:38:28.554: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:38:28.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3052" for this suite.
STEP: Destroying namespace "webhook-3052-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:10.514 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":276,"completed":54,"skipped":890,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:38:29.138: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4272
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 29 13:38:53.467: INFO: Container started at 2020-01-29 13:38:34 +0000 UTC, pod became ready at 2020-01-29 13:38:53 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:38:53.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4272" for this suite.

• [SLOW TEST:24.359 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":276,"completed":55,"skipped":902,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:38:53.498: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-2323
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-2323
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 29 13:38:53.739: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jan 29 13:39:18.174: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.2.47:8080/dial?request=hostname&protocol=udp&host=172.25.0.68&port=8081&tries=1'] Namespace:pod-network-test-2323 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 13:39:18.174: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
Jan 29 13:39:18.757: INFO: Waiting for responses: map[]
Jan 29 13:39:18.764: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.2.47:8080/dial?request=hostname&protocol=udp&host=172.25.2.46&port=8081&tries=1'] Namespace:pod-network-test-2323 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 13:39:18.764: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
Jan 29 13:39:19.380: INFO: Waiting for responses: map[]
Jan 29 13:39:19.388: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.2.47:8080/dial?request=hostname&protocol=udp&host=172.25.1.142&port=8081&tries=1'] Namespace:pod-network-test-2323 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 13:39:19.388: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
Jan 29 13:39:20.079: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:39:20.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2323" for this suite.

• [SLOW TEST:26.617 seconds]
[sig-network] Networking
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":276,"completed":56,"skipped":929,"failed":0}
SS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:39:20.118: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7628
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 29 13:39:20.344: INFO: Creating deployment "webserver-deployment"
Jan 29 13:39:20.355: INFO: Waiting for observed generation 1
Jan 29 13:39:22.424: INFO: Waiting for all required pods to come up
Jan 29 13:39:22.433: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Jan 29 13:39:32.461: INFO: Waiting for deployment "webserver-deployment" to complete
Jan 29 13:39:32.477: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jan 29 13:39:32.494: INFO: Updating deployment webserver-deployment
Jan 29 13:39:32.494: INFO: Waiting for observed generation 2
Jan 29 13:39:34.508: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jan 29 13:39:34.514: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jan 29 13:39:34.520: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 29 13:39:34.541: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jan 29 13:39:34.541: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jan 29 13:39:34.548: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 29 13:39:34.567: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jan 29 13:39:34.567: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jan 29 13:39:34.623: INFO: Updating deployment webserver-deployment
Jan 29 13:39:34.623: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jan 29 13:39:34.715: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jan 29 13:39:36.769: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Jan 29 13:39:36.803: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-7628 /apis/apps/v1/namespaces/deployment-7628/deployments/webserver-deployment fc5d8d9f-2d38-46eb-8645-d23bb7562616 65966 3 2020-01-29 13:39:20 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003204a98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-01-29 13:39:34 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2020-01-29 13:39:34 +0000 UTC,LastTransitionTime:2020-01-29 13:39:20 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jan 29 13:39:36.809: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-7628 /apis/apps/v1/namespaces/deployment-7628/replicasets/webserver-deployment-c7997dcc8 480c2aaa-44da-4d4d-bd2d-30aa3bc6f535 65954 3 2020-01-29 13:39:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment fc5d8d9f-2d38-46eb-8645-d23bb7562616 0xc0032051a7 0xc0032051a8}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003205368 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 29 13:39:36.809: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jan 29 13:39:36.809: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-7628 /apis/apps/v1/namespaces/deployment-7628/replicasets/webserver-deployment-595b5b9587 beb7b855-ce72-44fe-943e-759700086dfc 65964 3 2020-01-29 13:39:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment fc5d8d9f-2d38-46eb-8645-d23bb7562616 0xc003204ed7 0xc003204ed8}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003205148 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jan 29 13:39:36.834: INFO: Pod "webserver-deployment-595b5b9587-4sfd4" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-4sfd4 webserver-deployment-595b5b9587- deployment-7628 /api/v1/namespaces/deployment-7628/pods/webserver-deployment-595b5b9587-4sfd4 c9aa7f72-ef3c-48f3-b17a-25f03189c88b 65991 0 2020-01-29 13:39:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 beb7b855-ce72-44fe-943e-759700086dfc 0xc003205d07 0xc003205d08}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2rrkp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2rrkp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2rrkp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-cmccl-6d88bd94fc-87n7l,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.13,PodIP:,StartTime:2020-01-29 13:39:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 13:39:36.834: INFO: Pod "webserver-deployment-595b5b9587-5s9pw" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-5s9pw webserver-deployment-595b5b9587- deployment-7628 /api/v1/namespaces/deployment-7628/pods/webserver-deployment-595b5b9587-5s9pw c274d944-9d4c-45d3-9bbd-2110ce2cd6a5 65777 0 2020-01-29 13:39:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.25.1.144/32 cni.projectcalico.org/podIPs:172.25.1.144/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 beb7b855-ce72-44fe-943e-759700086dfc 0xc003205f67 0xc003205f68}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2rrkp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2rrkp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2rrkp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-cmccl-6d88bd94fc-znv5g,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.7,PodIP:172.25.1.144,StartTime:2020-01-29 13:39:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-29 13:39:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://273b7b569836bed72d80d22a768213c3ffbaf473e044454c5bf2d3f76a39140e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.144,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 13:39:36.835: INFO: Pod "webserver-deployment-595b5b9587-6vp2n" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-6vp2n webserver-deployment-595b5b9587- deployment-7628 /api/v1/namespaces/deployment-7628/pods/webserver-deployment-595b5b9587-6vp2n dbd8a58d-7c56-4f9c-abf7-ae2114bdb995 66001 0 2020-01-29 13:39:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 beb7b855-ce72-44fe-943e-759700086dfc 0xc0031b2227 0xc0031b2228}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2rrkp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2rrkp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2rrkp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-cmccl-6d88bd94fc-lqfxz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.5,PodIP:,StartTime:2020-01-29 13:39:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 13:39:36.835: INFO: Pod "webserver-deployment-595b5b9587-7j7bg" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-7j7bg webserver-deployment-595b5b9587- deployment-7628 /api/v1/namespaces/deployment-7628/pods/webserver-deployment-595b5b9587-7j7bg cff8a020-59bc-4a20-935f-aef8c41cac2c 65688 0 2020-01-29 13:39:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.25.0.71/32 cni.projectcalico.org/podIPs:172.25.0.71/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 beb7b855-ce72-44fe-943e-759700086dfc 0xc0031b2517 0xc0031b2518}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2rrkp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2rrkp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2rrkp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-cmccl-6d88bd94fc-87n7l,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.13,PodIP:172.25.0.71,StartTime:2020-01-29 13:39:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-29 13:39:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://a43b455dce49c30d3b11c4cc0d9a7fb3d94eedf7eb828db49cc67b7753bfc092,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.71,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 13:39:36.835: INFO: Pod "webserver-deployment-595b5b9587-9ks2z" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-9ks2z webserver-deployment-595b5b9587- deployment-7628 /api/v1/namespaces/deployment-7628/pods/webserver-deployment-595b5b9587-9ks2z 38857eed-a6d8-4b94-b84f-b33bedc5f53d 65983 0 2020-01-29 13:39:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 beb7b855-ce72-44fe-943e-759700086dfc 0xc0031b28d0 0xc0031b28d1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2rrkp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2rrkp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2rrkp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-cmccl-6d88bd94fc-znv5g,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.7,PodIP:,StartTime:2020-01-29 13:39:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 13:39:36.836: INFO: Pod "webserver-deployment-595b5b9587-9t2k8" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-9t2k8 webserver-deployment-595b5b9587- deployment-7628 /api/v1/namespaces/deployment-7628/pods/webserver-deployment-595b5b9587-9t2k8 a250e92f-4697-4d09-8ecb-7781eb2ae837 65920 0 2020-01-29 13:39:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 beb7b855-ce72-44fe-943e-759700086dfc 0xc0031b2b27 0xc0031b2b28}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2rrkp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2rrkp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2rrkp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-cmccl-6d88bd94fc-lqfxz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.5,PodIP:,StartTime:2020-01-29 13:39:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 13:39:36.836: INFO: Pod "webserver-deployment-595b5b9587-9v4vw" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-9v4vw webserver-deployment-595b5b9587- deployment-7628 /api/v1/namespaces/deployment-7628/pods/webserver-deployment-595b5b9587-9v4vw 91c3578d-91b4-4049-86a6-66349b904ec2 65792 0 2020-01-29 13:39:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.25.1.145/32 cni.projectcalico.org/podIPs:172.25.1.145/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 beb7b855-ce72-44fe-943e-759700086dfc 0xc0031b2e37 0xc0031b2e38}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2rrkp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2rrkp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2rrkp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-cmccl-6d88bd94fc-znv5g,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.7,PodIP:172.25.1.145,StartTime:2020-01-29 13:39:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-29 13:39:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://9c393a3b5b319d89a94c38dff0deec9b948ccf2a25b81e6d4c4364f80362806d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.145,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 13:39:36.836: INFO: Pod "webserver-deployment-595b5b9587-dbdms" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-dbdms webserver-deployment-595b5b9587- deployment-7628 /api/v1/namespaces/deployment-7628/pods/webserver-deployment-595b5b9587-dbdms 65c878a7-61f8-4855-af29-ccb1150de957 65816 0 2020-01-29 13:39:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.25.2.49/32 cni.projectcalico.org/podIPs:172.25.2.49/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 beb7b855-ce72-44fe-943e-759700086dfc 0xc0031b2fc7 0xc0031b2fc8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2rrkp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2rrkp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2rrkp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-cmccl-6d88bd94fc-lqfxz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.5,PodIP:172.25.2.49,StartTime:2020-01-29 13:39:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-29 13:39:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://57a68d90e818ee7df4bbb26b8742f55567b68a5233afc99e433e2547fc48ea78,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.49,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 13:39:36.837: INFO: Pod "webserver-deployment-595b5b9587-fnlwz" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-fnlwz webserver-deployment-595b5b9587- deployment-7628 /api/v1/namespaces/deployment-7628/pods/webserver-deployment-595b5b9587-fnlwz 2dde82f3-8051-4ad3-bad0-3bf1332ac658 65800 0 2020-01-29 13:39:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.25.2.48/32 cni.projectcalico.org/podIPs:172.25.2.48/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 beb7b855-ce72-44fe-943e-759700086dfc 0xc0031b3150 0xc0031b3151}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2rrkp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2rrkp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2rrkp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-cmccl-6d88bd94fc-lqfxz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.5,PodIP:172.25.2.48,StartTime:2020-01-29 13:39:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-29 13:39:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://03c961a12e1e2c39d43d4b6c46c118a2fda7a2bdec13efe0bf24d7be283b5946,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.48,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 13:39:36.837: INFO: Pod "webserver-deployment-595b5b9587-m49mg" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-m49mg webserver-deployment-595b5b9587- deployment-7628 /api/v1/namespaces/deployment-7628/pods/webserver-deployment-595b5b9587-m49mg 16f7de31-5a8e-4f18-b572-f1093ce72819 65972 0 2020-01-29 13:39:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 beb7b855-ce72-44fe-943e-759700086dfc 0xc0031b32b0 0xc0031b32b1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2rrkp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2rrkp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2rrkp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-cmccl-6d88bd94fc-znv5g,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.7,PodIP:,StartTime:2020-01-29 13:39:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 13:39:36.837: INFO: Pod "webserver-deployment-595b5b9587-mkqrm" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-mkqrm webserver-deployment-595b5b9587- deployment-7628 /api/v1/namespaces/deployment-7628/pods/webserver-deployment-595b5b9587-mkqrm 6b4d64da-a217-4ae3-8c75-5b19970d43a7 65690 0 2020-01-29 13:39:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.25.0.69/32 cni.projectcalico.org/podIPs:172.25.0.69/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 beb7b855-ce72-44fe-943e-759700086dfc 0xc0031b3417 0xc0031b3418}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2rrkp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2rrkp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2rrkp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-cmccl-6d88bd94fc-87n7l,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.13,PodIP:172.25.0.69,StartTime:2020-01-29 13:39:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-29 13:39:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://23a68f20386d748f44769d6344845886a035952bf9fa771308c0c6832ff0c290,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.69,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 13:39:36.837: INFO: Pod "webserver-deployment-595b5b9587-nf7tg" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-nf7tg webserver-deployment-595b5b9587- deployment-7628 /api/v1/namespaces/deployment-7628/pods/webserver-deployment-595b5b9587-nf7tg 1486ec40-bd1a-4407-ba86-7fac5428cef0 65694 0 2020-01-29 13:39:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.25.0.70/32 cni.projectcalico.org/podIPs:172.25.0.70/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 beb7b855-ce72-44fe-943e-759700086dfc 0xc0031b36a0 0xc0031b36a1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2rrkp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2rrkp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2rrkp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-cmccl-6d88bd94fc-87n7l,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.13,PodIP:172.25.0.70,StartTime:2020-01-29 13:39:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-29 13:39:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://bb5f4ebe61af272542a65e2974fecf1b072a4602c3e8040ba5bafddf0e6009e2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.70,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 13:39:36.838: INFO: Pod "webserver-deployment-595b5b9587-p4pxh" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-p4pxh webserver-deployment-595b5b9587- deployment-7628 /api/v1/namespaces/deployment-7628/pods/webserver-deployment-595b5b9587-p4pxh 6cdb0675-f9e7-4f92-a966-31baed6b8bfd 65987 0 2020-01-29 13:39:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 beb7b855-ce72-44fe-943e-759700086dfc 0xc0031b39d0 0xc0031b39d1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2rrkp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2rrkp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2rrkp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-cmccl-6d88bd94fc-znv5g,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.7,PodIP:,StartTime:2020-01-29 13:39:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 13:39:36.838: INFO: Pod "webserver-deployment-595b5b9587-pbppm" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-pbppm webserver-deployment-595b5b9587- deployment-7628 /api/v1/namespaces/deployment-7628/pods/webserver-deployment-595b5b9587-pbppm 58cc6078-82e2-4bf7-bd56-7c0d0cc4f959 65961 0 2020-01-29 13:39:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 beb7b855-ce72-44fe-943e-759700086dfc 0xc0031b3b67 0xc0031b3b68}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2rrkp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2rrkp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2rrkp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-cmccl-6d88bd94fc-lqfxz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.5,PodIP:,StartTime:2020-01-29 13:39:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 13:39:36.838: INFO: Pod "webserver-deployment-595b5b9587-tngsz" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-tngsz webserver-deployment-595b5b9587- deployment-7628 /api/v1/namespaces/deployment-7628/pods/webserver-deployment-595b5b9587-tngsz a55cd5a1-5481-4e9b-a9c6-233468c8aedb 65980 0 2020-01-29 13:39:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 beb7b855-ce72-44fe-943e-759700086dfc 0xc0031b3f17 0xc0031b3f18}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2rrkp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2rrkp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2rrkp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-cmccl-6d88bd94fc-87n7l,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.13,PodIP:,StartTime:2020-01-29 13:39:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 13:39:36.838: INFO: Pod "webserver-deployment-595b5b9587-w657b" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-w657b webserver-deployment-595b5b9587- deployment-7628 /api/v1/namespaces/deployment-7628/pods/webserver-deployment-595b5b9587-w657b 860fafec-40bc-42f0-b6ee-0b815cfeff41 65982 0 2020-01-29 13:39:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 beb7b855-ce72-44fe-943e-759700086dfc 0xc002f3c267 0xc002f3c268}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2rrkp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2rrkp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2rrkp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-cmccl-6d88bd94fc-lqfxz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.5,PodIP:,StartTime:2020-01-29 13:39:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 13:39:36.839: INFO: Pod "webserver-deployment-595b5b9587-xvrmf" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-xvrmf webserver-deployment-595b5b9587- deployment-7628 /api/v1/namespaces/deployment-7628/pods/webserver-deployment-595b5b9587-xvrmf c5d74131-c7f9-429f-a125-a6073c8d4790 65967 0 2020-01-29 13:39:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 beb7b855-ce72-44fe-943e-759700086dfc 0xc002f3c497 0xc002f3c498}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2rrkp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2rrkp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2rrkp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-cmccl-6d88bd94fc-87n7l,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.13,PodIP:,StartTime:2020-01-29 13:39:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 13:39:36.839: INFO: Pod "webserver-deployment-595b5b9587-xxv9c" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-xxv9c webserver-deployment-595b5b9587- deployment-7628 /api/v1/namespaces/deployment-7628/pods/webserver-deployment-595b5b9587-xxv9c d8c6159f-cfdb-42a2-8904-168c19ccb5bd 65975 0 2020-01-29 13:39:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 beb7b855-ce72-44fe-943e-759700086dfc 0xc002f3c767 0xc002f3c768}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2rrkp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2rrkp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2rrkp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-cmccl-6d88bd94fc-lqfxz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.5,PodIP:,StartTime:2020-01-29 13:39:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 13:39:36.839: INFO: Pod "webserver-deployment-595b5b9587-z9j6g" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-z9j6g webserver-deployment-595b5b9587- deployment-7628 /api/v1/namespaces/deployment-7628/pods/webserver-deployment-595b5b9587-z9j6g 4efa6bd3-0a4e-433c-893e-d2f7a665c07b 65989 0 2020-01-29 13:39:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 beb7b855-ce72-44fe-943e-759700086dfc 0xc002f3ca97 0xc002f3ca98}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2rrkp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2rrkp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2rrkp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-cmccl-6d88bd94fc-87n7l,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.13,PodIP:,StartTime:2020-01-29 13:39:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 13:39:36.839: INFO: Pod "webserver-deployment-595b5b9587-zl79x" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-zl79x webserver-deployment-595b5b9587- deployment-7628 /api/v1/namespaces/deployment-7628/pods/webserver-deployment-595b5b9587-zl79x 2fd3b900-040d-4c47-b052-33cd4064a534 65775 0 2020-01-29 13:39:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.25.1.143/32 cni.projectcalico.org/podIPs:172.25.1.143/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 beb7b855-ce72-44fe-943e-759700086dfc 0xc002f3cdc7 0xc002f3cdc8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2rrkp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2rrkp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2rrkp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-cmccl-6d88bd94fc-znv5g,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.7,PodIP:172.25.1.143,StartTime:2020-01-29 13:39:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-29 13:39:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://855ffb827e5c7f662a70310fe3d556c165d1b238cdf245888e7636b30c70181e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.143,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 13:39:36.839: INFO: Pod "webserver-deployment-c7997dcc8-4hqwk" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-4hqwk webserver-deployment-c7997dcc8- deployment-7628 /api/v1/namespaces/deployment-7628/pods/webserver-deployment-c7997dcc8-4hqwk 737c29e6-8cc1-43a8-83da-e25d1c0c95fa 65994 0 2020-01-29 13:39:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 480c2aaa-44da-4d4d-bd2d-30aa3bc6f535 0xc002f3cf37 0xc002f3cf38}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2rrkp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2rrkp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2rrkp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-cmccl-6d88bd94fc-lqfxz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.5,PodIP:,StartTime:2020-01-29 13:39:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 13:39:36.840: INFO: Pod "webserver-deployment-c7997dcc8-8fv95" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-8fv95 webserver-deployment-c7997dcc8- deployment-7628 /api/v1/namespaces/deployment-7628/pods/webserver-deployment-c7997dcc8-8fv95 e2849f98-8ab4-424a-b33b-521f53a193f8 65963 0 2020-01-29 13:39:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 480c2aaa-44da-4d4d-bd2d-30aa3bc6f535 0xc002f3d0b0 0xc002f3d0b1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2rrkp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2rrkp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2rrkp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-cmccl-6d88bd94fc-znv5g,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.7,PodIP:,StartTime:2020-01-29 13:39:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 13:39:36.840: INFO: Pod "webserver-deployment-c7997dcc8-9l9zd" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-9l9zd webserver-deployment-c7997dcc8- deployment-7628 /api/v1/namespaces/deployment-7628/pods/webserver-deployment-c7997dcc8-9l9zd ef1d3f01-f478-4c95-b2fa-073adfe092ca 65882 0 2020-01-29 13:39:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.25.1.146/32 cni.projectcalico.org/podIPs:172.25.1.146/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 480c2aaa-44da-4d4d-bd2d-30aa3bc6f535 0xc002f3d230 0xc002f3d231}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2rrkp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2rrkp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2rrkp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-cmccl-6d88bd94fc-znv5g,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.7,PodIP:,StartTime:2020-01-29 13:39:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 13:39:36.840: INFO: Pod "webserver-deployment-c7997dcc8-cd565" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-cd565 webserver-deployment-c7997dcc8- deployment-7628 /api/v1/namespaces/deployment-7628/pods/webserver-deployment-c7997dcc8-cd565 f5d0cbfd-bd0e-4c83-bef9-c7a1c5b77f1f 65880 0 2020-01-29 13:39:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.25.0.72/32 cni.projectcalico.org/podIPs:172.25.0.72/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 480c2aaa-44da-4d4d-bd2d-30aa3bc6f535 0xc002f3d3b0 0xc002f3d3b1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2rrkp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2rrkp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2rrkp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-cmccl-6d88bd94fc-87n7l,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.13,PodIP:,StartTime:2020-01-29 13:39:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 13:39:36.840: INFO: Pod "webserver-deployment-c7997dcc8-csstq" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-csstq webserver-deployment-c7997dcc8- deployment-7628 /api/v1/namespaces/deployment-7628/pods/webserver-deployment-c7997dcc8-csstq a114ea93-18b4-475b-a003-b1bbb8308609 65870 0 2020-01-29 13:39:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 480c2aaa-44da-4d4d-bd2d-30aa3bc6f535 0xc002f3d517 0xc002f3d518}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2rrkp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2rrkp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2rrkp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-cmccl-6d88bd94fc-lqfxz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.5,PodIP:,StartTime:2020-01-29 13:39:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 13:39:36.840: INFO: Pod "webserver-deployment-c7997dcc8-cz4lz" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-cz4lz webserver-deployment-c7997dcc8- deployment-7628 /api/v1/namespaces/deployment-7628/pods/webserver-deployment-c7997dcc8-cz4lz 36612b42-da7b-4c76-a3d0-6f011434d468 65978 0 2020-01-29 13:39:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 480c2aaa-44da-4d4d-bd2d-30aa3bc6f535 0xc002f3d680 0xc002f3d681}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2rrkp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2rrkp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2rrkp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-cmccl-6d88bd94fc-znv5g,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.7,PodIP:,StartTime:2020-01-29 13:39:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 13:39:36.842: INFO: Pod "webserver-deployment-c7997dcc8-dtvq6" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-dtvq6 webserver-deployment-c7997dcc8- deployment-7628 /api/v1/namespaces/deployment-7628/pods/webserver-deployment-c7997dcc8-dtvq6 c0ea5eb7-d6aa-4861-a147-4dd758ea0d89 66002 0 2020-01-29 13:39:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 480c2aaa-44da-4d4d-bd2d-30aa3bc6f535 0xc002f3d7e0 0xc002f3d7e1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2rrkp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2rrkp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2rrkp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-cmccl-6d88bd94fc-87n7l,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.13,PodIP:,StartTime:2020-01-29 13:39:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 13:39:36.843: INFO: Pod "webserver-deployment-c7997dcc8-fs2d8" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-fs2d8 webserver-deployment-c7997dcc8- deployment-7628 /api/v1/namespaces/deployment-7628/pods/webserver-deployment-c7997dcc8-fs2d8 38cd35f1-4d9b-4fda-b61f-115d98c9d554 65992 0 2020-01-29 13:39:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 480c2aaa-44da-4d4d-bd2d-30aa3bc6f535 0xc002f3d967 0xc002f3d968}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2rrkp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2rrkp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2rrkp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-cmccl-6d88bd94fc-lqfxz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.5,PodIP:,StartTime:2020-01-29 13:39:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 13:39:36.856: INFO: Pod "webserver-deployment-c7997dcc8-g2sch" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-g2sch webserver-deployment-c7997dcc8- deployment-7628 /api/v1/namespaces/deployment-7628/pods/webserver-deployment-c7997dcc8-g2sch f681198b-8356-416d-96ff-44bc01c8eb80 65974 0 2020-01-29 13:39:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 480c2aaa-44da-4d4d-bd2d-30aa3bc6f535 0xc002f3dd50 0xc002f3dd51}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2rrkp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2rrkp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2rrkp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-cmccl-6d88bd94fc-87n7l,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.13,PodIP:,StartTime:2020-01-29 13:39:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 13:39:36.856: INFO: Pod "webserver-deployment-c7997dcc8-jd647" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-jd647 webserver-deployment-c7997dcc8- deployment-7628 /api/v1/namespaces/deployment-7628/pods/webserver-deployment-c7997dcc8-jd647 1ef1261e-72d3-4886-866d-1b4e9e97c91d 65873 0 2020-01-29 13:39:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 480c2aaa-44da-4d4d-bd2d-30aa3bc6f535 0xc002f3df37 0xc002f3df38}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2rrkp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2rrkp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2rrkp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-cmccl-6d88bd94fc-lqfxz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.5,PodIP:,StartTime:2020-01-29 13:39:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 13:39:36.856: INFO: Pod "webserver-deployment-c7997dcc8-kkw2r" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-kkw2r webserver-deployment-c7997dcc8- deployment-7628 /api/v1/namespaces/deployment-7628/pods/webserver-deployment-c7997dcc8-kkw2r 80ac4046-eb0f-4c57-9057-94892f1e5344 66012 0 2020-01-29 13:39:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.25.2.52/32 cni.projectcalico.org/podIPs:172.25.2.52/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 480c2aaa-44da-4d4d-bd2d-30aa3bc6f535 0xc002e5a120 0xc002e5a121}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2rrkp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2rrkp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2rrkp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-cmccl-6d88bd94fc-lqfxz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.5,PodIP:,StartTime:2020-01-29 13:39:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 13:39:36.856: INFO: Pod "webserver-deployment-c7997dcc8-lcjcf" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-lcjcf webserver-deployment-c7997dcc8- deployment-7628 /api/v1/namespaces/deployment-7628/pods/webserver-deployment-c7997dcc8-lcjcf d3003a2d-f666-49e5-b624-50001e0d3981 65985 0 2020-01-29 13:39:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 480c2aaa-44da-4d4d-bd2d-30aa3bc6f535 0xc002e5a280 0xc002e5a281}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2rrkp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2rrkp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2rrkp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-cmccl-6d88bd94fc-87n7l,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.13,PodIP:,StartTime:2020-01-29 13:39:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 13:39:36.857: INFO: Pod "webserver-deployment-c7997dcc8-pxj4f" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-pxj4f webserver-deployment-c7997dcc8- deployment-7628 /api/v1/namespaces/deployment-7628/pods/webserver-deployment-c7997dcc8-pxj4f a76e6985-1538-4789-9322-b9d3355cbb6f 65970 0 2020-01-29 13:39:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 480c2aaa-44da-4d4d-bd2d-30aa3bc6f535 0xc002e5a3e7 0xc002e5a3e8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2rrkp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2rrkp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2rrkp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-cmccl-6d88bd94fc-znv5g,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:39:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.7,PodIP:,StartTime:2020-01-29 13:39:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:39:36.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7628" for this suite.

• [SLOW TEST:16.767 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":276,"completed":57,"skipped":931,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:39:36.889: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-1143
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jan 29 13:39:50.231: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:39:50.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1143" for this suite.

• [SLOW TEST:13.493 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":276,"completed":58,"skipped":973,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:39:50.385: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1218
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-map-9f367f85-6c68-4ac5-a907-2fc09d7ced9f
STEP: Creating a pod to test consume secrets
Jan 29 13:39:50.717: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-98a965c5-b925-4e10-b9ae-612543ab66ab" in namespace "projected-1218" to be "success or failure"
Jan 29 13:39:50.742: INFO: Pod "pod-projected-secrets-98a965c5-b925-4e10-b9ae-612543ab66ab": Phase="Pending", Reason="", readiness=false. Elapsed: 24.688816ms
Jan 29 13:39:52.750: INFO: Pod "pod-projected-secrets-98a965c5-b925-4e10-b9ae-612543ab66ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032071289s
Jan 29 13:39:54.757: INFO: Pod "pod-projected-secrets-98a965c5-b925-4e10-b9ae-612543ab66ab": Phase="Running", Reason="", readiness=true. Elapsed: 4.039472312s
Jan 29 13:39:56.764: INFO: Pod "pod-projected-secrets-98a965c5-b925-4e10-b9ae-612543ab66ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.046661899s
STEP: Saw pod success
Jan 29 13:39:56.764: INFO: Pod "pod-projected-secrets-98a965c5-b925-4e10-b9ae-612543ab66ab" satisfied condition "success or failure"
Jan 29 13:39:56.773: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-projected-secrets-98a965c5-b925-4e10-b9ae-612543ab66ab container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 29 13:39:56.889: INFO: Waiting for pod pod-projected-secrets-98a965c5-b925-4e10-b9ae-612543ab66ab to disappear
Jan 29 13:39:56.899: INFO: Pod pod-projected-secrets-98a965c5-b925-4e10-b9ae-612543ab66ab no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:39:56.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1218" for this suite.

• [SLOW TEST:6.548 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":276,"completed":59,"skipped":1006,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:39:56.939: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4438
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 29 13:39:57.160: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fc34449c-6c0a-4a99-97f9-f824dc7980c5" in namespace "projected-4438" to be "success or failure"
Jan 29 13:39:57.177: INFO: Pod "downwardapi-volume-fc34449c-6c0a-4a99-97f9-f824dc7980c5": Phase="Pending", Reason="", readiness=false. Elapsed: 17.158274ms
Jan 29 13:39:59.184: INFO: Pod "downwardapi-volume-fc34449c-6c0a-4a99-97f9-f824dc7980c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024329158s
Jan 29 13:40:01.192: INFO: Pod "downwardapi-volume-fc34449c-6c0a-4a99-97f9-f824dc7980c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031443035s
Jan 29 13:40:03.211: INFO: Pod "downwardapi-volume-fc34449c-6c0a-4a99-97f9-f824dc7980c5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.050661746s
Jan 29 13:40:05.219: INFO: Pod "downwardapi-volume-fc34449c-6c0a-4a99-97f9-f824dc7980c5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.058888044s
Jan 29 13:40:07.226: INFO: Pod "downwardapi-volume-fc34449c-6c0a-4a99-97f9-f824dc7980c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.065749941s
STEP: Saw pod success
Jan 29 13:40:07.226: INFO: Pod "downwardapi-volume-fc34449c-6c0a-4a99-97f9-f824dc7980c5" satisfied condition "success or failure"
Jan 29 13:40:07.233: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod downwardapi-volume-fc34449c-6c0a-4a99-97f9-f824dc7980c5 container client-container: <nil>
STEP: delete the pod
Jan 29 13:40:07.289: INFO: Waiting for pod downwardapi-volume-fc34449c-6c0a-4a99-97f9-f824dc7980c5 to disappear
Jan 29 13:40:07.324: INFO: Pod downwardapi-volume-fc34449c-6c0a-4a99-97f9-f824dc7980c5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:40:07.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4438" for this suite.

• [SLOW TEST:10.416 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":276,"completed":60,"skipped":1011,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:40:07.355: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-190
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap configmap-190/configmap-test-75a4e656-8337-46fa-83e6-aa084e838101
STEP: Creating a pod to test consume configMaps
Jan 29 13:40:07.630: INFO: Waiting up to 5m0s for pod "pod-configmaps-2d766fc0-43f0-4835-bb7b-ed1452a3ebf1" in namespace "configmap-190" to be "success or failure"
Jan 29 13:40:07.642: INFO: Pod "pod-configmaps-2d766fc0-43f0-4835-bb7b-ed1452a3ebf1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.406915ms
Jan 29 13:40:09.652: INFO: Pod "pod-configmaps-2d766fc0-43f0-4835-bb7b-ed1452a3ebf1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022398057s
Jan 29 13:40:11.659: INFO: Pod "pod-configmaps-2d766fc0-43f0-4835-bb7b-ed1452a3ebf1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029445662s
Jan 29 13:40:13.668: INFO: Pod "pod-configmaps-2d766fc0-43f0-4835-bb7b-ed1452a3ebf1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038877736s
STEP: Saw pod success
Jan 29 13:40:13.669: INFO: Pod "pod-configmaps-2d766fc0-43f0-4835-bb7b-ed1452a3ebf1" satisfied condition "success or failure"
Jan 29 13:40:13.679: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-configmaps-2d766fc0-43f0-4835-bb7b-ed1452a3ebf1 container env-test: <nil>
STEP: delete the pod
Jan 29 13:40:13.758: INFO: Waiting for pod pod-configmaps-2d766fc0-43f0-4835-bb7b-ed1452a3ebf1 to disappear
Jan 29 13:40:13.765: INFO: Pod pod-configmaps-2d766fc0-43f0-4835-bb7b-ed1452a3ebf1 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:40:13.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-190" for this suite.

• [SLOW TEST:6.441 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":276,"completed":61,"skipped":1027,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:40:13.796: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-5357
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 29 13:40:14.014: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-954a71e3-98ff-42c1-8c06-8612a68f1692" in namespace "security-context-test-5357" to be "success or failure"
Jan 29 13:40:14.022: INFO: Pod "busybox-privileged-false-954a71e3-98ff-42c1-8c06-8612a68f1692": Phase="Pending", Reason="", readiness=false. Elapsed: 8.485931ms
Jan 29 13:40:16.031: INFO: Pod "busybox-privileged-false-954a71e3-98ff-42c1-8c06-8612a68f1692": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017041109s
Jan 29 13:40:18.038: INFO: Pod "busybox-privileged-false-954a71e3-98ff-42c1-8c06-8612a68f1692": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024578383s
Jan 29 13:40:18.038: INFO: Pod "busybox-privileged-false-954a71e3-98ff-42c1-8c06-8612a68f1692" satisfied condition "success or failure"
Jan 29 13:40:18.094: INFO: Got logs for pod "busybox-privileged-false-954a71e3-98ff-42c1-8c06-8612a68f1692": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:40:18.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5357" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":276,"completed":62,"skipped":1035,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:40:18.124: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1905
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-upd-943eba93-2ce9-40ce-89e7-425a2b8e75f1
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-943eba93-2ce9-40ce-89e7-425a2b8e75f1
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:41:49.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1905" for this suite.

• [SLOW TEST:91.332 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":276,"completed":63,"skipped":1040,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:41:49.457: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-4343
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 29 13:41:49.880: INFO: Creating ReplicaSet my-hostname-basic-0c9d6a9d-0def-4c50-a698-a0516458a8ea
Jan 29 13:41:49.901: INFO: Pod name my-hostname-basic-0c9d6a9d-0def-4c50-a698-a0516458a8ea: Found 0 pods out of 1
Jan 29 13:41:54.912: INFO: Pod name my-hostname-basic-0c9d6a9d-0def-4c50-a698-a0516458a8ea: Found 1 pods out of 1
Jan 29 13:41:54.912: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-0c9d6a9d-0def-4c50-a698-a0516458a8ea" is running
Jan 29 13:41:54.920: INFO: Pod "my-hostname-basic-0c9d6a9d-0def-4c50-a698-a0516458a8ea-2f7ms" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-01-29 13:41:50 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-01-29 13:41:54 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-01-29 13:41:54 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-01-29 13:41:49 +0000 UTC Reason: Message:}])
Jan 29 13:41:54.920: INFO: Trying to dial the pod
Jan 29 13:42:00.053: INFO: Controller my-hostname-basic-0c9d6a9d-0def-4c50-a698-a0516458a8ea: Got expected result from replica 1 [my-hostname-basic-0c9d6a9d-0def-4c50-a698-a0516458a8ea-2f7ms]: "my-hostname-basic-0c9d6a9d-0def-4c50-a698-a0516458a8ea-2f7ms", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:42:00.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4343" for this suite.

• [SLOW TEST:10.687 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":276,"completed":64,"skipped":1057,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:42:00.146: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-975
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 29 13:42:01.524: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 29 13:42:03.611: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715902121, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715902121, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715902121, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715902121, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 13:42:05.621: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715902121, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715902121, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715902121, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715902121, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 29 13:42:08.648: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:42:09.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-975" for this suite.
STEP: Destroying namespace "webhook-975-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.610 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":276,"completed":65,"skipped":1062,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:42:09.760: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7793
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on node default medium
Jan 29 13:42:10.048: INFO: Waiting up to 5m0s for pod "pod-0bd8b33b-0f22-45aa-a760-1c6fbe658ad7" in namespace "emptydir-7793" to be "success or failure"
Jan 29 13:42:10.055: INFO: Pod "pod-0bd8b33b-0f22-45aa-a760-1c6fbe658ad7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.372065ms
Jan 29 13:42:12.062: INFO: Pod "pod-0bd8b33b-0f22-45aa-a760-1c6fbe658ad7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013510958s
Jan 29 13:42:14.075: INFO: Pod "pod-0bd8b33b-0f22-45aa-a760-1c6fbe658ad7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026508804s
Jan 29 13:42:16.336: INFO: Pod "pod-0bd8b33b-0f22-45aa-a760-1c6fbe658ad7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.287572736s
STEP: Saw pod success
Jan 29 13:42:16.336: INFO: Pod "pod-0bd8b33b-0f22-45aa-a760-1c6fbe658ad7" satisfied condition "success or failure"
Jan 29 13:42:16.439: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-0bd8b33b-0f22-45aa-a760-1c6fbe658ad7 container test-container: <nil>
STEP: delete the pod
Jan 29 13:42:16.507: INFO: Waiting for pod pod-0bd8b33b-0f22-45aa-a760-1c6fbe658ad7 to disappear
Jan 29 13:42:16.514: INFO: Pod pod-0bd8b33b-0f22-45aa-a760-1c6fbe658ad7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:42:16.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7793" for this suite.

• [SLOW TEST:6.775 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":276,"completed":66,"skipped":1067,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:42:16.537: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4596
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-245ca7fb-87a7-40f2-8f7d-6a0179afc9c2
STEP: Creating a pod to test consume configMaps
Jan 29 13:42:16.992: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c98affcc-d932-4d2f-bb6c-6a77f6d6a15b" in namespace "projected-4596" to be "success or failure"
Jan 29 13:42:16.998: INFO: Pod "pod-projected-configmaps-c98affcc-d932-4d2f-bb6c-6a77f6d6a15b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.262991ms
Jan 29 13:42:19.006: INFO: Pod "pod-projected-configmaps-c98affcc-d932-4d2f-bb6c-6a77f6d6a15b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013648245s
Jan 29 13:42:21.014: INFO: Pod "pod-projected-configmaps-c98affcc-d932-4d2f-bb6c-6a77f6d6a15b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02135134s
Jan 29 13:42:23.025: INFO: Pod "pod-projected-configmaps-c98affcc-d932-4d2f-bb6c-6a77f6d6a15b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032171313s
STEP: Saw pod success
Jan 29 13:42:23.025: INFO: Pod "pod-projected-configmaps-c98affcc-d932-4d2f-bb6c-6a77f6d6a15b" satisfied condition "success or failure"
Jan 29 13:42:23.032: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-projected-configmaps-c98affcc-d932-4d2f-bb6c-6a77f6d6a15b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 29 13:42:23.107: INFO: Waiting for pod pod-projected-configmaps-c98affcc-d932-4d2f-bb6c-6a77f6d6a15b to disappear
Jan 29 13:42:23.119: INFO: Pod pod-projected-configmaps-c98affcc-d932-4d2f-bb6c-6a77f6d6a15b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:42:23.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4596" for this suite.

• [SLOW TEST:6.617 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":276,"completed":67,"skipped":1087,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:42:23.157: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2133
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 29 13:42:23.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 create -f - --namespace=kubectl-2133'
Jan 29 13:42:23.814: INFO: stderr: ""
Jan 29 13:42:23.814: INFO: stdout: "replicationcontroller/agnhost-master created\n"
Jan 29 13:42:23.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 create -f - --namespace=kubectl-2133'
Jan 29 13:42:24.339: INFO: stderr: ""
Jan 29 13:42:24.339: INFO: stdout: "service/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Jan 29 13:42:25.347: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 29 13:42:25.348: INFO: Found 0 / 1
Jan 29 13:42:26.347: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 29 13:42:26.347: INFO: Found 0 / 1
Jan 29 13:42:27.346: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 29 13:42:27.346: INFO: Found 1 / 1
Jan 29 13:42:27.346: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 29 13:42:27.355: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 29 13:42:27.355: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 29 13:42:27.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 describe pod agnhost-master-wxkxq --namespace=kubectl-2133'
Jan 29 13:42:27.513: INFO: stderr: ""
Jan 29 13:42:27.513: INFO: stdout: "Name:         agnhost-master-wxkxq\nNamespace:    kubectl-2133\nPriority:     0\nNode:         metakube-worker-cmccl-6d88bd94fc-lqfxz/192.168.1.5\nStart Time:   Wed, 29 Jan 2020 13:42:23 +0000\nLabels:       app=agnhost\n              role=master\nAnnotations:  cni.projectcalico.org/podIP: 172.25.2.73/32\n              cni.projectcalico.org/podIPs: 172.25.2.73/32\nStatus:       Running\nIP:           172.25.2.73\nIPs:\n  IP:           172.25.2.73\nControlled By:  ReplicationController/agnhost-master\nContainers:\n  agnhost-master:\n    Container ID:   docker://f958f7e0f593627a0c405e2ffdffcf8aeabd0d2fb1e789de211ac690a9586337\n    Image:          gcr.io/kubernetes-e2e-test-images/agnhost:2.8\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 29 Jan 2020 13:42:26 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-mxr78 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-mxr78:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-mxr78\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From                                             Message\n  ----    ------     ----       ----                                             -------\n  Normal  Scheduled  <unknown>  default-scheduler                                Successfully assigned kubectl-2133/agnhost-master-wxkxq to metakube-worker-cmccl-6d88bd94fc-lqfxz\n  Normal  Pulled     2s         kubelet, metakube-worker-cmccl-6d88bd94fc-lqfxz  Container image \"gcr.io/kubernetes-e2e-test-images/agnhost:2.8\" already present on machine\n  Normal  Created    2s         kubelet, metakube-worker-cmccl-6d88bd94fc-lqfxz  Created container agnhost-master\n  Normal  Started    1s         kubelet, metakube-worker-cmccl-6d88bd94fc-lqfxz  Started container agnhost-master\n"
Jan 29 13:42:27.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 describe rc agnhost-master --namespace=kubectl-2133'
Jan 29 13:42:27.669: INFO: stderr: ""
Jan 29 13:42:27.669: INFO: stdout: "Name:         agnhost-master\nNamespace:    kubectl-2133\nSelector:     app=agnhost,role=master\nLabels:       app=agnhost\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=master\n  Containers:\n   agnhost-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/agnhost:2.8\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: agnhost-master-wxkxq\n"
Jan 29 13:42:27.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 describe service agnhost-master --namespace=kubectl-2133'
Jan 29 13:42:27.839: INFO: stderr: ""
Jan 29 13:42:27.839: INFO: stdout: "Name:              agnhost-master\nNamespace:         kubectl-2133\nLabels:            app=agnhost\n                   role=master\nAnnotations:       <none>\nSelector:          app=agnhost,role=master\nType:              ClusterIP\nIP:                10.240.24.141\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.25.2.73:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jan 29 13:42:27.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 describe node metakube-worker-cmccl-6d88bd94fc-87n7l'
Jan 29 13:42:28.053: INFO: stderr: ""
Jan 29 13:42:28.054: INFO: stdout: "Name:               metakube-worker-cmccl-6d88bd94fc-87n7l\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=216cc975-806e-44dd-93a1-c0b27513a975\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=dbl\n                    failure-domain.beta.kubernetes.io/zone=dbl1\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=metakube-worker-cmccl-6d88bd94fc-87n7l\n                    kubernetes.io/os=linux\n                    machine-controller/host-id=092df85491069dad7d0addf4f2714198f20dff5929f338f89f0abd9d\n                    machine-controller/owned-by=5b4f9d4a-8a44-4a5e-ab80-1ff4a918077d\n                    node.kubernetes.io/instance-type=216cc975-806e-44dd-93a1-c0b27513a975\n                    system/cluster=cwm62rh2d2\n                    system/project=cdfjnlgv6b\n                    topology.kubernetes.io/region=dbl\n                    topology.kubernetes.io/zone=dbl1\nAnnotations:        cluster.k8s.io/machine: kube-system/metakube-worker-cmccl-6d88bd94fc-87n7l\n                    flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"22:3d:64:12:a6:6b\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 192.168.1.13\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.25.0.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 29 Jan 2020 10:31:05 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  metakube-worker-cmccl-6d88bd94fc-87n7l\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 29 Jan 2020 13:42:20 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Wed, 29 Jan 2020 13:42:19 +0000   Wed, 29 Jan 2020 10:31:05 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Wed, 29 Jan 2020 13:42:19 +0000   Wed, 29 Jan 2020 10:31:05 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Wed, 29 Jan 2020 13:42:19 +0000   Wed, 29 Jan 2020 10:31:05 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Wed, 29 Jan 2020 13:42:19 +0000   Wed, 29 Jan 2020 10:31:35 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.1.13\n  ExternalIP:  195.192.129.48\n  Hostname:    metakube-worker-cmccl-6d88bd94fc-87n7l\nCapacity:\n  attachable-volumes-cinder:  25\n  cpu:                        2\n  ephemeral-storage:          50633164Ki\n  hugepages-2Mi:              0\n  memory:                     8167952Ki\n  pods:                       110\nAllocatable:\n  attachable-volumes-cinder:  25\n  cpu:                        1800m\n  ephemeral-storage:          44516040218\n  hugepages-2Mi:              0\n  memory:                     7860752Ki\n  pods:                       110\nSystem Info:\n  Machine ID:                 062287c2660941f5b88429fb520c289e\n  System UUID:                062287C2-6609-41F5-B884-29FB520C289E\n  Boot ID:                    e04f0b52-b3bc-4893-b5ff-9694a64b1490\n  Kernel Version:             4.15.0-74-generic\n  OS Image:                   Ubuntu 18.04.3 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://18.9.2\n  Kubelet Version:            v1.17.2\n  Kube-Proxy Version:         v1.17.2\nPodCIDR:                      172.25.0.0/24\nPodCIDRs:                     172.25.0.0/24\nProviderID:                   openstack:///062287c2-6609-41f5-b884-29fb520c289e\nNon-terminated Pods:          (10 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 canal-lpkd9                                                350m (19%)    200m (11%)  50Mi (0%)        256Mi (3%)     3h11m\n  kube-system                 cluster-autoscaler-8c65c7d54-z9ddg                         10m (0%)      100m (5%)   40Mi (0%)        300Mi (3%)     3h14m\n  kube-system                 coredns-6f745f6d74-8q86x                                   100m (5%)     0 (0%)      70Mi (0%)        170Mi (2%)     3h14m\n  kube-system                 coredns-6f745f6d74-kr8n2                                   100m (5%)     0 (0%)      70Mi (0%)        170Mi (2%)     3h14m\n  kube-system                 kube-proxy-7jg5r                                           75m (4%)      250m (13%)  50Mi (0%)        250Mi (3%)     3h11m\n  kube-system                 node-exporter-4wc5c                                        3m (0%)       200m (11%)  16Mi (0%)        50Mi (0%)      3h11m\n  kube-system                 node-local-dns-nhzkw                                       25m (1%)      0 (0%)      5Mi (0%)         30Mi (0%)      3h11m\n  kube-system                 openvpn-client-64df8b95c9-5fqpn                            30m (1%)      200m (11%)  30Mi (0%)        82Mi (1%)      3h14m\n  kube-system                 tiller-deploy-78f78bb476-hz2w6                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h14m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-6c9811082b824a7c-sbw8r    0 (0%)        0 (0%)      0 (0%)           0 (0%)         12m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                   Requests    Limits\n  --------                   --------    ------\n  cpu                        693m (38%)  950m (52%)\n  memory                     331Mi (4%)  1308Mi (17%)\n  ephemeral-storage          0 (0%)      0 (0%)\n  attachable-volumes-cinder  0           0\nEvents:                      <none>\n"
Jan 29 13:42:28.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 describe namespace kubectl-2133'
Jan 29 13:42:28.438: INFO: stderr: ""
Jan 29 13:42:28.438: INFO: stdout: "Name:         kubectl-2133\nLabels:       e2e-framework=kubectl\n              e2e-run=9d5c9a6d-a307-4091-8121-22facd703954\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:42:28.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2133" for this suite.

• [SLOW TEST:5.323 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1154
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":276,"completed":68,"skipped":1117,"failed":0}
SS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:42:28.481: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-8631
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override command
Jan 29 13:42:29.150: INFO: Waiting up to 5m0s for pod "client-containers-18461c37-6a51-4fe9-ae29-20529f7717e0" in namespace "containers-8631" to be "success or failure"
Jan 29 13:42:29.160: INFO: Pod "client-containers-18461c37-6a51-4fe9-ae29-20529f7717e0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.254303ms
Jan 29 13:42:31.169: INFO: Pod "client-containers-18461c37-6a51-4fe9-ae29-20529f7717e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018948901s
Jan 29 13:42:33.177: INFO: Pod "client-containers-18461c37-6a51-4fe9-ae29-20529f7717e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02738505s
STEP: Saw pod success
Jan 29 13:42:33.177: INFO: Pod "client-containers-18461c37-6a51-4fe9-ae29-20529f7717e0" satisfied condition "success or failure"
Jan 29 13:42:33.184: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod client-containers-18461c37-6a51-4fe9-ae29-20529f7717e0 container test-container: <nil>
STEP: delete the pod
Jan 29 13:42:33.244: INFO: Waiting for pod client-containers-18461c37-6a51-4fe9-ae29-20529f7717e0 to disappear
Jan 29 13:42:33.251: INFO: Pod client-containers-18461c37-6a51-4fe9-ae29-20529f7717e0 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:42:33.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8631" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":276,"completed":69,"skipped":1119,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:42:33.311: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9535
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-upd-2eaae396-27e9-4e69-b724-8ddf3fbd5ec1
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:42:45.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9535" for this suite.

• [SLOW TEST:12.467 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":276,"completed":70,"skipped":1135,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:42:45.781: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1336
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jan 29 13:42:52.065: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
W0129 13:42:52.064981      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jan 29 13:42:52.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1336" for this suite.

• [SLOW TEST:6.306 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":276,"completed":71,"skipped":1144,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:42:52.088: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2172
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jan 29 13:42:52.303: INFO: Waiting up to 5m0s for pod "pod-4b6baa3e-c5e3-4d46-9ede-d261c20f7517" in namespace "emptydir-2172" to be "success or failure"
Jan 29 13:42:52.310: INFO: Pod "pod-4b6baa3e-c5e3-4d46-9ede-d261c20f7517": Phase="Pending", Reason="", readiness=false. Elapsed: 6.376323ms
Jan 29 13:42:54.327: INFO: Pod "pod-4b6baa3e-c5e3-4d46-9ede-d261c20f7517": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023452592s
Jan 29 13:42:56.334: INFO: Pod "pod-4b6baa3e-c5e3-4d46-9ede-d261c20f7517": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030696819s
Jan 29 13:42:58.346: INFO: Pod "pod-4b6baa3e-c5e3-4d46-9ede-d261c20f7517": Phase="Pending", Reason="", readiness=false. Elapsed: 6.042780763s
Jan 29 13:43:00.363: INFO: Pod "pod-4b6baa3e-c5e3-4d46-9ede-d261c20f7517": Phase="Pending", Reason="", readiness=false. Elapsed: 8.059208401s
Jan 29 13:43:02.370: INFO: Pod "pod-4b6baa3e-c5e3-4d46-9ede-d261c20f7517": Phase="Pending", Reason="", readiness=false. Elapsed: 10.066802457s
Jan 29 13:43:04.383: INFO: Pod "pod-4b6baa3e-c5e3-4d46-9ede-d261c20f7517": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.079559637s
STEP: Saw pod success
Jan 29 13:43:04.383: INFO: Pod "pod-4b6baa3e-c5e3-4d46-9ede-d261c20f7517" satisfied condition "success or failure"
Jan 29 13:43:04.394: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-4b6baa3e-c5e3-4d46-9ede-d261c20f7517 container test-container: <nil>
STEP: delete the pod
Jan 29 13:43:04.485: INFO: Waiting for pod pod-4b6baa3e-c5e3-4d46-9ede-d261c20f7517 to disappear
Jan 29 13:43:04.494: INFO: Pod pod-4b6baa3e-c5e3-4d46-9ede-d261c20f7517 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:43:04.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2172" for this suite.

• [SLOW TEST:12.448 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":276,"completed":72,"skipped":1166,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:43:04.543: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4213
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Jan 29 13:43:09.382: INFO: Successfully updated pod "annotationupdate197c515a-cc9c-45f1-a158-d88af60d7298"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:43:13.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4213" for this suite.

• [SLOW TEST:8.985 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":276,"completed":73,"skipped":1169,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:43:13.528: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-824
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jan 29 13:43:17.879: INFO: &Pod{ObjectMeta:{send-events-bdfecb8c-cc8b-45d3-a9ba-d80c56728aa8  events-824 /api/v1/namespaces/events-824/pods/send-events-bdfecb8c-cc8b-45d3-a9ba-d80c56728aa8 bd6c2ba7-cd25-4ed1-a2e5-bed12a153ec2 67865 0 2020-01-29 13:43:13 +0000 UTC <nil> <nil> map[name:foo time:810413213] map[cni.projectcalico.org/podIP:172.25.2.82/32 cni.projectcalico.org/podIPs:172.25.2.82/32] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zrhvz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zrhvz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zrhvz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-cmccl-6d88bd94fc-lqfxz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:43:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:43:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:43:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:43:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.5,PodIP:172.25.2.82,StartTime:2020-01-29 13:43:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-29 13:43:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:docker://0df70d826ea6dea419078276b628b6c94e9bf9057839c8b1f4798f36ccf60b48,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.82,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Jan 29 13:43:19.889: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jan 29 13:43:21.897: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:43:21.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-824" for this suite.

• [SLOW TEST:8.410 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":276,"completed":74,"skipped":1178,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:43:21.939: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8690
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-19358757-4b81-4f40-98c6-78cc26384f93
STEP: Creating a pod to test consume secrets
Jan 29 13:43:22.349: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-43b2c969-7dc9-43ab-a450-110494dae975" in namespace "projected-8690" to be "success or failure"
Jan 29 13:43:22.357: INFO: Pod "pod-projected-secrets-43b2c969-7dc9-43ab-a450-110494dae975": Phase="Pending", Reason="", readiness=false. Elapsed: 7.572191ms
Jan 29 13:43:24.366: INFO: Pod "pod-projected-secrets-43b2c969-7dc9-43ab-a450-110494dae975": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016711334s
Jan 29 13:43:26.378: INFO: Pod "pod-projected-secrets-43b2c969-7dc9-43ab-a450-110494dae975": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028745633s
Jan 29 13:43:28.386: INFO: Pod "pod-projected-secrets-43b2c969-7dc9-43ab-a450-110494dae975": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036392086s
STEP: Saw pod success
Jan 29 13:43:28.386: INFO: Pod "pod-projected-secrets-43b2c969-7dc9-43ab-a450-110494dae975" satisfied condition "success or failure"
Jan 29 13:43:28.502: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-projected-secrets-43b2c969-7dc9-43ab-a450-110494dae975 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 29 13:43:28.887: INFO: Waiting for pod pod-projected-secrets-43b2c969-7dc9-43ab-a450-110494dae975 to disappear
Jan 29 13:43:28.894: INFO: Pod pod-projected-secrets-43b2c969-7dc9-43ab-a450-110494dae975 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:43:28.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8690" for this suite.

• [SLOW TEST:6.980 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":276,"completed":75,"skipped":1183,"failed":0}
SSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:43:28.920: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-1063
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jan 29 13:43:39.898: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 29 13:43:39.907: INFO: Pod pod-with-poststart-http-hook still exists
Jan 29 13:43:41.907: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 29 13:43:41.915: INFO: Pod pod-with-poststart-http-hook still exists
Jan 29 13:43:43.908: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 29 13:43:43.917: INFO: Pod pod-with-poststart-http-hook still exists
Jan 29 13:43:45.907: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 29 13:43:45.941: INFO: Pod pod-with-poststart-http-hook still exists
Jan 29 13:43:47.908: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 29 13:43:47.917: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:43:47.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1063" for this suite.

• [SLOW TEST:19.031 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":276,"completed":76,"skipped":1187,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:43:47.955: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2757
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 29 13:43:48.175: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Jan 29 13:43:52.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 --namespace=crd-publish-openapi-2757 create -f -'
Jan 29 13:43:53.809: INFO: stderr: ""
Jan 29 13:43:53.810: INFO: stdout: "e2e-test-crd-publish-openapi-7795-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 29 13:43:53.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 --namespace=crd-publish-openapi-2757 delete e2e-test-crd-publish-openapi-7795-crds test-foo'
Jan 29 13:43:53.961: INFO: stderr: ""
Jan 29 13:43:53.961: INFO: stdout: "e2e-test-crd-publish-openapi-7795-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jan 29 13:43:53.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 --namespace=crd-publish-openapi-2757 apply -f -'
Jan 29 13:43:54.358: INFO: stderr: ""
Jan 29 13:43:54.358: INFO: stdout: "e2e-test-crd-publish-openapi-7795-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 29 13:43:54.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 --namespace=crd-publish-openapi-2757 delete e2e-test-crd-publish-openapi-7795-crds test-foo'
Jan 29 13:43:54.544: INFO: stderr: ""
Jan 29 13:43:54.544: INFO: stdout: "e2e-test-crd-publish-openapi-7795-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Jan 29 13:43:54.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 --namespace=crd-publish-openapi-2757 create -f -'
Jan 29 13:43:54.958: INFO: rc: 1
Jan 29 13:43:54.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 --namespace=crd-publish-openapi-2757 apply -f -'
Jan 29 13:43:55.436: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Jan 29 13:43:55.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 --namespace=crd-publish-openapi-2757 create -f -'
Jan 29 13:43:55.767: INFO: rc: 1
Jan 29 13:43:55.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 --namespace=crd-publish-openapi-2757 apply -f -'
Jan 29 13:43:56.136: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Jan 29 13:43:56.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 explain e2e-test-crd-publish-openapi-7795-crds'
Jan 29 13:43:56.568: INFO: stderr: ""
Jan 29 13:43:56.568: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7795-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Jan 29 13:43:56.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 explain e2e-test-crd-publish-openapi-7795-crds.metadata'
Jan 29 13:43:56.891: INFO: stderr: ""
Jan 29 13:43:56.891: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7795-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jan 29 13:43:56.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 explain e2e-test-crd-publish-openapi-7795-crds.spec'
Jan 29 13:43:57.199: INFO: stderr: ""
Jan 29 13:43:57.199: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7795-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jan 29 13:43:57.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 explain e2e-test-crd-publish-openapi-7795-crds.spec.bars'
Jan 29 13:43:57.485: INFO: stderr: ""
Jan 29 13:43:57.485: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7795-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Jan 29 13:43:57.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 explain e2e-test-crd-publish-openapi-7795-crds.spec.bars2'
Jan 29 13:43:57.758: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:44:02.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2757" for this suite.

• [SLOW TEST:14.334 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":276,"completed":77,"skipped":1214,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:44:02.293: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5694
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 29 13:44:02.617: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f3233a27-251c-453c-b705-e34203446f3e" in namespace "downward-api-5694" to be "success or failure"
Jan 29 13:44:02.631: INFO: Pod "downwardapi-volume-f3233a27-251c-453c-b705-e34203446f3e": Phase="Pending", Reason="", readiness=false. Elapsed: 14.687616ms
Jan 29 13:44:04.644: INFO: Pod "downwardapi-volume-f3233a27-251c-453c-b705-e34203446f3e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027231129s
Jan 29 13:44:06.662: INFO: Pod "downwardapi-volume-f3233a27-251c-453c-b705-e34203446f3e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045528666s
Jan 29 13:44:08.680: INFO: Pod "downwardapi-volume-f3233a27-251c-453c-b705-e34203446f3e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.063277351s
STEP: Saw pod success
Jan 29 13:44:08.680: INFO: Pod "downwardapi-volume-f3233a27-251c-453c-b705-e34203446f3e" satisfied condition "success or failure"
Jan 29 13:44:08.691: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod downwardapi-volume-f3233a27-251c-453c-b705-e34203446f3e container client-container: <nil>
STEP: delete the pod
Jan 29 13:44:08.769: INFO: Waiting for pod downwardapi-volume-f3233a27-251c-453c-b705-e34203446f3e to disappear
Jan 29 13:44:08.779: INFO: Pod downwardapi-volume-f3233a27-251c-453c-b705-e34203446f3e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:44:08.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5694" for this suite.

• [SLOW TEST:6.520 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":276,"completed":78,"skipped":1228,"failed":0}
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:44:08.815: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4374
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-downwardapi-brg8
STEP: Creating a pod to test atomic-volume-subpath
Jan 29 13:44:09.112: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-brg8" in namespace "subpath-4374" to be "success or failure"
Jan 29 13:44:09.120: INFO: Pod "pod-subpath-test-downwardapi-brg8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.584455ms
Jan 29 13:44:11.130: INFO: Pod "pod-subpath-test-downwardapi-brg8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018052802s
Jan 29 13:44:13.141: INFO: Pod "pod-subpath-test-downwardapi-brg8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029065973s
Jan 29 13:44:15.155: INFO: Pod "pod-subpath-test-downwardapi-brg8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.043540054s
Jan 29 13:44:17.170: INFO: Pod "pod-subpath-test-downwardapi-brg8": Phase="Running", Reason="", readiness=true. Elapsed: 8.058630453s
Jan 29 13:44:19.186: INFO: Pod "pod-subpath-test-downwardapi-brg8": Phase="Running", Reason="", readiness=true. Elapsed: 10.074438056s
Jan 29 13:44:21.201: INFO: Pod "pod-subpath-test-downwardapi-brg8": Phase="Running", Reason="", readiness=true. Elapsed: 12.089568476s
Jan 29 13:44:23.224: INFO: Pod "pod-subpath-test-downwardapi-brg8": Phase="Running", Reason="", readiness=true. Elapsed: 14.111995599s
Jan 29 13:44:25.238: INFO: Pod "pod-subpath-test-downwardapi-brg8": Phase="Running", Reason="", readiness=true. Elapsed: 16.126556545s
Jan 29 13:44:27.248: INFO: Pod "pod-subpath-test-downwardapi-brg8": Phase="Running", Reason="", readiness=true. Elapsed: 18.136645853s
Jan 29 13:44:29.260: INFO: Pod "pod-subpath-test-downwardapi-brg8": Phase="Running", Reason="", readiness=true. Elapsed: 20.147676587s
Jan 29 13:44:31.537: INFO: Pod "pod-subpath-test-downwardapi-brg8": Phase="Running", Reason="", readiness=true. Elapsed: 22.425568144s
Jan 29 13:44:33.549: INFO: Pod "pod-subpath-test-downwardapi-brg8": Phase="Running", Reason="", readiness=true. Elapsed: 24.437263541s
Jan 29 13:44:35.575: INFO: Pod "pod-subpath-test-downwardapi-brg8": Phase="Running", Reason="", readiness=true. Elapsed: 26.463073359s
Jan 29 13:44:37.584: INFO: Pod "pod-subpath-test-downwardapi-brg8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.471884545s
STEP: Saw pod success
Jan 29 13:44:37.584: INFO: Pod "pod-subpath-test-downwardapi-brg8" satisfied condition "success or failure"
Jan 29 13:44:37.593: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-subpath-test-downwardapi-brg8 container test-container-subpath-downwardapi-brg8: <nil>
STEP: delete the pod
Jan 29 13:44:37.637: INFO: Waiting for pod pod-subpath-test-downwardapi-brg8 to disappear
Jan 29 13:44:37.644: INFO: Pod pod-subpath-test-downwardapi-brg8 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-brg8
Jan 29 13:44:37.644: INFO: Deleting pod "pod-subpath-test-downwardapi-brg8" in namespace "subpath-4374"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:44:37.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4374" for this suite.

• [SLOW TEST:28.876 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":276,"completed":79,"skipped":1233,"failed":0}
SSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:44:37.693: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-9643
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: getting the auto-created API token
Jan 29 13:44:38.557: INFO: created pod pod-service-account-defaultsa
Jan 29 13:44:38.558: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jan 29 13:44:38.573: INFO: created pod pod-service-account-mountsa
Jan 29 13:44:38.573: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jan 29 13:44:38.609: INFO: created pod pod-service-account-nomountsa
Jan 29 13:44:38.609: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jan 29 13:44:38.637: INFO: created pod pod-service-account-defaultsa-mountspec
Jan 29 13:44:38.637: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jan 29 13:44:38.656: INFO: created pod pod-service-account-mountsa-mountspec
Jan 29 13:44:38.656: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jan 29 13:44:38.799: INFO: created pod pod-service-account-nomountsa-mountspec
Jan 29 13:44:38.799: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jan 29 13:44:38.828: INFO: created pod pod-service-account-defaultsa-nomountspec
Jan 29 13:44:38.828: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jan 29 13:44:38.853: INFO: created pod pod-service-account-mountsa-nomountspec
Jan 29 13:44:38.853: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jan 29 13:44:38.892: INFO: created pod pod-service-account-nomountsa-nomountspec
Jan 29 13:44:38.892: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:44:38.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9643" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":276,"completed":80,"skipped":1239,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:44:39.035: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-9062
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Jan 29 13:44:39.464: INFO: PodSpec: initContainers in spec.initContainers
Jan 29 13:45:28.005: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-743447cc-1927-49af-9748-0aab727a7662", GenerateName:"", Namespace:"init-container-9062", SelfLink:"/api/v1/namespaces/init-container-9062/pods/pod-init-743447cc-1927-49af-9748-0aab727a7662", UID:"29ff34e8-3366-4a31-aefa-dabbfa13dd75", ResourceVersion:"68720", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63715902279, loc:(*time.Location)(0x7db4bc0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"464601057"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"172.25.1.156/32", "cni.projectcalico.org/podIPs":"172.25.1.156/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-p24q9", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc005807f40), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-p24q9", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-p24q9", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-p24q9", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00378ebf8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"metakube-worker-cmccl-6d88bd94fc-znv5g", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002ed0f00), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00378ec90)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00378ecb0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00378ecb8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00378ecbc), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715902279, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715902279, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715902279, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715902279, loc:(*time.Location)(0x7db4bc0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.1.7", PodIP:"172.25.1.156", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.25.1.156"}}, StartTime:(*v1.Time)(0xc002d69da0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000485880)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0004858f0)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://9fefac6c4203c5de16b0c9d39a92425192ad64f5b1864f4bed32b1a7e8c79ec5", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002d69de0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002d69dc0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc00378ed4f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:45:28.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9062" for this suite.

• [SLOW TEST:49.027 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":276,"completed":81,"skipped":1252,"failed":0}
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:45:28.064: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-2736
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Jan 29 13:45:28.297: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 29 13:45:28.348: INFO: Waiting for terminating namespaces to be deleted...
Jan 29 13:45:28.356: INFO: 
Logging pods the kubelet thinks is on node metakube-worker-cmccl-6d88bd94fc-87n7l before test
Jan 29 13:45:28.400: INFO: openvpn-client-64df8b95c9-5fqpn from kube-system started at 2020-01-29 10:31:44 +0000 UTC (2 container statuses recorded)
Jan 29 13:45:28.400: INFO: 	Container dnat-controller ready: true, restart count 0
Jan 29 13:45:28.400: INFO: 	Container openvpn-client ready: true, restart count 1
Jan 29 13:45:28.400: INFO: coredns-6f745f6d74-kr8n2 from kube-system started at 2020-01-29 10:31:44 +0000 UTC (1 container statuses recorded)
Jan 29 13:45:28.401: INFO: 	Container coredns ready: true, restart count 0
Jan 29 13:45:28.401: INFO: sonobuoy-systemd-logs-daemon-set-6c9811082b824a7c-sbw8r from sonobuoy started at 2020-01-29 13:30:03 +0000 UTC (2 container statuses recorded)
Jan 29 13:45:28.401: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 29 13:45:28.401: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 29 13:45:28.401: INFO: kube-proxy-7jg5r from kube-system started at 2020-01-29 10:31:06 +0000 UTC (1 container statuses recorded)
Jan 29 13:45:28.401: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 29 13:45:28.401: INFO: node-local-dns-nhzkw from kube-system started at 2020-01-29 10:31:06 +0000 UTC (1 container statuses recorded)
Jan 29 13:45:28.401: INFO: 	Container node-cache ready: true, restart count 0
Jan 29 13:45:28.402: INFO: cluster-autoscaler-8c65c7d54-z9ddg from kube-system started at 2020-01-29 10:31:35 +0000 UTC (1 container statuses recorded)
Jan 29 13:45:28.402: INFO: 	Container cluster-autoscaler ready: true, restart count 1
Jan 29 13:45:28.402: INFO: tiller-deploy-78f78bb476-hz2w6 from kube-system started at 2020-01-29 10:31:35 +0000 UTC (1 container statuses recorded)
Jan 29 13:45:28.402: INFO: 	Container tiller ready: true, restart count 0
Jan 29 13:45:28.402: INFO: coredns-6f745f6d74-8q86x from kube-system started at 2020-01-29 10:31:43 +0000 UTC (1 container statuses recorded)
Jan 29 13:45:28.402: INFO: 	Container coredns ready: true, restart count 0
Jan 29 13:45:28.403: INFO: canal-lpkd9 from kube-system started at 2020-01-29 10:31:07 +0000 UTC (2 container statuses recorded)
Jan 29 13:45:28.403: INFO: 	Container calico-node ready: true, restart count 0
Jan 29 13:45:28.403: INFO: 	Container kube-flannel ready: true, restart count 0
Jan 29 13:45:28.403: INFO: node-exporter-4wc5c from kube-system started at 2020-01-29 10:31:07 +0000 UTC (1 container statuses recorded)
Jan 29 13:45:28.403: INFO: 	Container node-exporter ready: true, restart count 0
Jan 29 13:45:28.403: INFO: 
Logging pods the kubelet thinks is on node metakube-worker-cmccl-6d88bd94fc-lqfxz before test
Jan 29 13:45:28.477: INFO: canal-4zhl8 from kube-system started at 2020-01-29 10:32:02 +0000 UTC (2 container statuses recorded)
Jan 29 13:45:28.477: INFO: 	Container calico-node ready: true, restart count 0
Jan 29 13:45:28.477: INFO: 	Container kube-flannel ready: true, restart count 0
Jan 29 13:45:28.478: INFO: kube-proxy-rr5gp from kube-system started at 2020-01-29 10:32:02 +0000 UTC (1 container statuses recorded)
Jan 29 13:45:28.478: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 29 13:45:28.478: INFO: node-exporter-99p82 from kube-system started at 2020-01-29 10:32:02 +0000 UTC (1 container statuses recorded)
Jan 29 13:45:28.479: INFO: 	Container node-exporter ready: true, restart count 0
Jan 29 13:45:28.479: INFO: node-local-dns-72k8g from kube-system started at 2020-01-29 10:32:02 +0000 UTC (1 container statuses recorded)
Jan 29 13:45:28.479: INFO: 	Container node-cache ready: true, restart count 0
Jan 29 13:45:28.479: INFO: sonobuoy-systemd-logs-daemon-set-6c9811082b824a7c-9g4cq from sonobuoy started at 2020-01-29 13:30:03 +0000 UTC (2 container statuses recorded)
Jan 29 13:45:28.480: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 29 13:45:28.480: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 29 13:45:28.480: INFO: 
Logging pods the kubelet thinks is on node metakube-worker-cmccl-6d88bd94fc-znv5g before test
Jan 29 13:45:28.556: INFO: kube-proxy-h6b8h from kube-system started at 2020-01-29 10:31:22 +0000 UTC (1 container statuses recorded)
Jan 29 13:45:28.556: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 29 13:45:28.556: INFO: pod-init-743447cc-1927-49af-9748-0aab727a7662 from init-container-9062 started at 2020-01-29 13:44:39 +0000 UTC (1 container statuses recorded)
Jan 29 13:45:28.556: INFO: 	Container run1 ready: false, restart count 0
Jan 29 13:45:28.556: INFO: sonobuoy from sonobuoy started at 2020-01-29 13:30:00 +0000 UTC (1 container statuses recorded)
Jan 29 13:45:28.556: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 29 13:45:28.557: INFO: sonobuoy-e2e-job-bb288164d824466f from sonobuoy started at 2020-01-29 13:30:03 +0000 UTC (2 container statuses recorded)
Jan 29 13:45:28.557: INFO: 	Container e2e ready: true, restart count 0
Jan 29 13:45:28.557: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 29 13:45:28.557: INFO: node-local-dns-2tvcw from kube-system started at 2020-01-29 10:31:23 +0000 UTC (1 container statuses recorded)
Jan 29 13:45:28.557: INFO: 	Container node-cache ready: true, restart count 0
Jan 29 13:45:28.557: INFO: canal-d5wpn from kube-system started at 2020-01-29 10:31:23 +0000 UTC (2 container statuses recorded)
Jan 29 13:45:28.557: INFO: 	Container calico-node ready: true, restart count 0
Jan 29 13:45:28.557: INFO: 	Container kube-flannel ready: true, restart count 0
Jan 29 13:45:28.557: INFO: node-exporter-xblbq from kube-system started at 2020-01-29 10:31:23 +0000 UTC (1 container statuses recorded)
Jan 29 13:45:28.557: INFO: 	Container node-exporter ready: true, restart count 0
Jan 29 13:45:28.557: INFO: sonobuoy-systemd-logs-daemon-set-6c9811082b824a7c-mxxcq from sonobuoy started at 2020-01-29 13:30:03 +0000 UTC (2 container statuses recorded)
Jan 29 13:45:28.557: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 29 13:45:28.557: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15ee5f81664c1e18], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:45:29.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2736" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":276,"completed":82,"skipped":1256,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:45:29.685: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9099
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1692
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jan 29 13:45:29.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-9099'
Jan 29 13:45:30.126: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan 29 13:45:30.126: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
Jan 29 13:45:30.149: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Jan 29 13:45:30.167: INFO: scanned /root for discovery docs: <nil>
Jan 29 13:45:30.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-9099'
Jan 29 13:45:51.006: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jan 29 13:45:51.006: INFO: stdout: "Created e2e-test-httpd-rc-782aadfccc04197a703ed23d67627ed5\nScaling up e2e-test-httpd-rc-782aadfccc04197a703ed23d67627ed5 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-782aadfccc04197a703ed23d67627ed5 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-782aadfccc04197a703ed23d67627ed5 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Jan 29 13:45:51.006: INFO: stdout: "Created e2e-test-httpd-rc-782aadfccc04197a703ed23d67627ed5\nScaling up e2e-test-httpd-rc-782aadfccc04197a703ed23d67627ed5 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-782aadfccc04197a703ed23d67627ed5 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-782aadfccc04197a703ed23d67627ed5 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Jan 29 13:45:51.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-9099'
Jan 29 13:45:51.109: INFO: stderr: ""
Jan 29 13:45:51.109: INFO: stdout: "e2e-test-httpd-rc-782aadfccc04197a703ed23d67627ed5-qdncf e2e-test-httpd-rc-g4bxz "
STEP: Replicas for run=e2e-test-httpd-rc: expected=1 actual=2
Jan 29 13:45:56.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-9099'
Jan 29 13:45:56.264: INFO: stderr: ""
Jan 29 13:45:56.264: INFO: stdout: "e2e-test-httpd-rc-782aadfccc04197a703ed23d67627ed5-qdncf e2e-test-httpd-rc-g4bxz "
STEP: Replicas for run=e2e-test-httpd-rc: expected=1 actual=2
Jan 29 13:46:01.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-9099'
Jan 29 13:46:01.446: INFO: stderr: ""
Jan 29 13:46:01.446: INFO: stdout: "e2e-test-httpd-rc-782aadfccc04197a703ed23d67627ed5-qdncf "
Jan 29 13:46:01.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods e2e-test-httpd-rc-782aadfccc04197a703ed23d67627ed5-qdncf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9099'
Jan 29 13:46:01.751: INFO: stderr: ""
Jan 29 13:46:01.751: INFO: stdout: "true"
Jan 29 13:46:01.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods e2e-test-httpd-rc-782aadfccc04197a703ed23d67627ed5-qdncf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9099'
Jan 29 13:46:01.874: INFO: stderr: ""
Jan 29 13:46:01.874: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Jan 29 13:46:01.874: INFO: e2e-test-httpd-rc-782aadfccc04197a703ed23d67627ed5-qdncf is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1698
Jan 29 13:46:01.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 delete rc e2e-test-httpd-rc --namespace=kubectl-9099'
Jan 29 13:46:02.024: INFO: stderr: ""
Jan 29 13:46:02.024: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:46:02.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9099" for this suite.

• [SLOW TEST:32.573 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1687
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl rolling-update should support rolling-update to same image  [Conformance]","total":276,"completed":83,"skipped":1264,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:46:02.264: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-667
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 29 13:46:03.400: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 29 13:46:05.458: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715902363, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715902363, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715902363, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715902363, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 13:46:07.481: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715902363, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715902363, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715902363, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715902363, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 29 13:46:10.518: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Jan 29 13:46:16.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 attach --namespace=webhook-667 to-be-attached-pod -i -c=container1'
Jan 29 13:46:17.148: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:46:17.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-667" for this suite.
STEP: Destroying namespace "webhook-667-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:15.110 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":276,"completed":84,"skipped":1281,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:46:17.375: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8019
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl replace
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1897
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jan 29 13:46:17.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-8019'
Jan 29 13:46:17.755: INFO: stderr: ""
Jan 29 13:46:17.755: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Jan 29 13:46:22.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pod e2e-test-httpd-pod --namespace=kubectl-8019 -o json'
Jan 29 13:46:22.962: INFO: stderr: ""
Jan 29 13:46:22.962: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"172.25.2.97/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.25.2.97/32\"\n        },\n        \"creationTimestamp\": \"2020-01-29T13:46:17Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-8019\",\n        \"resourceVersion\": \"69119\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-8019/pods/e2e-test-httpd-pod\",\n        \"uid\": \"b444c1c0-35d9-433c-8d88-8bd38b6994c8\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-ghvg5\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"metakube-worker-cmccl-6d88bd94fc-lqfxz\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-ghvg5\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-ghvg5\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-01-29T13:46:17Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-01-29T13:46:20Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-01-29T13:46:20Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-01-29T13:46:17Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://a6d42cc2506348c466abac3ba73d44e3d754804021119692a406d78751e46fd2\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-01-29T13:46:20Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.1.5\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.25.2.97\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.25.2.97\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-01-29T13:46:17Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jan 29 13:46:22.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 replace -f - --namespace=kubectl-8019'
Jan 29 13:46:23.472: INFO: stderr: ""
Jan 29 13:46:23.472: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1902
Jan 29 13:46:23.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 delete pods e2e-test-httpd-pod --namespace=kubectl-8019'
Jan 29 13:46:36.428: INFO: stderr: ""
Jan 29 13:46:36.428: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:46:36.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8019" for this suite.

• [SLOW TEST:19.090 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1893
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":276,"completed":85,"skipped":1300,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:46:36.469: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2688
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:46:36.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2688" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":276,"completed":86,"skipped":1310,"failed":0}
SSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:46:36.949: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-7775
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 29 13:46:37.371: INFO: (0) /api/v1/nodes/metakube-worker-cmccl-6d88bd94fc-lqfxz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 100.147871ms)
Jan 29 13:46:37.439: INFO: (1) /api/v1/nodes/metakube-worker-cmccl-6d88bd94fc-lqfxz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 67.957155ms)
Jan 29 13:46:37.474: INFO: (2) /api/v1/nodes/metakube-worker-cmccl-6d88bd94fc-lqfxz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 34.603595ms)
Jan 29 13:46:37.490: INFO: (3) /api/v1/nodes/metakube-worker-cmccl-6d88bd94fc-lqfxz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 16.127762ms)
Jan 29 13:46:37.504: INFO: (4) /api/v1/nodes/metakube-worker-cmccl-6d88bd94fc-lqfxz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 13.868668ms)
Jan 29 13:46:37.515: INFO: (5) /api/v1/nodes/metakube-worker-cmccl-6d88bd94fc-lqfxz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 10.734332ms)
Jan 29 13:46:37.527: INFO: (6) /api/v1/nodes/metakube-worker-cmccl-6d88bd94fc-lqfxz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 12.118568ms)
Jan 29 13:46:37.541: INFO: (7) /api/v1/nodes/metakube-worker-cmccl-6d88bd94fc-lqfxz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 12.970118ms)
Jan 29 13:46:37.561: INFO: (8) /api/v1/nodes/metakube-worker-cmccl-6d88bd94fc-lqfxz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 19.845862ms)
Jan 29 13:46:37.582: INFO: (9) /api/v1/nodes/metakube-worker-cmccl-6d88bd94fc-lqfxz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 21.347844ms)
Jan 29 13:46:37.610: INFO: (10) /api/v1/nodes/metakube-worker-cmccl-6d88bd94fc-lqfxz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 27.862622ms)
Jan 29 13:46:37.629: INFO: (11) /api/v1/nodes/metakube-worker-cmccl-6d88bd94fc-lqfxz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 19.135648ms)
Jan 29 13:46:37.647: INFO: (12) /api/v1/nodes/metakube-worker-cmccl-6d88bd94fc-lqfxz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 17.928279ms)
Jan 29 13:46:37.681: INFO: (13) /api/v1/nodes/metakube-worker-cmccl-6d88bd94fc-lqfxz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 32.908228ms)
Jan 29 13:46:37.720: INFO: (14) /api/v1/nodes/metakube-worker-cmccl-6d88bd94fc-lqfxz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 39.106314ms)
Jan 29 13:46:37.757: INFO: (15) /api/v1/nodes/metakube-worker-cmccl-6d88bd94fc-lqfxz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 36.690856ms)
Jan 29 13:46:37.775: INFO: (16) /api/v1/nodes/metakube-worker-cmccl-6d88bd94fc-lqfxz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 17.848599ms)
Jan 29 13:46:37.804: INFO: (17) /api/v1/nodes/metakube-worker-cmccl-6d88bd94fc-lqfxz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 29.154305ms)
Jan 29 13:46:37.826: INFO: (18) /api/v1/nodes/metakube-worker-cmccl-6d88bd94fc-lqfxz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 21.744923ms)
Jan 29 13:46:37.863: INFO: (19) /api/v1/nodes/metakube-worker-cmccl-6d88bd94fc-lqfxz/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 37.030469ms)
[AfterEach] version v1
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:46:37.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7775" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node using proxy subresource  [Conformance]","total":276,"completed":87,"skipped":1317,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:46:37.906: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-8378
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Jan 29 13:46:44.708: INFO: Successfully updated pod "adopt-release-cd78j"
STEP: Checking that the Job readopts the Pod
Jan 29 13:46:44.708: INFO: Waiting up to 15m0s for pod "adopt-release-cd78j" in namespace "job-8378" to be "adopted"
Jan 29 13:46:44.719: INFO: Pod "adopt-release-cd78j": Phase="Running", Reason="", readiness=true. Elapsed: 11.239864ms
Jan 29 13:46:46.740: INFO: Pod "adopt-release-cd78j": Phase="Running", Reason="", readiness=true. Elapsed: 2.032481322s
Jan 29 13:46:46.741: INFO: Pod "adopt-release-cd78j" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Jan 29 13:46:47.270: INFO: Successfully updated pod "adopt-release-cd78j"
STEP: Checking that the Job releases the Pod
Jan 29 13:46:47.270: INFO: Waiting up to 15m0s for pod "adopt-release-cd78j" in namespace "job-8378" to be "released"
Jan 29 13:46:47.279: INFO: Pod "adopt-release-cd78j": Phase="Running", Reason="", readiness=true. Elapsed: 9.121937ms
Jan 29 13:46:49.291: INFO: Pod "adopt-release-cd78j": Phase="Running", Reason="", readiness=true. Elapsed: 2.020940982s
Jan 29 13:46:49.291: INFO: Pod "adopt-release-cd78j" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:46:49.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8378" for this suite.

• [SLOW TEST:11.417 seconds]
[sig-apps] Job
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":276,"completed":88,"skipped":1326,"failed":0}
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:46:49.325: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3240
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-3240
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating statefulset ss in namespace statefulset-3240
Jan 29 13:46:49.822: INFO: Found 0 stateful pods, waiting for 1
Jan 29 13:46:59.830: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jan 29 13:46:59.918: INFO: Deleting all statefulset in ns statefulset-3240
Jan 29 13:46:59.932: INFO: Scaling statefulset ss to 0
Jan 29 13:47:20.012: INFO: Waiting for statefulset status.replicas updated to 0
Jan 29 13:47:20.042: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:47:20.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3240" for this suite.

• [SLOW TEST:30.836 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":276,"completed":89,"skipped":1326,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:47:20.163: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-5701
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test substitution in container's command
Jan 29 13:47:20.433: INFO: Waiting up to 5m0s for pod "var-expansion-03acb468-fe2d-4871-81fe-b38a4e62ddbc" in namespace "var-expansion-5701" to be "success or failure"
Jan 29 13:47:20.445: INFO: Pod "var-expansion-03acb468-fe2d-4871-81fe-b38a4e62ddbc": Phase="Pending", Reason="", readiness=false. Elapsed: 11.805991ms
Jan 29 13:47:22.509: INFO: Pod "var-expansion-03acb468-fe2d-4871-81fe-b38a4e62ddbc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.075503766s
Jan 29 13:47:24.534: INFO: Pod "var-expansion-03acb468-fe2d-4871-81fe-b38a4e62ddbc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.101331022s
STEP: Saw pod success
Jan 29 13:47:24.535: INFO: Pod "var-expansion-03acb468-fe2d-4871-81fe-b38a4e62ddbc" satisfied condition "success or failure"
Jan 29 13:47:24.561: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-znv5g pod var-expansion-03acb468-fe2d-4871-81fe-b38a4e62ddbc container dapi-container: <nil>
STEP: delete the pod
Jan 29 13:47:24.679: INFO: Waiting for pod var-expansion-03acb468-fe2d-4871-81fe-b38a4e62ddbc to disappear
Jan 29 13:47:24.689: INFO: Pod var-expansion-03acb468-fe2d-4871-81fe-b38a4e62ddbc no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:47:24.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5701" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":276,"completed":90,"skipped":1334,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:47:24.720: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8806
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on node default medium
Jan 29 13:47:25.095: INFO: Waiting up to 5m0s for pod "pod-b3578de5-235a-4dbb-ba92-2ab50e996754" in namespace "emptydir-8806" to be "success or failure"
Jan 29 13:47:25.125: INFO: Pod "pod-b3578de5-235a-4dbb-ba92-2ab50e996754": Phase="Pending", Reason="", readiness=false. Elapsed: 29.091058ms
Jan 29 13:47:27.135: INFO: Pod "pod-b3578de5-235a-4dbb-ba92-2ab50e996754": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039114887s
Jan 29 13:47:29.153: INFO: Pod "pod-b3578de5-235a-4dbb-ba92-2ab50e996754": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05719219s
STEP: Saw pod success
Jan 29 13:47:29.154: INFO: Pod "pod-b3578de5-235a-4dbb-ba92-2ab50e996754" satisfied condition "success or failure"
Jan 29 13:47:29.165: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-b3578de5-235a-4dbb-ba92-2ab50e996754 container test-container: <nil>
STEP: delete the pod
Jan 29 13:47:29.265: INFO: Waiting for pod pod-b3578de5-235a-4dbb-ba92-2ab50e996754 to disappear
Jan 29 13:47:29.274: INFO: Pod pod-b3578de5-235a-4dbb-ba92-2ab50e996754 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:47:29.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8806" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":276,"completed":91,"skipped":1350,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:47:29.317: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8755
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap that has name configmap-test-emptyKey-7421de3c-c551-474c-a579-360f6f969840
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:47:29.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8755" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":276,"completed":92,"skipped":1366,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:47:29.605: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-9845
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test substitution in container's args
Jan 29 13:47:29.861: INFO: Waiting up to 5m0s for pod "var-expansion-21366f5d-2dee-4b60-9af1-92d529ae9c3b" in namespace "var-expansion-9845" to be "success or failure"
Jan 29 13:47:29.876: INFO: Pod "var-expansion-21366f5d-2dee-4b60-9af1-92d529ae9c3b": Phase="Pending", Reason="", readiness=false. Elapsed: 15.610155ms
Jan 29 13:47:31.883: INFO: Pod "var-expansion-21366f5d-2dee-4b60-9af1-92d529ae9c3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02248261s
Jan 29 13:47:33.893: INFO: Pod "var-expansion-21366f5d-2dee-4b60-9af1-92d529ae9c3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032838578s
STEP: Saw pod success
Jan 29 13:47:33.894: INFO: Pod "var-expansion-21366f5d-2dee-4b60-9af1-92d529ae9c3b" satisfied condition "success or failure"
Jan 29 13:47:33.903: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod var-expansion-21366f5d-2dee-4b60-9af1-92d529ae9c3b container dapi-container: <nil>
STEP: delete the pod
Jan 29 13:47:34.002: INFO: Waiting for pod var-expansion-21366f5d-2dee-4b60-9af1-92d529ae9c3b to disappear
Jan 29 13:47:34.030: INFO: Pod var-expansion-21366f5d-2dee-4b60-9af1-92d529ae9c3b no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:47:34.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9845" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":276,"completed":93,"skipped":1406,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:47:34.088: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6556
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:48:06.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6556" for this suite.

• [SLOW TEST:32.496 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  blackbox test
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":276,"completed":94,"skipped":1442,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:48:06.586: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8411
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jan 29 13:48:11.461: INFO: Successfully updated pod "pod-update-activedeadlineseconds-d1ebecbe-e5b1-403e-9edd-6c8a10abd09c"
Jan 29 13:48:11.461: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-d1ebecbe-e5b1-403e-9edd-6c8a10abd09c" in namespace "pods-8411" to be "terminated due to deadline exceeded"
Jan 29 13:48:11.473: INFO: Pod "pod-update-activedeadlineseconds-d1ebecbe-e5b1-403e-9edd-6c8a10abd09c": Phase="Running", Reason="", readiness=true. Elapsed: 11.200283ms
Jan 29 13:48:13.481: INFO: Pod "pod-update-activedeadlineseconds-d1ebecbe-e5b1-403e-9edd-6c8a10abd09c": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.01961349s
Jan 29 13:48:13.482: INFO: Pod "pod-update-activedeadlineseconds-d1ebecbe-e5b1-403e-9edd-6c8a10abd09c" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:48:13.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8411" for this suite.

• [SLOW TEST:6.926 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":276,"completed":95,"skipped":1452,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:48:13.522: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8025
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8025
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-8025
I0129 13:48:13.904969      22 runners.go:189] Created replication controller with name: externalname-service, namespace: services-8025, replica count: 2
I0129 13:48:16.956288      22 runners.go:189] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 29 13:48:16.956: INFO: Creating new exec pod
Jan 29 13:48:22.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=services-8025 execpodj9gp9 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jan 29 13:48:23.697: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 29 13:48:23.697: INFO: stdout: ""
Jan 29 13:48:23.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=services-8025 execpodj9gp9 -- /bin/sh -x -c nc -zv -t -w 2 10.240.28.10 80'
Jan 29 13:48:24.521: INFO: stderr: "+ nc -zv -t -w 2 10.240.28.10 80\nConnection to 10.240.28.10 80 port [tcp/http] succeeded!\n"
Jan 29 13:48:24.521: INFO: stdout: ""
Jan 29 13:48:24.521: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:48:24.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8025" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:11.077 seconds]
[sig-network] Services
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":276,"completed":96,"skipped":1495,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:48:24.601: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6986
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 29 13:48:24.822: INFO: Creating deployment "test-recreate-deployment"
Jan 29 13:48:24.837: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jan 29 13:48:24.904: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jan 29 13:48:26.945: INFO: Waiting deployment "test-recreate-deployment" to complete
Jan 29 13:48:26.953: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715902504, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715902504, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715902504, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715902504, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-799c574856\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 13:48:28.961: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jan 29 13:48:28.987: INFO: Updating deployment test-recreate-deployment
Jan 29 13:48:28.988: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Jan 29 13:48:29.206: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-6986 /apis/apps/v1/namespaces/deployment-6986/deployments/test-recreate-deployment f38933b4-4782-45b7-81de-2f07c53b90ce 70184 2 2020-01-29 13:48:24 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001f2a248 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-01-29 13:48:29 +0000 UTC,LastTransitionTime:2020-01-29 13:48:29 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2020-01-29 13:48:29 +0000 UTC,LastTransitionTime:2020-01-29 13:48:24 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jan 29 13:48:29.219: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-6986 /apis/apps/v1/namespaces/deployment-6986/replicasets/test-recreate-deployment-5f94c574ff b3a200d1-081e-447f-9d78-1f4d37a43247 70183 1 2020-01-29 13:48:29 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment f38933b4-4782-45b7-81de-2f07c53b90ce 0xc001f2a5f7 0xc001f2a5f8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001f2a658 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 29 13:48:29.219: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jan 29 13:48:29.220: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-799c574856  deployment-6986 /apis/apps/v1/namespaces/deployment-6986/replicasets/test-recreate-deployment-799c574856 e8e36ded-b997-41bc-a0fa-e8015042e02e 70172 2 2020-01-29 13:48:24 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:799c574856] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment f38933b4-4782-45b7-81de-2f07c53b90ce 0xc001f2a6c7 0xc001f2a6c8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 799c574856,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:799c574856] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001f2a738 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 29 13:48:29.231: INFO: Pod "test-recreate-deployment-5f94c574ff-kf4rs" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-kf4rs test-recreate-deployment-5f94c574ff- deployment-6986 /api/v1/namespaces/deployment-6986/pods/test-recreate-deployment-5f94c574ff-kf4rs 9c6839b7-0ca8-41cc-8f7e-985e4d1f5429 70186 0 2020-01-29 13:48:29 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff b3a200d1-081e-447f-9d78-1f4d37a43247 0xc001f2aba7 0xc001f2aba8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wjp8h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wjp8h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wjp8h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-cmccl-6d88bd94fc-lqfxz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:48:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:48:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:48:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 13:48:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.5,PodIP:,StartTime:2020-01-29 13:48:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:48:29.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6986" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":276,"completed":97,"skipped":1509,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:48:29.272: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-329
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 29 13:48:30.432: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 29 13:48:32.455: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715902510, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715902510, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715902510, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715902510, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 29 13:48:35.504: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:48:35.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-329" for this suite.
STEP: Destroying namespace "webhook-329-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.987 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":276,"completed":98,"skipped":1523,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:48:36.267: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-6247
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Jan 29 13:48:36.516: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:48:43.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6247" for this suite.

• [SLOW TEST:6.940 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":276,"completed":99,"skipped":1550,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:48:43.210: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8031
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8031.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8031.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8031.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8031.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 29 13:48:49.761: INFO: DNS probes using dns-test-eaf98e8b-af92-4cea-a037-3cbc6e2887ec succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8031.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8031.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8031.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8031.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 29 13:48:56.241: INFO: DNS probes using dns-test-2ced6e79-a773-4eb0-b0b0-fef3acdeedc5 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8031.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-8031.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8031.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-8031.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 29 13:49:10.707: INFO: DNS probes using dns-test-f79e1eea-009d-4020-a8f0-7dff8d768cf1 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:49:10.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8031" for this suite.

• [SLOW TEST:27.649 seconds]
[sig-network] DNS
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":276,"completed":100,"skipped":1577,"failed":0}
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:49:10.859: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-105
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Jan 29 13:49:11.117: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 29 13:49:11.171: INFO: Waiting for terminating namespaces to be deleted...
Jan 29 13:49:11.182: INFO: 
Logging pods the kubelet thinks is on node metakube-worker-cmccl-6d88bd94fc-87n7l before test
Jan 29 13:49:11.229: INFO: cluster-autoscaler-8c65c7d54-z9ddg from kube-system started at 2020-01-29 10:31:35 +0000 UTC (1 container statuses recorded)
Jan 29 13:49:11.229: INFO: 	Container cluster-autoscaler ready: true, restart count 1
Jan 29 13:49:11.229: INFO: tiller-deploy-78f78bb476-hz2w6 from kube-system started at 2020-01-29 10:31:35 +0000 UTC (1 container statuses recorded)
Jan 29 13:49:11.229: INFO: 	Container tiller ready: true, restart count 0
Jan 29 13:49:11.229: INFO: coredns-6f745f6d74-8q86x from kube-system started at 2020-01-29 10:31:43 +0000 UTC (1 container statuses recorded)
Jan 29 13:49:11.229: INFO: 	Container coredns ready: true, restart count 0
Jan 29 13:49:11.229: INFO: canal-lpkd9 from kube-system started at 2020-01-29 10:31:07 +0000 UTC (2 container statuses recorded)
Jan 29 13:49:11.229: INFO: 	Container calico-node ready: true, restart count 0
Jan 29 13:49:11.229: INFO: 	Container kube-flannel ready: true, restart count 0
Jan 29 13:49:11.229: INFO: node-exporter-4wc5c from kube-system started at 2020-01-29 10:31:07 +0000 UTC (1 container statuses recorded)
Jan 29 13:49:11.229: INFO: 	Container node-exporter ready: true, restart count 0
Jan 29 13:49:11.229: INFO: openvpn-client-64df8b95c9-5fqpn from kube-system started at 2020-01-29 10:31:44 +0000 UTC (2 container statuses recorded)
Jan 29 13:49:11.229: INFO: 	Container dnat-controller ready: true, restart count 0
Jan 29 13:49:11.229: INFO: 	Container openvpn-client ready: true, restart count 1
Jan 29 13:49:11.229: INFO: coredns-6f745f6d74-kr8n2 from kube-system started at 2020-01-29 10:31:44 +0000 UTC (1 container statuses recorded)
Jan 29 13:49:11.229: INFO: 	Container coredns ready: true, restart count 0
Jan 29 13:49:11.229: INFO: sonobuoy-systemd-logs-daemon-set-6c9811082b824a7c-sbw8r from sonobuoy started at 2020-01-29 13:30:03 +0000 UTC (2 container statuses recorded)
Jan 29 13:49:11.229: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 29 13:49:11.229: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 29 13:49:11.229: INFO: kube-proxy-7jg5r from kube-system started at 2020-01-29 10:31:06 +0000 UTC (1 container statuses recorded)
Jan 29 13:49:11.229: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 29 13:49:11.229: INFO: node-local-dns-nhzkw from kube-system started at 2020-01-29 10:31:06 +0000 UTC (1 container statuses recorded)
Jan 29 13:49:11.229: INFO: 	Container node-cache ready: true, restart count 0
Jan 29 13:49:11.229: INFO: 
Logging pods the kubelet thinks is on node metakube-worker-cmccl-6d88bd94fc-lqfxz before test
Jan 29 13:49:11.292: INFO: kube-proxy-rr5gp from kube-system started at 2020-01-29 10:32:02 +0000 UTC (1 container statuses recorded)
Jan 29 13:49:11.293: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 29 13:49:11.294: INFO: node-exporter-99p82 from kube-system started at 2020-01-29 10:32:02 +0000 UTC (1 container statuses recorded)
Jan 29 13:49:11.294: INFO: 	Container node-exporter ready: true, restart count 0
Jan 29 13:49:11.294: INFO: node-local-dns-72k8g from kube-system started at 2020-01-29 10:32:02 +0000 UTC (1 container statuses recorded)
Jan 29 13:49:11.294: INFO: 	Container node-cache ready: true, restart count 0
Jan 29 13:49:11.294: INFO: sonobuoy-systemd-logs-daemon-set-6c9811082b824a7c-9g4cq from sonobuoy started at 2020-01-29 13:30:03 +0000 UTC (2 container statuses recorded)
Jan 29 13:49:11.295: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 29 13:49:11.295: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 29 13:49:11.295: INFO: canal-4zhl8 from kube-system started at 2020-01-29 10:32:02 +0000 UTC (2 container statuses recorded)
Jan 29 13:49:11.295: INFO: 	Container calico-node ready: true, restart count 0
Jan 29 13:49:11.296: INFO: 	Container kube-flannel ready: true, restart count 0
Jan 29 13:49:11.296: INFO: 
Logging pods the kubelet thinks is on node metakube-worker-cmccl-6d88bd94fc-znv5g before test
Jan 29 13:49:11.360: INFO: sonobuoy from sonobuoy started at 2020-01-29 13:30:00 +0000 UTC (1 container statuses recorded)
Jan 29 13:49:11.360: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 29 13:49:11.360: INFO: node-exporter-xblbq from kube-system started at 2020-01-29 10:31:23 +0000 UTC (1 container statuses recorded)
Jan 29 13:49:11.360: INFO: 	Container node-exporter ready: true, restart count 0
Jan 29 13:49:11.360: INFO: sonobuoy-e2e-job-bb288164d824466f from sonobuoy started at 2020-01-29 13:30:03 +0000 UTC (2 container statuses recorded)
Jan 29 13:49:11.360: INFO: 	Container e2e ready: true, restart count 0
Jan 29 13:49:11.360: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 29 13:49:11.360: INFO: node-local-dns-2tvcw from kube-system started at 2020-01-29 10:31:23 +0000 UTC (1 container statuses recorded)
Jan 29 13:49:11.360: INFO: 	Container node-cache ready: true, restart count 0
Jan 29 13:49:11.360: INFO: canal-d5wpn from kube-system started at 2020-01-29 10:31:23 +0000 UTC (2 container statuses recorded)
Jan 29 13:49:11.360: INFO: 	Container calico-node ready: true, restart count 0
Jan 29 13:49:11.360: INFO: 	Container kube-flannel ready: true, restart count 0
Jan 29 13:49:11.360: INFO: sonobuoy-systemd-logs-daemon-set-6c9811082b824a7c-mxxcq from sonobuoy started at 2020-01-29 13:30:03 +0000 UTC (2 container statuses recorded)
Jan 29 13:49:11.360: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 29 13:49:11.360: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 29 13:49:11.360: INFO: kube-proxy-h6b8h from kube-system started at 2020-01-29 10:31:22 +0000 UTC (1 container statuses recorded)
Jan 29 13:49:11.360: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-5fc6fc5d-7f2a-449b-a2c1-abc9f117f920 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-5fc6fc5d-7f2a-449b-a2c1-abc9f117f920 off the node metakube-worker-cmccl-6d88bd94fc-lqfxz
STEP: verifying the node doesn't have the label kubernetes.io/e2e-5fc6fc5d-7f2a-449b-a2c1-abc9f117f920
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:49:20.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-105" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:9.196 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":276,"completed":101,"skipped":1581,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:49:20.057: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9020
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 29 13:49:21.376: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 29 13:49:23.403: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715902561, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715902561, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715902561, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715902561, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 29 13:49:26.482: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:49:39.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9020" for this suite.
STEP: Destroying namespace "webhook-9020-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:19.335 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":276,"completed":102,"skipped":1581,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:49:39.398: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-9031
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-9031, will wait for the garbage collector to delete the pods
Jan 29 13:49:45.791: INFO: Deleting Job.batch foo took: 17.18379ms
Jan 29 13:49:45.891: INFO: Terminating Job.batch foo pods took: 100.351664ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:50:26.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9031" for this suite.

• [SLOW TEST:47.245 seconds]
[sig-apps] Job
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":276,"completed":103,"skipped":1591,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:50:26.643: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9404
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 29 13:50:27.857: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 29 13:50:29.890: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715902627, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715902627, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715902627, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715902627, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 13:50:31.898: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715902627, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715902627, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715902627, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715902627, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 13:50:33.909: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715902627, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715902627, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715902627, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715902627, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 29 13:50:36.929: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Jan 29 13:50:37.104: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:50:37.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9404" for this suite.
STEP: Destroying namespace "webhook-9404-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:10.786 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":276,"completed":104,"skipped":1595,"failed":0}
SSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:50:37.436: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-7391
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 29 13:50:37.671: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-d9bac7c2-2639-4a1d-80af-b5e7bddaf935" in namespace "security-context-test-7391" to be "success or failure"
Jan 29 13:50:37.679: INFO: Pod "busybox-readonly-false-d9bac7c2-2639-4a1d-80af-b5e7bddaf935": Phase="Pending", Reason="", readiness=false. Elapsed: 7.534459ms
Jan 29 13:50:39.690: INFO: Pod "busybox-readonly-false-d9bac7c2-2639-4a1d-80af-b5e7bddaf935": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018351564s
Jan 29 13:50:41.700: INFO: Pod "busybox-readonly-false-d9bac7c2-2639-4a1d-80af-b5e7bddaf935": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028588499s
Jan 29 13:50:43.728: INFO: Pod "busybox-readonly-false-d9bac7c2-2639-4a1d-80af-b5e7bddaf935": Phase="Pending", Reason="", readiness=false. Elapsed: 6.056476559s
Jan 29 13:50:45.744: INFO: Pod "busybox-readonly-false-d9bac7c2-2639-4a1d-80af-b5e7bddaf935": Phase="Pending", Reason="", readiness=false. Elapsed: 8.072843596s
Jan 29 13:50:47.762: INFO: Pod "busybox-readonly-false-d9bac7c2-2639-4a1d-80af-b5e7bddaf935": Phase="Pending", Reason="", readiness=false. Elapsed: 10.090672107s
Jan 29 13:50:49.774: INFO: Pod "busybox-readonly-false-d9bac7c2-2639-4a1d-80af-b5e7bddaf935": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.102498161s
Jan 29 13:50:49.774: INFO: Pod "busybox-readonly-false-d9bac7c2-2639-4a1d-80af-b5e7bddaf935" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:50:49.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7391" for this suite.

• [SLOW TEST:12.836 seconds]
[k8s.io] Security Context
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  When creating a pod with readOnlyRootFilesystem
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:164
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":276,"completed":105,"skipped":1599,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:50:50.275: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9308
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:51:01.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9308" for this suite.

• [SLOW TEST:11.528 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":276,"completed":106,"skipped":1622,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:51:01.804: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-6739
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jan 29 13:51:02.105: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6739 /api/v1/namespaces/watch-6739/configmaps/e2e-watch-test-watch-closed 23c6f627-49a2-4944-b880-64201a62c224 71392 0 2020-01-29 13:51:02 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 29 13:51:02.126: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6739 /api/v1/namespaces/watch-6739/configmaps/e2e-watch-test-watch-closed 23c6f627-49a2-4944-b880-64201a62c224 71393 0 2020-01-29 13:51:02 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jan 29 13:51:02.217: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6739 /api/v1/namespaces/watch-6739/configmaps/e2e-watch-test-watch-closed 23c6f627-49a2-4944-b880-64201a62c224 71395 0 2020-01-29 13:51:02 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 29 13:51:02.218: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6739 /api/v1/namespaces/watch-6739/configmaps/e2e-watch-test-watch-closed 23c6f627-49a2-4944-b880-64201a62c224 71396 0 2020-01-29 13:51:02 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:51:02.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6739" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":276,"completed":107,"skipped":1633,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:51:02.248: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3416
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 29 13:51:02.500: INFO: Waiting up to 5m0s for pod "downwardapi-volume-eab606cd-b622-45dc-a209-5419ed0a03e6" in namespace "downward-api-3416" to be "success or failure"
Jan 29 13:51:02.538: INFO: Pod "downwardapi-volume-eab606cd-b622-45dc-a209-5419ed0a03e6": Phase="Pending", Reason="", readiness=false. Elapsed: 38.080306ms
Jan 29 13:51:04.547: INFO: Pod "downwardapi-volume-eab606cd-b622-45dc-a209-5419ed0a03e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046913729s
Jan 29 13:51:06.558: INFO: Pod "downwardapi-volume-eab606cd-b622-45dc-a209-5419ed0a03e6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058354686s
Jan 29 13:51:08.622: INFO: Pod "downwardapi-volume-eab606cd-b622-45dc-a209-5419ed0a03e6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.12168041s
Jan 29 13:51:10.634: INFO: Pod "downwardapi-volume-eab606cd-b622-45dc-a209-5419ed0a03e6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.13379661s
Jan 29 13:51:12.649: INFO: Pod "downwardapi-volume-eab606cd-b622-45dc-a209-5419ed0a03e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.148725405s
STEP: Saw pod success
Jan 29 13:51:12.649: INFO: Pod "downwardapi-volume-eab606cd-b622-45dc-a209-5419ed0a03e6" satisfied condition "success or failure"
Jan 29 13:51:12.664: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod downwardapi-volume-eab606cd-b622-45dc-a209-5419ed0a03e6 container client-container: <nil>
STEP: delete the pod
Jan 29 13:51:12.776: INFO: Waiting for pod downwardapi-volume-eab606cd-b622-45dc-a209-5419ed0a03e6 to disappear
Jan 29 13:51:12.784: INFO: Pod downwardapi-volume-eab606cd-b622-45dc-a209-5419ed0a03e6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:51:12.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3416" for this suite.

• [SLOW TEST:10.567 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":276,"completed":108,"skipped":1637,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:51:12.821: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6881
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 29 13:51:13.202: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b0177fc0-8054-4ba0-a907-85a60879a00e" in namespace "downward-api-6881" to be "success or failure"
Jan 29 13:51:13.209: INFO: Pod "downwardapi-volume-b0177fc0-8054-4ba0-a907-85a60879a00e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.24993ms
Jan 29 13:51:15.220: INFO: Pod "downwardapi-volume-b0177fc0-8054-4ba0-a907-85a60879a00e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01736062s
Jan 29 13:51:17.228: INFO: Pod "downwardapi-volume-b0177fc0-8054-4ba0-a907-85a60879a00e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02529398s
STEP: Saw pod success
Jan 29 13:51:17.228: INFO: Pod "downwardapi-volume-b0177fc0-8054-4ba0-a907-85a60879a00e" satisfied condition "success or failure"
Jan 29 13:51:17.235: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod downwardapi-volume-b0177fc0-8054-4ba0-a907-85a60879a00e container client-container: <nil>
STEP: delete the pod
Jan 29 13:51:17.333: INFO: Waiting for pod downwardapi-volume-b0177fc0-8054-4ba0-a907-85a60879a00e to disappear
Jan 29 13:51:17.373: INFO: Pod downwardapi-volume-b0177fc0-8054-4ba0-a907-85a60879a00e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:51:17.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6881" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":276,"completed":109,"skipped":1665,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:51:17.401: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6849
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service endpoint-test2 in namespace services-6849
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6849 to expose endpoints map[]
Jan 29 13:51:17.708: INFO: successfully validated that service endpoint-test2 in namespace services-6849 exposes endpoints map[] (25.95424ms elapsed)
STEP: Creating pod pod1 in namespace services-6849
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6849 to expose endpoints map[pod1:[80]]
Jan 29 13:51:21.886: INFO: successfully validated that service endpoint-test2 in namespace services-6849 exposes endpoints map[pod1:[80]] (4.155110097s elapsed)
STEP: Creating pod pod2 in namespace services-6849
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6849 to expose endpoints map[pod1:[80] pod2:[80]]
Jan 29 13:51:25.058: INFO: successfully validated that service endpoint-test2 in namespace services-6849 exposes endpoints map[pod1:[80] pod2:[80]] (3.145428209s elapsed)
STEP: Deleting pod pod1 in namespace services-6849
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6849 to expose endpoints map[pod2:[80]]
Jan 29 13:51:25.113: INFO: successfully validated that service endpoint-test2 in namespace services-6849 exposes endpoints map[pod2:[80]] (31.849765ms elapsed)
STEP: Deleting pod pod2 in namespace services-6849
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6849 to expose endpoints map[]
Jan 29 13:51:25.145: INFO: successfully validated that service endpoint-test2 in namespace services-6849 exposes endpoints map[] (10.46334ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:51:25.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6849" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:7.828 seconds]
[sig-network] Services
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":276,"completed":110,"skipped":1693,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:51:25.229: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-217
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating Agnhost RC
Jan 29 13:51:25.535: INFO: namespace kubectl-217
Jan 29 13:51:25.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 create -f - --namespace=kubectl-217'
Jan 29 13:51:25.900: INFO: stderr: ""
Jan 29 13:51:25.900: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Jan 29 13:51:26.910: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 29 13:51:26.910: INFO: Found 0 / 1
Jan 29 13:51:27.918: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 29 13:51:27.918: INFO: Found 0 / 1
Jan 29 13:51:28.923: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 29 13:51:28.923: INFO: Found 0 / 1
Jan 29 13:51:29.908: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 29 13:51:29.909: INFO: Found 1 / 1
Jan 29 13:51:29.909: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 29 13:51:29.918: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 29 13:51:29.918: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 29 13:51:29.918: INFO: wait on agnhost-master startup in kubectl-217 
Jan 29 13:51:29.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 logs agnhost-master-qqdhz agnhost-master --namespace=kubectl-217'
Jan 29 13:51:30.126: INFO: stderr: ""
Jan 29 13:51:30.126: INFO: stdout: "Paused\n"
STEP: exposing RC
Jan 29 13:51:30.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 expose rc agnhost-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-217'
Jan 29 13:51:30.314: INFO: stderr: ""
Jan 29 13:51:30.314: INFO: stdout: "service/rm2 exposed\n"
Jan 29 13:51:30.323: INFO: Service rm2 in namespace kubectl-217 found.
STEP: exposing service
Jan 29 13:51:32.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-217'
Jan 29 13:51:32.531: INFO: stderr: ""
Jan 29 13:51:32.531: INFO: stdout: "service/rm3 exposed\n"
Jan 29 13:51:32.566: INFO: Service rm3 in namespace kubectl-217 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:51:34.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-217" for this suite.

• [SLOW TEST:9.438 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1295
    should create services for rc  [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":276,"completed":111,"skipped":1700,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:51:34.667: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-9555
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jan 29 13:51:35.955: INFO: Pod name wrapped-volume-race-1e72ab11-6e82-45b8-82f0-302e54473ed2: Found 1 pods out of 5
Jan 29 13:51:40.973: INFO: Pod name wrapped-volume-race-1e72ab11-6e82-45b8-82f0-302e54473ed2: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-1e72ab11-6e82-45b8-82f0-302e54473ed2 in namespace emptydir-wrapper-9555, will wait for the garbage collector to delete the pods
Jan 29 13:51:53.196: INFO: Deleting ReplicationController wrapped-volume-race-1e72ab11-6e82-45b8-82f0-302e54473ed2 took: 21.792207ms
Jan 29 13:51:53.796: INFO: Terminating ReplicationController wrapped-volume-race-1e72ab11-6e82-45b8-82f0-302e54473ed2 pods took: 600.557908ms
STEP: Creating RC which spawns configmap-volume pods
Jan 29 13:52:00.776: INFO: Pod name wrapped-volume-race-9ea25414-1d72-4ebb-ae67-38bee6a148e5: Found 1 pods out of 5
Jan 29 13:52:05.957: INFO: Pod name wrapped-volume-race-9ea25414-1d72-4ebb-ae67-38bee6a148e5: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-9ea25414-1d72-4ebb-ae67-38bee6a148e5 in namespace emptydir-wrapper-9555, will wait for the garbage collector to delete the pods
Jan 29 13:52:20.335: INFO: Deleting ReplicationController wrapped-volume-race-9ea25414-1d72-4ebb-ae67-38bee6a148e5 took: 27.455667ms
Jan 29 13:52:20.535: INFO: Terminating ReplicationController wrapped-volume-race-9ea25414-1d72-4ebb-ae67-38bee6a148e5 pods took: 200.753229ms
STEP: Creating RC which spawns configmap-volume pods
Jan 29 13:52:32.117: INFO: Pod name wrapped-volume-race-8c078cc4-804d-4dc0-9c11-cdaa24b70105: Found 2 pods out of 5
Jan 29 13:52:37.146: INFO: Pod name wrapped-volume-race-8c078cc4-804d-4dc0-9c11-cdaa24b70105: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-8c078cc4-804d-4dc0-9c11-cdaa24b70105 in namespace emptydir-wrapper-9555, will wait for the garbage collector to delete the pods
Jan 29 13:52:51.327: INFO: Deleting ReplicationController wrapped-volume-race-8c078cc4-804d-4dc0-9c11-cdaa24b70105 took: 38.584654ms
Jan 29 13:52:51.927: INFO: Terminating ReplicationController wrapped-volume-race-8c078cc4-804d-4dc0-9c11-cdaa24b70105 pods took: 600.431987ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:53:07.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9555" for this suite.

• [SLOW TEST:93.396 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":276,"completed":112,"skipped":1713,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:53:08.065: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-9987
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-p5h85 in namespace proxy-9987
I0129 13:53:09.460789      22 runners.go:189] Created replication controller with name: proxy-service-p5h85, namespace: proxy-9987, replica count: 1
I0129 13:53:10.512378      22 runners.go:189] proxy-service-p5h85 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0129 13:53:11.512783      22 runners.go:189] proxy-service-p5h85 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0129 13:53:12.513246      22 runners.go:189] proxy-service-p5h85 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0129 13:53:13.513612      22 runners.go:189] proxy-service-p5h85 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0129 13:53:14.514300      22 runners.go:189] proxy-service-p5h85 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0129 13:53:15.514573      22 runners.go:189] proxy-service-p5h85 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0129 13:53:16.514810      22 runners.go:189] proxy-service-p5h85 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0129 13:53:17.515131      22 runners.go:189] proxy-service-p5h85 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0129 13:53:18.515398      22 runners.go:189] proxy-service-p5h85 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0129 13:53:19.515656      22 runners.go:189] proxy-service-p5h85 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 29 13:53:19.533: INFO: setup took 10.814630946s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jan 29 13:53:19.598: INFO: (0) /api/v1/namespaces/proxy-9987/services/proxy-service-p5h85:portname1/proxy/: foo (200; 62.142465ms)
Jan 29 13:53:19.598: INFO: (0) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:160/proxy/: foo (200; 63.518644ms)
Jan 29 13:53:19.599: INFO: (0) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:1080/proxy/rewriteme">test<... (200; 63.826199ms)
Jan 29 13:53:19.602: INFO: (0) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:1080/proxy/rewriteme">... (200; 68.034468ms)
Jan 29 13:53:19.602: INFO: (0) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:162/proxy/: bar (200; 67.201722ms)
Jan 29 13:53:19.603: INFO: (0) /api/v1/namespaces/proxy-9987/services/http:proxy-service-p5h85:portname2/proxy/: bar (200; 66.789697ms)
Jan 29 13:53:19.604: INFO: (0) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:162/proxy/: bar (200; 67.380653ms)
Jan 29 13:53:19.604: INFO: (0) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4/proxy/rewriteme">test</a> (200; 69.032216ms)
Jan 29 13:53:19.604: INFO: (0) /api/v1/namespaces/proxy-9987/services/http:proxy-service-p5h85:portname1/proxy/: foo (200; 68.001379ms)
Jan 29 13:53:19.604: INFO: (0) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:160/proxy/: foo (200; 67.979588ms)
Jan 29 13:53:19.604: INFO: (0) /api/v1/namespaces/proxy-9987/services/proxy-service-p5h85:portname2/proxy/: bar (200; 70.706134ms)
Jan 29 13:53:19.610: INFO: (0) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:460/proxy/: tls baz (200; 76.546496ms)
Jan 29 13:53:19.610: INFO: (0) /api/v1/namespaces/proxy-9987/services/https:proxy-service-p5h85:tlsportname1/proxy/: tls baz (200; 73.899317ms)
Jan 29 13:53:19.610: INFO: (0) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:462/proxy/: tls qux (200; 74.523467ms)
Jan 29 13:53:19.611: INFO: (0) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:443/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:443/proxy/tlsrewritem... (200; 74.484806ms)
Jan 29 13:53:19.611: INFO: (0) /api/v1/namespaces/proxy-9987/services/https:proxy-service-p5h85:tlsportname2/proxy/: tls qux (200; 76.442448ms)
Jan 29 13:53:19.629: INFO: (1) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:1080/proxy/rewriteme">test<... (200; 17.667039ms)
Jan 29 13:53:19.629: INFO: (1) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4/proxy/rewriteme">test</a> (200; 17.369823ms)
Jan 29 13:53:19.634: INFO: (1) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:1080/proxy/rewriteme">... (200; 22.454959ms)
Jan 29 13:53:19.634: INFO: (1) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:462/proxy/: tls qux (200; 21.522579ms)
Jan 29 13:53:19.635: INFO: (1) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:443/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:443/proxy/tlsrewritem... (200; 21.801329ms)
Jan 29 13:53:19.645: INFO: (1) /api/v1/namespaces/proxy-9987/services/http:proxy-service-p5h85:portname1/proxy/: foo (200; 31.966899ms)
Jan 29 13:53:19.645: INFO: (1) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:162/proxy/: bar (200; 33.483904ms)
Jan 29 13:53:19.646: INFO: (1) /api/v1/namespaces/proxy-9987/services/https:proxy-service-p5h85:tlsportname1/proxy/: tls baz (200; 33.411814ms)
Jan 29 13:53:19.646: INFO: (1) /api/v1/namespaces/proxy-9987/services/proxy-service-p5h85:portname2/proxy/: bar (200; 32.956981ms)
Jan 29 13:53:19.646: INFO: (1) /api/v1/namespaces/proxy-9987/services/https:proxy-service-p5h85:tlsportname2/proxy/: tls qux (200; 33.772019ms)
Jan 29 13:53:19.646: INFO: (1) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:460/proxy/: tls baz (200; 33.593667ms)
Jan 29 13:53:19.646: INFO: (1) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:160/proxy/: foo (200; 33.741641ms)
Jan 29 13:53:19.657: INFO: (1) /api/v1/namespaces/proxy-9987/services/http:proxy-service-p5h85:portname2/proxy/: bar (200; 43.843659ms)
Jan 29 13:53:19.657: INFO: (1) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:162/proxy/: bar (200; 44.277394ms)
Jan 29 13:53:19.657: INFO: (1) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:160/proxy/: foo (200; 45.647409ms)
Jan 29 13:53:19.669: INFO: (1) /api/v1/namespaces/proxy-9987/services/proxy-service-p5h85:portname1/proxy/: foo (200; 56.374733ms)
Jan 29 13:53:19.702: INFO: (2) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:160/proxy/: foo (200; 32.37096ms)
Jan 29 13:53:19.703: INFO: (2) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4/proxy/rewriteme">test</a> (200; 32.359156ms)
Jan 29 13:53:19.704: INFO: (2) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:1080/proxy/rewriteme">... (200; 34.186551ms)
Jan 29 13:53:19.704: INFO: (2) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:1080/proxy/rewriteme">test<... (200; 33.637134ms)
Jan 29 13:53:19.704: INFO: (2) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:162/proxy/: bar (200; 33.889796ms)
Jan 29 13:53:19.705: INFO: (2) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:160/proxy/: foo (200; 34.792649ms)
Jan 29 13:53:19.717: INFO: (2) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:443/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:443/proxy/tlsrewritem... (200; 46.656624ms)
Jan 29 13:53:19.717: INFO: (2) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:462/proxy/: tls qux (200; 46.783563ms)
Jan 29 13:53:19.717: INFO: (2) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:162/proxy/: bar (200; 47.131573ms)
Jan 29 13:53:19.717: INFO: (2) /api/v1/namespaces/proxy-9987/services/https:proxy-service-p5h85:tlsportname2/proxy/: tls qux (200; 46.90387ms)
Jan 29 13:53:19.717: INFO: (2) /api/v1/namespaces/proxy-9987/services/http:proxy-service-p5h85:portname2/proxy/: bar (200; 47.953377ms)
Jan 29 13:53:19.718: INFO: (2) /api/v1/namespaces/proxy-9987/services/http:proxy-service-p5h85:portname1/proxy/: foo (200; 48.15676ms)
Jan 29 13:53:19.718: INFO: (2) /api/v1/namespaces/proxy-9987/services/https:proxy-service-p5h85:tlsportname1/proxy/: tls baz (200; 48.12578ms)
Jan 29 13:53:19.718: INFO: (2) /api/v1/namespaces/proxy-9987/services/proxy-service-p5h85:portname1/proxy/: foo (200; 48.07038ms)
Jan 29 13:53:19.718: INFO: (2) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:460/proxy/: tls baz (200; 47.84093ms)
Jan 29 13:53:19.725: INFO: (2) /api/v1/namespaces/proxy-9987/services/proxy-service-p5h85:portname2/proxy/: bar (200; 55.338639ms)
Jan 29 13:53:19.761: INFO: (3) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:1080/proxy/rewriteme">test<... (200; 34.29901ms)
Jan 29 13:53:19.761: INFO: (3) /api/v1/namespaces/proxy-9987/services/http:proxy-service-p5h85:portname1/proxy/: foo (200; 34.766605ms)
Jan 29 13:53:19.765: INFO: (3) /api/v1/namespaces/proxy-9987/services/https:proxy-service-p5h85:tlsportname1/proxy/: tls baz (200; 38.784072ms)
Jan 29 13:53:19.765: INFO: (3) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:460/proxy/: tls baz (200; 39.009572ms)
Jan 29 13:53:19.765: INFO: (3) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:160/proxy/: foo (200; 38.791961ms)
Jan 29 13:53:19.765: INFO: (3) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:443/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:443/proxy/tlsrewritem... (200; 38.547502ms)
Jan 29 13:53:19.765: INFO: (3) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:1080/proxy/rewriteme">... (200; 38.851011ms)
Jan 29 13:53:19.765: INFO: (3) /api/v1/namespaces/proxy-9987/services/proxy-service-p5h85:portname2/proxy/: bar (200; 39.101902ms)
Jan 29 13:53:19.765: INFO: (3) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:462/proxy/: tls qux (200; 38.645051ms)
Jan 29 13:53:19.765: INFO: (3) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4/proxy/rewriteme">test</a> (200; 39.068946ms)
Jan 29 13:53:19.765: INFO: (3) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:162/proxy/: bar (200; 38.878954ms)
Jan 29 13:53:19.800: INFO: (3) /api/v1/namespaces/proxy-9987/services/https:proxy-service-p5h85:tlsportname2/proxy/: tls qux (200; 73.30343ms)
Jan 29 13:53:19.800: INFO: (3) /api/v1/namespaces/proxy-9987/services/proxy-service-p5h85:portname1/proxy/: foo (200; 74.83662ms)
Jan 29 13:53:19.801: INFO: (3) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:160/proxy/: foo (200; 74.314202ms)
Jan 29 13:53:19.801: INFO: (3) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:162/proxy/: bar (200; 74.373587ms)
Jan 29 13:53:19.801: INFO: (3) /api/v1/namespaces/proxy-9987/services/http:proxy-service-p5h85:portname2/proxy/: bar (200; 74.277558ms)
Jan 29 13:53:19.824: INFO: (4) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:160/proxy/: foo (200; 23.208191ms)
Jan 29 13:53:19.832: INFO: (4) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:1080/proxy/rewriteme">test<... (200; 29.905522ms)
Jan 29 13:53:19.833: INFO: (4) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:443/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:443/proxy/tlsrewritem... (200; 30.966012ms)
Jan 29 13:53:19.834: INFO: (4) /api/v1/namespaces/proxy-9987/services/http:proxy-service-p5h85:portname2/proxy/: bar (200; 32.192521ms)
Jan 29 13:53:19.834: INFO: (4) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:162/proxy/: bar (200; 31.887665ms)
Jan 29 13:53:19.834: INFO: (4) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:462/proxy/: tls qux (200; 32.0916ms)
Jan 29 13:53:19.834: INFO: (4) /api/v1/namespaces/proxy-9987/services/https:proxy-service-p5h85:tlsportname2/proxy/: tls qux (200; 32.540493ms)
Jan 29 13:53:19.835: INFO: (4) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4/proxy/rewriteme">test</a> (200; 33.247173ms)
Jan 29 13:53:19.835: INFO: (4) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:160/proxy/: foo (200; 32.667184ms)
Jan 29 13:53:19.835: INFO: (4) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:1080/proxy/rewriteme">... (200; 33.787425ms)
Jan 29 13:53:19.835: INFO: (4) /api/v1/namespaces/proxy-9987/services/https:proxy-service-p5h85:tlsportname1/proxy/: tls baz (200; 32.943288ms)
Jan 29 13:53:19.835: INFO: (4) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:460/proxy/: tls baz (200; 32.711341ms)
Jan 29 13:53:19.843: INFO: (4) /api/v1/namespaces/proxy-9987/services/http:proxy-service-p5h85:portname1/proxy/: foo (200; 40.780415ms)
Jan 29 13:53:19.843: INFO: (4) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:162/proxy/: bar (200; 40.945128ms)
Jan 29 13:53:19.845: INFO: (4) /api/v1/namespaces/proxy-9987/services/proxy-service-p5h85:portname2/proxy/: bar (200; 43.340538ms)
Jan 29 13:53:19.845: INFO: (4) /api/v1/namespaces/proxy-9987/services/proxy-service-p5h85:portname1/proxy/: foo (200; 42.589544ms)
Jan 29 13:53:19.880: INFO: (5) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:162/proxy/: bar (200; 34.091343ms)
Jan 29 13:53:19.880: INFO: (5) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:1080/proxy/rewriteme">test<... (200; 33.893656ms)
Jan 29 13:53:19.881: INFO: (5) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:462/proxy/: tls qux (200; 34.38925ms)
Jan 29 13:53:19.881: INFO: (5) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4/proxy/rewriteme">test</a> (200; 34.945566ms)
Jan 29 13:53:19.881: INFO: (5) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:443/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:443/proxy/tlsrewritem... (200; 34.537265ms)
Jan 29 13:53:19.881: INFO: (5) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:160/proxy/: foo (200; 35.477577ms)
Jan 29 13:53:19.881: INFO: (5) /api/v1/namespaces/proxy-9987/services/https:proxy-service-p5h85:tlsportname2/proxy/: tls qux (200; 35.47769ms)
Jan 29 13:53:19.881: INFO: (5) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:160/proxy/: foo (200; 35.413873ms)
Jan 29 13:53:19.881: INFO: (5) /api/v1/namespaces/proxy-9987/services/https:proxy-service-p5h85:tlsportname1/proxy/: tls baz (200; 36.354787ms)
Jan 29 13:53:19.881: INFO: (5) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:460/proxy/: tls baz (200; 35.360996ms)
Jan 29 13:53:19.882: INFO: (5) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:1080/proxy/rewriteme">... (200; 36.289924ms)
Jan 29 13:53:19.882: INFO: (5) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:162/proxy/: bar (200; 35.859474ms)
Jan 29 13:53:19.924: INFO: (5) /api/v1/namespaces/proxy-9987/services/proxy-service-p5h85:portname1/proxy/: foo (200; 77.812119ms)
Jan 29 13:53:19.924: INFO: (5) /api/v1/namespaces/proxy-9987/services/http:proxy-service-p5h85:portname1/proxy/: foo (200; 77.781317ms)
Jan 29 13:53:19.924: INFO: (5) /api/v1/namespaces/proxy-9987/services/proxy-service-p5h85:portname2/proxy/: bar (200; 77.989561ms)
Jan 29 13:53:19.924: INFO: (5) /api/v1/namespaces/proxy-9987/services/http:proxy-service-p5h85:portname2/proxy/: bar (200; 78.142754ms)
Jan 29 13:53:19.949: INFO: (6) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:162/proxy/: bar (200; 23.9284ms)
Jan 29 13:53:19.949: INFO: (6) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:162/proxy/: bar (200; 23.366164ms)
Jan 29 13:53:19.951: INFO: (6) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:462/proxy/: tls qux (200; 24.984409ms)
Jan 29 13:53:19.951: INFO: (6) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:460/proxy/: tls baz (200; 25.853104ms)
Jan 29 13:53:19.951: INFO: (6) /api/v1/namespaces/proxy-9987/services/https:proxy-service-p5h85:tlsportname2/proxy/: tls qux (200; 25.680111ms)
Jan 29 13:53:19.951: INFO: (6) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:160/proxy/: foo (200; 25.647063ms)
Jan 29 13:53:19.951: INFO: (6) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:443/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:443/proxy/tlsrewritem... (200; 26.535466ms)
Jan 29 13:53:19.951: INFO: (6) /api/v1/namespaces/proxy-9987/services/https:proxy-service-p5h85:tlsportname1/proxy/: tls baz (200; 25.54915ms)
Jan 29 13:53:19.952: INFO: (6) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:160/proxy/: foo (200; 26.299052ms)
Jan 29 13:53:19.952: INFO: (6) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:1080/proxy/rewriteme">... (200; 26.566526ms)
Jan 29 13:53:19.952: INFO: (6) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4/proxy/rewriteme">test</a> (200; 26.692145ms)
Jan 29 13:53:19.952: INFO: (6) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:1080/proxy/rewriteme">test<... (200; 26.852815ms)
Jan 29 13:53:19.958: INFO: (6) /api/v1/namespaces/proxy-9987/services/proxy-service-p5h85:portname2/proxy/: bar (200; 32.447397ms)
Jan 29 13:53:19.958: INFO: (6) /api/v1/namespaces/proxy-9987/services/http:proxy-service-p5h85:portname2/proxy/: bar (200; 31.884741ms)
Jan 29 13:53:19.961: INFO: (6) /api/v1/namespaces/proxy-9987/services/http:proxy-service-p5h85:portname1/proxy/: foo (200; 35.449948ms)
Jan 29 13:53:20.004: INFO: (6) /api/v1/namespaces/proxy-9987/services/proxy-service-p5h85:portname1/proxy/: foo (200; 78.358961ms)
Jan 29 13:53:20.072: INFO: (7) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:162/proxy/: bar (200; 66.424744ms)
Jan 29 13:53:20.072: INFO: (7) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:460/proxy/: tls baz (200; 67.132986ms)
Jan 29 13:53:20.072: INFO: (7) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:1080/proxy/rewriteme">test<... (200; 67.289916ms)
Jan 29 13:53:20.073: INFO: (7) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:443/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:443/proxy/tlsrewritem... (200; 68.140541ms)
Jan 29 13:53:20.073: INFO: (7) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:160/proxy/: foo (200; 67.814858ms)
Jan 29 13:53:20.073: INFO: (7) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4/proxy/rewriteme">test</a> (200; 67.477652ms)
Jan 29 13:53:20.073: INFO: (7) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:160/proxy/: foo (200; 67.999674ms)
Jan 29 13:53:20.073: INFO: (7) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:1080/proxy/rewriteme">... (200; 68.206571ms)
Jan 29 13:53:20.073: INFO: (7) /api/v1/namespaces/proxy-9987/services/proxy-service-p5h85:portname2/proxy/: bar (200; 68.30395ms)
Jan 29 13:53:20.074: INFO: (7) /api/v1/namespaces/proxy-9987/services/https:proxy-service-p5h85:tlsportname1/proxy/: tls baz (200; 68.271257ms)
Jan 29 13:53:20.089: INFO: (7) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:462/proxy/: tls qux (200; 83.504119ms)
Jan 29 13:53:20.094: INFO: (7) /api/v1/namespaces/proxy-9987/services/http:proxy-service-p5h85:portname2/proxy/: bar (200; 88.461673ms)
Jan 29 13:53:20.094: INFO: (7) /api/v1/namespaces/proxy-9987/services/http:proxy-service-p5h85:portname1/proxy/: foo (200; 88.73025ms)
Jan 29 13:53:20.094: INFO: (7) /api/v1/namespaces/proxy-9987/services/proxy-service-p5h85:portname1/proxy/: foo (200; 88.803185ms)
Jan 29 13:53:20.095: INFO: (7) /api/v1/namespaces/proxy-9987/services/https:proxy-service-p5h85:tlsportname2/proxy/: tls qux (200; 89.147843ms)
Jan 29 13:53:20.095: INFO: (7) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:162/proxy/: bar (200; 89.335697ms)
Jan 29 13:53:20.118: INFO: (8) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:1080/proxy/rewriteme">... (200; 23.060503ms)
Jan 29 13:53:20.131: INFO: (8) /api/v1/namespaces/proxy-9987/services/http:proxy-service-p5h85:portname2/proxy/: bar (200; 34.563445ms)
Jan 29 13:53:20.131: INFO: (8) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:160/proxy/: foo (200; 34.727277ms)
Jan 29 13:53:20.131: INFO: (8) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:1080/proxy/rewriteme">test<... (200; 35.503089ms)
Jan 29 13:53:20.151: INFO: (8) /api/v1/namespaces/proxy-9987/services/https:proxy-service-p5h85:tlsportname2/proxy/: tls qux (200; 55.105679ms)
Jan 29 13:53:20.151: INFO: (8) /api/v1/namespaces/proxy-9987/services/http:proxy-service-p5h85:portname1/proxy/: foo (200; 55.525842ms)
Jan 29 13:53:20.152: INFO: (8) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:462/proxy/: tls qux (200; 56.141735ms)
Jan 29 13:53:20.152: INFO: (8) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:443/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:443/proxy/tlsrewritem... (200; 55.439516ms)
Jan 29 13:53:20.152: INFO: (8) /api/v1/namespaces/proxy-9987/services/proxy-service-p5h85:portname2/proxy/: bar (200; 55.693163ms)
Jan 29 13:53:20.152: INFO: (8) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:460/proxy/: tls baz (200; 56.087437ms)
Jan 29 13:53:20.152: INFO: (8) /api/v1/namespaces/proxy-9987/services/https:proxy-service-p5h85:tlsportname1/proxy/: tls baz (200; 55.807402ms)
Jan 29 13:53:20.152: INFO: (8) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4/proxy/rewriteme">test</a> (200; 55.934249ms)
Jan 29 13:53:20.152: INFO: (8) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:160/proxy/: foo (200; 56.201658ms)
Jan 29 13:53:20.152: INFO: (8) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:162/proxy/: bar (200; 56.226266ms)
Jan 29 13:53:20.158: INFO: (8) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:162/proxy/: bar (200; 62.818721ms)
Jan 29 13:53:20.203: INFO: (8) /api/v1/namespaces/proxy-9987/services/proxy-service-p5h85:portname1/proxy/: foo (200; 107.452225ms)
Jan 29 13:53:20.227: INFO: (9) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:162/proxy/: bar (200; 22.836471ms)
Jan 29 13:53:20.227: INFO: (9) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4/proxy/rewriteme">test</a> (200; 23.137453ms)
Jan 29 13:53:20.227: INFO: (9) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:162/proxy/: bar (200; 22.259296ms)
Jan 29 13:53:20.227: INFO: (9) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:1080/proxy/rewriteme">... (200; 24.006812ms)
Jan 29 13:53:20.233: INFO: (9) /api/v1/namespaces/proxy-9987/services/https:proxy-service-p5h85:tlsportname2/proxy/: tls qux (200; 29.573449ms)
Jan 29 13:53:20.234: INFO: (9) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:462/proxy/: tls qux (200; 29.438422ms)
Jan 29 13:53:20.235: INFO: (9) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:460/proxy/: tls baz (200; 29.497134ms)
Jan 29 13:53:20.237: INFO: (9) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:160/proxy/: foo (200; 32.748267ms)
Jan 29 13:53:20.237: INFO: (9) /api/v1/namespaces/proxy-9987/services/proxy-service-p5h85:portname1/proxy/: foo (200; 33.189391ms)
Jan 29 13:53:20.248: INFO: (9) /api/v1/namespaces/proxy-9987/services/proxy-service-p5h85:portname2/proxy/: bar (200; 42.246073ms)
Jan 29 13:53:20.248: INFO: (9) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:160/proxy/: foo (200; 44.278397ms)
Jan 29 13:53:20.249: INFO: (9) /api/v1/namespaces/proxy-9987/services/http:proxy-service-p5h85:portname1/proxy/: foo (200; 44.261864ms)
Jan 29 13:53:20.249: INFO: (9) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:443/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:443/proxy/tlsrewritem... (200; 43.526919ms)
Jan 29 13:53:20.253: INFO: (9) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:1080/proxy/rewriteme">test<... (200; 49.116737ms)
Jan 29 13:53:20.260: INFO: (9) /api/v1/namespaces/proxy-9987/services/http:proxy-service-p5h85:portname2/proxy/: bar (200; 55.294597ms)
Jan 29 13:53:20.261: INFO: (9) /api/v1/namespaces/proxy-9987/services/https:proxy-service-p5h85:tlsportname1/proxy/: tls baz (200; 57.820144ms)
Jan 29 13:53:20.284: INFO: (10) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4/proxy/rewriteme">test</a> (200; 21.726296ms)
Jan 29 13:53:20.284: INFO: (10) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:160/proxy/: foo (200; 21.667271ms)
Jan 29 13:53:20.286: INFO: (10) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:1080/proxy/rewriteme">... (200; 24.67723ms)
Jan 29 13:53:20.286: INFO: (10) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:162/proxy/: bar (200; 24.15378ms)
Jan 29 13:53:20.286: INFO: (10) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:162/proxy/: bar (200; 23.554247ms)
Jan 29 13:53:20.288: INFO: (10) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:443/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:443/proxy/tlsrewritem... (200; 25.129714ms)
Jan 29 13:53:20.296: INFO: (10) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:160/proxy/: foo (200; 33.632096ms)
Jan 29 13:53:20.296: INFO: (10) /api/v1/namespaces/proxy-9987/services/https:proxy-service-p5h85:tlsportname2/proxy/: tls qux (200; 34.363259ms)
Jan 29 13:53:20.300: INFO: (10) /api/v1/namespaces/proxy-9987/services/http:proxy-service-p5h85:portname2/proxy/: bar (200; 37.543251ms)
Jan 29 13:53:20.301: INFO: (10) /api/v1/namespaces/proxy-9987/services/proxy-service-p5h85:portname2/proxy/: bar (200; 37.760954ms)
Jan 29 13:53:20.306: INFO: (10) /api/v1/namespaces/proxy-9987/services/proxy-service-p5h85:portname1/proxy/: foo (200; 43.817562ms)
Jan 29 13:53:20.331: INFO: (10) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:1080/proxy/rewriteme">test<... (200; 68.772757ms)
Jan 29 13:53:20.331: INFO: (10) /api/v1/namespaces/proxy-9987/services/https:proxy-service-p5h85:tlsportname1/proxy/: tls baz (200; 68.370477ms)
Jan 29 13:53:20.331: INFO: (10) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:462/proxy/: tls qux (200; 68.571504ms)
Jan 29 13:53:20.331: INFO: (10) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:460/proxy/: tls baz (200; 68.409253ms)
Jan 29 13:53:20.331: INFO: (10) /api/v1/namespaces/proxy-9987/services/http:proxy-service-p5h85:portname1/proxy/: foo (200; 69.851539ms)
Jan 29 13:53:20.358: INFO: (11) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:160/proxy/: foo (200; 25.044642ms)
Jan 29 13:53:20.358: INFO: (11) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:462/proxy/: tls qux (200; 25.195682ms)
Jan 29 13:53:20.358: INFO: (11) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:160/proxy/: foo (200; 25.385507ms)
Jan 29 13:53:20.359: INFO: (11) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4/proxy/rewriteme">test</a> (200; 26.336635ms)
Jan 29 13:53:20.359: INFO: (11) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:460/proxy/: tls baz (200; 25.86868ms)
Jan 29 13:53:20.359: INFO: (11) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:1080/proxy/rewriteme">test<... (200; 26.558587ms)
Jan 29 13:53:20.359: INFO: (11) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:162/proxy/: bar (200; 26.13521ms)
Jan 29 13:53:20.359: INFO: (11) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:443/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:443/proxy/tlsrewritem... (200; 26.400428ms)
Jan 29 13:53:20.359: INFO: (11) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:162/proxy/: bar (200; 26.678521ms)
Jan 29 13:53:20.360: INFO: (11) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:1080/proxy/rewriteme">... (200; 27.814328ms)
Jan 29 13:53:20.365: INFO: (11) /api/v1/namespaces/proxy-9987/services/https:proxy-service-p5h85:tlsportname2/proxy/: tls qux (200; 32.967234ms)
Jan 29 13:53:20.366: INFO: (11) /api/v1/namespaces/proxy-9987/services/https:proxy-service-p5h85:tlsportname1/proxy/: tls baz (200; 33.88057ms)
Jan 29 13:53:20.366: INFO: (11) /api/v1/namespaces/proxy-9987/services/http:proxy-service-p5h85:portname1/proxy/: foo (200; 32.624836ms)
Jan 29 13:53:20.366: INFO: (11) /api/v1/namespaces/proxy-9987/services/proxy-service-p5h85:portname2/proxy/: bar (200; 33.087339ms)
Jan 29 13:53:20.469: INFO: (11) /api/v1/namespaces/proxy-9987/services/proxy-service-p5h85:portname1/proxy/: foo (200; 135.805571ms)
Jan 29 13:53:20.469: INFO: (11) /api/v1/namespaces/proxy-9987/services/http:proxy-service-p5h85:portname2/proxy/: bar (200; 136.101675ms)
Jan 29 13:53:20.488: INFO: (12) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:460/proxy/: tls baz (200; 18.380249ms)
Jan 29 13:53:20.488: INFO: (12) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:162/proxy/: bar (200; 17.646724ms)
Jan 29 13:53:20.488: INFO: (12) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4/proxy/rewriteme">test</a> (200; 19.427149ms)
Jan 29 13:53:20.491: INFO: (12) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:1080/proxy/rewriteme">test<... (200; 20.608577ms)
Jan 29 13:53:20.491: INFO: (12) /api/v1/namespaces/proxy-9987/services/http:proxy-service-p5h85:portname1/proxy/: foo (200; 19.5262ms)
Jan 29 13:53:20.492: INFO: (12) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:462/proxy/: tls qux (200; 21.431039ms)
Jan 29 13:53:20.496: INFO: (12) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:160/proxy/: foo (200; 25.199109ms)
Jan 29 13:53:20.496: INFO: (12) /api/v1/namespaces/proxy-9987/services/proxy-service-p5h85:portname2/proxy/: bar (200; 26.311716ms)
Jan 29 13:53:20.496: INFO: (12) /api/v1/namespaces/proxy-9987/services/https:proxy-service-p5h85:tlsportname2/proxy/: tls qux (200; 25.603273ms)
Jan 29 13:53:20.496: INFO: (12) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:160/proxy/: foo (200; 25.422191ms)
Jan 29 13:53:20.496: INFO: (12) /api/v1/namespaces/proxy-9987/services/http:proxy-service-p5h85:portname2/proxy/: bar (200; 26.80717ms)
Jan 29 13:53:20.496: INFO: (12) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:1080/proxy/rewriteme">... (200; 26.262785ms)
Jan 29 13:53:20.497: INFO: (12) /api/v1/namespaces/proxy-9987/services/https:proxy-service-p5h85:tlsportname1/proxy/: tls baz (200; 27.167363ms)
Jan 29 13:53:20.497: INFO: (12) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:443/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:443/proxy/tlsrewritem... (200; 27.226199ms)
Jan 29 13:53:20.532: INFO: (12) /api/v1/namespaces/proxy-9987/services/proxy-service-p5h85:portname1/proxy/: foo (200; 60.734065ms)
Jan 29 13:53:20.532: INFO: (12) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:162/proxy/: bar (200; 60.924033ms)
Jan 29 13:53:20.561: INFO: (13) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:160/proxy/: foo (200; 27.557731ms)
Jan 29 13:53:20.561: INFO: (13) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4/proxy/rewriteme">test</a> (200; 28.68948ms)
Jan 29 13:53:20.561: INFO: (13) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:162/proxy/: bar (200; 28.448405ms)
Jan 29 13:53:20.561: INFO: (13) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:160/proxy/: foo (200; 28.50924ms)
Jan 29 13:53:20.561: INFO: (13) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:443/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:443/proxy/tlsrewritem... (200; 28.23035ms)
Jan 29 13:53:20.562: INFO: (13) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:1080/proxy/rewriteme">... (200; 29.481251ms)
Jan 29 13:53:20.562: INFO: (13) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:162/proxy/: bar (200; 28.576564ms)
Jan 29 13:53:20.562: INFO: (13) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:1080/proxy/rewriteme">test<... (200; 29.008707ms)
Jan 29 13:53:20.564: INFO: (13) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:462/proxy/: tls qux (200; 31.340248ms)
Jan 29 13:53:20.564: INFO: (13) /api/v1/namespaces/proxy-9987/services/https:proxy-service-p5h85:tlsportname2/proxy/: tls qux (200; 32.171805ms)
Jan 29 13:53:20.569: INFO: (13) /api/v1/namespaces/proxy-9987/services/https:proxy-service-p5h85:tlsportname1/proxy/: tls baz (200; 36.373285ms)
Jan 29 13:53:20.569: INFO: (13) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:460/proxy/: tls baz (200; 36.034563ms)
Jan 29 13:53:20.582: INFO: (13) /api/v1/namespaces/proxy-9987/services/proxy-service-p5h85:portname1/proxy/: foo (200; 49.519883ms)
Jan 29 13:53:20.583: INFO: (13) /api/v1/namespaces/proxy-9987/services/proxy-service-p5h85:portname2/proxy/: bar (200; 49.586136ms)
Jan 29 13:53:20.583: INFO: (13) /api/v1/namespaces/proxy-9987/services/http:proxy-service-p5h85:portname2/proxy/: bar (200; 49.843462ms)
Jan 29 13:53:20.583: INFO: (13) /api/v1/namespaces/proxy-9987/services/http:proxy-service-p5h85:portname1/proxy/: foo (200; 49.971634ms)
Jan 29 13:53:20.601: INFO: (14) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4/proxy/rewriteme">test</a> (200; 17.511689ms)
Jan 29 13:53:20.626: INFO: (14) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:462/proxy/: tls qux (200; 41.747146ms)
Jan 29 13:53:20.626: INFO: (14) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:460/proxy/: tls baz (200; 41.778467ms)
Jan 29 13:53:20.626: INFO: (14) /api/v1/namespaces/proxy-9987/services/https:proxy-service-p5h85:tlsportname1/proxy/: tls baz (200; 42.506582ms)
Jan 29 13:53:20.626: INFO: (14) /api/v1/namespaces/proxy-9987/services/http:proxy-service-p5h85:portname1/proxy/: foo (200; 41.897264ms)
Jan 29 13:53:20.627: INFO: (14) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:160/proxy/: foo (200; 42.543902ms)
Jan 29 13:53:20.627: INFO: (14) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:1080/proxy/rewriteme">... (200; 42.983087ms)
Jan 29 13:53:20.627: INFO: (14) /api/v1/namespaces/proxy-9987/services/proxy-service-p5h85:portname2/proxy/: bar (200; 42.757306ms)
Jan 29 13:53:20.627: INFO: (14) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:162/proxy/: bar (200; 42.983931ms)
Jan 29 13:53:20.627: INFO: (14) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:443/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:443/proxy/tlsrewritem... (200; 42.942117ms)
Jan 29 13:53:20.634: INFO: (14) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:160/proxy/: foo (200; 49.51477ms)
Jan 29 13:53:20.635: INFO: (14) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:1080/proxy/rewriteme">test<... (200; 50.332974ms)
Jan 29 13:53:20.635: INFO: (14) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:162/proxy/: bar (200; 50.401814ms)
Jan 29 13:53:20.638: INFO: (14) /api/v1/namespaces/proxy-9987/services/proxy-service-p5h85:portname1/proxy/: foo (200; 53.821559ms)
Jan 29 13:53:20.641: INFO: (14) /api/v1/namespaces/proxy-9987/services/http:proxy-service-p5h85:portname2/proxy/: bar (200; 56.899767ms)
Jan 29 13:53:20.644: INFO: (14) /api/v1/namespaces/proxy-9987/services/https:proxy-service-p5h85:tlsportname2/proxy/: tls qux (200; 60.475432ms)
Jan 29 13:53:20.695: INFO: (15) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:162/proxy/: bar (200; 49.584904ms)
Jan 29 13:53:20.695: INFO: (15) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:1080/proxy/rewriteme">test<... (200; 49.540219ms)
Jan 29 13:53:20.695: INFO: (15) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:443/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:443/proxy/tlsrewritem... (200; 49.93512ms)
Jan 29 13:53:20.695: INFO: (15) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:160/proxy/: foo (200; 50.865874ms)
Jan 29 13:53:20.696: INFO: (15) /api/v1/namespaces/proxy-9987/services/https:proxy-service-p5h85:tlsportname1/proxy/: tls baz (200; 50.426107ms)
Jan 29 13:53:20.696: INFO: (15) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:160/proxy/: foo (200; 50.637709ms)
Jan 29 13:53:20.696: INFO: (15) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4/proxy/rewriteme">test</a> (200; 51.20738ms)
Jan 29 13:53:20.696: INFO: (15) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:460/proxy/: tls baz (200; 51.009396ms)
Jan 29 13:53:20.696: INFO: (15) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:1080/proxy/rewriteme">... (200; 50.939395ms)
Jan 29 13:53:20.696: INFO: (15) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:162/proxy/: bar (200; 50.697853ms)
Jan 29 13:53:20.696: INFO: (15) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:462/proxy/: tls qux (200; 50.654778ms)
Jan 29 13:53:20.696: INFO: (15) /api/v1/namespaces/proxy-9987/services/https:proxy-service-p5h85:tlsportname2/proxy/: tls qux (200; 51.091209ms)
Jan 29 13:53:20.710: INFO: (15) /api/v1/namespaces/proxy-9987/services/http:proxy-service-p5h85:portname2/proxy/: bar (200; 65.166327ms)
Jan 29 13:53:20.710: INFO: (15) /api/v1/namespaces/proxy-9987/services/proxy-service-p5h85:portname2/proxy/: bar (200; 65.065477ms)
Jan 29 13:53:20.711: INFO: (15) /api/v1/namespaces/proxy-9987/services/http:proxy-service-p5h85:portname1/proxy/: foo (200; 65.492045ms)
Jan 29 13:53:20.711: INFO: (15) /api/v1/namespaces/proxy-9987/services/proxy-service-p5h85:portname1/proxy/: foo (200; 65.882249ms)
Jan 29 13:53:20.741: INFO: (16) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:1080/proxy/rewriteme">... (200; 28.752583ms)
Jan 29 13:53:20.741: INFO: (16) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:1080/proxy/rewriteme">test<... (200; 28.387532ms)
Jan 29 13:53:20.747: INFO: (16) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:160/proxy/: foo (200; 34.602715ms)
Jan 29 13:53:20.747: INFO: (16) /api/v1/namespaces/proxy-9987/services/https:proxy-service-p5h85:tlsportname2/proxy/: tls qux (200; 34.875317ms)
Jan 29 13:53:20.747: INFO: (16) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:162/proxy/: bar (200; 34.404317ms)
Jan 29 13:53:20.747: INFO: (16) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:162/proxy/: bar (200; 34.828161ms)
Jan 29 13:53:20.747: INFO: (16) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:160/proxy/: foo (200; 34.730055ms)
Jan 29 13:53:20.748: INFO: (16) /api/v1/namespaces/proxy-9987/services/https:proxy-service-p5h85:tlsportname1/proxy/: tls baz (200; 35.995298ms)
Jan 29 13:53:20.748: INFO: (16) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:460/proxy/: tls baz (200; 36.499007ms)
Jan 29 13:53:20.748: INFO: (16) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:443/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:443/proxy/tlsrewritem... (200; 35.907677ms)
Jan 29 13:53:20.748: INFO: (16) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:462/proxy/: tls qux (200; 36.011466ms)
Jan 29 13:53:20.749: INFO: (16) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4/proxy/rewriteme">test</a> (200; 36.377514ms)
Jan 29 13:53:20.753: INFO: (16) /api/v1/namespaces/proxy-9987/services/http:proxy-service-p5h85:portname1/proxy/: foo (200; 41.340286ms)
Jan 29 13:53:20.753: INFO: (16) /api/v1/namespaces/proxy-9987/services/http:proxy-service-p5h85:portname2/proxy/: bar (200; 41.061229ms)
Jan 29 13:53:20.754: INFO: (16) /api/v1/namespaces/proxy-9987/services/proxy-service-p5h85:portname1/proxy/: foo (200; 42.684244ms)
Jan 29 13:53:20.754: INFO: (16) /api/v1/namespaces/proxy-9987/services/proxy-service-p5h85:portname2/proxy/: bar (200; 42.552073ms)
Jan 29 13:53:20.778: INFO: (17) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4/proxy/rewriteme">test</a> (200; 22.418287ms)
Jan 29 13:53:20.778: INFO: (17) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:160/proxy/: foo (200; 22.741814ms)
Jan 29 13:53:20.779: INFO: (17) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:160/proxy/: foo (200; 23.604738ms)
Jan 29 13:53:20.779: INFO: (17) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:462/proxy/: tls qux (200; 23.608545ms)
Jan 29 13:53:20.780: INFO: (17) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:443/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:443/proxy/tlsrewritem... (200; 24.485457ms)
Jan 29 13:53:20.780: INFO: (17) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:162/proxy/: bar (200; 24.383688ms)
Jan 29 13:53:20.780: INFO: (17) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:460/proxy/: tls baz (200; 24.462504ms)
Jan 29 13:53:20.780: INFO: (17) /api/v1/namespaces/proxy-9987/services/https:proxy-service-p5h85:tlsportname1/proxy/: tls baz (200; 24.290969ms)
Jan 29 13:53:20.780: INFO: (17) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:1080/proxy/rewriteme">test<... (200; 25.183087ms)
Jan 29 13:53:20.785: INFO: (17) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:162/proxy/: bar (200; 28.822688ms)
Jan 29 13:53:20.786: INFO: (17) /api/v1/namespaces/proxy-9987/services/proxy-service-p5h85:portname1/proxy/: foo (200; 30.839742ms)
Jan 29 13:53:20.786: INFO: (17) /api/v1/namespaces/proxy-9987/services/http:proxy-service-p5h85:portname2/proxy/: bar (200; 30.781889ms)
Jan 29 13:53:20.787: INFO: (17) /api/v1/namespaces/proxy-9987/services/https:proxy-service-p5h85:tlsportname2/proxy/: tls qux (200; 30.955241ms)
Jan 29 13:53:20.787: INFO: (17) /api/v1/namespaces/proxy-9987/services/http:proxy-service-p5h85:portname1/proxy/: foo (200; 30.958591ms)
Jan 29 13:53:20.787: INFO: (17) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:1080/proxy/rewriteme">... (200; 31.090937ms)
Jan 29 13:53:20.834: INFO: (17) /api/v1/namespaces/proxy-9987/services/proxy-service-p5h85:portname2/proxy/: bar (200; 78.172604ms)
Jan 29 13:53:20.860: INFO: (18) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:160/proxy/: foo (200; 25.42577ms)
Jan 29 13:53:20.863: INFO: (18) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:162/proxy/: bar (200; 27.74635ms)
Jan 29 13:53:20.863: INFO: (18) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:443/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:443/proxy/tlsrewritem... (200; 28.172631ms)
Jan 29 13:53:20.863: INFO: (18) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:1080/proxy/rewriteme">test<... (200; 28.052246ms)
Jan 29 13:53:20.863: INFO: (18) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:162/proxy/: bar (200; 28.399572ms)
Jan 29 13:53:20.863: INFO: (18) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:462/proxy/: tls qux (200; 28.352851ms)
Jan 29 13:53:20.863: INFO: (18) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:460/proxy/: tls baz (200; 28.262116ms)
Jan 29 13:53:20.863: INFO: (18) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4/proxy/rewriteme">test</a> (200; 28.246141ms)
Jan 29 13:53:20.863: INFO: (18) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:1080/proxy/rewriteme">... (200; 27.867421ms)
Jan 29 13:53:20.863: INFO: (18) /api/v1/namespaces/proxy-9987/services/proxy-service-p5h85:portname1/proxy/: foo (200; 28.537818ms)
Jan 29 13:53:20.869: INFO: (18) /api/v1/namespaces/proxy-9987/services/https:proxy-service-p5h85:tlsportname1/proxy/: tls baz (200; 33.592106ms)
Jan 29 13:53:20.871: INFO: (18) /api/v1/namespaces/proxy-9987/services/http:proxy-service-p5h85:portname1/proxy/: foo (200; 35.75829ms)
Jan 29 13:53:20.875: INFO: (18) /api/v1/namespaces/proxy-9987/services/https:proxy-service-p5h85:tlsportname2/proxy/: tls qux (200; 39.924191ms)
Jan 29 13:53:20.875: INFO: (18) /api/v1/namespaces/proxy-9987/services/http:proxy-service-p5h85:portname2/proxy/: bar (200; 40.294647ms)
Jan 29 13:53:20.906: INFO: (18) /api/v1/namespaces/proxy-9987/services/proxy-service-p5h85:portname2/proxy/: bar (200; 71.77109ms)
Jan 29 13:53:20.906: INFO: (18) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:160/proxy/: foo (200; 70.841142ms)
Jan 29 13:53:20.932: INFO: (19) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4/proxy/rewriteme">test</a> (200; 25.653584ms)
Jan 29 13:53:20.932: INFO: (19) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:162/proxy/: bar (200; 26.293798ms)
Jan 29 13:53:20.933: INFO: (19) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:443/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:443/proxy/tlsrewritem... (200; 26.633644ms)
Jan 29 13:53:20.938: INFO: (19) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:1080/proxy/rewriteme">test<... (200; 31.110163ms)
Jan 29 13:53:20.938: INFO: (19) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:460/proxy/: tls baz (200; 31.379793ms)
Jan 29 13:53:20.938: INFO: (19) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:162/proxy/: bar (200; 31.836181ms)
Jan 29 13:53:20.938: INFO: (19) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:1080/proxy/rewriteme">... (200; 31.680899ms)
Jan 29 13:53:20.938: INFO: (19) /api/v1/namespaces/proxy-9987/pods/http:proxy-service-p5h85-dwwn4:160/proxy/: foo (200; 31.579848ms)
Jan 29 13:53:20.938: INFO: (19) /api/v1/namespaces/proxy-9987/pods/proxy-service-p5h85-dwwn4:160/proxy/: foo (200; 32.080026ms)
Jan 29 13:53:20.939: INFO: (19) /api/v1/namespaces/proxy-9987/services/proxy-service-p5h85:portname2/proxy/: bar (200; 31.84546ms)
Jan 29 13:53:20.939: INFO: (19) /api/v1/namespaces/proxy-9987/pods/https:proxy-service-p5h85-dwwn4:462/proxy/: tls qux (200; 32.126011ms)
Jan 29 13:53:20.941: INFO: (19) /api/v1/namespaces/proxy-9987/services/https:proxy-service-p5h85:tlsportname2/proxy/: tls qux (200; 34.528611ms)
Jan 29 13:53:20.943: INFO: (19) /api/v1/namespaces/proxy-9987/services/http:proxy-service-p5h85:portname2/proxy/: bar (200; 36.262554ms)
Jan 29 13:53:20.987: INFO: (19) /api/v1/namespaces/proxy-9987/services/https:proxy-service-p5h85:tlsportname1/proxy/: tls baz (200; 80.190657ms)
Jan 29 13:53:20.987: INFO: (19) /api/v1/namespaces/proxy-9987/services/proxy-service-p5h85:portname1/proxy/: foo (200; 80.419967ms)
Jan 29 13:53:20.988: INFO: (19) /api/v1/namespaces/proxy-9987/services/http:proxy-service-p5h85:portname1/proxy/: foo (200; 81.205704ms)
STEP: deleting ReplicationController proxy-service-p5h85 in namespace proxy-9987, will wait for the garbage collector to delete the pods
Jan 29 13:53:21.081: INFO: Deleting ReplicationController proxy-service-p5h85 took: 25.510636ms
Jan 29 13:53:21.583: INFO: Terminating ReplicationController proxy-service-p5h85 pods took: 502.046926ms
[AfterEach] version v1
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:53:26.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9987" for this suite.

• [SLOW TEST:18.358 seconds]
[sig-network] Proxy
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":276,"completed":113,"skipped":1730,"failed":0}
SSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:53:26.424: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-2534
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating replication controller my-hostname-basic-f602ce19-fce5-479b-b059-879d95100a54
Jan 29 13:53:26.684: INFO: Pod name my-hostname-basic-f602ce19-fce5-479b-b059-879d95100a54: Found 0 pods out of 1
Jan 29 13:53:31.694: INFO: Pod name my-hostname-basic-f602ce19-fce5-479b-b059-879d95100a54: Found 1 pods out of 1
Jan 29 13:53:31.694: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-f602ce19-fce5-479b-b059-879d95100a54" are running
Jan 29 13:53:32.331: INFO: Pod "my-hostname-basic-f602ce19-fce5-479b-b059-879d95100a54-jc7xl" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-01-29 13:53:26 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-01-29 13:53:29 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-01-29 13:53:29 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-01-29 13:53:26 +0000 UTC Reason: Message:}])
Jan 29 13:53:32.331: INFO: Trying to dial the pod
Jan 29 13:53:37.466: INFO: Controller my-hostname-basic-f602ce19-fce5-479b-b059-879d95100a54: Got expected result from replica 1 [my-hostname-basic-f602ce19-fce5-479b-b059-879d95100a54-jc7xl]: "my-hostname-basic-f602ce19-fce5-479b-b059-879d95100a54-jc7xl", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:53:37.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2534" for this suite.

• [SLOW TEST:11.067 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":276,"completed":114,"skipped":1733,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:53:37.492: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-329
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name s-test-opt-del-a8203a6d-d633-4ca9-950c-e01f65333e64
STEP: Creating secret with name s-test-opt-upd-3715e681-ccd7-4ec2-ba13-2219fbfd33db
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-a8203a6d-d633-4ca9-950c-e01f65333e64
STEP: Updating secret s-test-opt-upd-3715e681-ccd7-4ec2-ba13-2219fbfd33db
STEP: Creating secret with name s-test-opt-create-ca5cc236-407b-42e2-8a21-51ac0cdc45f8
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:53:46.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-329" for this suite.

• [SLOW TEST:9.137 seconds]
[sig-storage] Secrets
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":276,"completed":115,"skipped":1744,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:53:46.630: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-5720
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 29 13:53:46.954: INFO: Waiting up to 5m0s for pod "busybox-user-65534-b4585aef-2dcf-4f35-9554-0e32d32c523e" in namespace "security-context-test-5720" to be "success or failure"
Jan 29 13:53:46.967: INFO: Pod "busybox-user-65534-b4585aef-2dcf-4f35-9554-0e32d32c523e": Phase="Pending", Reason="", readiness=false. Elapsed: 13.187375ms
Jan 29 13:53:48.984: INFO: Pod "busybox-user-65534-b4585aef-2dcf-4f35-9554-0e32d32c523e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029998372s
Jan 29 13:53:50.991: INFO: Pod "busybox-user-65534-b4585aef-2dcf-4f35-9554-0e32d32c523e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037589388s
Jan 29 13:53:50.992: INFO: Pod "busybox-user-65534-b4585aef-2dcf-4f35-9554-0e32d32c523e" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:53:50.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5720" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":276,"completed":116,"skipped":1767,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:53:51.022: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6849
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 29 13:53:51.828: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2d9ca833-c642-43f3-9307-dccb7cf30daa" in namespace "downward-api-6849" to be "success or failure"
Jan 29 13:53:51.846: INFO: Pod "downwardapi-volume-2d9ca833-c642-43f3-9307-dccb7cf30daa": Phase="Pending", Reason="", readiness=false. Elapsed: 17.515884ms
Jan 29 13:53:53.881: INFO: Pod "downwardapi-volume-2d9ca833-c642-43f3-9307-dccb7cf30daa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053096095s
Jan 29 13:53:55.891: INFO: Pod "downwardapi-volume-2d9ca833-c642-43f3-9307-dccb7cf30daa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.062985089s
STEP: Saw pod success
Jan 29 13:53:55.891: INFO: Pod "downwardapi-volume-2d9ca833-c642-43f3-9307-dccb7cf30daa" satisfied condition "success or failure"
Jan 29 13:53:55.901: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-znv5g pod downwardapi-volume-2d9ca833-c642-43f3-9307-dccb7cf30daa container client-container: <nil>
STEP: delete the pod
Jan 29 13:53:56.199: INFO: Waiting for pod downwardapi-volume-2d9ca833-c642-43f3-9307-dccb7cf30daa to disappear
Jan 29 13:53:56.208: INFO: Pod downwardapi-volume-2d9ca833-c642-43f3-9307-dccb7cf30daa no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:53:56.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6849" for this suite.

• [SLOW TEST:5.216 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":276,"completed":117,"skipped":1778,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:53:56.244: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4974
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-4974
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a new StatefulSet
Jan 29 13:53:57.067: INFO: Found 0 stateful pods, waiting for 3
Jan 29 13:54:07.080: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 13:54:07.080: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 13:54:07.080: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jan 29 13:54:17.084: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 13:54:17.084: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 13:54:17.085: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 13:54:17.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4974 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 29 13:54:19.283: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 29 13:54:19.283: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 29 13:54:19.283: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jan 29 13:54:29.398: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jan 29 13:54:39.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4974 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 13:54:40.433: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 29 13:54:40.433: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 29 13:54:40.433: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision
Jan 29 13:55:20.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4974 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 29 13:55:21.318: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 29 13:55:21.318: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 29 13:55:21.318: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 29 13:55:31.422: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jan 29 13:55:41.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4974 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 13:55:42.219: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 29 13:55:42.219: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 29 13:55:42.219: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 29 13:55:52.362: INFO: Waiting for StatefulSet statefulset-4974/ss2 to complete update
Jan 29 13:55:52.362: INFO: Waiting for Pod statefulset-4974/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jan 29 13:55:52.362: INFO: Waiting for Pod statefulset-4974/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jan 29 13:55:52.362: INFO: Waiting for Pod statefulset-4974/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jan 29 13:56:02.394: INFO: Waiting for StatefulSet statefulset-4974/ss2 to complete update
Jan 29 13:56:02.395: INFO: Waiting for Pod statefulset-4974/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jan 29 13:56:02.395: INFO: Waiting for Pod statefulset-4974/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jan 29 13:56:12.402: INFO: Waiting for StatefulSet statefulset-4974/ss2 to complete update
Jan 29 13:56:12.402: INFO: Waiting for Pod statefulset-4974/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jan 29 13:56:22.381: INFO: Deleting all statefulset in ns statefulset-4974
Jan 29 13:56:22.388: INFO: Scaling statefulset ss2 to 0
Jan 29 13:56:52.908: INFO: Waiting for statefulset status.replicas updated to 0
Jan 29 13:56:52.994: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:56:53.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4974" for this suite.

• [SLOW TEST:177.000 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":276,"completed":118,"skipped":1807,"failed":0}
SS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:56:53.244: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-9969
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 29 13:56:58.760: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:56:58.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9969" for this suite.

• [SLOW TEST:5.662 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  blackbox test
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:131
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":276,"completed":119,"skipped":1809,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:56:58.915: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8265
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0129 13:57:09.369977      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jan 29 13:57:09.370: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 13:57:09.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8265" for this suite.

• [SLOW TEST:10.496 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":276,"completed":120,"skipped":1853,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 13:57:09.415: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7657
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod test-webserver-570ba213-10b4-4332-ad0a-94d64134c42d in namespace container-probe-7657
Jan 29 13:57:14.049: INFO: Started pod test-webserver-570ba213-10b4-4332-ad0a-94d64134c42d in namespace container-probe-7657
STEP: checking the pod's current state and verifying that restartCount is present
Jan 29 13:57:14.063: INFO: Initial restart count of pod test-webserver-570ba213-10b4-4332-ad0a-94d64134c42d is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:01:15.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7657" for this suite.

• [SLOW TEST:246.148 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":276,"completed":121,"skipped":1897,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:01:15.568: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-5208
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Jan 29 14:01:16.617: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:01:22.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5208" for this suite.

• [SLOW TEST:6.956 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":276,"completed":122,"skipped":1948,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:01:22.526: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3615
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:46
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Jan 29 14:01:26.932: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-836921002 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Jan 29 14:01:37.128: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:01:37.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3615" for this suite.

• [SLOW TEST:14.651 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should be submitted and removed [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period should be submitted and removed [Conformance]","total":276,"completed":123,"skipped":1959,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:01:37.178: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1184
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jan 29 14:01:37.489: INFO: Waiting up to 5m0s for pod "pod-ceb2407b-916d-4d27-a19d-5b90700d6fff" in namespace "emptydir-1184" to be "success or failure"
Jan 29 14:01:37.496: INFO: Pod "pod-ceb2407b-916d-4d27-a19d-5b90700d6fff": Phase="Pending", Reason="", readiness=false. Elapsed: 6.765317ms
Jan 29 14:01:39.513: INFO: Pod "pod-ceb2407b-916d-4d27-a19d-5b90700d6fff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023824292s
Jan 29 14:01:41.522: INFO: Pod "pod-ceb2407b-916d-4d27-a19d-5b90700d6fff": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032308936s
Jan 29 14:01:43.547: INFO: Pod "pod-ceb2407b-916d-4d27-a19d-5b90700d6fff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.057660454s
STEP: Saw pod success
Jan 29 14:01:43.547: INFO: Pod "pod-ceb2407b-916d-4d27-a19d-5b90700d6fff" satisfied condition "success or failure"
Jan 29 14:01:43.568: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-ceb2407b-916d-4d27-a19d-5b90700d6fff container test-container: <nil>
STEP: delete the pod
Jan 29 14:01:43.814: INFO: Waiting for pod pod-ceb2407b-916d-4d27-a19d-5b90700d6fff to disappear
Jan 29 14:01:43.825: INFO: Pod pod-ceb2407b-916d-4d27-a19d-5b90700d6fff no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:01:43.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1184" for this suite.

• [SLOW TEST:6.701 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":276,"completed":124,"skipped":1972,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:01:43.883: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7804
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 29 14:01:44.931: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 29 14:01:46.961: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903304, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903304, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903304, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903304, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 14:01:48.970: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903304, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903304, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903304, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903304, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 29 14:01:51.995: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 29 14:01:52.004: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4914-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:01:53.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7804" for this suite.
STEP: Destroying namespace "webhook-7804-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:10.451 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":276,"completed":125,"skipped":1975,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:01:54.346: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-1371
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jan 29 14:01:54.711: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1371 /api/v1/namespaces/watch-1371/configmaps/e2e-watch-test-resource-version 4984b68f-3e84-4454-81bc-600e3b687758 75886 0 2020-01-29 14:01:54 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 29 14:01:54.711: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1371 /api/v1/namespaces/watch-1371/configmaps/e2e-watch-test-resource-version 4984b68f-3e84-4454-81bc-600e3b687758 75887 0 2020-01-29 14:01:54 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:01:54.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1371" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":276,"completed":126,"skipped":2008,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:01:54.761: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-3281
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:01:59.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3281" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":276,"completed":127,"skipped":2022,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:01:59.393: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-105
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 29 14:01:59.708: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c5dbaa09-3249-43c0-bdb7-7d06765aad8c" in namespace "downward-api-105" to be "success or failure"
Jan 29 14:01:59.729: INFO: Pod "downwardapi-volume-c5dbaa09-3249-43c0-bdb7-7d06765aad8c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.421386ms
Jan 29 14:02:01.742: INFO: Pod "downwardapi-volume-c5dbaa09-3249-43c0-bdb7-7d06765aad8c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032911455s
Jan 29 14:02:03.750: INFO: Pod "downwardapi-volume-c5dbaa09-3249-43c0-bdb7-7d06765aad8c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041342157s
Jan 29 14:02:05.761: INFO: Pod "downwardapi-volume-c5dbaa09-3249-43c0-bdb7-7d06765aad8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.052235715s
STEP: Saw pod success
Jan 29 14:02:05.761: INFO: Pod "downwardapi-volume-c5dbaa09-3249-43c0-bdb7-7d06765aad8c" satisfied condition "success or failure"
Jan 29 14:02:05.769: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod downwardapi-volume-c5dbaa09-3249-43c0-bdb7-7d06765aad8c container client-container: <nil>
STEP: delete the pod
Jan 29 14:02:05.864: INFO: Waiting for pod downwardapi-volume-c5dbaa09-3249-43c0-bdb7-7d06765aad8c to disappear
Jan 29 14:02:05.874: INFO: Pod downwardapi-volume-c5dbaa09-3249-43c0-bdb7-7d06765aad8c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:02:05.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-105" for this suite.

• [SLOW TEST:6.648 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":276,"completed":128,"skipped":2024,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:02:06.043: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7460
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jan 29 14:02:06.386: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:02:16.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7460" for this suite.

• [SLOW TEST:10.393 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":276,"completed":129,"skipped":2038,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:02:16.436: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-6247
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override all
Jan 29 14:02:16.825: INFO: Waiting up to 5m0s for pod "client-containers-22bb791d-9850-4f8f-a631-07b390095469" in namespace "containers-6247" to be "success or failure"
Jan 29 14:02:16.853: INFO: Pod "client-containers-22bb791d-9850-4f8f-a631-07b390095469": Phase="Pending", Reason="", readiness=false. Elapsed: 27.390688ms
Jan 29 14:02:19.019: INFO: Pod "client-containers-22bb791d-9850-4f8f-a631-07b390095469": Phase="Pending", Reason="", readiness=false. Elapsed: 2.193703571s
Jan 29 14:02:21.029: INFO: Pod "client-containers-22bb791d-9850-4f8f-a631-07b390095469": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.20338243s
STEP: Saw pod success
Jan 29 14:02:21.029: INFO: Pod "client-containers-22bb791d-9850-4f8f-a631-07b390095469" satisfied condition "success or failure"
Jan 29 14:02:21.035: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod client-containers-22bb791d-9850-4f8f-a631-07b390095469 container test-container: <nil>
STEP: delete the pod
Jan 29 14:02:21.156: INFO: Waiting for pod client-containers-22bb791d-9850-4f8f-a631-07b390095469 to disappear
Jan 29 14:02:21.166: INFO: Pod client-containers-22bb791d-9850-4f8f-a631-07b390095469 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:02:21.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6247" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":276,"completed":130,"skipped":2049,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:02:21.202: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9504
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jan 29 14:02:21.463: INFO: Waiting up to 5m0s for pod "pod-2bf1c7f6-aa89-4ac9-862f-f9a13060bc58" in namespace "emptydir-9504" to be "success or failure"
Jan 29 14:02:21.480: INFO: Pod "pod-2bf1c7f6-aa89-4ac9-862f-f9a13060bc58": Phase="Pending", Reason="", readiness=false. Elapsed: 10.283535ms
Jan 29 14:02:23.505: INFO: Pod "pod-2bf1c7f6-aa89-4ac9-862f-f9a13060bc58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034858211s
Jan 29 14:02:25.531: INFO: Pod "pod-2bf1c7f6-aa89-4ac9-862f-f9a13060bc58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060604577s
STEP: Saw pod success
Jan 29 14:02:25.531: INFO: Pod "pod-2bf1c7f6-aa89-4ac9-862f-f9a13060bc58" satisfied condition "success or failure"
Jan 29 14:02:25.547: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-2bf1c7f6-aa89-4ac9-862f-f9a13060bc58 container test-container: <nil>
STEP: delete the pod
Jan 29 14:02:25.658: INFO: Waiting for pod pod-2bf1c7f6-aa89-4ac9-862f-f9a13060bc58 to disappear
Jan 29 14:02:25.680: INFO: Pod pod-2bf1c7f6-aa89-4ac9-862f-f9a13060bc58 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:02:25.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9504" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":276,"completed":131,"skipped":2065,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:02:25.750: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3409
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: set up a multi version CRD
Jan 29 14:02:26.014: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:02:46.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3409" for this suite.

• [SLOW TEST:20.687 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":276,"completed":132,"skipped":2082,"failed":0}
S
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:02:46.437: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4004
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-1fa4a2a5-5c18-4edc-9951-3a066fbb81b5
STEP: Creating a pod to test consume secrets
Jan 29 14:02:47.943: INFO: Waiting up to 5m0s for pod "pod-secrets-3a2ccc60-e394-43a6-9ee0-c03ce9b25a20" in namespace "secrets-4004" to be "success or failure"
Jan 29 14:02:48.726: INFO: Pod "pod-secrets-3a2ccc60-e394-43a6-9ee0-c03ce9b25a20": Phase="Pending", Reason="", readiness=false. Elapsed: 782.747874ms
Jan 29 14:02:51.198: INFO: Pod "pod-secrets-3a2ccc60-e394-43a6-9ee0-c03ce9b25a20": Phase="Pending", Reason="", readiness=false. Elapsed: 3.255086974s
Jan 29 14:02:53.212: INFO: Pod "pod-secrets-3a2ccc60-e394-43a6-9ee0-c03ce9b25a20": Phase="Succeeded", Reason="", readiness=false. Elapsed: 5.269193693s
STEP: Saw pod success
Jan 29 14:02:53.212: INFO: Pod "pod-secrets-3a2ccc60-e394-43a6-9ee0-c03ce9b25a20" satisfied condition "success or failure"
Jan 29 14:02:53.241: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-secrets-3a2ccc60-e394-43a6-9ee0-c03ce9b25a20 container secret-volume-test: <nil>
STEP: delete the pod
Jan 29 14:02:53.365: INFO: Waiting for pod pod-secrets-3a2ccc60-e394-43a6-9ee0-c03ce9b25a20 to disappear
Jan 29 14:02:53.378: INFO: Pod pod-secrets-3a2ccc60-e394-43a6-9ee0-c03ce9b25a20 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:02:53.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4004" for this suite.

• [SLOW TEST:7.012 seconds]
[sig-storage] Secrets
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":276,"completed":133,"skipped":2083,"failed":0}
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:02:53.451: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-186
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:03:00.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-186" for this suite.

• [SLOW TEST:7.420 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":276,"completed":134,"skipped":2090,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:03:00.871: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-2170
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:03:01.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2170" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":276,"completed":135,"skipped":2113,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:03:01.201: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7234
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 29 14:03:01.422: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jan 29 14:03:05.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 --namespace=crd-publish-openapi-7234 create -f -'
Jan 29 14:03:08.584: INFO: stderr: ""
Jan 29 14:03:08.584: INFO: stdout: "e2e-test-crd-publish-openapi-9444-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 29 14:03:08.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 --namespace=crd-publish-openapi-7234 delete e2e-test-crd-publish-openapi-9444-crds test-cr'
Jan 29 14:03:08.749: INFO: stderr: ""
Jan 29 14:03:08.750: INFO: stdout: "e2e-test-crd-publish-openapi-9444-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jan 29 14:03:08.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 --namespace=crd-publish-openapi-7234 apply -f -'
Jan 29 14:03:09.238: INFO: stderr: ""
Jan 29 14:03:09.238: INFO: stdout: "e2e-test-crd-publish-openapi-9444-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 29 14:03:09.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 --namespace=crd-publish-openapi-7234 delete e2e-test-crd-publish-openapi-9444-crds test-cr'
Jan 29 14:03:09.380: INFO: stderr: ""
Jan 29 14:03:09.380: INFO: stdout: "e2e-test-crd-publish-openapi-9444-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jan 29 14:03:09.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 explain e2e-test-crd-publish-openapi-9444-crds'
Jan 29 14:03:09.697: INFO: stderr: ""
Jan 29 14:03:09.697: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9444-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:03:13.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7234" for this suite.

• [SLOW TEST:12.388 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":276,"completed":136,"skipped":2140,"failed":0}
S
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:03:13.593: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1683
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 29 14:03:13.930: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jan 29 14:03:18.949: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 29 14:03:18.950: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Jan 29 14:03:25.080: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-1683 /apis/apps/v1/namespaces/deployment-1683/deployments/test-cleanup-deployment 1d588392-afb8-4415-9752-f90b846cf7fe 76577 1 2020-01-29 14:03:18 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005ab1928 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-01-29 14:03:19 +0000 UTC,LastTransitionTime:2020-01-29 14:03:19 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-55ffc6b7b6" has successfully progressed.,LastUpdateTime:2020-01-29 14:03:23 +0000 UTC,LastTransitionTime:2020-01-29 14:03:19 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 29 14:03:25.091: INFO: New ReplicaSet "test-cleanup-deployment-55ffc6b7b6" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-55ffc6b7b6  deployment-1683 /apis/apps/v1/namespaces/deployment-1683/replicasets/test-cleanup-deployment-55ffc6b7b6 a5a440c7-8383-477e-aa4c-26ce3ac53098 76566 1 2020-01-29 14:03:19 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:55ffc6b7b6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 1d588392-afb8-4415-9752-f90b846cf7fe 0xc005ab1dc7 0xc005ab1dc8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55ffc6b7b6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:55ffc6b7b6] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005ab1ea8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 29 14:03:25.102: INFO: Pod "test-cleanup-deployment-55ffc6b7b6-9rntx" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-55ffc6b7b6-9rntx test-cleanup-deployment-55ffc6b7b6- deployment-1683 /api/v1/namespaces/deployment-1683/pods/test-cleanup-deployment-55ffc6b7b6-9rntx e381c7bd-fe24-49ca-9b82-b8a39460eb03 76565 0 2020-01-29 14:03:19 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:55ffc6b7b6] map[cni.projectcalico.org/podIP:172.25.2.149/32 cni.projectcalico.org/podIPs:172.25.2.149/32] [{apps/v1 ReplicaSet test-cleanup-deployment-55ffc6b7b6 a5a440c7-8383-477e-aa4c-26ce3ac53098 0xc001f00697 0xc001f00698}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rfbbz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rfbbz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rfbbz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-cmccl-6d88bd94fc-lqfxz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 14:03:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 14:03:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 14:03:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 14:03:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.5,PodIP:172.25.2.149,StartTime:2020-01-29 14:03:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-29 14:03:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:docker://7d664f5635b8cde305a04082c0cf33913d0420a54cd9a85860e610af0cecca01,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.149,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:03:25.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1683" for this suite.

• [SLOW TEST:11.545 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":276,"completed":137,"skipped":2141,"failed":0}
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:03:25.141: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-966
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jan 29 14:03:39.710: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 29 14:03:40.052: INFO: Pod pod-with-prestop-http-hook still exists
Jan 29 14:03:42.052: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 29 14:03:42.067: INFO: Pod pod-with-prestop-http-hook still exists
Jan 29 14:03:44.053: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 29 14:03:44.394: INFO: Pod pod-with-prestop-http-hook still exists
Jan 29 14:03:46.053: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 29 14:03:46.094: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:03:46.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-966" for this suite.

• [SLOW TEST:21.224 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":276,"completed":138,"skipped":2141,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:03:46.365: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8912
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1632
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jan 29 14:03:46.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-8912'
Jan 29 14:03:46.857: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan 29 14:03:46.857: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Jan 29 14:03:46.918: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-h9dgt]
Jan 29 14:03:46.918: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-h9dgt" in namespace "kubectl-8912" to be "running and ready"
Jan 29 14:03:46.959: INFO: Pod "e2e-test-httpd-rc-h9dgt": Phase="Pending", Reason="", readiness=false. Elapsed: 40.915308ms
Jan 29 14:03:48.982: INFO: Pod "e2e-test-httpd-rc-h9dgt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.064423216s
Jan 29 14:03:50.994: INFO: Pod "e2e-test-httpd-rc-h9dgt": Phase="Pending", Reason="", readiness=false. Elapsed: 4.076093647s
Jan 29 14:03:53.003: INFO: Pod "e2e-test-httpd-rc-h9dgt": Phase="Running", Reason="", readiness=true. Elapsed: 6.085467598s
Jan 29 14:03:53.003: INFO: Pod "e2e-test-httpd-rc-h9dgt" satisfied condition "running and ready"
Jan 29 14:03:53.003: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-h9dgt]
Jan 29 14:03:53.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 logs rc/e2e-test-httpd-rc --namespace=kubectl-8912'
Jan 29 14:03:53.215: INFO: stderr: ""
Jan 29 14:03:53.215: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.25.2.152. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.25.2.152. Set the 'ServerName' directive globally to suppress this message\n[Wed Jan 29 14:03:50.147194 2020] [mpm_event:notice] [pid 1:tid 140176141802344] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Wed Jan 29 14:03:50.147623 2020] [core:notice] [pid 1:tid 140176141802344] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1637
Jan 29 14:03:53.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 delete rc e2e-test-httpd-rc --namespace=kubectl-8912'
Jan 29 14:03:53.384: INFO: stderr: ""
Jan 29 14:03:53.384: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:03:53.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8912" for this suite.

• [SLOW TEST:7.087 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run rc
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1628
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run rc should create an rc from an image  [Conformance]","total":276,"completed":139,"skipped":2144,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:03:53.456: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-2748
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-2748
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 29 14:03:53.730: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jan 29 14:04:16.083: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.2.154:8080/dial?request=hostname&protocol=http&host=172.25.0.91&port=8080&tries=1'] Namespace:pod-network-test-2748 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 14:04:16.083: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
Jan 29 14:04:16.805: INFO: Waiting for responses: map[]
Jan 29 14:04:16.822: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.2.154:8080/dial?request=hostname&protocol=http&host=172.25.2.153&port=8080&tries=1'] Namespace:pod-network-test-2748 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 14:04:16.822: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
Jan 29 14:04:17.351: INFO: Waiting for responses: map[]
Jan 29 14:04:17.361: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.2.154:8080/dial?request=hostname&protocol=http&host=172.25.1.177&port=8080&tries=1'] Namespace:pod-network-test-2748 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 14:04:17.361: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
Jan 29 14:04:17.933: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:04:17.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2748" for this suite.

• [SLOW TEST:24.521 seconds]
[sig-network] Networking
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":276,"completed":140,"skipped":2201,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:04:17.987: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2884
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating pod
Jan 29 14:04:22.355: INFO: Pod pod-hostip-97b798df-11aa-4f4c-9889-c4cfb360aec0 has hostIP: 192.168.1.5
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:04:22.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2884" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":276,"completed":141,"skipped":2221,"failed":0}

------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:04:22.391: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-3207
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Jan 29 14:04:22.627: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the sample API server.
Jan 29 14:04:23.398: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jan 29 14:04:25.644: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903463, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903463, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903463, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903463, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 14:04:27.653: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903463, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903463, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903463, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903463, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 14:04:31.041: INFO: Waited 1.339758263s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:04:32.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-3207" for this suite.

• [SLOW TEST:10.369 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]","total":276,"completed":142,"skipped":2221,"failed":0}
SSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:04:32.761: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9090
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 29 14:04:39.164: INFO: Waiting up to 5m0s for pod "client-envvars-b276781e-85de-4168-8b69-5bcfaac6f9f0" in namespace "pods-9090" to be "success or failure"
Jan 29 14:04:39.176: INFO: Pod "client-envvars-b276781e-85de-4168-8b69-5bcfaac6f9f0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.633538ms
Jan 29 14:04:41.185: INFO: Pod "client-envvars-b276781e-85de-4168-8b69-5bcfaac6f9f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020758455s
Jan 29 14:04:43.206: INFO: Pod "client-envvars-b276781e-85de-4168-8b69-5bcfaac6f9f0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042064592s
Jan 29 14:04:45.243: INFO: Pod "client-envvars-b276781e-85de-4168-8b69-5bcfaac6f9f0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.078895011s
Jan 29 14:04:47.257: INFO: Pod "client-envvars-b276781e-85de-4168-8b69-5bcfaac6f9f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.093033937s
STEP: Saw pod success
Jan 29 14:04:47.257: INFO: Pod "client-envvars-b276781e-85de-4168-8b69-5bcfaac6f9f0" satisfied condition "success or failure"
Jan 29 14:04:47.268: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod client-envvars-b276781e-85de-4168-8b69-5bcfaac6f9f0 container env3cont: <nil>
STEP: delete the pod
Jan 29 14:04:47.409: INFO: Waiting for pod client-envvars-b276781e-85de-4168-8b69-5bcfaac6f9f0 to disappear
Jan 29 14:04:47.418: INFO: Pod client-envvars-b276781e-85de-4168-8b69-5bcfaac6f9f0 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:04:47.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9090" for this suite.

• [SLOW TEST:14.697 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":276,"completed":143,"skipped":2224,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:04:47.468: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-5759
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 29 14:04:47.685: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jan 29 14:04:49.809: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:04:49.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5759" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":276,"completed":144,"skipped":2257,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:04:49.901: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9803
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jan 29 14:04:50.200: INFO: Waiting up to 5m0s for pod "pod-10ce5e2f-0508-4fd3-9e99-c199aeb7c20b" in namespace "emptydir-9803" to be "success or failure"
Jan 29 14:04:50.235: INFO: Pod "pod-10ce5e2f-0508-4fd3-9e99-c199aeb7c20b": Phase="Pending", Reason="", readiness=false. Elapsed: 35.164086ms
Jan 29 14:04:52.274: INFO: Pod "pod-10ce5e2f-0508-4fd3-9e99-c199aeb7c20b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.073592657s
Jan 29 14:04:54.373: INFO: Pod "pod-10ce5e2f-0508-4fd3-9e99-c199aeb7c20b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.172511486s
Jan 29 14:04:56.385: INFO: Pod "pod-10ce5e2f-0508-4fd3-9e99-c199aeb7c20b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.184539903s
STEP: Saw pod success
Jan 29 14:04:56.385: INFO: Pod "pod-10ce5e2f-0508-4fd3-9e99-c199aeb7c20b" satisfied condition "success or failure"
Jan 29 14:04:56.394: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-10ce5e2f-0508-4fd3-9e99-c199aeb7c20b container test-container: <nil>
STEP: delete the pod
Jan 29 14:04:56.520: INFO: Waiting for pod pod-10ce5e2f-0508-4fd3-9e99-c199aeb7c20b to disappear
Jan 29 14:04:56.528: INFO: Pod pod-10ce5e2f-0508-4fd3-9e99-c199aeb7c20b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:04:56.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9803" for this suite.

• [SLOW TEST:6.657 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":276,"completed":145,"skipped":2269,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:04:56.564: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-9748
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override arguments
Jan 29 14:04:56.901: INFO: Waiting up to 5m0s for pod "client-containers-6f5e10a6-1838-4eeb-a3ac-4b69b7e80130" in namespace "containers-9748" to be "success or failure"
Jan 29 14:04:56.916: INFO: Pod "client-containers-6f5e10a6-1838-4eeb-a3ac-4b69b7e80130": Phase="Pending", Reason="", readiness=false. Elapsed: 15.360215ms
Jan 29 14:04:58.928: INFO: Pod "client-containers-6f5e10a6-1838-4eeb-a3ac-4b69b7e80130": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027033386s
Jan 29 14:05:00.944: INFO: Pod "client-containers-6f5e10a6-1838-4eeb-a3ac-4b69b7e80130": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042647529s
STEP: Saw pod success
Jan 29 14:05:00.944: INFO: Pod "client-containers-6f5e10a6-1838-4eeb-a3ac-4b69b7e80130" satisfied condition "success or failure"
Jan 29 14:05:00.956: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod client-containers-6f5e10a6-1838-4eeb-a3ac-4b69b7e80130 container test-container: <nil>
STEP: delete the pod
Jan 29 14:05:01.044: INFO: Waiting for pod client-containers-6f5e10a6-1838-4eeb-a3ac-4b69b7e80130 to disappear
Jan 29 14:05:01.056: INFO: Pod client-containers-6f5e10a6-1838-4eeb-a3ac-4b69b7e80130 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:05:01.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9748" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":276,"completed":146,"skipped":2288,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:05:01.091: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8737
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name cm-test-opt-del-262c1a15-fcf3-4c01-921a-3633897cfe7f
STEP: Creating configMap with name cm-test-opt-upd-d4ffa937-d12c-4b17-a5ae-6dd2cf3cbd69
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-262c1a15-fcf3-4c01-921a-3633897cfe7f
STEP: Updating configmap cm-test-opt-upd-d4ffa937-d12c-4b17-a5ae-6dd2cf3cbd69
STEP: Creating configMap with name cm-test-opt-create-6e33bd37-304d-4934-b349-615f67a5eed4
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:06:22.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8737" for this suite.

• [SLOW TEST:81.085 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":276,"completed":147,"skipped":2291,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:06:22.177: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1011
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Jan 29 14:06:22.441: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
Jan 29 14:06:26.318: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:06:42.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1011" for this suite.

• [SLOW TEST:20.043 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":276,"completed":148,"skipped":2296,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:06:42.228: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8521
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 29 14:06:42.493: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jan 29 14:06:42.517: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 29 14:06:47.528: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 29 14:06:47.528: INFO: Creating deployment "test-rolling-update-deployment"
Jan 29 14:06:47.542: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jan 29 14:06:47.563: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jan 29 14:06:49.625: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jan 29 14:06:49.632: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903607, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903607, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903607, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903607, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67cf4f6444\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 14:06:51.653: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903607, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903607, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903607, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903607, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67cf4f6444\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 14:06:53.649: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Jan 29 14:06:53.682: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-8521 /apis/apps/v1/namespaces/deployment-8521/deployments/test-rolling-update-deployment fffb8ada-2078-4e4e-a632-cca44d1a6097 78029 1 2020-01-29 14:06:47 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001e3c648 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-01-29 14:06:47 +0000 UTC,LastTransitionTime:2020-01-29 14:06:47 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-67cf4f6444" has successfully progressed.,LastUpdateTime:2020-01-29 14:06:51 +0000 UTC,LastTransitionTime:2020-01-29 14:06:47 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 29 14:06:53.704: INFO: New ReplicaSet "test-rolling-update-deployment-67cf4f6444" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-67cf4f6444  deployment-8521 /apis/apps/v1/namespaces/deployment-8521/replicasets/test-rolling-update-deployment-67cf4f6444 7b5057aa-3a9a-4528-baef-26d2f7f14962 78016 1 2020-01-29 14:06:47 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment fffb8ada-2078-4e4e-a632-cca44d1a6097 0xc001e3cd77 0xc001e3cd78}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67cf4f6444,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001e3cde8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 29 14:06:53.704: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jan 29 14:06:53.705: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-8521 /apis/apps/v1/namespaces/deployment-8521/replicasets/test-rolling-update-controller 48d1bb29-9a22-43d9-b636-29193bceb2e1 78028 2 2020-01-29 14:06:42 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment fffb8ada-2078-4e4e-a632-cca44d1a6097 0xc001e3cc77 0xc001e3cc78}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc001e3ccf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 29 14:06:53.721: INFO: Pod "test-rolling-update-deployment-67cf4f6444-j8x7s" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-67cf4f6444-j8x7s test-rolling-update-deployment-67cf4f6444- deployment-8521 /api/v1/namespaces/deployment-8521/pods/test-rolling-update-deployment-67cf4f6444-j8x7s 8a2a7e45-6d4d-48c8-8838-e6b628d4c83d 78015 0 2020-01-29 14:06:47 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[cni.projectcalico.org/podIP:172.25.2.164/32 cni.projectcalico.org/podIPs:172.25.2.164/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-67cf4f6444 7b5057aa-3a9a-4528-baef-26d2f7f14962 0xc001e3d357 0xc001e3d358}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-n9892,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-n9892,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-n9892,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-cmccl-6d88bd94fc-lqfxz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 14:06:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 14:06:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 14:06:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 14:06:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.5,PodIP:172.25.2.164,StartTime:2020-01-29 14:06:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-29 14:06:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:docker://ed30e2356efba289da9d1279974dd839a31ce3123374de3cde1fc7cd3eeda351,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.164,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:06:53.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8521" for this suite.

• [SLOW TEST:11.537 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":276,"completed":149,"skipped":2322,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:06:53.773: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-504
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 29 14:06:54.964: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903614, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903614, loc:(*time.Location)(0x7db4bc0)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-5f65f8c764\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903614, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903614, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Jan 29 14:06:56.982: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903614, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903614, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903615, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903614, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 14:06:58.972: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903614, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903614, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903615, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903614, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 29 14:07:02.008: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 29 14:07:02.031: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:07:03.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-504" for this suite.
STEP: Destroying namespace "webhook-504-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:10.435 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":276,"completed":150,"skipped":2340,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:07:04.212: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-3406
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jan 29 14:07:04.502: INFO: Pod name pod-release: Found 0 pods out of 1
Jan 29 14:07:09.559: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:07:09.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3406" for this suite.

• [SLOW TEST:5.506 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":276,"completed":151,"skipped":2351,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:07:09.720: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6169
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6169.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6169.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6169.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6169.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6169.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6169.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 29 14:07:23.992: INFO: DNS probes using dns-6169/dns-test-b4a4614f-db19-4395-a38b-0c710d3b80e2 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:07:24.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6169" for this suite.

• [SLOW TEST:15.726 seconds]
[sig-network] DNS
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":276,"completed":152,"skipped":2374,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:07:25.449: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2562
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-9933
STEP: Creating secret with name secret-test-a8ba2e79-f827-4e89-8d23-e3b1cc5b4acf
STEP: Creating a pod to test consume secrets
Jan 29 14:07:26.176: INFO: Waiting up to 5m0s for pod "pod-secrets-54e401ea-34b2-412f-81af-205fa8972ca2" in namespace "secrets-2562" to be "success or failure"
Jan 29 14:07:26.193: INFO: Pod "pod-secrets-54e401ea-34b2-412f-81af-205fa8972ca2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.831504ms
Jan 29 14:07:28.202: INFO: Pod "pod-secrets-54e401ea-34b2-412f-81af-205fa8972ca2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025693191s
Jan 29 14:07:30.215: INFO: Pod "pod-secrets-54e401ea-34b2-412f-81af-205fa8972ca2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039089519s
STEP: Saw pod success
Jan 29 14:07:30.215: INFO: Pod "pod-secrets-54e401ea-34b2-412f-81af-205fa8972ca2" satisfied condition "success or failure"
Jan 29 14:07:30.227: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-secrets-54e401ea-34b2-412f-81af-205fa8972ca2 container secret-volume-test: <nil>
STEP: delete the pod
Jan 29 14:07:30.322: INFO: Waiting for pod pod-secrets-54e401ea-34b2-412f-81af-205fa8972ca2 to disappear
Jan 29 14:07:30.374: INFO: Pod pod-secrets-54e401ea-34b2-412f-81af-205fa8972ca2 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:07:30.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2562" for this suite.
STEP: Destroying namespace "secret-namespace-9933" for this suite.

• [SLOW TEST:5.007 seconds]
[sig-storage] Secrets
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":276,"completed":153,"skipped":2408,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:07:30.457: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9055
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 29 14:07:31.690: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 29 14:07:33.732: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903651, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903651, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903651, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903651, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 29 14:07:36.795: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 29 14:07:36.809: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7929-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:07:38.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9055" for this suite.
STEP: Destroying namespace "webhook-9055-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.133 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":276,"completed":154,"skipped":2412,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:07:38.591: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-8169
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:125
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jan 29 14:07:40.102: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jan 29 14:07:42.163: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903660, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903660, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903660, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903660, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 14:07:44.175: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903660, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903660, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903660, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903660, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 14:07:46.181: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903660, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903660, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903660, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903660, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 14:07:48.173: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903660, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903660, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903660, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903660, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 29 14:07:51.250: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 29 14:07:51.264: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:07:53.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-8169" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:136

• [SLOW TEST:14.794 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":276,"completed":155,"skipped":2414,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:07:53.386: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9754
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 29 14:07:54.424: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 29 14:07:56.476: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903674, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903674, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903674, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903674, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 14:07:58.511: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903674, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903674, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903674, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903674, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 29 14:08:01.533: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:08:01.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9754" for this suite.
STEP: Destroying namespace "webhook-9754-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.579 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":276,"completed":156,"skipped":2424,"failed":0}
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:08:01.967: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-627
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-projected-all-test-volume-0ebadd59-928b-4741-b22f-039707fde794
STEP: Creating secret with name secret-projected-all-test-volume-6cd818b2-1a77-45d1-9afc-b5b29fac0a49
STEP: Creating a pod to test Check all projections for projected volume plugin
Jan 29 14:08:02.301: INFO: Waiting up to 5m0s for pod "projected-volume-887c1024-cbb3-4551-9512-505d9b0fa424" in namespace "projected-627" to be "success or failure"
Jan 29 14:08:02.310: INFO: Pod "projected-volume-887c1024-cbb3-4551-9512-505d9b0fa424": Phase="Pending", Reason="", readiness=false. Elapsed: 8.914397ms
Jan 29 14:08:04.339: INFO: Pod "projected-volume-887c1024-cbb3-4551-9512-505d9b0fa424": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038023029s
Jan 29 14:08:06.348: INFO: Pod "projected-volume-887c1024-cbb3-4551-9512-505d9b0fa424": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047372475s
Jan 29 14:08:08.355: INFO: Pod "projected-volume-887c1024-cbb3-4551-9512-505d9b0fa424": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.054449141s
STEP: Saw pod success
Jan 29 14:08:08.355: INFO: Pod "projected-volume-887c1024-cbb3-4551-9512-505d9b0fa424" satisfied condition "success or failure"
Jan 29 14:08:08.362: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod projected-volume-887c1024-cbb3-4551-9512-505d9b0fa424 container projected-all-volume-test: <nil>
STEP: delete the pod
Jan 29 14:08:08.488: INFO: Waiting for pod projected-volume-887c1024-cbb3-4551-9512-505d9b0fa424 to disappear
Jan 29 14:08:08.496: INFO: Pod projected-volume-887c1024-cbb3-4551-9512-505d9b0fa424 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:08:08.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-627" for this suite.

• [SLOW TEST:6.575 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":276,"completed":157,"skipped":2424,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:08:08.543: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-9381
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 29 14:08:08.898: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jan 29 14:08:08.982: INFO: Number of nodes with available pods: 0
Jan 29 14:08:08.982: INFO: Node metakube-worker-cmccl-6d88bd94fc-87n7l is running more than one daemon pod
Jan 29 14:08:10.005: INFO: Number of nodes with available pods: 0
Jan 29 14:08:10.005: INFO: Node metakube-worker-cmccl-6d88bd94fc-87n7l is running more than one daemon pod
Jan 29 14:08:11.022: INFO: Number of nodes with available pods: 0
Jan 29 14:08:11.022: INFO: Node metakube-worker-cmccl-6d88bd94fc-87n7l is running more than one daemon pod
Jan 29 14:08:12.174: INFO: Number of nodes with available pods: 2
Jan 29 14:08:12.174: INFO: Node metakube-worker-cmccl-6d88bd94fc-lqfxz is running more than one daemon pod
Jan 29 14:08:13.003: INFO: Number of nodes with available pods: 2
Jan 29 14:08:13.003: INFO: Node metakube-worker-cmccl-6d88bd94fc-lqfxz is running more than one daemon pod
Jan 29 14:08:14.018: INFO: Number of nodes with available pods: 2
Jan 29 14:08:14.018: INFO: Node metakube-worker-cmccl-6d88bd94fc-lqfxz is running more than one daemon pod
Jan 29 14:08:15.006: INFO: Number of nodes with available pods: 2
Jan 29 14:08:15.006: INFO: Node metakube-worker-cmccl-6d88bd94fc-lqfxz is running more than one daemon pod
Jan 29 14:08:16.005: INFO: Number of nodes with available pods: 3
Jan 29 14:08:16.005: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jan 29 14:08:16.385: INFO: Wrong image for pod: daemon-set-tdnz4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:16.385: INFO: Wrong image for pod: daemon-set-w6kfl. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:16.385: INFO: Wrong image for pod: daemon-set-whsqf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:17.430: INFO: Wrong image for pod: daemon-set-tdnz4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:17.430: INFO: Wrong image for pod: daemon-set-w6kfl. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:17.430: INFO: Wrong image for pod: daemon-set-whsqf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:18.432: INFO: Wrong image for pod: daemon-set-tdnz4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:18.432: INFO: Wrong image for pod: daemon-set-w6kfl. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:18.432: INFO: Wrong image for pod: daemon-set-whsqf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:19.432: INFO: Wrong image for pod: daemon-set-tdnz4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:19.432: INFO: Wrong image for pod: daemon-set-w6kfl. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:19.433: INFO: Pod daemon-set-w6kfl is not available
Jan 29 14:08:19.433: INFO: Wrong image for pod: daemon-set-whsqf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:20.447: INFO: Wrong image for pod: daemon-set-tdnz4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:20.447: INFO: Wrong image for pod: daemon-set-w6kfl. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:20.447: INFO: Pod daemon-set-w6kfl is not available
Jan 29 14:08:20.447: INFO: Wrong image for pod: daemon-set-whsqf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:21.429: INFO: Wrong image for pod: daemon-set-tdnz4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:21.429: INFO: Wrong image for pod: daemon-set-w6kfl. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:21.429: INFO: Pod daemon-set-w6kfl is not available
Jan 29 14:08:21.429: INFO: Wrong image for pod: daemon-set-whsqf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:22.429: INFO: Wrong image for pod: daemon-set-tdnz4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:22.430: INFO: Wrong image for pod: daemon-set-w6kfl. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:22.430: INFO: Pod daemon-set-w6kfl is not available
Jan 29 14:08:22.430: INFO: Wrong image for pod: daemon-set-whsqf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:23.438: INFO: Wrong image for pod: daemon-set-tdnz4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:23.438: INFO: Wrong image for pod: daemon-set-whsqf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:23.438: INFO: Pod daemon-set-wqq86 is not available
Jan 29 14:08:24.430: INFO: Wrong image for pod: daemon-set-tdnz4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:24.430: INFO: Wrong image for pod: daemon-set-whsqf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:24.430: INFO: Pod daemon-set-wqq86 is not available
Jan 29 14:08:25.436: INFO: Wrong image for pod: daemon-set-tdnz4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:25.436: INFO: Wrong image for pod: daemon-set-whsqf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:25.436: INFO: Pod daemon-set-wqq86 is not available
Jan 29 14:08:26.427: INFO: Wrong image for pod: daemon-set-tdnz4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:26.428: INFO: Wrong image for pod: daemon-set-whsqf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:27.463: INFO: Wrong image for pod: daemon-set-tdnz4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:27.463: INFO: Wrong image for pod: daemon-set-whsqf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:27.463: INFO: Pod daemon-set-whsqf is not available
Jan 29 14:08:28.444: INFO: Wrong image for pod: daemon-set-tdnz4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:28.444: INFO: Wrong image for pod: daemon-set-whsqf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:28.444: INFO: Pod daemon-set-whsqf is not available
Jan 29 14:08:29.428: INFO: Wrong image for pod: daemon-set-tdnz4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:29.428: INFO: Wrong image for pod: daemon-set-whsqf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:29.428: INFO: Pod daemon-set-whsqf is not available
Jan 29 14:08:30.455: INFO: Wrong image for pod: daemon-set-tdnz4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:30.455: INFO: Wrong image for pod: daemon-set-whsqf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:30.455: INFO: Pod daemon-set-whsqf is not available
Jan 29 14:08:31.667: INFO: Pod daemon-set-q429k is not available
Jan 29 14:08:31.667: INFO: Wrong image for pod: daemon-set-tdnz4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:32.433: INFO: Pod daemon-set-q429k is not available
Jan 29 14:08:32.433: INFO: Wrong image for pod: daemon-set-tdnz4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:33.452: INFO: Pod daemon-set-q429k is not available
Jan 29 14:08:33.452: INFO: Wrong image for pod: daemon-set-tdnz4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:34.430: INFO: Pod daemon-set-q429k is not available
Jan 29 14:08:34.430: INFO: Wrong image for pod: daemon-set-tdnz4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:35.439: INFO: Wrong image for pod: daemon-set-tdnz4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:36.430: INFO: Wrong image for pod: daemon-set-tdnz4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:37.430: INFO: Wrong image for pod: daemon-set-tdnz4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:37.430: INFO: Pod daemon-set-tdnz4 is not available
Jan 29 14:08:38.429: INFO: Wrong image for pod: daemon-set-tdnz4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:38.429: INFO: Pod daemon-set-tdnz4 is not available
Jan 29 14:08:39.430: INFO: Wrong image for pod: daemon-set-tdnz4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:39.430: INFO: Pod daemon-set-tdnz4 is not available
Jan 29 14:08:40.436: INFO: Wrong image for pod: daemon-set-tdnz4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:40.436: INFO: Pod daemon-set-tdnz4 is not available
Jan 29 14:08:41.437: INFO: Wrong image for pod: daemon-set-tdnz4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:41.437: INFO: Pod daemon-set-tdnz4 is not available
Jan 29 14:08:42.433: INFO: Wrong image for pod: daemon-set-tdnz4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:42.433: INFO: Pod daemon-set-tdnz4 is not available
Jan 29 14:08:43.429: INFO: Wrong image for pod: daemon-set-tdnz4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:43.429: INFO: Pod daemon-set-tdnz4 is not available
Jan 29 14:08:44.433: INFO: Wrong image for pod: daemon-set-tdnz4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:44.433: INFO: Pod daemon-set-tdnz4 is not available
Jan 29 14:08:45.430: INFO: Wrong image for pod: daemon-set-tdnz4. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 29 14:08:45.430: INFO: Pod daemon-set-tdnz4 is not available
Jan 29 14:08:46.445: INFO: Pod daemon-set-54rln is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Jan 29 14:08:46.506: INFO: Number of nodes with available pods: 2
Jan 29 14:08:46.507: INFO: Node metakube-worker-cmccl-6d88bd94fc-lqfxz is running more than one daemon pod
Jan 29 14:08:47.527: INFO: Number of nodes with available pods: 2
Jan 29 14:08:47.528: INFO: Node metakube-worker-cmccl-6d88bd94fc-lqfxz is running more than one daemon pod
Jan 29 14:08:48.543: INFO: Number of nodes with available pods: 2
Jan 29 14:08:48.543: INFO: Node metakube-worker-cmccl-6d88bd94fc-lqfxz is running more than one daemon pod
Jan 29 14:08:49.983: INFO: Number of nodes with available pods: 2
Jan 29 14:08:49.983: INFO: Node metakube-worker-cmccl-6d88bd94fc-lqfxz is running more than one daemon pod
Jan 29 14:08:50.559: INFO: Number of nodes with available pods: 2
Jan 29 14:08:50.559: INFO: Node metakube-worker-cmccl-6d88bd94fc-lqfxz is running more than one daemon pod
Jan 29 14:08:51.535: INFO: Number of nodes with available pods: 3
Jan 29 14:08:51.536: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9381, will wait for the garbage collector to delete the pods
Jan 29 14:08:51.664: INFO: Deleting DaemonSet.extensions daemon-set took: 19.201236ms
Jan 29 14:08:52.364: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.233965ms
Jan 29 14:09:03.177: INFO: Number of nodes with available pods: 0
Jan 29 14:09:03.177: INFO: Number of running nodes: 0, number of available pods: 0
Jan 29 14:09:03.187: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9381/daemonsets","resourceVersion":"79141"},"items":null}

Jan 29 14:09:03.195: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9381/pods","resourceVersion":"79141"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:09:03.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9381" for this suite.

• [SLOW TEST:54.731 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":276,"completed":158,"skipped":2431,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:09:03.276: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5415
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name secret-emptykey-test-2131b638-5822-44cb-825a-17ab9174a02f
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:09:03.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5415" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":276,"completed":159,"skipped":2435,"failed":0}
SSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:09:03.679: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8235
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8235 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8235;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8235 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8235;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8235.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8235.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8235.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8235.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8235.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8235.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8235.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8235.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8235.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8235.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8235.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8235.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8235.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 132.29.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.29.132_udp@PTR;check="$$(dig +tcp +noall +answer +search 132.29.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.29.132_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8235 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8235;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8235 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8235;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8235.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8235.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8235.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8235.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8235.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8235.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8235.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8235.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8235.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8235.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8235.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8235.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8235.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 132.29.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.29.132_udp@PTR;check="$$(dig +tcp +noall +answer +search 132.29.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.29.132_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 29 14:09:10.204: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8235/dns-test-27d10da5-2535-4261-ae25-ed825378f0b7: the server could not find the requested resource (get pods dns-test-27d10da5-2535-4261-ae25-ed825378f0b7)
Jan 29 14:09:10.252: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8235/dns-test-27d10da5-2535-4261-ae25-ed825378f0b7: the server could not find the requested resource (get pods dns-test-27d10da5-2535-4261-ae25-ed825378f0b7)
Jan 29 14:09:10.380: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8235 from pod dns-8235/dns-test-27d10da5-2535-4261-ae25-ed825378f0b7: the server could not find the requested resource (get pods dns-test-27d10da5-2535-4261-ae25-ed825378f0b7)
Jan 29 14:09:10.506: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8235.svc from pod dns-8235/dns-test-27d10da5-2535-4261-ae25-ed825378f0b7: the server could not find the requested resource (get pods dns-test-27d10da5-2535-4261-ae25-ed825378f0b7)
Jan 29 14:09:10.524: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8235.svc from pod dns-8235/dns-test-27d10da5-2535-4261-ae25-ed825378f0b7: the server could not find the requested resource (get pods dns-test-27d10da5-2535-4261-ae25-ed825378f0b7)
Jan 29 14:09:10.542: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8235.svc from pod dns-8235/dns-test-27d10da5-2535-4261-ae25-ed825378f0b7: the server could not find the requested resource (get pods dns-test-27d10da5-2535-4261-ae25-ed825378f0b7)
Jan 29 14:09:11.077: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8235/dns-test-27d10da5-2535-4261-ae25-ed825378f0b7: the server could not find the requested resource (get pods dns-test-27d10da5-2535-4261-ae25-ed825378f0b7)
Jan 29 14:09:11.130: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8235/dns-test-27d10da5-2535-4261-ae25-ed825378f0b7: the server could not find the requested resource (get pods dns-test-27d10da5-2535-4261-ae25-ed825378f0b7)
Jan 29 14:09:11.145: INFO: Unable to read jessie_udp@dns-test-service.dns-8235 from pod dns-8235/dns-test-27d10da5-2535-4261-ae25-ed825378f0b7: the server could not find the requested resource (get pods dns-test-27d10da5-2535-4261-ae25-ed825378f0b7)
Jan 29 14:09:11.175: INFO: Unable to read jessie_tcp@dns-test-service.dns-8235 from pod dns-8235/dns-test-27d10da5-2535-4261-ae25-ed825378f0b7: the server could not find the requested resource (get pods dns-test-27d10da5-2535-4261-ae25-ed825378f0b7)
Jan 29 14:09:11.209: INFO: Unable to read jessie_udp@dns-test-service.dns-8235.svc from pod dns-8235/dns-test-27d10da5-2535-4261-ae25-ed825378f0b7: the server could not find the requested resource (get pods dns-test-27d10da5-2535-4261-ae25-ed825378f0b7)
Jan 29 14:09:11.228: INFO: Unable to read jessie_tcp@dns-test-service.dns-8235.svc from pod dns-8235/dns-test-27d10da5-2535-4261-ae25-ed825378f0b7: the server could not find the requested resource (get pods dns-test-27d10da5-2535-4261-ae25-ed825378f0b7)
Jan 29 14:09:11.264: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8235.svc from pod dns-8235/dns-test-27d10da5-2535-4261-ae25-ed825378f0b7: the server could not find the requested resource (get pods dns-test-27d10da5-2535-4261-ae25-ed825378f0b7)
Jan 29 14:09:11.312: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8235.svc from pod dns-8235/dns-test-27d10da5-2535-4261-ae25-ed825378f0b7: the server could not find the requested resource (get pods dns-test-27d10da5-2535-4261-ae25-ed825378f0b7)
Jan 29 14:09:11.869: INFO: Lookups using dns-8235/dns-test-27d10da5-2535-4261-ae25-ed825378f0b7 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_tcp@dns-test-service.dns-8235 wheezy_tcp@dns-test-service.dns-8235.svc wheezy_udp@_http._tcp.dns-test-service.dns-8235.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8235.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8235 jessie_tcp@dns-test-service.dns-8235 jessie_udp@dns-test-service.dns-8235.svc jessie_tcp@dns-test-service.dns-8235.svc jessie_udp@_http._tcp.dns-test-service.dns-8235.svc jessie_tcp@_http._tcp.dns-test-service.dns-8235.svc]

Jan 29 14:09:19.342: INFO: DNS probes using dns-8235/dns-test-27d10da5-2535-4261-ae25-ed825378f0b7 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:09:19.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8235" for this suite.

• [SLOW TEST:15.922 seconds]
[sig-network] DNS
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":276,"completed":160,"skipped":2439,"failed":0}
S
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:09:19.601: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6005
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6005.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6005.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6005.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6005.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6005.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6005.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6005.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6005.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6005.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6005.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6005.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6005.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6005.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 23.28.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.28.23_udp@PTR;check="$$(dig +tcp +noall +answer +search 23.28.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.28.23_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6005.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6005.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6005.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6005.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6005.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6005.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6005.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6005.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6005.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6005.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6005.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6005.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6005.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 23.28.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.28.23_udp@PTR;check="$$(dig +tcp +noall +answer +search 23.28.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.28.23_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 29 14:09:26.213: INFO: Unable to read wheezy_udp@dns-test-service.dns-6005.svc.cluster.local from pod dns-6005/dns-test-212337d1-4bad-4490-a204-fc875f02deef: the server could not find the requested resource (get pods dns-test-212337d1-4bad-4490-a204-fc875f02deef)
Jan 29 14:09:26.264: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6005.svc.cluster.local from pod dns-6005/dns-test-212337d1-4bad-4490-a204-fc875f02deef: the server could not find the requested resource (get pods dns-test-212337d1-4bad-4490-a204-fc875f02deef)
Jan 29 14:09:26.299: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6005.svc.cluster.local from pod dns-6005/dns-test-212337d1-4bad-4490-a204-fc875f02deef: the server could not find the requested resource (get pods dns-test-212337d1-4bad-4490-a204-fc875f02deef)
Jan 29 14:09:26.314: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6005.svc.cluster.local from pod dns-6005/dns-test-212337d1-4bad-4490-a204-fc875f02deef: the server could not find the requested resource (get pods dns-test-212337d1-4bad-4490-a204-fc875f02deef)
Jan 29 14:09:26.941: INFO: Unable to read jessie_udp@dns-test-service.dns-6005.svc.cluster.local from pod dns-6005/dns-test-212337d1-4bad-4490-a204-fc875f02deef: the server could not find the requested resource (get pods dns-test-212337d1-4bad-4490-a204-fc875f02deef)
Jan 29 14:09:26.975: INFO: Unable to read jessie_tcp@dns-test-service.dns-6005.svc.cluster.local from pod dns-6005/dns-test-212337d1-4bad-4490-a204-fc875f02deef: the server could not find the requested resource (get pods dns-test-212337d1-4bad-4490-a204-fc875f02deef)
Jan 29 14:09:26.994: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6005.svc.cluster.local from pod dns-6005/dns-test-212337d1-4bad-4490-a204-fc875f02deef: the server could not find the requested resource (get pods dns-test-212337d1-4bad-4490-a204-fc875f02deef)
Jan 29 14:09:27.010: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6005.svc.cluster.local from pod dns-6005/dns-test-212337d1-4bad-4490-a204-fc875f02deef: the server could not find the requested resource (get pods dns-test-212337d1-4bad-4490-a204-fc875f02deef)
Jan 29 14:09:27.542: INFO: Lookups using dns-6005/dns-test-212337d1-4bad-4490-a204-fc875f02deef failed for: [wheezy_udp@dns-test-service.dns-6005.svc.cluster.local wheezy_tcp@dns-test-service.dns-6005.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6005.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6005.svc.cluster.local jessie_udp@dns-test-service.dns-6005.svc.cluster.local jessie_tcp@dns-test-service.dns-6005.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6005.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6005.svc.cluster.local]

Jan 29 14:09:34.149: INFO: DNS probes using dns-6005/dns-test-212337d1-4bad-4490-a204-fc875f02deef succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:09:34.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6005" for this suite.

• [SLOW TEST:14.828 seconds]
[sig-network] DNS
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":276,"completed":161,"skipped":2440,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:09:34.433: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2624
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Jan 29 14:09:39.576: INFO: Successfully updated pod "labelsupdate5df6d6c3-f38e-44f8-bc1a-43981ed52127"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:09:41.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2624" for this suite.

• [SLOW TEST:7.263 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":276,"completed":162,"skipped":2443,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:09:41.705: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7309
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod busybox-74afbe9a-8aac-4fd6-beb6-370acd36dc53 in namespace container-probe-7309
Jan 29 14:09:46.064: INFO: Started pod busybox-74afbe9a-8aac-4fd6-beb6-370acd36dc53 in namespace container-probe-7309
STEP: checking the pod's current state and verifying that restartCount is present
Jan 29 14:09:46.071: INFO: Initial restart count of pod busybox-74afbe9a-8aac-4fd6-beb6-370acd36dc53 is 0
Jan 29 14:10:36.916: INFO: Restart count of pod container-probe-7309/busybox-74afbe9a-8aac-4fd6-beb6-370acd36dc53 is now 1 (50.844492404s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:10:37.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7309" for this suite.

• [SLOW TEST:55.836 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":276,"completed":163,"skipped":2467,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:10:37.541: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7836
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap configmap-7836/configmap-test-edc0ccb9-44ab-4a37-8b19-7b211c909980
STEP: Creating a pod to test consume configMaps
Jan 29 14:10:38.073: INFO: Waiting up to 5m0s for pod "pod-configmaps-0ca1493e-3ae6-414b-a30c-f30e411394e4" in namespace "configmap-7836" to be "success or failure"
Jan 29 14:10:38.086: INFO: Pod "pod-configmaps-0ca1493e-3ae6-414b-a30c-f30e411394e4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.984031ms
Jan 29 14:10:40.098: INFO: Pod "pod-configmaps-0ca1493e-3ae6-414b-a30c-f30e411394e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024540195s
Jan 29 14:10:42.106: INFO: Pod "pod-configmaps-0ca1493e-3ae6-414b-a30c-f30e411394e4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032328384s
Jan 29 14:10:44.125: INFO: Pod "pod-configmaps-0ca1493e-3ae6-414b-a30c-f30e411394e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.051621448s
STEP: Saw pod success
Jan 29 14:10:44.125: INFO: Pod "pod-configmaps-0ca1493e-3ae6-414b-a30c-f30e411394e4" satisfied condition "success or failure"
Jan 29 14:10:44.140: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-configmaps-0ca1493e-3ae6-414b-a30c-f30e411394e4 container env-test: <nil>
STEP: delete the pod
Jan 29 14:10:44.205: INFO: Waiting for pod pod-configmaps-0ca1493e-3ae6-414b-a30c-f30e411394e4 to disappear
Jan 29 14:10:44.215: INFO: Pod pod-configmaps-0ca1493e-3ae6-414b-a30c-f30e411394e4 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:10:44.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7836" for this suite.

• [SLOW TEST:6.700 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":276,"completed":164,"skipped":2478,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:10:44.243: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2527
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Jan 29 14:10:49.129: INFO: Successfully updated pod "annotationupdate734be9d7-0a63-4eb5-8046-a0727bcf1cc8"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:10:51.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2527" for this suite.

• [SLOW TEST:7.021 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":276,"completed":165,"skipped":2501,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:10:51.265: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-3749
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 29 14:10:51.640: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 29 14:10:55.683: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jan 29 14:10:57.691: INFO: Creating deployment "test-rollover-deployment"
Jan 29 14:10:57.735: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jan 29 14:10:59.767: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jan 29 14:10:59.805: INFO: Ensure that both replica sets have 1 created replica
Jan 29 14:10:59.832: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jan 29 14:10:59.864: INFO: Updating deployment test-rollover-deployment
Jan 29 14:10:59.865: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jan 29 14:11:01.902: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jan 29 14:11:01.931: INFO: Make sure deployment "test-rollover-deployment" is complete
Jan 29 14:11:01.950: INFO: all replica sets need to contain the pod-template-hash label
Jan 29 14:11:01.950: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903857, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903857, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903860, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903857, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 14:11:03.979: INFO: all replica sets need to contain the pod-template-hash label
Jan 29 14:11:03.979: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903857, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903857, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903863, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903857, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 14:11:05.969: INFO: all replica sets need to contain the pod-template-hash label
Jan 29 14:11:05.970: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903857, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903857, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903863, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903857, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 14:11:07.974: INFO: all replica sets need to contain the pod-template-hash label
Jan 29 14:11:07.975: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903857, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903857, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903863, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903857, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 14:11:10.016: INFO: all replica sets need to contain the pod-template-hash label
Jan 29 14:11:10.017: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903857, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903857, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903863, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903857, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 14:11:12.030: INFO: all replica sets need to contain the pod-template-hash label
Jan 29 14:11:12.030: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903857, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903857, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903863, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715903857, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 14:11:13.992: INFO: 
Jan 29 14:11:13.992: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Jan 29 14:11:14.086: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-3749 /apis/apps/v1/namespaces/deployment-3749/deployments/test-rollover-deployment a1e71906-af45-42a6-be0e-110d748b5995 80058 2 2020-01-29 14:10:57 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002f3d358 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-01-29 14:10:57 +0000 UTC,LastTransitionTime:2020-01-29 14:10:57 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-574d6dfbff" has successfully progressed.,LastUpdateTime:2020-01-29 14:11:13 +0000 UTC,LastTransitionTime:2020-01-29 14:10:57 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 29 14:11:14.099: INFO: New ReplicaSet "test-rollover-deployment-574d6dfbff" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-574d6dfbff  deployment-3749 /apis/apps/v1/namespaces/deployment-3749/replicasets/test-rollover-deployment-574d6dfbff 7f805d0d-d53f-4709-bc9f-3c8fcecde0ef 80047 2 2020-01-29 14:10:59 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment a1e71906-af45-42a6-be0e-110d748b5995 0xc004bab6c7 0xc004bab6c8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 574d6dfbff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004bab738 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 29 14:11:14.099: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jan 29 14:11:14.100: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-3749 /apis/apps/v1/namespaces/deployment-3749/replicasets/test-rollover-controller ab9df557-014e-4ae1-bb3a-735c4ec2a2c8 80056 2 2020-01-29 14:10:51 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment a1e71906-af45-42a6-be0e-110d748b5995 0xc004bab5e7 0xc004bab5e8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004bab658 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 29 14:11:14.100: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-3749 /apis/apps/v1/namespaces/deployment-3749/replicasets/test-rollover-deployment-f6c94f66c 20a281c3-dad9-4c8f-ada3-ed1aa915ad2c 79975 2 2020-01-29 14:10:57 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment a1e71906-af45-42a6-be0e-110d748b5995 0xc004bab7b0 0xc004bab7b1}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004bab828 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 29 14:11:14.128: INFO: Pod "test-rollover-deployment-574d6dfbff-tnj49" is available:
&Pod{ObjectMeta:{test-rollover-deployment-574d6dfbff-tnj49 test-rollover-deployment-574d6dfbff- deployment-3749 /api/v1/namespaces/deployment-3749/pods/test-rollover-deployment-574d6dfbff-tnj49 f6b04b8e-05ee-46bb-a97e-6d4a14ee198f 80003 0 2020-01-29 14:10:59 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[cni.projectcalico.org/podIP:172.25.2.183/32 cni.projectcalico.org/podIPs:172.25.2.183/32] [{apps/v1 ReplicaSet test-rollover-deployment-574d6dfbff 7f805d0d-d53f-4709-bc9f-3c8fcecde0ef 0xc004babdb7 0xc004babdb8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-txpv8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-txpv8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-txpv8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metakube-worker-cmccl-6d88bd94fc-lqfxz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 14:11:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 14:11:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 14:11:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-29 14:10:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.5,PodIP:172.25.2.183,StartTime:2020-01-29 14:11:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-29 14:11:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:docker://f490d5bae7506abfdfdc8aa00d7624ec3145097a23117a616f5ef1f49c370c49,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.183,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:11:14.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3749" for this suite.

• [SLOW TEST:22.925 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":276,"completed":166,"skipped":2513,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:11:14.198: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2020
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod liveness-c072a23f-56a9-4d3e-a727-4e43270d46a5 in namespace container-probe-2020
Jan 29 14:11:20.627: INFO: Started pod liveness-c072a23f-56a9-4d3e-a727-4e43270d46a5 in namespace container-probe-2020
STEP: checking the pod's current state and verifying that restartCount is present
Jan 29 14:11:20.637: INFO: Initial restart count of pod liveness-c072a23f-56a9-4d3e-a727-4e43270d46a5 is 0
Jan 29 14:11:37.095: INFO: Restart count of pod container-probe-2020/liveness-c072a23f-56a9-4d3e-a727-4e43270d46a5 is now 1 (16.458578422s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:11:37.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2020" for this suite.

• [SLOW TEST:23.058 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":276,"completed":167,"skipped":2537,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:11:37.261: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2742
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod busybox-21abd8a3-6804-46ba-9314-1c474c43fbcb in namespace container-probe-2742
Jan 29 14:11:41.735: INFO: Started pod busybox-21abd8a3-6804-46ba-9314-1c474c43fbcb in namespace container-probe-2742
STEP: checking the pod's current state and verifying that restartCount is present
Jan 29 14:11:41.744: INFO: Initial restart count of pod busybox-21abd8a3-6804-46ba-9314-1c474c43fbcb is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:15:42.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2742" for this suite.

• [SLOW TEST:245.299 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":276,"completed":168,"skipped":2567,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:15:42.562: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9895
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:15:54.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9895" for this suite.

• [SLOW TEST:11.477 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":276,"completed":169,"skipped":2580,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:15:54.041: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-471
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Jan 29 14:15:54.367: INFO: Waiting up to 5m0s for pod "downward-api-e5410498-2186-423e-b683-456892475a7f" in namespace "downward-api-471" to be "success or failure"
Jan 29 14:15:54.384: INFO: Pod "downward-api-e5410498-2186-423e-b683-456892475a7f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.859021ms
Jan 29 14:15:56.394: INFO: Pod "downward-api-e5410498-2186-423e-b683-456892475a7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026752057s
Jan 29 14:15:58.417: INFO: Pod "downward-api-e5410498-2186-423e-b683-456892475a7f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049476331s
STEP: Saw pod success
Jan 29 14:15:58.417: INFO: Pod "downward-api-e5410498-2186-423e-b683-456892475a7f" satisfied condition "success or failure"
Jan 29 14:15:58.435: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod downward-api-e5410498-2186-423e-b683-456892475a7f container dapi-container: <nil>
STEP: delete the pod
Jan 29 14:15:58.511: INFO: Waiting for pod downward-api-e5410498-2186-423e-b683-456892475a7f to disappear
Jan 29 14:15:58.520: INFO: Pod downward-api-e5410498-2186-423e-b683-456892475a7f no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:15:58.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-471" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":276,"completed":170,"skipped":2591,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:15:58.663: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3069
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-3069
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-3069
STEP: Creating statefulset with conflicting port in namespace statefulset-3069
STEP: Waiting until pod test-pod will start running in namespace statefulset-3069
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3069
Jan 29 14:16:05.032: INFO: Observed stateful pod in namespace: statefulset-3069, name: ss-0, uid: 3a9e0b46-1d49-4367-9e34-e85fd611d994, status phase: Pending. Waiting for statefulset controller to delete.
Jan 29 14:16:05.607: INFO: Observed stateful pod in namespace: statefulset-3069, name: ss-0, uid: 3a9e0b46-1d49-4367-9e34-e85fd611d994, status phase: Failed. Waiting for statefulset controller to delete.
Jan 29 14:16:05.624: INFO: Observed stateful pod in namespace: statefulset-3069, name: ss-0, uid: 3a9e0b46-1d49-4367-9e34-e85fd611d994, status phase: Failed. Waiting for statefulset controller to delete.
Jan 29 14:16:05.647: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3069
STEP: Removing pod with conflicting port in namespace statefulset-3069
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3069 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jan 29 14:16:09.754: INFO: Deleting all statefulset in ns statefulset-3069
Jan 29 14:16:09.764: INFO: Scaling statefulset ss to 0
Jan 29 14:16:29.915: INFO: Waiting for statefulset status.replicas updated to 0
Jan 29 14:16:29.929: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:16:30.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3069" for this suite.

• [SLOW TEST:31.423 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":276,"completed":171,"skipped":2645,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:16:30.092: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-4749
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jan 29 14:16:30.404: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4749 /api/v1/namespaces/watch-4749/configmaps/e2e-watch-test-configmap-a e2206b2f-e53b-434f-a539-7c1231416a8a 81621 0 2020-01-29 14:16:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 29 14:16:30.404: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4749 /api/v1/namespaces/watch-4749/configmaps/e2e-watch-test-configmap-a e2206b2f-e53b-434f-a539-7c1231416a8a 81621 0 2020-01-29 14:16:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jan 29 14:16:40.436: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4749 /api/v1/namespaces/watch-4749/configmaps/e2e-watch-test-configmap-a e2206b2f-e53b-434f-a539-7c1231416a8a 81696 0 2020-01-29 14:16:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jan 29 14:16:40.436: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4749 /api/v1/namespaces/watch-4749/configmaps/e2e-watch-test-configmap-a e2206b2f-e53b-434f-a539-7c1231416a8a 81696 0 2020-01-29 14:16:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jan 29 14:16:50.486: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4749 /api/v1/namespaces/watch-4749/configmaps/e2e-watch-test-configmap-a e2206b2f-e53b-434f-a539-7c1231416a8a 81738 0 2020-01-29 14:16:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 29 14:16:50.486: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4749 /api/v1/namespaces/watch-4749/configmaps/e2e-watch-test-configmap-a e2206b2f-e53b-434f-a539-7c1231416a8a 81738 0 2020-01-29 14:16:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jan 29 14:17:00.522: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4749 /api/v1/namespaces/watch-4749/configmaps/e2e-watch-test-configmap-a e2206b2f-e53b-434f-a539-7c1231416a8a 81778 0 2020-01-29 14:16:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 29 14:17:00.522: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4749 /api/v1/namespaces/watch-4749/configmaps/e2e-watch-test-configmap-a e2206b2f-e53b-434f-a539-7c1231416a8a 81778 0 2020-01-29 14:16:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jan 29 14:17:10.546: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4749 /api/v1/namespaces/watch-4749/configmaps/e2e-watch-test-configmap-b 82e6efc2-fee2-4383-904c-954ee7159086 81820 0 2020-01-29 14:17:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 29 14:17:10.546: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4749 /api/v1/namespaces/watch-4749/configmaps/e2e-watch-test-configmap-b 82e6efc2-fee2-4383-904c-954ee7159086 81820 0 2020-01-29 14:17:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jan 29 14:17:20.587: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4749 /api/v1/namespaces/watch-4749/configmaps/e2e-watch-test-configmap-b 82e6efc2-fee2-4383-904c-954ee7159086 81862 0 2020-01-29 14:17:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 29 14:17:20.587: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4749 /api/v1/namespaces/watch-4749/configmaps/e2e-watch-test-configmap-b 82e6efc2-fee2-4383-904c-954ee7159086 81862 0 2020-01-29 14:17:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:17:30.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4749" for this suite.

• [SLOW TEST:60.537 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":276,"completed":172,"skipped":2657,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:17:30.635: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6111
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-5a9bc1af-f499-485b-bdd2-d0349845ee99
STEP: Creating a pod to test consume configMaps
Jan 29 14:17:30.922: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-63af90b1-289c-42eb-933e-a8929d7ed100" in namespace "projected-6111" to be "success or failure"
Jan 29 14:17:30.936: INFO: Pod "pod-projected-configmaps-63af90b1-289c-42eb-933e-a8929d7ed100": Phase="Pending", Reason="", readiness=false. Elapsed: 14.112227ms
Jan 29 14:17:32.946: INFO: Pod "pod-projected-configmaps-63af90b1-289c-42eb-933e-a8929d7ed100": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024394902s
Jan 29 14:17:34.961: INFO: Pod "pod-projected-configmaps-63af90b1-289c-42eb-933e-a8929d7ed100": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039118375s
STEP: Saw pod success
Jan 29 14:17:34.961: INFO: Pod "pod-projected-configmaps-63af90b1-289c-42eb-933e-a8929d7ed100" satisfied condition "success or failure"
Jan 29 14:17:34.972: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-projected-configmaps-63af90b1-289c-42eb-933e-a8929d7ed100 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 29 14:17:35.041: INFO: Waiting for pod pod-projected-configmaps-63af90b1-289c-42eb-933e-a8929d7ed100 to disappear
Jan 29 14:17:35.063: INFO: Pod pod-projected-configmaps-63af90b1-289c-42eb-933e-a8929d7ed100 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:17:35.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6111" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":276,"completed":173,"skipped":2683,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:17:35.278: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7736
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 29 14:17:35.552: INFO: Waiting up to 5m0s for pod "downwardapi-volume-66775fc5-2962-46fe-9d77-7ba5bcd6a829" in namespace "projected-7736" to be "success or failure"
Jan 29 14:17:35.558: INFO: Pod "downwardapi-volume-66775fc5-2962-46fe-9d77-7ba5bcd6a829": Phase="Pending", Reason="", readiness=false. Elapsed: 6.280381ms
Jan 29 14:17:37.566: INFO: Pod "downwardapi-volume-66775fc5-2962-46fe-9d77-7ba5bcd6a829": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014558108s
Jan 29 14:17:39.575: INFO: Pod "downwardapi-volume-66775fc5-2962-46fe-9d77-7ba5bcd6a829": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023803687s
Jan 29 14:17:41.586: INFO: Pod "downwardapi-volume-66775fc5-2962-46fe-9d77-7ba5bcd6a829": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033963526s
STEP: Saw pod success
Jan 29 14:17:41.586: INFO: Pod "downwardapi-volume-66775fc5-2962-46fe-9d77-7ba5bcd6a829" satisfied condition "success or failure"
Jan 29 14:17:41.595: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod downwardapi-volume-66775fc5-2962-46fe-9d77-7ba5bcd6a829 container client-container: <nil>
STEP: delete the pod
Jan 29 14:17:41.683: INFO: Waiting for pod downwardapi-volume-66775fc5-2962-46fe-9d77-7ba5bcd6a829 to disappear
Jan 29 14:17:41.689: INFO: Pod downwardapi-volume-66775fc5-2962-46fe-9d77-7ba5bcd6a829 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:17:41.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7736" for this suite.

• [SLOW TEST:6.445 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":276,"completed":174,"skipped":2690,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:17:41.729: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9727
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Jan 29 14:17:41.995: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
Jan 29 14:17:45.986: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:18:02.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9727" for this suite.

• [SLOW TEST:20.480 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":276,"completed":175,"skipped":2691,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:18:02.216: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9361
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1861
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jan 29 14:18:02.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-9361'
Jan 29 14:18:05.150: INFO: stderr: ""
Jan 29 14:18:05.150: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1866
Jan 29 14:18:05.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 delete pods e2e-test-httpd-pod --namespace=kubectl-9361'
Jan 29 14:18:06.941: INFO: stderr: ""
Jan 29 14:18:06.941: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:18:06.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9361" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":276,"completed":176,"skipped":2702,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:18:07.150: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1953
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl run job
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1788
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jan 29 14:18:07.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-1953'
Jan 29 14:18:08.477: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan 29 14:18:08.477: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1793
Jan 29 14:18:08.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 delete jobs e2e-test-httpd-job --namespace=kubectl-1953'
Jan 29 14:18:08.653: INFO: stderr: ""
Jan 29 14:18:08.653: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:18:08.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1953" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run job should create a job from an image when restart is OnFailure  [Conformance]","total":276,"completed":177,"skipped":2709,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:18:09.131: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6821
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jan 29 14:18:11.424: INFO: Waiting up to 5m0s for pod "pod-82af6dbf-d53f-43c4-ac2f-e31ef5ffbab3" in namespace "emptydir-6821" to be "success or failure"
Jan 29 14:18:11.430: INFO: Pod "pod-82af6dbf-d53f-43c4-ac2f-e31ef5ffbab3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.840004ms
Jan 29 14:18:13.447: INFO: Pod "pod-82af6dbf-d53f-43c4-ac2f-e31ef5ffbab3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023154795s
Jan 29 14:18:15.601: INFO: Pod "pod-82af6dbf-d53f-43c4-ac2f-e31ef5ffbab3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.177111909s
STEP: Saw pod success
Jan 29 14:18:15.601: INFO: Pod "pod-82af6dbf-d53f-43c4-ac2f-e31ef5ffbab3" satisfied condition "success or failure"
Jan 29 14:18:15.617: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-82af6dbf-d53f-43c4-ac2f-e31ef5ffbab3 container test-container: <nil>
STEP: delete the pod
Jan 29 14:18:15.723: INFO: Waiting for pod pod-82af6dbf-d53f-43c4-ac2f-e31ef5ffbab3 to disappear
Jan 29 14:18:15.731: INFO: Pod pod-82af6dbf-d53f-43c4-ac2f-e31ef5ffbab3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:18:15.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6821" for this suite.

• [SLOW TEST:6.626 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":276,"completed":178,"skipped":2752,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:18:15.758: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-1051
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6588
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-5929
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:18:34.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1051" for this suite.
STEP: Destroying namespace "nsdeletetest-6588" for this suite.
Jan 29 14:18:34.852: INFO: Namespace nsdeletetest-6588 was already deleted
STEP: Destroying namespace "nsdeletetest-5929" for this suite.

• [SLOW TEST:19.114 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":276,"completed":179,"skipped":2776,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:18:34.873: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2773
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating Agnhost RC
Jan 29 14:18:35.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 create -f - --namespace=kubectl-2773'
Jan 29 14:18:35.830: INFO: stderr: ""
Jan 29 14:18:35.830: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Jan 29 14:18:36.851: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 29 14:18:36.851: INFO: Found 0 / 1
Jan 29 14:18:37.844: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 29 14:18:37.845: INFO: Found 0 / 1
Jan 29 14:18:38.841: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 29 14:18:38.841: INFO: Found 0 / 1
Jan 29 14:18:39.864: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 29 14:18:39.864: INFO: Found 0 / 1
Jan 29 14:18:41.513: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 29 14:18:41.513: INFO: Found 1 / 1
Jan 29 14:18:41.513: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jan 29 14:18:41.537: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 29 14:18:41.537: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 29 14:18:41.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 patch pod agnhost-master-lvg7t --namespace=kubectl-2773 -p {"metadata":{"annotations":{"x":"y"}}}'
Jan 29 14:18:42.118: INFO: stderr: ""
Jan 29 14:18:42.118: INFO: stdout: "pod/agnhost-master-lvg7t patched\n"
STEP: checking annotations
Jan 29 14:18:42.126: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 29 14:18:42.127: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:18:42.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2773" for this suite.

• [SLOW TEST:7.316 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1539
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":276,"completed":180,"skipped":2784,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:18:42.190: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-2132
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-2132
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a new StatefulSet
Jan 29 14:18:43.605: INFO: Found 1 stateful pods, waiting for 3
Jan 29 14:18:53.631: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 14:18:53.631: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 14:18:53.631: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jan 29 14:19:03.622: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 14:19:03.622: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 14:19:03.622: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jan 29 14:19:03.683: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jan 29 14:19:13.820: INFO: Updating stateful set ss2
Jan 29 14:19:13.879: INFO: Waiting for Pod statefulset-2132/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Jan 29 14:19:24.051: INFO: Found 2 stateful pods, waiting for 3
Jan 29 14:19:34.071: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 14:19:34.071: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 14:19:34.071: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jan 29 14:19:34.136: INFO: Updating stateful set ss2
Jan 29 14:19:34.158: INFO: Waiting for Pod statefulset-2132/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jan 29 14:19:44.307: INFO: Updating stateful set ss2
Jan 29 14:19:44.369: INFO: Waiting for StatefulSet statefulset-2132/ss2 to complete update
Jan 29 14:19:44.369: INFO: Waiting for Pod statefulset-2132/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jan 29 14:19:54.440: INFO: Waiting for StatefulSet statefulset-2132/ss2 to complete update
Jan 29 14:19:54.440: INFO: Waiting for Pod statefulset-2132/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jan 29 14:20:04.391: INFO: Deleting all statefulset in ns statefulset-2132
Jan 29 14:20:04.401: INFO: Scaling statefulset ss2 to 0
Jan 29 14:20:34.458: INFO: Waiting for statefulset status.replicas updated to 0
Jan 29 14:20:34.466: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:20:34.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2132" for this suite.

• [SLOW TEST:112.350 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":276,"completed":181,"skipped":2803,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:20:34.544: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8344
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-8344
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-8344
STEP: creating replication controller externalsvc in namespace services-8344
I0129 14:20:34.893199      22 runners.go:189] Created replication controller with name: externalsvc, namespace: services-8344, replica count: 2
I0129 14:20:37.948580      22 runners.go:189] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Jan 29 14:20:38.000: INFO: Creating new exec pod
Jan 29 14:20:42.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=services-8344 execpod9ndf9 -- /bin/sh -x -c nslookup clusterip-service'
Jan 29 14:20:43.726: INFO: stderr: "+ nslookup clusterip-service\n"
Jan 29 14:20:43.726: INFO: stdout: "Server:\t\t169.254.20.10\nAddress:\t169.254.20.10#53\n\nclusterip-service.services-8344.svc.cluster.local\tcanonical name = externalsvc.services-8344.svc.cluster.local.\nName:\texternalsvc.services-8344.svc.cluster.local\nAddress: 10.240.16.12\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8344, will wait for the garbage collector to delete the pods
Jan 29 14:20:43.809: INFO: Deleting ReplicationController externalsvc took: 21.826741ms
Jan 29 14:20:44.110: INFO: Terminating ReplicationController externalsvc pods took: 300.324865ms
Jan 29 14:21:01.092: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:21:01.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8344" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:26.699 seconds]
[sig-network] Services
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":276,"completed":182,"skipped":2807,"failed":0}
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:21:01.246: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-5703
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:21:20.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5703" for this suite.

• [SLOW TEST:20.374 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when scheduling a read only busybox container
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":276,"completed":183,"skipped":2807,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:21:21.622: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-1802
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-configmap-hbwl
STEP: Creating a pod to test atomic-volume-subpath
Jan 29 14:21:23.033: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-hbwl" in namespace "subpath-1802" to be "success or failure"
Jan 29 14:21:23.096: INFO: Pod "pod-subpath-test-configmap-hbwl": Phase="Pending", Reason="", readiness=false. Elapsed: 63.112906ms
Jan 29 14:21:25.111: INFO: Pod "pod-subpath-test-configmap-hbwl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.07758215s
Jan 29 14:21:27.241: INFO: Pod "pod-subpath-test-configmap-hbwl": Phase="Running", Reason="", readiness=true. Elapsed: 4.207868038s
Jan 29 14:21:29.253: INFO: Pod "pod-subpath-test-configmap-hbwl": Phase="Running", Reason="", readiness=true. Elapsed: 6.2194917s
Jan 29 14:21:31.267: INFO: Pod "pod-subpath-test-configmap-hbwl": Phase="Running", Reason="", readiness=true. Elapsed: 8.233571682s
Jan 29 14:21:33.298: INFO: Pod "pod-subpath-test-configmap-hbwl": Phase="Running", Reason="", readiness=true. Elapsed: 10.264354196s
Jan 29 14:21:36.264: INFO: Pod "pod-subpath-test-configmap-hbwl": Phase="Running", Reason="", readiness=true. Elapsed: 13.231174533s
Jan 29 14:21:38.324: INFO: Pod "pod-subpath-test-configmap-hbwl": Phase="Running", Reason="", readiness=true. Elapsed: 15.290785983s
Jan 29 14:21:40.357: INFO: Pod "pod-subpath-test-configmap-hbwl": Phase="Running", Reason="", readiness=true. Elapsed: 17.323435835s
Jan 29 14:21:42.378: INFO: Pod "pod-subpath-test-configmap-hbwl": Phase="Running", Reason="", readiness=true. Elapsed: 19.344410606s
Jan 29 14:21:44.429: INFO: Pod "pod-subpath-test-configmap-hbwl": Phase="Running", Reason="", readiness=true. Elapsed: 21.395861889s
Jan 29 14:21:46.448: INFO: Pod "pod-subpath-test-configmap-hbwl": Phase="Running", Reason="", readiness=true. Elapsed: 23.414402595s
Jan 29 14:21:48.465: INFO: Pod "pod-subpath-test-configmap-hbwl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 25.432137501s
STEP: Saw pod success
Jan 29 14:21:48.466: INFO: Pod "pod-subpath-test-configmap-hbwl" satisfied condition "success or failure"
Jan 29 14:21:48.477: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-subpath-test-configmap-hbwl container test-container-subpath-configmap-hbwl: <nil>
STEP: delete the pod
Jan 29 14:21:48.549: INFO: Waiting for pod pod-subpath-test-configmap-hbwl to disappear
Jan 29 14:21:48.565: INFO: Pod pod-subpath-test-configmap-hbwl no longer exists
STEP: Deleting pod pod-subpath-test-configmap-hbwl
Jan 29 14:21:48.565: INFO: Deleting pod "pod-subpath-test-configmap-hbwl" in namespace "subpath-1802"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:21:48.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1802" for this suite.

• [SLOW TEST:27.018 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":276,"completed":184,"skipped":2834,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:21:48.646: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6277
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-73203b46-f2e7-4650-9c16-d4c5204d2a7b
STEP: Creating a pod to test consume configMaps
Jan 29 14:21:48.922: INFO: Waiting up to 5m0s for pod "pod-configmaps-df3860ea-75d4-471e-85d9-75bc55e477f3" in namespace "configmap-6277" to be "success or failure"
Jan 29 14:21:48.958: INFO: Pod "pod-configmaps-df3860ea-75d4-471e-85d9-75bc55e477f3": Phase="Pending", Reason="", readiness=false. Elapsed: 35.803849ms
Jan 29 14:21:50.994: INFO: Pod "pod-configmaps-df3860ea-75d4-471e-85d9-75bc55e477f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.072412457s
Jan 29 14:21:53.011: INFO: Pod "pod-configmaps-df3860ea-75d4-471e-85d9-75bc55e477f3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.088583038s
Jan 29 14:21:55.594: INFO: Pod "pod-configmaps-df3860ea-75d4-471e-85d9-75bc55e477f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.671464971s
STEP: Saw pod success
Jan 29 14:21:55.594: INFO: Pod "pod-configmaps-df3860ea-75d4-471e-85d9-75bc55e477f3" satisfied condition "success or failure"
Jan 29 14:21:55.600: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-configmaps-df3860ea-75d4-471e-85d9-75bc55e477f3 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 29 14:21:55.930: INFO: Waiting for pod pod-configmaps-df3860ea-75d4-471e-85d9-75bc55e477f3 to disappear
Jan 29 14:21:55.955: INFO: Pod pod-configmaps-df3860ea-75d4-471e-85d9-75bc55e477f3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:21:55.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6277" for this suite.

• [SLOW TEST:7.390 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":276,"completed":185,"skipped":2891,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:21:56.037: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3379
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-2c5edd22-fe1a-4da3-8f6d-592b1efc2abc
STEP: Creating a pod to test consume secrets
Jan 29 14:21:56.465: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8caef6ef-5dd8-4669-98a2-3995226bd289" in namespace "projected-3379" to be "success or failure"
Jan 29 14:21:56.495: INFO: Pod "pod-projected-secrets-8caef6ef-5dd8-4669-98a2-3995226bd289": Phase="Pending", Reason="", readiness=false. Elapsed: 29.065006ms
Jan 29 14:21:58.529: INFO: Pod "pod-projected-secrets-8caef6ef-5dd8-4669-98a2-3995226bd289": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06338549s
Jan 29 14:22:00.544: INFO: Pod "pod-projected-secrets-8caef6ef-5dd8-4669-98a2-3995226bd289": Phase="Pending", Reason="", readiness=false. Elapsed: 4.077917404s
Jan 29 14:22:02.556: INFO: Pod "pod-projected-secrets-8caef6ef-5dd8-4669-98a2-3995226bd289": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.089956708s
STEP: Saw pod success
Jan 29 14:22:02.556: INFO: Pod "pod-projected-secrets-8caef6ef-5dd8-4669-98a2-3995226bd289" satisfied condition "success or failure"
Jan 29 14:22:02.578: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-projected-secrets-8caef6ef-5dd8-4669-98a2-3995226bd289 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 29 14:22:02.686: INFO: Waiting for pod pod-projected-secrets-8caef6ef-5dd8-4669-98a2-3995226bd289 to disappear
Jan 29 14:22:02.694: INFO: Pod pod-projected-secrets-8caef6ef-5dd8-4669-98a2-3995226bd289 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:22:02.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3379" for this suite.

• [SLOW TEST:6.687 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":276,"completed":186,"skipped":2928,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:22:02.729: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-2570
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: getting the auto-created API token
STEP: reading a file in the container
Jan 29 14:22:09.604: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2570 pod-service-account-c785bd51-8f29-40b7-8e39-eba872b2c8bd -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jan 29 14:22:10.635: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2570 pod-service-account-c785bd51-8f29-40b7-8e39-eba872b2c8bd -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jan 29 14:22:11.475: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2570 pod-service-account-c785bd51-8f29-40b7-8e39-eba872b2c8bd -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:22:12.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2570" for this suite.

• [SLOW TEST:9.655 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":276,"completed":187,"skipped":2974,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:22:12.386: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8263
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir volume type on tmpfs
Jan 29 14:22:12.737: INFO: Waiting up to 5m0s for pod "pod-699d6ce0-b09b-49fe-9fcc-293d33a03807" in namespace "emptydir-8263" to be "success or failure"
Jan 29 14:22:12.754: INFO: Pod "pod-699d6ce0-b09b-49fe-9fcc-293d33a03807": Phase="Pending", Reason="", readiness=false. Elapsed: 16.572355ms
Jan 29 14:22:14.778: INFO: Pod "pod-699d6ce0-b09b-49fe-9fcc-293d33a03807": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040884836s
Jan 29 14:22:16.796: INFO: Pod "pod-699d6ce0-b09b-49fe-9fcc-293d33a03807": Phase="Pending", Reason="", readiness=false. Elapsed: 4.059355431s
Jan 29 14:22:18.806: INFO: Pod "pod-699d6ce0-b09b-49fe-9fcc-293d33a03807": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.068658362s
STEP: Saw pod success
Jan 29 14:22:18.806: INFO: Pod "pod-699d6ce0-b09b-49fe-9fcc-293d33a03807" satisfied condition "success or failure"
Jan 29 14:22:18.813: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-699d6ce0-b09b-49fe-9fcc-293d33a03807 container test-container: <nil>
STEP: delete the pod
Jan 29 14:22:19.050: INFO: Waiting for pod pod-699d6ce0-b09b-49fe-9fcc-293d33a03807 to disappear
Jan 29 14:22:19.060: INFO: Pod pod-699d6ce0-b09b-49fe-9fcc-293d33a03807 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:22:19.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8263" for this suite.

• [SLOW TEST:7.068 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":276,"completed":188,"skipped":3018,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:22:19.457: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5720
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 29 14:22:21.765: INFO: Waiting up to 5m0s for pod "downwardapi-volume-13b71b55-b366-4951-a21d-67838eea95a0" in namespace "projected-5720" to be "success or failure"
Jan 29 14:22:21.778: INFO: Pod "downwardapi-volume-13b71b55-b366-4951-a21d-67838eea95a0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.300929ms
Jan 29 14:22:23.790: INFO: Pod "downwardapi-volume-13b71b55-b366-4951-a21d-67838eea95a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023960764s
Jan 29 14:22:25.976: INFO: Pod "downwardapi-volume-13b71b55-b366-4951-a21d-67838eea95a0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.210208064s
Jan 29 14:22:27.989: INFO: Pod "downwardapi-volume-13b71b55-b366-4951-a21d-67838eea95a0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.223697892s
Jan 29 14:22:30.010: INFO: Pod "downwardapi-volume-13b71b55-b366-4951-a21d-67838eea95a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.243895656s
STEP: Saw pod success
Jan 29 14:22:30.010: INFO: Pod "downwardapi-volume-13b71b55-b366-4951-a21d-67838eea95a0" satisfied condition "success or failure"
Jan 29 14:22:30.022: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod downwardapi-volume-13b71b55-b366-4951-a21d-67838eea95a0 container client-container: <nil>
STEP: delete the pod
Jan 29 14:22:30.103: INFO: Waiting for pod downwardapi-volume-13b71b55-b366-4951-a21d-67838eea95a0 to disappear
Jan 29 14:22:30.113: INFO: Pod downwardapi-volume-13b71b55-b366-4951-a21d-67838eea95a0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:22:30.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5720" for this suite.

• [SLOW TEST:10.803 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":276,"completed":189,"skipped":3025,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:22:30.264: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8242
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 29 14:22:30.748: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jan 29 14:22:34.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 --namespace=crd-publish-openapi-8242 create -f -'
Jan 29 14:22:38.116: INFO: stderr: ""
Jan 29 14:22:38.116: INFO: stdout: "e2e-test-crd-publish-openapi-7124-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 29 14:22:38.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 --namespace=crd-publish-openapi-8242 delete e2e-test-crd-publish-openapi-7124-crds test-cr'
Jan 29 14:22:38.291: INFO: stderr: ""
Jan 29 14:22:38.291: INFO: stdout: "e2e-test-crd-publish-openapi-7124-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jan 29 14:22:38.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 --namespace=crd-publish-openapi-8242 apply -f -'
Jan 29 14:22:39.629: INFO: stderr: ""
Jan 29 14:22:39.629: INFO: stdout: "e2e-test-crd-publish-openapi-7124-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 29 14:22:39.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 --namespace=crd-publish-openapi-8242 delete e2e-test-crd-publish-openapi-7124-crds test-cr'
Jan 29 14:22:39.858: INFO: stderr: ""
Jan 29 14:22:39.858: INFO: stdout: "e2e-test-crd-publish-openapi-7124-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Jan 29 14:22:39.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 explain e2e-test-crd-publish-openapi-7124-crds'
Jan 29 14:22:40.619: INFO: stderr: ""
Jan 29 14:22:40.619: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7124-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:22:44.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8242" for this suite.

• [SLOW TEST:14.527 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":276,"completed":190,"skipped":3035,"failed":0}
SS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:22:44.793: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-6342
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jan 29 14:22:58.729: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6342 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 14:22:58.729: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
Jan 29 14:22:59.307: INFO: Exec stderr: ""
Jan 29 14:22:59.307: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6342 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 14:22:59.307: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
Jan 29 14:22:59.938: INFO: Exec stderr: ""
Jan 29 14:22:59.938: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6342 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 14:22:59.938: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
Jan 29 14:23:01.097: INFO: Exec stderr: ""
Jan 29 14:23:01.098: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6342 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 14:23:01.098: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
Jan 29 14:23:01.837: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jan 29 14:23:01.837: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6342 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 14:23:01.837: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
Jan 29 14:23:02.498: INFO: Exec stderr: ""
Jan 29 14:23:02.498: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6342 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 14:23:02.498: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
Jan 29 14:23:03.133: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jan 29 14:23:03.134: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6342 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 14:23:03.134: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
Jan 29 14:23:03.756: INFO: Exec stderr: ""
Jan 29 14:23:03.756: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6342 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 14:23:03.756: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
Jan 29 14:23:04.408: INFO: Exec stderr: ""
Jan 29 14:23:04.408: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6342 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 14:23:04.408: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
Jan 29 14:23:05.016: INFO: Exec stderr: ""
Jan 29 14:23:05.017: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6342 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 14:23:05.017: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
Jan 29 14:23:05.660: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:23:05.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-6342" for this suite.

• [SLOW TEST:20.907 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":276,"completed":191,"skipped":3037,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:23:05.702: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4737
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-projected-pg8d
STEP: Creating a pod to test atomic-volume-subpath
Jan 29 14:23:06.004: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-pg8d" in namespace "subpath-4737" to be "success or failure"
Jan 29 14:23:06.018: INFO: Pod "pod-subpath-test-projected-pg8d": Phase="Pending", Reason="", readiness=false. Elapsed: 14.602606ms
Jan 29 14:23:08.034: INFO: Pod "pod-subpath-test-projected-pg8d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030140078s
Jan 29 14:23:10.054: INFO: Pod "pod-subpath-test-projected-pg8d": Phase="Running", Reason="", readiness=true. Elapsed: 4.050140735s
Jan 29 14:23:12.065: INFO: Pod "pod-subpath-test-projected-pg8d": Phase="Running", Reason="", readiness=true. Elapsed: 6.061050232s
Jan 29 14:23:14.081: INFO: Pod "pod-subpath-test-projected-pg8d": Phase="Running", Reason="", readiness=true. Elapsed: 8.076995598s
Jan 29 14:23:16.091: INFO: Pod "pod-subpath-test-projected-pg8d": Phase="Running", Reason="", readiness=true. Elapsed: 10.087476346s
Jan 29 14:23:18.104: INFO: Pod "pod-subpath-test-projected-pg8d": Phase="Running", Reason="", readiness=true. Elapsed: 12.100172703s
Jan 29 14:23:20.125: INFO: Pod "pod-subpath-test-projected-pg8d": Phase="Running", Reason="", readiness=true. Elapsed: 14.121194463s
Jan 29 14:23:22.136: INFO: Pod "pod-subpath-test-projected-pg8d": Phase="Running", Reason="", readiness=true. Elapsed: 16.132623263s
Jan 29 14:23:24.146: INFO: Pod "pod-subpath-test-projected-pg8d": Phase="Running", Reason="", readiness=true. Elapsed: 18.141896623s
Jan 29 14:23:26.163: INFO: Pod "pod-subpath-test-projected-pg8d": Phase="Running", Reason="", readiness=true. Elapsed: 20.159157891s
Jan 29 14:23:28.176: INFO: Pod "pod-subpath-test-projected-pg8d": Phase="Running", Reason="", readiness=true. Elapsed: 22.172739916s
Jan 29 14:23:30.194: INFO: Pod "pod-subpath-test-projected-pg8d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.190294814s
STEP: Saw pod success
Jan 29 14:23:30.194: INFO: Pod "pod-subpath-test-projected-pg8d" satisfied condition "success or failure"
Jan 29 14:23:30.204: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-znv5g pod pod-subpath-test-projected-pg8d container test-container-subpath-projected-pg8d: <nil>
STEP: delete the pod
Jan 29 14:23:30.285: INFO: Waiting for pod pod-subpath-test-projected-pg8d to disappear
Jan 29 14:23:30.298: INFO: Pod pod-subpath-test-projected-pg8d no longer exists
STEP: Deleting pod pod-subpath-test-projected-pg8d
Jan 29 14:23:30.298: INFO: Deleting pod "pod-subpath-test-projected-pg8d" in namespace "subpath-4737"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:23:30.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4737" for this suite.

• [SLOW TEST:24.685 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":276,"completed":192,"skipped":3080,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:23:30.389: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2603
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on node default medium
Jan 29 14:23:30.780: INFO: Waiting up to 5m0s for pod "pod-f16346a7-b296-4e08-a042-3a1d8ff56e8f" in namespace "emptydir-2603" to be "success or failure"
Jan 29 14:23:30.788: INFO: Pod "pod-f16346a7-b296-4e08-a042-3a1d8ff56e8f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.27183ms
Jan 29 14:23:32.799: INFO: Pod "pod-f16346a7-b296-4e08-a042-3a1d8ff56e8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019068997s
Jan 29 14:23:34.846: INFO: Pod "pod-f16346a7-b296-4e08-a042-3a1d8ff56e8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065898889s
STEP: Saw pod success
Jan 29 14:23:34.846: INFO: Pod "pod-f16346a7-b296-4e08-a042-3a1d8ff56e8f" satisfied condition "success or failure"
Jan 29 14:23:34.858: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-znv5g pod pod-f16346a7-b296-4e08-a042-3a1d8ff56e8f container test-container: <nil>
STEP: delete the pod
Jan 29 14:23:34.988: INFO: Waiting for pod pod-f16346a7-b296-4e08-a042-3a1d8ff56e8f to disappear
Jan 29 14:23:34.998: INFO: Pod pod-f16346a7-b296-4e08-a042-3a1d8ff56e8f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:23:34.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2603" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":276,"completed":193,"skipped":3098,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:23:35.042: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-5758
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-configmap-x98r
STEP: Creating a pod to test atomic-volume-subpath
Jan 29 14:23:35.545: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-x98r" in namespace "subpath-5758" to be "success or failure"
Jan 29 14:23:35.584: INFO: Pod "pod-subpath-test-configmap-x98r": Phase="Pending", Reason="", readiness=false. Elapsed: 38.746047ms
Jan 29 14:23:37.592: INFO: Pod "pod-subpath-test-configmap-x98r": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046875505s
Jan 29 14:23:39.606: INFO: Pod "pod-subpath-test-configmap-x98r": Phase="Running", Reason="", readiness=true. Elapsed: 4.060820272s
Jan 29 14:23:41.614: INFO: Pod "pod-subpath-test-configmap-x98r": Phase="Running", Reason="", readiness=true. Elapsed: 6.069410143s
Jan 29 14:23:43.633: INFO: Pod "pod-subpath-test-configmap-x98r": Phase="Running", Reason="", readiness=true. Elapsed: 8.08843574s
Jan 29 14:23:45.648: INFO: Pod "pod-subpath-test-configmap-x98r": Phase="Running", Reason="", readiness=true. Elapsed: 10.103003522s
Jan 29 14:23:47.658: INFO: Pod "pod-subpath-test-configmap-x98r": Phase="Running", Reason="", readiness=true. Elapsed: 12.113323504s
Jan 29 14:23:49.668: INFO: Pod "pod-subpath-test-configmap-x98r": Phase="Running", Reason="", readiness=true. Elapsed: 14.1227799s
Jan 29 14:23:51.677: INFO: Pod "pod-subpath-test-configmap-x98r": Phase="Running", Reason="", readiness=true. Elapsed: 16.131692052s
Jan 29 14:23:53.690: INFO: Pod "pod-subpath-test-configmap-x98r": Phase="Running", Reason="", readiness=true. Elapsed: 18.145341427s
Jan 29 14:23:55.725: INFO: Pod "pod-subpath-test-configmap-x98r": Phase="Running", Reason="", readiness=true. Elapsed: 20.180251612s
Jan 29 14:23:57.735: INFO: Pod "pod-subpath-test-configmap-x98r": Phase="Running", Reason="", readiness=true. Elapsed: 22.189948577s
Jan 29 14:23:59.753: INFO: Pod "pod-subpath-test-configmap-x98r": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.208300142s
STEP: Saw pod success
Jan 29 14:23:59.754: INFO: Pod "pod-subpath-test-configmap-x98r" satisfied condition "success or failure"
Jan 29 14:23:59.765: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-znv5g pod pod-subpath-test-configmap-x98r container test-container-subpath-configmap-x98r: <nil>
STEP: delete the pod
Jan 29 14:23:59.839: INFO: Waiting for pod pod-subpath-test-configmap-x98r to disappear
Jan 29 14:23:59.851: INFO: Pod pod-subpath-test-configmap-x98r no longer exists
STEP: Deleting pod pod-subpath-test-configmap-x98r
Jan 29 14:23:59.852: INFO: Deleting pod "pod-subpath-test-configmap-x98r" in namespace "subpath-5758"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:23:59.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5758" for this suite.

• [SLOW TEST:24.861 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":276,"completed":194,"skipped":3136,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:23:59.909: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-669
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 29 14:24:00.149: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Creating first CR 
Jan 29 14:24:00.432: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-01-29T14:24:00Z generation:1 name:name1 resourceVersion:84486 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:6f6d7906-e5a4-43d0-b4dc-91c0db8074d9] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Jan 29 14:24:10.445: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-01-29T14:24:10Z generation:1 name:name2 resourceVersion:84542 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:d729de42-101f-43f1-9b90-c57bdb0f086d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Jan 29 14:24:20.652: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-01-29T14:24:00Z generation:2 name:name1 resourceVersion:84584 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:6f6d7906-e5a4-43d0-b4dc-91c0db8074d9] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Jan 29 14:24:30.665: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-01-29T14:24:10Z generation:2 name:name2 resourceVersion:84626 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:d729de42-101f-43f1-9b90-c57bdb0f086d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Jan 29 14:24:40.705: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-01-29T14:24:00Z generation:2 name:name1 resourceVersion:84666 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:6f6d7906-e5a4-43d0-b4dc-91c0db8074d9] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Jan 29 14:24:50.870: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-01-29T14:24:10Z generation:2 name:name2 resourceVersion:84709 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:d729de42-101f-43f1-9b90-c57bdb0f086d] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:25:01.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-669" for this suite.

• [SLOW TEST:61.549 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:41
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":276,"completed":195,"skipped":3139,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:25:01.463: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6105
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6105.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-6105.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6105.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6105.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6105.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-6105.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6105.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-6105.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6105.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6105.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-6105.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6105.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-6105.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6105.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-6105.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6105.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-6105.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6105.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 29 14:25:08.097: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6105.svc.cluster.local from pod dns-6105/dns-test-6b6b7c8c-eacf-4e61-9aa8-841faf43dbd0: the server could not find the requested resource (get pods dns-test-6b6b7c8c-eacf-4e61-9aa8-841faf43dbd0)
Jan 29 14:25:08.156: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6105.svc.cluster.local from pod dns-6105/dns-test-6b6b7c8c-eacf-4e61-9aa8-841faf43dbd0: the server could not find the requested resource (get pods dns-test-6b6b7c8c-eacf-4e61-9aa8-841faf43dbd0)
Jan 29 14:25:08.173: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6105.svc.cluster.local from pod dns-6105/dns-test-6b6b7c8c-eacf-4e61-9aa8-841faf43dbd0: the server could not find the requested resource (get pods dns-test-6b6b7c8c-eacf-4e61-9aa8-841faf43dbd0)
Jan 29 14:25:08.446: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6105.svc.cluster.local from pod dns-6105/dns-test-6b6b7c8c-eacf-4e61-9aa8-841faf43dbd0: the server could not find the requested resource (get pods dns-test-6b6b7c8c-eacf-4e61-9aa8-841faf43dbd0)
Jan 29 14:25:08.657: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6105.svc.cluster.local from pod dns-6105/dns-test-6b6b7c8c-eacf-4e61-9aa8-841faf43dbd0: the server could not find the requested resource (get pods dns-test-6b6b7c8c-eacf-4e61-9aa8-841faf43dbd0)
Jan 29 14:25:08.671: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6105.svc.cluster.local from pod dns-6105/dns-test-6b6b7c8c-eacf-4e61-9aa8-841faf43dbd0: the server could not find the requested resource (get pods dns-test-6b6b7c8c-eacf-4e61-9aa8-841faf43dbd0)
Jan 29 14:25:08.688: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6105.svc.cluster.local from pod dns-6105/dns-test-6b6b7c8c-eacf-4e61-9aa8-841faf43dbd0: the server could not find the requested resource (get pods dns-test-6b6b7c8c-eacf-4e61-9aa8-841faf43dbd0)
Jan 29 14:25:08.699: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6105.svc.cluster.local from pod dns-6105/dns-test-6b6b7c8c-eacf-4e61-9aa8-841faf43dbd0: the server could not find the requested resource (get pods dns-test-6b6b7c8c-eacf-4e61-9aa8-841faf43dbd0)
Jan 29 14:25:08.857: INFO: Lookups using dns-6105/dns-test-6b6b7c8c-eacf-4e61-9aa8-841faf43dbd0 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6105.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6105.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6105.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6105.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6105.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6105.svc.cluster.local jessie_udp@dns-test-service-2.dns-6105.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6105.svc.cluster.local]

Jan 29 14:25:14.954: INFO: DNS probes using dns-6105/dns-test-6b6b7c8c-eacf-4e61-9aa8-841faf43dbd0 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:25:15.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6105" for this suite.

• [SLOW TEST:13.721 seconds]
[sig-network] DNS
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":276,"completed":196,"skipped":3150,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:25:15.189: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1357
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jan 29 14:25:24.102: INFO: Successfully updated pod "pod-update-dc08df8e-ba7f-4c61-921b-fcae50bc5508"
STEP: verifying the updated pod is in kubernetes
Jan 29 14:25:24.135: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:25:24.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1357" for this suite.

• [SLOW TEST:8.986 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":276,"completed":197,"skipped":3190,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:25:24.184: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1123
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Jan 29 14:25:24.457: INFO: Created pod &Pod{ObjectMeta:{dns-1123  dns-1123 /api/v1/namespaces/dns-1123/pods/dns-1123 257c9b7e-9f2e-4dec-b5c9-ec06a9f6a31f 84931 0 2020-01-29 14:25:24 +0000 UTC <nil> <nil> map[] map[] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6x6fr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6x6fr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6x6fr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
STEP: Verifying customized DNS suffix list is configured on pod...
Jan 29 14:25:28.485: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1123 PodName:dns-1123 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 14:25:28.485: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Verifying customized DNS server is configured on pod...
Jan 29 14:25:29.048: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1123 PodName:dns-1123 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 14:25:29.048: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
Jan 29 14:25:29.870: INFO: Deleting pod dns-1123...
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:25:29.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1123" for this suite.

• [SLOW TEST:5.792 seconds]
[sig-network] DNS
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":276,"completed":198,"skipped":3220,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:25:29.979: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-2380
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 29 14:25:30.243: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: creating replication controller svc-latency-rc in namespace svc-latency-2380
I0129 14:25:30.269388      22 runners.go:189] Created replication controller with name: svc-latency-rc, namespace: svc-latency-2380, replica count: 1
I0129 14:25:31.320920      22 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0129 14:25:32.321259      22 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0129 14:25:33.321572      22 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 29 14:25:33.474: INFO: Created: latency-svc-g6vlf
Jan 29 14:25:33.499: INFO: Got endpoints: latency-svc-g6vlf [77.174462ms]
Jan 29 14:25:33.541: INFO: Created: latency-svc-rknrd
Jan 29 14:25:33.555: INFO: Created: latency-svc-tfldv
Jan 29 14:25:33.559: INFO: Got endpoints: latency-svc-rknrd [59.008489ms]
Jan 29 14:25:33.575: INFO: Got endpoints: latency-svc-tfldv [74.945564ms]
Jan 29 14:25:33.576: INFO: Created: latency-svc-zxlrv
Jan 29 14:25:33.593: INFO: Created: latency-svc-cvfxv
Jan 29 14:25:33.593: INFO: Got endpoints: latency-svc-zxlrv [92.223819ms]
Jan 29 14:25:33.627: INFO: Got endpoints: latency-svc-cvfxv [125.940749ms]
Jan 29 14:25:33.642: INFO: Created: latency-svc-77v7r
Jan 29 14:25:33.666: INFO: Got endpoints: latency-svc-77v7r [164.629326ms]
Jan 29 14:25:33.669: INFO: Created: latency-svc-4r45s
Jan 29 14:25:33.693: INFO: Created: latency-svc-mrtng
Jan 29 14:25:33.698: INFO: Got endpoints: latency-svc-4r45s [197.011695ms]
Jan 29 14:25:33.709: INFO: Got endpoints: latency-svc-mrtng [207.148544ms]
Jan 29 14:25:33.715: INFO: Created: latency-svc-qq67p
Jan 29 14:25:33.730: INFO: Created: latency-svc-lwgnn
Jan 29 14:25:33.788: INFO: Got endpoints: latency-svc-qq67p [286.158224ms]
Jan 29 14:25:33.789: INFO: Got endpoints: latency-svc-lwgnn [287.348746ms]
Jan 29 14:25:33.796: INFO: Created: latency-svc-pms7r
Jan 29 14:25:33.803: INFO: Got endpoints: latency-svc-pms7r [301.532441ms]
Jan 29 14:25:33.843: INFO: Created: latency-svc-fbksx
Jan 29 14:25:33.862: INFO: Created: latency-svc-g98x7
Jan 29 14:25:33.868: INFO: Got endpoints: latency-svc-fbksx [366.794447ms]
Jan 29 14:25:33.886: INFO: Created: latency-svc-m8rdz
Jan 29 14:25:33.925: INFO: Got endpoints: latency-svc-g98x7 [423.472109ms]
Jan 29 14:25:33.925: INFO: Got endpoints: latency-svc-m8rdz [423.660716ms]
Jan 29 14:25:33.927: INFO: Created: latency-svc-74tqp
Jan 29 14:25:33.945: INFO: Got endpoints: latency-svc-74tqp [442.979591ms]
Jan 29 14:25:33.961: INFO: Created: latency-svc-lxxfg
Jan 29 14:25:33.981: INFO: Created: latency-svc-dfdnw
Jan 29 14:25:34.006: INFO: Got endpoints: latency-svc-lxxfg [504.82103ms]
Jan 29 14:25:34.011: INFO: Got endpoints: latency-svc-dfdnw [452.183729ms]
Jan 29 14:25:34.034: INFO: Created: latency-svc-mxrlr
Jan 29 14:25:34.054: INFO: Got endpoints: latency-svc-mxrlr [478.299469ms]
Jan 29 14:25:34.056: INFO: Created: latency-svc-lgt98
Jan 29 14:25:34.082: INFO: Created: latency-svc-42jg2
Jan 29 14:25:34.083: INFO: Got endpoints: latency-svc-lgt98 [489.001651ms]
Jan 29 14:25:34.101: INFO: Got endpoints: latency-svc-42jg2 [473.079125ms]
Jan 29 14:25:34.110: INFO: Created: latency-svc-crkwh
Jan 29 14:25:34.141: INFO: Created: latency-svc-kch2x
Jan 29 14:25:34.144: INFO: Got endpoints: latency-svc-crkwh [478.133277ms]
Jan 29 14:25:34.162: INFO: Created: latency-svc-fzgs5
Jan 29 14:25:34.168: INFO: Got endpoints: latency-svc-kch2x [469.188195ms]
Jan 29 14:25:34.184: INFO: Got endpoints: latency-svc-fzgs5 [474.662809ms]
Jan 29 14:25:34.185: INFO: Created: latency-svc-2f55x
Jan 29 14:25:34.210: INFO: Got endpoints: latency-svc-2f55x [421.773298ms]
Jan 29 14:25:34.212: INFO: Created: latency-svc-mbmhv
Jan 29 14:25:34.231: INFO: Created: latency-svc-7qgww
Jan 29 14:25:34.244: INFO: Got endpoints: latency-svc-mbmhv [454.677169ms]
Jan 29 14:25:34.248: INFO: Got endpoints: latency-svc-7qgww [444.96716ms]
Jan 29 14:25:34.271: INFO: Created: latency-svc-mrfhp
Jan 29 14:25:34.286: INFO: Created: latency-svc-hp94m
Jan 29 14:25:34.292: INFO: Created: latency-svc-28zwq
Jan 29 14:25:34.294: INFO: Got endpoints: latency-svc-mrfhp [425.086444ms]
Jan 29 14:25:34.309: INFO: Got endpoints: latency-svc-hp94m [384.055523ms]
Jan 29 14:25:34.320: INFO: Got endpoints: latency-svc-28zwq [393.699842ms]
Jan 29 14:25:34.343: INFO: Created: latency-svc-f59s5
Jan 29 14:25:34.366: INFO: Got endpoints: latency-svc-f59s5 [420.707632ms]
Jan 29 14:25:34.372: INFO: Created: latency-svc-h8pkf
Jan 29 14:25:34.408: INFO: Got endpoints: latency-svc-h8pkf [402.001686ms]
Jan 29 14:25:34.409: INFO: Created: latency-svc-zhnzq
Jan 29 14:25:34.418: INFO: Got endpoints: latency-svc-zhnzq [406.284219ms]
Jan 29 14:25:34.430: INFO: Created: latency-svc-xb9dl
Jan 29 14:25:34.470: INFO: Got endpoints: latency-svc-xb9dl [416.719153ms]
Jan 29 14:25:34.493: INFO: Created: latency-svc-rjdbw
Jan 29 14:25:34.508: INFO: Created: latency-svc-6jz2n
Jan 29 14:25:34.508: INFO: Got endpoints: latency-svc-rjdbw [425.001002ms]
Jan 29 14:25:34.535: INFO: Got endpoints: latency-svc-6jz2n [432.072127ms]
Jan 29 14:25:34.548: INFO: Created: latency-svc-nwtwk
Jan 29 14:25:34.560: INFO: Got endpoints: latency-svc-nwtwk [415.072665ms]
Jan 29 14:25:34.564: INFO: Created: latency-svc-ptrpk
Jan 29 14:25:34.605: INFO: Got endpoints: latency-svc-ptrpk [436.467761ms]
Jan 29 14:25:34.606: INFO: Created: latency-svc-t929v
Jan 29 14:25:34.624: INFO: Got endpoints: latency-svc-t929v [440.172785ms]
Jan 29 14:25:34.629: INFO: Created: latency-svc-9kvcz
Jan 29 14:25:34.654: INFO: Got endpoints: latency-svc-9kvcz [444.26014ms]
Jan 29 14:25:34.665: INFO: Created: latency-svc-vnn7p
Jan 29 14:25:34.675: INFO: Got endpoints: latency-svc-vnn7p [430.743468ms]
Jan 29 14:25:34.681: INFO: Created: latency-svc-29gwv
Jan 29 14:25:34.707: INFO: Got endpoints: latency-svc-29gwv [458.802514ms]
Jan 29 14:25:34.733: INFO: Created: latency-svc-2vtd2
Jan 29 14:25:34.800: INFO: Got endpoints: latency-svc-2vtd2 [506.363739ms]
Jan 29 14:25:34.801: INFO: Created: latency-svc-9vn9s
Jan 29 14:25:34.807: INFO: Created: latency-svc-dhcss
Jan 29 14:25:34.821: INFO: Got endpoints: latency-svc-9vn9s [511.980191ms]
Jan 29 14:25:34.853: INFO: Got endpoints: latency-svc-dhcss [533.093825ms]
Jan 29 14:25:34.854: INFO: Created: latency-svc-zbhc7
Jan 29 14:25:34.878: INFO: Created: latency-svc-46zzg
Jan 29 14:25:34.878: INFO: Got endpoints: latency-svc-zbhc7 [512.111422ms]
Jan 29 14:25:34.886: INFO: Created: latency-svc-k795b
Jan 29 14:25:34.906: INFO: Got endpoints: latency-svc-46zzg [497.465654ms]
Jan 29 14:25:34.916: INFO: Got endpoints: latency-svc-k795b [498.512034ms]
Jan 29 14:25:34.938: INFO: Created: latency-svc-l2cw7
Jan 29 14:25:34.953: INFO: Got endpoints: latency-svc-l2cw7 [482.384724ms]
Jan 29 14:25:34.995: INFO: Created: latency-svc-9t7kt
Jan 29 14:25:35.014: INFO: Got endpoints: latency-svc-9t7kt [505.84615ms]
Jan 29 14:25:35.014: INFO: Created: latency-svc-8vwhc
Jan 29 14:25:35.040: INFO: Got endpoints: latency-svc-8vwhc [505.397202ms]
Jan 29 14:25:35.045: INFO: Created: latency-svc-fzv72
Jan 29 14:25:35.093: INFO: Created: latency-svc-mr745
Jan 29 14:25:35.093: INFO: Got endpoints: latency-svc-fzv72 [533.425245ms]
Jan 29 14:25:35.104: INFO: Created: latency-svc-g4wnw
Jan 29 14:25:35.104: INFO: Got endpoints: latency-svc-mr745 [499.514252ms]
Jan 29 14:25:35.125: INFO: Got endpoints: latency-svc-g4wnw [500.569527ms]
Jan 29 14:25:35.135: INFO: Created: latency-svc-h654k
Jan 29 14:25:35.158: INFO: Created: latency-svc-89pmq
Jan 29 14:25:35.170: INFO: Got endpoints: latency-svc-h654k [516.070323ms]
Jan 29 14:25:35.182: INFO: Got endpoints: latency-svc-89pmq [506.261039ms]
Jan 29 14:25:35.184: INFO: Created: latency-svc-jjpct
Jan 29 14:25:35.207: INFO: Got endpoints: latency-svc-jjpct [500.024263ms]
Jan 29 14:25:35.220: INFO: Created: latency-svc-nnsf9
Jan 29 14:25:35.310: INFO: Got endpoints: latency-svc-nnsf9 [509.447752ms]
Jan 29 14:25:35.313: INFO: Created: latency-svc-mzwpv
Jan 29 14:25:35.394: INFO: Got endpoints: latency-svc-mzwpv [572.708628ms]
Jan 29 14:25:35.395: INFO: Created: latency-svc-cq6w8
Jan 29 14:25:35.395: INFO: Got endpoints: latency-svc-cq6w8 [541.719402ms]
Jan 29 14:25:35.399: INFO: Created: latency-svc-khbvp
Jan 29 14:25:35.452: INFO: Got endpoints: latency-svc-khbvp [573.72037ms]
Jan 29 14:25:35.453: INFO: Created: latency-svc-qpjms
Jan 29 14:25:35.472: INFO: Got endpoints: latency-svc-qpjms [565.251854ms]
Jan 29 14:25:35.492: INFO: Created: latency-svc-8xq7j
Jan 29 14:25:35.502: INFO: Got endpoints: latency-svc-8xq7j [585.971321ms]
Jan 29 14:25:35.504: INFO: Created: latency-svc-ghl8t
Jan 29 14:25:35.530: INFO: Got endpoints: latency-svc-ghl8t [576.838598ms]
Jan 29 14:25:35.550: INFO: Created: latency-svc-zxnfk
Jan 29 14:25:35.560: INFO: Got endpoints: latency-svc-zxnfk [546.139083ms]
Jan 29 14:25:35.580: INFO: Created: latency-svc-jppgj
Jan 29 14:25:35.611: INFO: Got endpoints: latency-svc-jppgj [570.96094ms]
Jan 29 14:25:35.627: INFO: Created: latency-svc-jczcn
Jan 29 14:25:35.638: INFO: Got endpoints: latency-svc-jczcn [543.396486ms]
Jan 29 14:25:35.664: INFO: Created: latency-svc-jjrmw
Jan 29 14:25:35.734: INFO: Got endpoints: latency-svc-jjrmw [630.26596ms]
Jan 29 14:25:35.739: INFO: Created: latency-svc-x5jsw
Jan 29 14:25:35.761: INFO: Got endpoints: latency-svc-x5jsw [635.890023ms]
Jan 29 14:25:35.764: INFO: Created: latency-svc-hd4bh
Jan 29 14:25:35.786: INFO: Got endpoints: latency-svc-hd4bh [615.836558ms]
Jan 29 14:25:35.799: INFO: Created: latency-svc-kdcxr
Jan 29 14:25:35.818: INFO: Created: latency-svc-jz2t7
Jan 29 14:25:35.833: INFO: Got endpoints: latency-svc-kdcxr [650.612217ms]
Jan 29 14:25:35.839: INFO: Got endpoints: latency-svc-jz2t7 [631.110502ms]
Jan 29 14:25:35.894: INFO: Created: latency-svc-crhq9
Jan 29 14:25:35.895: INFO: Got endpoints: latency-svc-crhq9 [584.85161ms]
Jan 29 14:25:35.910: INFO: Created: latency-svc-72rcr
Jan 29 14:25:35.935: INFO: Got endpoints: latency-svc-72rcr [540.906308ms]
Jan 29 14:25:35.943: INFO: Created: latency-svc-7s722
Jan 29 14:25:35.970: INFO: Created: latency-svc-ckgmb
Jan 29 14:25:35.970: INFO: Got endpoints: latency-svc-7s722 [574.826359ms]
Jan 29 14:25:36.002: INFO: Got endpoints: latency-svc-ckgmb [549.690793ms]
Jan 29 14:25:36.024: INFO: Created: latency-svc-qtb4t
Jan 29 14:25:36.040: INFO: Got endpoints: latency-svc-qtb4t [568.460448ms]
Jan 29 14:25:36.045: INFO: Created: latency-svc-s6ml4
Jan 29 14:25:36.061: INFO: Got endpoints: latency-svc-s6ml4 [558.905876ms]
Jan 29 14:25:36.075: INFO: Created: latency-svc-74fvz
Jan 29 14:25:36.093: INFO: Got endpoints: latency-svc-74fvz [562.182196ms]
Jan 29 14:25:36.126: INFO: Created: latency-svc-wnxx4
Jan 29 14:25:36.156: INFO: Got endpoints: latency-svc-wnxx4 [594.52717ms]
Jan 29 14:25:36.198: INFO: Created: latency-svc-dzrwt
Jan 29 14:25:36.205: INFO: Got endpoints: latency-svc-dzrwt [593.632812ms]
Jan 29 14:25:36.214: INFO: Created: latency-svc-mrm52
Jan 29 14:25:36.293: INFO: Got endpoints: latency-svc-mrm52 [654.513906ms]
Jan 29 14:25:36.315: INFO: Created: latency-svc-tgpsv
Jan 29 14:25:36.325: INFO: Created: latency-svc-kdpv5
Jan 29 14:25:36.346: INFO: Got endpoints: latency-svc-tgpsv [611.316268ms]
Jan 29 14:25:36.350: INFO: Got endpoints: latency-svc-kdpv5 [588.129315ms]
Jan 29 14:25:36.352: INFO: Created: latency-svc-nppjm
Jan 29 14:25:36.372: INFO: Got endpoints: latency-svc-nppjm [585.341417ms]
Jan 29 14:25:36.373: INFO: Created: latency-svc-pwgl7
Jan 29 14:25:36.408: INFO: Got endpoints: latency-svc-pwgl7 [574.867689ms]
Jan 29 14:25:36.409: INFO: Created: latency-svc-h2w4r
Jan 29 14:25:36.426: INFO: Created: latency-svc-qqjpw
Jan 29 14:25:36.459: INFO: Got endpoints: latency-svc-h2w4r [619.482208ms]
Jan 29 14:25:36.462: INFO: Got endpoints: latency-svc-qqjpw [567.240759ms]
Jan 29 14:25:36.475: INFO: Created: latency-svc-wsspk
Jan 29 14:25:36.499: INFO: Got endpoints: latency-svc-wsspk [564.052513ms]
Jan 29 14:25:36.513: INFO: Created: latency-svc-n2pjr
Jan 29 14:25:36.565: INFO: Created: latency-svc-kmc45
Jan 29 14:25:36.566: INFO: Got endpoints: latency-svc-n2pjr [103.526414ms]
Jan 29 14:25:36.591: INFO: Got endpoints: latency-svc-kmc45 [619.827359ms]
Jan 29 14:25:36.592: INFO: Created: latency-svc-x6bkh
Jan 29 14:25:36.612: INFO: Got endpoints: latency-svc-x6bkh [609.831533ms]
Jan 29 14:25:36.617: INFO: Created: latency-svc-k9lq8
Jan 29 14:25:36.729: INFO: Created: latency-svc-fzvnd
Jan 29 14:25:36.731: INFO: Got endpoints: latency-svc-k9lq8 [689.820258ms]
Jan 29 14:25:36.747: INFO: Got endpoints: latency-svc-fzvnd [685.801423ms]
Jan 29 14:25:36.781: INFO: Created: latency-svc-r7dnp
Jan 29 14:25:36.822: INFO: Created: latency-svc-wtd6t
Jan 29 14:25:36.822: INFO: Got endpoints: latency-svc-r7dnp [728.663531ms]
Jan 29 14:25:36.835: INFO: Created: latency-svc-5dtmg
Jan 29 14:25:36.836: INFO: Got endpoints: latency-svc-wtd6t [679.345832ms]
Jan 29 14:25:36.874: INFO: Got endpoints: latency-svc-5dtmg [668.588754ms]
Jan 29 14:25:36.896: INFO: Created: latency-svc-n49q2
Jan 29 14:25:36.915: INFO: Created: latency-svc-5hnrb
Jan 29 14:25:36.916: INFO: Got endpoints: latency-svc-n49q2 [614.38039ms]
Jan 29 14:25:36.933: INFO: Got endpoints: latency-svc-5hnrb [586.49598ms]
Jan 29 14:25:36.943: INFO: Created: latency-svc-4pl8v
Jan 29 14:25:36.988: INFO: Got endpoints: latency-svc-4pl8v [636.54598ms]
Jan 29 14:25:37.002: INFO: Created: latency-svc-89tsk
Jan 29 14:25:37.018: INFO: Got endpoints: latency-svc-89tsk [645.599457ms]
Jan 29 14:25:37.023: INFO: Created: latency-svc-hqlh8
Jan 29 14:25:37.033: INFO: Created: latency-svc-942vg
Jan 29 14:25:37.052: INFO: Created: latency-svc-rfbdp
Jan 29 14:25:37.062: INFO: Got endpoints: latency-svc-hqlh8 [654.156091ms]
Jan 29 14:25:37.076: INFO: Created: latency-svc-zd6zn
Jan 29 14:25:37.093: INFO: Created: latency-svc-qhn4d
Jan 29 14:25:37.112: INFO: Got endpoints: latency-svc-942vg [652.727031ms]
Jan 29 14:25:37.136: INFO: Created: latency-svc-g66zn
Jan 29 14:25:37.146: INFO: Created: latency-svc-t9k9s
Jan 29 14:25:37.160: INFO: Created: latency-svc-bsvpp
Jan 29 14:25:37.164: INFO: Got endpoints: latency-svc-rfbdp [665.070188ms]
Jan 29 14:25:37.178: INFO: Created: latency-svc-tmf2g
Jan 29 14:25:37.215: INFO: Created: latency-svc-l9rfr
Jan 29 14:25:37.217: INFO: Got endpoints: latency-svc-zd6zn [651.174948ms]
Jan 29 14:25:37.245: INFO: Created: latency-svc-lzd5d
Jan 29 14:25:37.257: INFO: Got endpoints: latency-svc-qhn4d [665.748415ms]
Jan 29 14:25:37.267: INFO: Created: latency-svc-vtf5v
Jan 29 14:25:37.275: INFO: Created: latency-svc-75txf
Jan 29 14:25:37.310: INFO: Created: latency-svc-gw7dc
Jan 29 14:25:37.326: INFO: Got endpoints: latency-svc-g66zn [713.253705ms]
Jan 29 14:25:37.363: INFO: Got endpoints: latency-svc-t9k9s [631.753216ms]
Jan 29 14:25:37.364: INFO: Created: latency-svc-945r7
Jan 29 14:25:37.400: INFO: Created: latency-svc-jtbxj
Jan 29 14:25:37.437: INFO: Got endpoints: latency-svc-bsvpp [689.714415ms]
Jan 29 14:25:37.439: INFO: Created: latency-svc-r8kp4
Jan 29 14:25:37.458: INFO: Created: latency-svc-5stxj
Jan 29 14:25:37.474: INFO: Got endpoints: latency-svc-tmf2g [651.632819ms]
Jan 29 14:25:37.481: INFO: Created: latency-svc-wqrd8
Jan 29 14:25:37.511: INFO: Created: latency-svc-gx9dc
Jan 29 14:25:37.511: INFO: Got endpoints: latency-svc-l9rfr [675.527434ms]
Jan 29 14:25:37.522: INFO: Created: latency-svc-8nnm5
Jan 29 14:25:37.550: INFO: Created: latency-svc-jhh9p
Jan 29 14:25:37.570: INFO: Got endpoints: latency-svc-lzd5d [696.463251ms]
Jan 29 14:25:37.571: INFO: Created: latency-svc-drlxh
Jan 29 14:25:37.626: INFO: Created: latency-svc-sfb52
Jan 29 14:25:37.631: INFO: Got endpoints: latency-svc-vtf5v [713.944834ms]
Jan 29 14:25:37.681: INFO: Created: latency-svc-9srn6
Jan 29 14:25:37.682: INFO: Got endpoints: latency-svc-75txf [748.629959ms]
Jan 29 14:25:37.694: INFO: Created: latency-svc-9xhlj
Jan 29 14:25:37.710: INFO: Got endpoints: latency-svc-gw7dc [721.742529ms]
Jan 29 14:25:37.730: INFO: Created: latency-svc-bbj9m
Jan 29 14:25:37.750: INFO: Created: latency-svc-xmr8v
Jan 29 14:25:37.771: INFO: Got endpoints: latency-svc-945r7 [752.617979ms]
Jan 29 14:25:37.809: INFO: Created: latency-svc-zt8bz
Jan 29 14:25:37.818: INFO: Created: latency-svc-vj862
Jan 29 14:25:37.819: INFO: Got endpoints: latency-svc-jtbxj [757.278332ms]
Jan 29 14:25:37.837: INFO: Created: latency-svc-dsr79
Jan 29 14:25:37.865: INFO: Got endpoints: latency-svc-r8kp4 [753.601409ms]
Jan 29 14:25:37.893: INFO: Created: latency-svc-wjzgj
Jan 29 14:25:37.916: INFO: Got endpoints: latency-svc-5stxj [751.311962ms]
Jan 29 14:25:37.956: INFO: Created: latency-svc-cbt4z
Jan 29 14:25:37.958: INFO: Got endpoints: latency-svc-wqrd8 [740.731731ms]
Jan 29 14:25:37.989: INFO: Created: latency-svc-2vts8
Jan 29 14:25:38.014: INFO: Got endpoints: latency-svc-gx9dc [757.127931ms]
Jan 29 14:25:38.041: INFO: Created: latency-svc-8xc7w
Jan 29 14:25:38.073: INFO: Got endpoints: latency-svc-8nnm5 [746.875014ms]
Jan 29 14:25:38.133: INFO: Got endpoints: latency-svc-jhh9p [770.574237ms]
Jan 29 14:25:38.145: INFO: Created: latency-svc-6ldz7
Jan 29 14:25:38.156: INFO: Got endpoints: latency-svc-drlxh [718.735546ms]
Jan 29 14:25:38.231: INFO: Created: latency-svc-sbj2h
Jan 29 14:25:38.253: INFO: Got endpoints: latency-svc-sfb52 [778.077239ms]
Jan 29 14:25:38.270: INFO: Created: latency-svc-98ctg
Jan 29 14:25:38.270: INFO: Got endpoints: latency-svc-9srn6 [758.058797ms]
Jan 29 14:25:38.294: INFO: Created: latency-svc-h2stx
Jan 29 14:25:38.331: INFO: Created: latency-svc-t4955
Jan 29 14:25:38.333: INFO: Got endpoints: latency-svc-9xhlj [762.071292ms]
Jan 29 14:25:38.378: INFO: Got endpoints: latency-svc-bbj9m [747.176297ms]
Jan 29 14:25:38.392: INFO: Created: latency-svc-ttg2n
Jan 29 14:25:38.424: INFO: Got endpoints: latency-svc-xmr8v [741.955854ms]
Jan 29 14:25:38.425: INFO: Created: latency-svc-5vdc6
Jan 29 14:25:38.463: INFO: Got endpoints: latency-svc-zt8bz [752.742542ms]
Jan 29 14:25:38.469: INFO: Created: latency-svc-t44rr
Jan 29 14:25:38.505: INFO: Created: latency-svc-nvk7d
Jan 29 14:25:38.522: INFO: Got endpoints: latency-svc-vj862 [751.035229ms]
Jan 29 14:25:38.566: INFO: Created: latency-svc-nhq4h
Jan 29 14:25:38.567: INFO: Got endpoints: latency-svc-dsr79 [747.657461ms]
Jan 29 14:25:38.621: INFO: Created: latency-svc-t97c7
Jan 29 14:25:38.626: INFO: Got endpoints: latency-svc-wjzgj [760.139835ms]
Jan 29 14:25:38.653: INFO: Created: latency-svc-t87rv
Jan 29 14:25:38.674: INFO: Got endpoints: latency-svc-cbt4z [758.200458ms]
Jan 29 14:25:38.708: INFO: Created: latency-svc-x64sq
Jan 29 14:25:38.716: INFO: Got endpoints: latency-svc-2vts8 [757.649287ms]
Jan 29 14:25:38.750: INFO: Created: latency-svc-q8s5h
Jan 29 14:25:38.788: INFO: Got endpoints: latency-svc-8xc7w [773.646022ms]
Jan 29 14:25:38.825: INFO: Created: latency-svc-grffg
Jan 29 14:25:38.825: INFO: Got endpoints: latency-svc-6ldz7 [751.316337ms]
Jan 29 14:25:38.866: INFO: Got endpoints: latency-svc-sbj2h [732.807936ms]
Jan 29 14:25:38.867: INFO: Created: latency-svc-jkbwn
Jan 29 14:25:38.922: INFO: Got endpoints: latency-svc-98ctg [765.532069ms]
Jan 29 14:25:38.931: INFO: Created: latency-svc-vs4kc
Jan 29 14:25:38.961: INFO: Got endpoints: latency-svc-h2stx [708.202034ms]
Jan 29 14:25:38.965: INFO: Created: latency-svc-59tj6
Jan 29 14:25:39.005: INFO: Created: latency-svc-p2dkx
Jan 29 14:25:39.021: INFO: Got endpoints: latency-svc-t4955 [751.737346ms]
Jan 29 14:25:39.059: INFO: Created: latency-svc-z2bzj
Jan 29 14:25:39.065: INFO: Got endpoints: latency-svc-ttg2n [732.392531ms]
Jan 29 14:25:39.093: INFO: Created: latency-svc-9hpjg
Jan 29 14:25:39.141: INFO: Got endpoints: latency-svc-5vdc6 [762.698626ms]
Jan 29 14:25:39.165: INFO: Got endpoints: latency-svc-t44rr [739.793606ms]
Jan 29 14:25:39.181: INFO: Created: latency-svc-6w6nt
Jan 29 14:25:39.191: INFO: Created: latency-svc-6vjww
Jan 29 14:25:39.218: INFO: Got endpoints: latency-svc-nvk7d [755.020987ms]
Jan 29 14:25:39.252: INFO: Created: latency-svc-6djnp
Jan 29 14:25:39.259: INFO: Got endpoints: latency-svc-nhq4h [737.04485ms]
Jan 29 14:25:39.293: INFO: Created: latency-svc-dw2bc
Jan 29 14:25:39.309: INFO: Got endpoints: latency-svc-t97c7 [741.554896ms]
Jan 29 14:25:39.337: INFO: Created: latency-svc-2c22l
Jan 29 14:25:39.367: INFO: Got endpoints: latency-svc-t87rv [741.491702ms]
Jan 29 14:25:39.409: INFO: Created: latency-svc-fl624
Jan 29 14:25:39.410: INFO: Got endpoints: latency-svc-x64sq [735.144512ms]
Jan 29 14:25:39.444: INFO: Created: latency-svc-c4lfv
Jan 29 14:25:39.457: INFO: Got endpoints: latency-svc-q8s5h [740.006754ms]
Jan 29 14:25:39.484: INFO: Created: latency-svc-hsqsw
Jan 29 14:25:39.541: INFO: Got endpoints: latency-svc-grffg [752.533064ms]
Jan 29 14:25:39.563: INFO: Created: latency-svc-z7w97
Jan 29 14:25:39.571: INFO: Got endpoints: latency-svc-jkbwn [745.457824ms]
Jan 29 14:25:39.595: INFO: Created: latency-svc-g52fv
Jan 29 14:25:39.619: INFO: Got endpoints: latency-svc-vs4kc [752.914484ms]
Jan 29 14:25:39.642: INFO: Created: latency-svc-9ns9r
Jan 29 14:25:39.670: INFO: Got endpoints: latency-svc-59tj6 [747.329975ms]
Jan 29 14:25:39.705: INFO: Created: latency-svc-lb6sc
Jan 29 14:25:39.713: INFO: Got endpoints: latency-svc-p2dkx [752.360098ms]
Jan 29 14:25:39.755: INFO: Created: latency-svc-skc8q
Jan 29 14:25:39.765: INFO: Got endpoints: latency-svc-z2bzj [743.377252ms]
Jan 29 14:25:39.795: INFO: Created: latency-svc-q7vgs
Jan 29 14:25:39.814: INFO: Got endpoints: latency-svc-9hpjg [748.945906ms]
Jan 29 14:25:39.853: INFO: Created: latency-svc-fb2hd
Jan 29 14:25:39.859: INFO: Got endpoints: latency-svc-6w6nt [717.647176ms]
Jan 29 14:25:39.894: INFO: Created: latency-svc-k4tb4
Jan 29 14:25:39.915: INFO: Got endpoints: latency-svc-6vjww [749.618155ms]
Jan 29 14:25:39.972: INFO: Got endpoints: latency-svc-6djnp [752.962124ms]
Jan 29 14:25:39.976: INFO: Created: latency-svc-vwz2v
Jan 29 14:25:39.993: INFO: Created: latency-svc-nljzq
Jan 29 14:25:40.010: INFO: Got endpoints: latency-svc-dw2bc [750.202281ms]
Jan 29 14:25:40.058: INFO: Created: latency-svc-gt7rp
Jan 29 14:25:40.088: INFO: Got endpoints: latency-svc-2c22l [778.642128ms]
Jan 29 14:25:40.114: INFO: Got endpoints: latency-svc-fl624 [746.809008ms]
Jan 29 14:25:40.176: INFO: Got endpoints: latency-svc-c4lfv [765.571177ms]
Jan 29 14:25:40.192: INFO: Created: latency-svc-hxjwf
Jan 29 14:25:40.216: INFO: Created: latency-svc-2gscx
Jan 29 14:25:40.217: INFO: Got endpoints: latency-svc-hsqsw [759.974699ms]
Jan 29 14:25:40.239: INFO: Created: latency-svc-tnl8n
Jan 29 14:25:40.261: INFO: Got endpoints: latency-svc-z7w97 [720.630019ms]
Jan 29 14:25:40.275: INFO: Created: latency-svc-kx8zm
Jan 29 14:25:40.300: INFO: Created: latency-svc-bn58l
Jan 29 14:25:40.313: INFO: Got endpoints: latency-svc-g52fv [741.672094ms]
Jan 29 14:25:40.366: INFO: Got endpoints: latency-svc-9ns9r [746.091923ms]
Jan 29 14:25:40.380: INFO: Created: latency-svc-xmcpz
Jan 29 14:25:40.401: INFO: Created: latency-svc-llvk5
Jan 29 14:25:40.414: INFO: Got endpoints: latency-svc-lb6sc [744.781859ms]
Jan 29 14:25:40.462: INFO: Created: latency-svc-4jksb
Jan 29 14:25:40.462: INFO: Got endpoints: latency-svc-skc8q [748.781125ms]
Jan 29 14:25:40.509: INFO: Created: latency-svc-52vp4
Jan 29 14:25:40.513: INFO: Got endpoints: latency-svc-q7vgs [747.47032ms]
Jan 29 14:25:40.566: INFO: Created: latency-svc-f74ql
Jan 29 14:25:40.575: INFO: Got endpoints: latency-svc-fb2hd [759.887585ms]
Jan 29 14:25:40.606: INFO: Created: latency-svc-nz4xg
Jan 29 14:25:40.632: INFO: Got endpoints: latency-svc-k4tb4 [772.790605ms]
Jan 29 14:25:40.697: INFO: Got endpoints: latency-svc-vwz2v [782.127563ms]
Jan 29 14:25:40.702: INFO: Created: latency-svc-hs5gd
Jan 29 14:25:40.712: INFO: Got endpoints: latency-svc-nljzq [739.667574ms]
Jan 29 14:25:40.743: INFO: Created: latency-svc-4wv9h
Jan 29 14:25:40.804: INFO: Created: latency-svc-d6k9w
Jan 29 14:25:40.825: INFO: Got endpoints: latency-svc-gt7rp [815.175424ms]
Jan 29 14:25:40.826: INFO: Got endpoints: latency-svc-hxjwf [738.340141ms]
Jan 29 14:25:40.880: INFO: Got endpoints: latency-svc-2gscx [765.267781ms]
Jan 29 14:25:40.881: INFO: Created: latency-svc-nsnz5
Jan 29 14:25:40.892: INFO: Created: latency-svc-wwm6w
Jan 29 14:25:40.906: INFO: Created: latency-svc-vnnn8
Jan 29 14:25:40.910: INFO: Got endpoints: latency-svc-tnl8n [734.705119ms]
Jan 29 14:25:40.954: INFO: Created: latency-svc-thtzk
Jan 29 14:25:40.958: INFO: Got endpoints: latency-svc-kx8zm [740.038772ms]
Jan 29 14:25:41.015: INFO: Got endpoints: latency-svc-bn58l [753.121773ms]
Jan 29 14:25:41.018: INFO: Created: latency-svc-nrnrp
Jan 29 14:25:41.059: INFO: Created: latency-svc-wwzmn
Jan 29 14:25:41.077: INFO: Got endpoints: latency-svc-xmcpz [764.28995ms]
Jan 29 14:25:41.123: INFO: Got endpoints: latency-svc-llvk5 [756.4812ms]
Jan 29 14:25:41.123: INFO: Created: latency-svc-vcrxf
Jan 29 14:25:41.153: INFO: Created: latency-svc-w79xf
Jan 29 14:25:41.179: INFO: Got endpoints: latency-svc-4jksb [763.558297ms]
Jan 29 14:25:41.220: INFO: Created: latency-svc-4xz9k
Jan 29 14:25:41.221: INFO: Got endpoints: latency-svc-52vp4 [758.522779ms]
Jan 29 14:25:41.269: INFO: Created: latency-svc-gsgz4
Jan 29 14:25:41.276: INFO: Got endpoints: latency-svc-f74ql [763.015959ms]
Jan 29 14:25:41.302: INFO: Created: latency-svc-mxcfd
Jan 29 14:25:41.307: INFO: Got endpoints: latency-svc-nz4xg [732.579672ms]
Jan 29 14:25:41.333: INFO: Created: latency-svc-mzm9n
Jan 29 14:25:41.359: INFO: Got endpoints: latency-svc-hs5gd [727.567517ms]
Jan 29 14:25:41.411: INFO: Got endpoints: latency-svc-4wv9h [713.586342ms]
Jan 29 14:25:41.458: INFO: Got endpoints: latency-svc-d6k9w [745.804234ms]
Jan 29 14:25:41.509: INFO: Got endpoints: latency-svc-nsnz5 [683.092421ms]
Jan 29 14:25:41.556: INFO: Got endpoints: latency-svc-wwm6w [730.076004ms]
Jan 29 14:25:41.610: INFO: Got endpoints: latency-svc-vnnn8 [730.261719ms]
Jan 29 14:25:41.681: INFO: Got endpoints: latency-svc-thtzk [769.988743ms]
Jan 29 14:25:41.714: INFO: Got endpoints: latency-svc-nrnrp [755.282139ms]
Jan 29 14:25:41.763: INFO: Got endpoints: latency-svc-wwzmn [748.060795ms]
Jan 29 14:25:41.812: INFO: Got endpoints: latency-svc-vcrxf [734.483824ms]
Jan 29 14:25:41.856: INFO: Got endpoints: latency-svc-w79xf [733.168324ms]
Jan 29 14:25:41.913: INFO: Got endpoints: latency-svc-4xz9k [734.155723ms]
Jan 29 14:25:41.972: INFO: Got endpoints: latency-svc-gsgz4 [751.194466ms]
Jan 29 14:25:42.020: INFO: Got endpoints: latency-svc-mxcfd [743.521994ms]
Jan 29 14:25:42.059: INFO: Got endpoints: latency-svc-mzm9n [751.200934ms]
Jan 29 14:25:42.059: INFO: Latencies: [59.008489ms 74.945564ms 92.223819ms 103.526414ms 125.940749ms 164.629326ms 197.011695ms 207.148544ms 286.158224ms 287.348746ms 301.532441ms 366.794447ms 384.055523ms 393.699842ms 402.001686ms 406.284219ms 415.072665ms 416.719153ms 420.707632ms 421.773298ms 423.472109ms 423.660716ms 425.001002ms 425.086444ms 430.743468ms 432.072127ms 436.467761ms 440.172785ms 442.979591ms 444.26014ms 444.96716ms 452.183729ms 454.677169ms 458.802514ms 469.188195ms 473.079125ms 474.662809ms 478.133277ms 478.299469ms 482.384724ms 489.001651ms 497.465654ms 498.512034ms 499.514252ms 500.024263ms 500.569527ms 504.82103ms 505.397202ms 505.84615ms 506.261039ms 506.363739ms 509.447752ms 511.980191ms 512.111422ms 516.070323ms 533.093825ms 533.425245ms 540.906308ms 541.719402ms 543.396486ms 546.139083ms 549.690793ms 558.905876ms 562.182196ms 564.052513ms 565.251854ms 567.240759ms 568.460448ms 570.96094ms 572.708628ms 573.72037ms 574.826359ms 574.867689ms 576.838598ms 584.85161ms 585.341417ms 585.971321ms 586.49598ms 588.129315ms 593.632812ms 594.52717ms 609.831533ms 611.316268ms 614.38039ms 615.836558ms 619.482208ms 619.827359ms 630.26596ms 631.110502ms 631.753216ms 635.890023ms 636.54598ms 645.599457ms 650.612217ms 651.174948ms 651.632819ms 652.727031ms 654.156091ms 654.513906ms 665.070188ms 665.748415ms 668.588754ms 675.527434ms 679.345832ms 683.092421ms 685.801423ms 689.714415ms 689.820258ms 696.463251ms 708.202034ms 713.253705ms 713.586342ms 713.944834ms 717.647176ms 718.735546ms 720.630019ms 721.742529ms 727.567517ms 728.663531ms 730.076004ms 730.261719ms 732.392531ms 732.579672ms 732.807936ms 733.168324ms 734.155723ms 734.483824ms 734.705119ms 735.144512ms 737.04485ms 738.340141ms 739.667574ms 739.793606ms 740.006754ms 740.038772ms 740.731731ms 741.491702ms 741.554896ms 741.672094ms 741.955854ms 743.377252ms 743.521994ms 744.781859ms 745.457824ms 745.804234ms 746.091923ms 746.809008ms 746.875014ms 747.176297ms 747.329975ms 747.47032ms 747.657461ms 748.060795ms 748.629959ms 748.781125ms 748.945906ms 749.618155ms 750.202281ms 751.035229ms 751.194466ms 751.200934ms 751.311962ms 751.316337ms 751.737346ms 752.360098ms 752.533064ms 752.617979ms 752.742542ms 752.914484ms 752.962124ms 753.121773ms 753.601409ms 755.020987ms 755.282139ms 756.4812ms 757.127931ms 757.278332ms 757.649287ms 758.058797ms 758.200458ms 758.522779ms 759.887585ms 759.974699ms 760.139835ms 762.071292ms 762.698626ms 763.015959ms 763.558297ms 764.28995ms 765.267781ms 765.532069ms 765.571177ms 769.988743ms 770.574237ms 772.790605ms 773.646022ms 778.077239ms 778.642128ms 782.127563ms 815.175424ms]
Jan 29 14:25:42.060: INFO: 50 %ile: 665.748415ms
Jan 29 14:25:42.060: INFO: 90 %ile: 758.522779ms
Jan 29 14:25:42.060: INFO: 99 %ile: 782.127563ms
Jan 29 14:25:42.060: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:25:42.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-2380" for this suite.

• [SLOW TEST:12.121 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":276,"completed":199,"skipped":3268,"failed":0}
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:25:42.101: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-9276
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Jan 29 14:25:42.339: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 29 14:25:42.375: INFO: Waiting for terminating namespaces to be deleted...
Jan 29 14:25:42.383: INFO: 
Logging pods the kubelet thinks is on node metakube-worker-cmccl-6d88bd94fc-87n7l before test
Jan 29 14:25:42.425: INFO: node-local-dns-nhzkw from kube-system started at 2020-01-29 10:31:06 +0000 UTC (1 container statuses recorded)
Jan 29 14:25:42.425: INFO: 	Container node-cache ready: true, restart count 0
Jan 29 14:25:42.425: INFO: sonobuoy-systemd-logs-daemon-set-6c9811082b824a7c-sbw8r from sonobuoy started at 2020-01-29 13:30:03 +0000 UTC (2 container statuses recorded)
Jan 29 14:25:42.425: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 29 14:25:42.425: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 29 14:25:42.425: INFO: kube-proxy-7jg5r from kube-system started at 2020-01-29 10:31:06 +0000 UTC (1 container statuses recorded)
Jan 29 14:25:42.425: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 29 14:25:42.425: INFO: node-exporter-4wc5c from kube-system started at 2020-01-29 10:31:07 +0000 UTC (1 container statuses recorded)
Jan 29 14:25:42.425: INFO: 	Container node-exporter ready: true, restart count 0
Jan 29 14:25:42.426: INFO: cluster-autoscaler-8c65c7d54-z9ddg from kube-system started at 2020-01-29 10:31:35 +0000 UTC (1 container statuses recorded)
Jan 29 14:25:42.426: INFO: 	Container cluster-autoscaler ready: true, restart count 1
Jan 29 14:25:42.426: INFO: tiller-deploy-78f78bb476-hz2w6 from kube-system started at 2020-01-29 10:31:35 +0000 UTC (1 container statuses recorded)
Jan 29 14:25:42.426: INFO: 	Container tiller ready: true, restart count 0
Jan 29 14:25:42.426: INFO: coredns-6f745f6d74-8q86x from kube-system started at 2020-01-29 10:31:43 +0000 UTC (1 container statuses recorded)
Jan 29 14:25:42.426: INFO: 	Container coredns ready: true, restart count 0
Jan 29 14:25:42.426: INFO: canal-lpkd9 from kube-system started at 2020-01-29 10:31:07 +0000 UTC (2 container statuses recorded)
Jan 29 14:25:42.426: INFO: 	Container calico-node ready: true, restart count 0
Jan 29 14:25:42.426: INFO: 	Container kube-flannel ready: true, restart count 0
Jan 29 14:25:42.426: INFO: openvpn-client-64df8b95c9-5fqpn from kube-system started at 2020-01-29 10:31:44 +0000 UTC (2 container statuses recorded)
Jan 29 14:25:42.426: INFO: 	Container dnat-controller ready: true, restart count 0
Jan 29 14:25:42.426: INFO: 	Container openvpn-client ready: true, restart count 1
Jan 29 14:25:42.427: INFO: coredns-6f745f6d74-kr8n2 from kube-system started at 2020-01-29 10:31:44 +0000 UTC (1 container statuses recorded)
Jan 29 14:25:42.427: INFO: 	Container coredns ready: true, restart count 0
Jan 29 14:25:42.427: INFO: 
Logging pods the kubelet thinks is on node metakube-worker-cmccl-6d88bd94fc-lqfxz before test
Jan 29 14:25:42.474: INFO: canal-4zhl8 from kube-system started at 2020-01-29 10:32:02 +0000 UTC (2 container statuses recorded)
Jan 29 14:25:42.474: INFO: 	Container calico-node ready: true, restart count 0
Jan 29 14:25:42.474: INFO: 	Container kube-flannel ready: true, restart count 0
Jan 29 14:25:42.474: INFO: kube-proxy-rr5gp from kube-system started at 2020-01-29 10:32:02 +0000 UTC (1 container statuses recorded)
Jan 29 14:25:42.474: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 29 14:25:42.474: INFO: svc-latency-rc-p2jk8 from svc-latency-2380 started at 2020-01-29 14:25:30 +0000 UTC (1 container statuses recorded)
Jan 29 14:25:42.474: INFO: 	Container svc-latency-rc ready: true, restart count 0
Jan 29 14:25:42.474: INFO: node-exporter-99p82 from kube-system started at 2020-01-29 10:32:02 +0000 UTC (1 container statuses recorded)
Jan 29 14:25:42.474: INFO: 	Container node-exporter ready: true, restart count 0
Jan 29 14:25:42.474: INFO: node-local-dns-72k8g from kube-system started at 2020-01-29 10:32:02 +0000 UTC (1 container statuses recorded)
Jan 29 14:25:42.474: INFO: 	Container node-cache ready: true, restart count 0
Jan 29 14:25:42.474: INFO: sonobuoy-systemd-logs-daemon-set-6c9811082b824a7c-9g4cq from sonobuoy started at 2020-01-29 13:30:03 +0000 UTC (2 container statuses recorded)
Jan 29 14:25:42.474: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 29 14:25:42.474: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 29 14:25:42.474: INFO: 
Logging pods the kubelet thinks is on node metakube-worker-cmccl-6d88bd94fc-znv5g before test
Jan 29 14:25:42.555: INFO: sonobuoy-systemd-logs-daemon-set-6c9811082b824a7c-mxxcq from sonobuoy started at 2020-01-29 13:30:03 +0000 UTC (2 container statuses recorded)
Jan 29 14:25:42.555: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 29 14:25:42.555: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 29 14:25:42.555: INFO: kube-proxy-h6b8h from kube-system started at 2020-01-29 10:31:22 +0000 UTC (1 container statuses recorded)
Jan 29 14:25:42.555: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 29 14:25:42.555: INFO: sonobuoy from sonobuoy started at 2020-01-29 13:30:00 +0000 UTC (1 container statuses recorded)
Jan 29 14:25:42.555: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 29 14:25:42.556: INFO: node-local-dns-2tvcw from kube-system started at 2020-01-29 10:31:23 +0000 UTC (1 container statuses recorded)
Jan 29 14:25:42.556: INFO: 	Container node-cache ready: true, restart count 0
Jan 29 14:25:42.556: INFO: canal-d5wpn from kube-system started at 2020-01-29 10:31:23 +0000 UTC (2 container statuses recorded)
Jan 29 14:25:42.556: INFO: 	Container calico-node ready: true, restart count 0
Jan 29 14:25:42.556: INFO: 	Container kube-flannel ready: true, restart count 0
Jan 29 14:25:42.556: INFO: node-exporter-xblbq from kube-system started at 2020-01-29 10:31:23 +0000 UTC (1 container statuses recorded)
Jan 29 14:25:42.556: INFO: 	Container node-exporter ready: true, restart count 0
Jan 29 14:25:42.556: INFO: sonobuoy-e2e-job-bb288164d824466f from sonobuoy started at 2020-01-29 13:30:03 +0000 UTC (2 container statuses recorded)
Jan 29 14:25:42.556: INFO: 	Container e2e ready: true, restart count 0
Jan 29 14:25:42.556: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-e7991a74-c3d8-4a16-94f6-9bd3959fe26d 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-e7991a74-c3d8-4a16-94f6-9bd3959fe26d off the node metakube-worker-cmccl-6d88bd94fc-lqfxz
STEP: verifying the node doesn't have the label kubernetes.io/e2e-e7991a74-c3d8-4a16-94f6-9bd3959fe26d
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:30:56.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9276" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:314.878 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":276,"completed":200,"skipped":3275,"failed":0}
SSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:30:56.983: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2902
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Jan 29 14:30:57.294: INFO: Waiting up to 5m0s for pod "downward-api-8d1398cd-8fd3-400c-8318-f52ce520109d" in namespace "downward-api-2902" to be "success or failure"
Jan 29 14:30:57.302: INFO: Pod "downward-api-8d1398cd-8fd3-400c-8318-f52ce520109d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.195495ms
Jan 29 14:30:59.321: INFO: Pod "downward-api-8d1398cd-8fd3-400c-8318-f52ce520109d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026981835s
Jan 29 14:31:01.338: INFO: Pod "downward-api-8d1398cd-8fd3-400c-8318-f52ce520109d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043657802s
STEP: Saw pod success
Jan 29 14:31:01.338: INFO: Pod "downward-api-8d1398cd-8fd3-400c-8318-f52ce520109d" satisfied condition "success or failure"
Jan 29 14:31:01.349: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod downward-api-8d1398cd-8fd3-400c-8318-f52ce520109d container dapi-container: <nil>
STEP: delete the pod
Jan 29 14:31:01.498: INFO: Waiting for pod downward-api-8d1398cd-8fd3-400c-8318-f52ce520109d to disappear
Jan 29 14:31:01.539: INFO: Pod downward-api-8d1398cd-8fd3-400c-8318-f52ce520109d no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:31:01.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2902" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":276,"completed":201,"skipped":3279,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:31:01.585: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2899
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-secret-9cvq
STEP: Creating a pod to test atomic-volume-subpath
Jan 29 14:31:02.023: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-9cvq" in namespace "subpath-2899" to be "success or failure"
Jan 29 14:31:02.045: INFO: Pod "pod-subpath-test-secret-9cvq": Phase="Pending", Reason="", readiness=false. Elapsed: 21.459261ms
Jan 29 14:31:04.077: INFO: Pod "pod-subpath-test-secret-9cvq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053678632s
Jan 29 14:31:06.089: INFO: Pod "pod-subpath-test-secret-9cvq": Phase="Pending", Reason="", readiness=false. Elapsed: 4.065969826s
Jan 29 14:31:08.109: INFO: Pod "pod-subpath-test-secret-9cvq": Phase="Running", Reason="", readiness=true. Elapsed: 6.085030396s
Jan 29 14:31:10.119: INFO: Pod "pod-subpath-test-secret-9cvq": Phase="Running", Reason="", readiness=true. Elapsed: 8.095465779s
Jan 29 14:31:12.141: INFO: Pod "pod-subpath-test-secret-9cvq": Phase="Running", Reason="", readiness=true. Elapsed: 10.117413884s
Jan 29 14:31:14.150: INFO: Pod "pod-subpath-test-secret-9cvq": Phase="Running", Reason="", readiness=true. Elapsed: 12.126392513s
Jan 29 14:31:16.159: INFO: Pod "pod-subpath-test-secret-9cvq": Phase="Running", Reason="", readiness=true. Elapsed: 14.135167561s
Jan 29 14:31:18.168: INFO: Pod "pod-subpath-test-secret-9cvq": Phase="Running", Reason="", readiness=true. Elapsed: 16.1447838s
Jan 29 14:31:20.179: INFO: Pod "pod-subpath-test-secret-9cvq": Phase="Running", Reason="", readiness=true. Elapsed: 18.155092449s
Jan 29 14:31:22.187: INFO: Pod "pod-subpath-test-secret-9cvq": Phase="Running", Reason="", readiness=true. Elapsed: 20.163617472s
Jan 29 14:31:24.201: INFO: Pod "pod-subpath-test-secret-9cvq": Phase="Running", Reason="", readiness=true. Elapsed: 22.177277479s
Jan 29 14:31:26.238: INFO: Pod "pod-subpath-test-secret-9cvq": Phase="Running", Reason="", readiness=true. Elapsed: 24.214431007s
Jan 29 14:31:28.261: INFO: Pod "pod-subpath-test-secret-9cvq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.237272777s
STEP: Saw pod success
Jan 29 14:31:28.261: INFO: Pod "pod-subpath-test-secret-9cvq" satisfied condition "success or failure"
Jan 29 14:31:28.345: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-subpath-test-secret-9cvq container test-container-subpath-secret-9cvq: <nil>
STEP: delete the pod
Jan 29 14:31:28.623: INFO: Waiting for pod pod-subpath-test-secret-9cvq to disappear
Jan 29 14:31:28.673: INFO: Pod pod-subpath-test-secret-9cvq no longer exists
STEP: Deleting pod pod-subpath-test-secret-9cvq
Jan 29 14:31:28.673: INFO: Deleting pod "pod-subpath-test-secret-9cvq" in namespace "subpath-2899"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:31:28.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2899" for this suite.

• [SLOW TEST:27.258 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":276,"completed":202,"skipped":3302,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:31:28.845: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3154
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-713da56f-4e22-47ec-be7f-d458d2e7fb1c
STEP: Creating a pod to test consume configMaps
Jan 29 14:31:30.117: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9436c180-ee26-48b0-b7af-46ad514a1141" in namespace "projected-3154" to be "success or failure"
Jan 29 14:31:30.177: INFO: Pod "pod-projected-configmaps-9436c180-ee26-48b0-b7af-46ad514a1141": Phase="Pending", Reason="", readiness=false. Elapsed: 59.630809ms
Jan 29 14:31:32.200: INFO: Pod "pod-projected-configmaps-9436c180-ee26-48b0-b7af-46ad514a1141": Phase="Pending", Reason="", readiness=false. Elapsed: 2.082423751s
Jan 29 14:31:34.222: INFO: Pod "pod-projected-configmaps-9436c180-ee26-48b0-b7af-46ad514a1141": Phase="Pending", Reason="", readiness=false. Elapsed: 4.105188208s
Jan 29 14:31:36.235: INFO: Pod "pod-projected-configmaps-9436c180-ee26-48b0-b7af-46ad514a1141": Phase="Pending", Reason="", readiness=false. Elapsed: 6.118034609s
Jan 29 14:31:38.244: INFO: Pod "pod-projected-configmaps-9436c180-ee26-48b0-b7af-46ad514a1141": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.12700471s
STEP: Saw pod success
Jan 29 14:31:38.244: INFO: Pod "pod-projected-configmaps-9436c180-ee26-48b0-b7af-46ad514a1141" satisfied condition "success or failure"
Jan 29 14:31:38.259: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-projected-configmaps-9436c180-ee26-48b0-b7af-46ad514a1141 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 29 14:31:38.331: INFO: Waiting for pod pod-projected-configmaps-9436c180-ee26-48b0-b7af-46ad514a1141 to disappear
Jan 29 14:31:38.339: INFO: Pod pod-projected-configmaps-9436c180-ee26-48b0-b7af-46ad514a1141 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:31:38.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3154" for this suite.

• [SLOW TEST:9.534 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":276,"completed":203,"skipped":3306,"failed":0}
SSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:31:38.380: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1044
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 29 14:31:41.676: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:31:41.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1044" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":276,"completed":204,"skipped":3312,"failed":0}
S
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:31:41.780: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-916
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:178
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:31:42.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-916" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":276,"completed":205,"skipped":3313,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:31:42.090: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7225
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-41295682-f6ae-4514-aac8-ba5731b1fb97
STEP: Creating a pod to test consume secrets
Jan 29 14:31:42.362: INFO: Waiting up to 5m0s for pod "pod-secrets-68d6a43d-195b-4a7d-bc14-8748b1956674" in namespace "secrets-7225" to be "success or failure"
Jan 29 14:31:42.375: INFO: Pod "pod-secrets-68d6a43d-195b-4a7d-bc14-8748b1956674": Phase="Pending", Reason="", readiness=false. Elapsed: 13.267113ms
Jan 29 14:31:44.387: INFO: Pod "pod-secrets-68d6a43d-195b-4a7d-bc14-8748b1956674": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025688059s
Jan 29 14:31:46.405: INFO: Pod "pod-secrets-68d6a43d-195b-4a7d-bc14-8748b1956674": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043643402s
STEP: Saw pod success
Jan 29 14:31:46.406: INFO: Pod "pod-secrets-68d6a43d-195b-4a7d-bc14-8748b1956674" satisfied condition "success or failure"
Jan 29 14:31:46.425: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-secrets-68d6a43d-195b-4a7d-bc14-8748b1956674 container secret-env-test: <nil>
STEP: delete the pod
Jan 29 14:31:46.485: INFO: Waiting for pod pod-secrets-68d6a43d-195b-4a7d-bc14-8748b1956674 to disappear
Jan 29 14:31:46.491: INFO: Pod pod-secrets-68d6a43d-195b-4a7d-bc14-8748b1956674 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:31:46.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7225" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":276,"completed":206,"skipped":3322,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:31:46.519: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7148
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl logs
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1464
STEP: creating an pod
Jan 29 14:31:46.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.8 --namespace=kubectl-7148 -- logs-generator --log-lines-total 100 --run-duration 20s'
Jan 29 14:31:46.930: INFO: stderr: ""
Jan 29 14:31:46.930: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Waiting for log generator to start.
Jan 29 14:31:46.930: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jan 29 14:31:46.930: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-7148" to be "running and ready, or succeeded"
Jan 29 14:31:46.939: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 8.97786ms
Jan 29 14:31:48.959: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028535313s
Jan 29 14:31:50.975: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.045249168s
Jan 29 14:31:50.975: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jan 29 14:31:50.975: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Jan 29 14:31:50.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 logs logs-generator logs-generator --namespace=kubectl-7148'
Jan 29 14:31:51.211: INFO: stderr: ""
Jan 29 14:31:51.211: INFO: stdout: "I0129 14:31:49.482754       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/wqwz 502\nI0129 14:31:49.684176       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/vsz 577\nI0129 14:31:49.882994       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/j65b 503\nI0129 14:31:50.083293       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/jj7 230\nI0129 14:31:50.282916       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/8kpg 202\nI0129 14:31:50.482956       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/xmb 509\nI0129 14:31:50.684347       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/hqbm 371\nI0129 14:31:50.882943       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/h6td 290\nI0129 14:31:51.083033       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/fggq 579\n"
STEP: limiting log lines
Jan 29 14:31:51.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 logs logs-generator logs-generator --namespace=kubectl-7148 --tail=1'
Jan 29 14:31:51.492: INFO: stderr: ""
Jan 29 14:31:51.492: INFO: stdout: "I0129 14:31:51.482960       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/tp5 399\n"
Jan 29 14:31:51.492: INFO: got output "I0129 14:31:51.482960       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/tp5 399\n"
STEP: limiting log bytes
Jan 29 14:31:51.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 logs logs-generator logs-generator --namespace=kubectl-7148 --limit-bytes=1'
Jan 29 14:31:51.654: INFO: stderr: ""
Jan 29 14:31:51.654: INFO: stdout: "I"
Jan 29 14:31:51.654: INFO: got output "I"
STEP: exposing timestamps
Jan 29 14:31:51.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 logs logs-generator logs-generator --namespace=kubectl-7148 --tail=1 --timestamps'
Jan 29 14:31:51.876: INFO: stderr: ""
Jan 29 14:31:51.876: INFO: stdout: "2020-01-29T14:31:51.68408796Z I0129 14:31:51.683859       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/cg7 552\n"
Jan 29 14:31:51.876: INFO: got output "2020-01-29T14:31:51.68408796Z I0129 14:31:51.683859       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/cg7 552\n"
STEP: restricting to a time range
Jan 29 14:31:54.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 logs logs-generator logs-generator --namespace=kubectl-7148 --since=1s'
Jan 29 14:31:54.575: INFO: stderr: ""
Jan 29 14:31:54.575: INFO: stdout: "I0129 14:31:53.683337       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/lwvc 217\nI0129 14:31:53.883035       1 logs_generator.go:76] 22 POST /api/v1/namespaces/kube-system/pods/2nq 494\nI0129 14:31:54.083491       1 logs_generator.go:76] 23 GET /api/v1/namespaces/default/pods/qvm 592\nI0129 14:31:54.283323       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/default/pods/5nbl 427\nI0129 14:31:54.482897       1 logs_generator.go:76] 25 GET /api/v1/namespaces/default/pods/pjzd 282\n"
Jan 29 14:31:54.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 logs logs-generator logs-generator --namespace=kubectl-7148 --since=24h'
Jan 29 14:31:54.761: INFO: stderr: ""
Jan 29 14:31:54.762: INFO: stdout: "I0129 14:31:49.482754       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/wqwz 502\nI0129 14:31:49.684176       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/vsz 577\nI0129 14:31:49.882994       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/j65b 503\nI0129 14:31:50.083293       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/jj7 230\nI0129 14:31:50.282916       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/8kpg 202\nI0129 14:31:50.482956       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/xmb 509\nI0129 14:31:50.684347       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/hqbm 371\nI0129 14:31:50.882943       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/h6td 290\nI0129 14:31:51.083033       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/fggq 579\nI0129 14:31:51.283008       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/zs7b 354\nI0129 14:31:51.482960       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/tp5 399\nI0129 14:31:51.683859       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/cg7 552\nI0129 14:31:51.883045       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/j66 475\nI0129 14:31:52.083493       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/622 551\nI0129 14:31:52.293946       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/4ls 588\nI0129 14:31:52.483041       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/bcb2 404\nI0129 14:31:52.683318       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/dbq 499\nI0129 14:31:52.882997       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/9rx9 355\nI0129 14:31:53.083495       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/njjw 453\nI0129 14:31:53.283343       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/h8b 444\nI0129 14:31:53.483274       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/g7f 201\nI0129 14:31:53.683337       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/lwvc 217\nI0129 14:31:53.883035       1 logs_generator.go:76] 22 POST /api/v1/namespaces/kube-system/pods/2nq 494\nI0129 14:31:54.083491       1 logs_generator.go:76] 23 GET /api/v1/namespaces/default/pods/qvm 592\nI0129 14:31:54.283323       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/default/pods/5nbl 427\nI0129 14:31:54.482897       1 logs_generator.go:76] 25 GET /api/v1/namespaces/default/pods/pjzd 282\nI0129 14:31:54.683022       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/default/pods/nxjk 322\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1470
Jan 29 14:31:54.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 delete pod logs-generator --namespace=kubectl-7148'
Jan 29 14:32:00.694: INFO: stderr: ""
Jan 29 14:32:00.694: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:32:00.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7148" for this suite.

• [SLOW TEST:14.213 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1460
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":276,"completed":207,"skipped":3324,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:32:00.733: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3307
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Jan 29 14:32:01.128: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Jan 29 14:32:16.594: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
Jan 29 14:32:20.544: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:32:35.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3307" for this suite.

• [SLOW TEST:35.174 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":276,"completed":208,"skipped":3328,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:32:35.916: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3618
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:32:52.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3618" for this suite.

• [SLOW TEST:16.472 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":276,"completed":209,"skipped":3351,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:32:52.389: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5160
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod liveness-91abd855-688c-4131-8a11-a06f7249ed17 in namespace container-probe-5160
Jan 29 14:32:56.691: INFO: Started pod liveness-91abd855-688c-4131-8a11-a06f7249ed17 in namespace container-probe-5160
STEP: checking the pod's current state and verifying that restartCount is present
Jan 29 14:32:56.699: INFO: Initial restart count of pod liveness-91abd855-688c-4131-8a11-a06f7249ed17 is 0
Jan 29 14:33:16.829: INFO: Restart count of pod container-probe-5160/liveness-91abd855-688c-4131-8a11-a06f7249ed17 is now 1 (20.129631491s elapsed)
Jan 29 14:33:34.945: INFO: Restart count of pod container-probe-5160/liveness-91abd855-688c-4131-8a11-a06f7249ed17 is now 2 (38.245133687s elapsed)
Jan 29 14:33:57.334: INFO: Restart count of pod container-probe-5160/liveness-91abd855-688c-4131-8a11-a06f7249ed17 is now 3 (1m0.634537599s elapsed)
Jan 29 14:34:19.500: INFO: Restart count of pod container-probe-5160/liveness-91abd855-688c-4131-8a11-a06f7249ed17 is now 4 (1m22.800552606s elapsed)
Jan 29 14:35:24.445: INFO: Restart count of pod container-probe-5160/liveness-91abd855-688c-4131-8a11-a06f7249ed17 is now 5 (2m27.745531482s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:35:24.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5160" for this suite.

• [SLOW TEST:152.143 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":276,"completed":210,"skipped":3376,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:35:24.532: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4737
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 29 14:35:24.767: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:35:28.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4737" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":276,"completed":211,"skipped":3396,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:35:28.937: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-4680
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 29 14:35:29.221: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:35:30.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4680" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":276,"completed":212,"skipped":3398,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:35:30.403: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4823
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl label
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1382
STEP: creating the pod
Jan 29 14:35:30.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 create -f - --namespace=kubectl-4823'
Jan 29 14:35:33.622: INFO: stderr: ""
Jan 29 14:35:33.623: INFO: stdout: "pod/pause created\n"
Jan 29 14:35:33.623: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jan 29 14:35:33.623: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-4823" to be "running and ready"
Jan 29 14:35:33.635: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 11.876647ms
Jan 29 14:35:35.643: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020226851s
Jan 29 14:35:37.653: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.02985291s
Jan 29 14:35:37.653: INFO: Pod "pause" satisfied condition "running and ready"
Jan 29 14:35:37.653: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: adding the label testing-label with value testing-label-value to a pod
Jan 29 14:35:37.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 label pods pause testing-label=testing-label-value --namespace=kubectl-4823'
Jan 29 14:35:37.793: INFO: stderr: ""
Jan 29 14:35:37.793: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jan 29 14:35:37.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pod pause -L testing-label --namespace=kubectl-4823'
Jan 29 14:35:37.925: INFO: stderr: ""
Jan 29 14:35:37.925: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jan 29 14:35:37.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 label pods pause testing-label- --namespace=kubectl-4823'
Jan 29 14:35:38.080: INFO: stderr: ""
Jan 29 14:35:38.081: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jan 29 14:35:38.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pod pause -L testing-label --namespace=kubectl-4823'
Jan 29 14:35:38.204: INFO: stderr: ""
Jan 29 14:35:38.204: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1389
STEP: using delete to clean up resources
Jan 29 14:35:38.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 delete --grace-period=0 --force -f - --namespace=kubectl-4823'
Jan 29 14:35:38.359: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 29 14:35:38.360: INFO: stdout: "pod \"pause\" force deleted\n"
Jan 29 14:35:38.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get rc,svc -l name=pause --no-headers --namespace=kubectl-4823'
Jan 29 14:35:38.502: INFO: stderr: "No resources found in kubectl-4823 namespace.\n"
Jan 29 14:35:38.502: INFO: stdout: ""
Jan 29 14:35:38.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods -l name=pause --namespace=kubectl-4823 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 29 14:35:38.644: INFO: stderr: ""
Jan 29 14:35:38.644: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:35:38.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4823" for this suite.

• [SLOW TEST:8.270 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1379
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":276,"completed":213,"skipped":3399,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:35:38.675: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-9261
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 29 14:35:38.982: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:35:44.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9261" for this suite.

• [SLOW TEST:5.957 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:47
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":276,"completed":214,"skipped":3416,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:35:44.634: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5114
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-566a1270-b1f2-45ba-b660-0781b0a60612
STEP: Creating a pod to test consume configMaps
Jan 29 14:35:47.283: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-305ced26-b3a4-4105-bc8d-cac49ae5233c" in namespace "projected-5114" to be "success or failure"
Jan 29 14:35:47.317: INFO: Pod "pod-projected-configmaps-305ced26-b3a4-4105-bc8d-cac49ae5233c": Phase="Pending", Reason="", readiness=false. Elapsed: 34.473266ms
Jan 29 14:35:49.397: INFO: Pod "pod-projected-configmaps-305ced26-b3a4-4105-bc8d-cac49ae5233c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.114427537s
Jan 29 14:35:52.558: INFO: Pod "pod-projected-configmaps-305ced26-b3a4-4105-bc8d-cac49ae5233c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.275441231s
Jan 29 14:35:54.582: INFO: Pod "pod-projected-configmaps-305ced26-b3a4-4105-bc8d-cac49ae5233c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.298699619s
STEP: Saw pod success
Jan 29 14:35:54.583: INFO: Pod "pod-projected-configmaps-305ced26-b3a4-4105-bc8d-cac49ae5233c" satisfied condition "success or failure"
Jan 29 14:35:54.593: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-projected-configmaps-305ced26-b3a4-4105-bc8d-cac49ae5233c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 29 14:35:54.758: INFO: Waiting for pod pod-projected-configmaps-305ced26-b3a4-4105-bc8d-cac49ae5233c to disappear
Jan 29 14:35:54.768: INFO: Pod pod-projected-configmaps-305ced26-b3a4-4105-bc8d-cac49ae5233c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:35:54.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5114" for this suite.

• [SLOW TEST:10.177 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":276,"completed":215,"skipped":3428,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:35:54.811: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9751
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:36:12.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9751" for this suite.

• [SLOW TEST:17.478 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":276,"completed":216,"skipped":3429,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:36:12.294: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3384
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir volume type on node default medium
Jan 29 14:36:12.604: INFO: Waiting up to 5m0s for pod "pod-49949579-ad42-448a-843d-565781853f10" in namespace "emptydir-3384" to be "success or failure"
Jan 29 14:36:12.629: INFO: Pod "pod-49949579-ad42-448a-843d-565781853f10": Phase="Pending", Reason="", readiness=false. Elapsed: 25.489673ms
Jan 29 14:36:14.657: INFO: Pod "pod-49949579-ad42-448a-843d-565781853f10": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052952215s
Jan 29 14:36:16.672: INFO: Pod "pod-49949579-ad42-448a-843d-565781853f10": Phase="Pending", Reason="", readiness=false. Elapsed: 4.067852832s
Jan 29 14:36:18.685: INFO: Pod "pod-49949579-ad42-448a-843d-565781853f10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.080666475s
STEP: Saw pod success
Jan 29 14:36:18.685: INFO: Pod "pod-49949579-ad42-448a-843d-565781853f10" satisfied condition "success or failure"
Jan 29 14:36:18.695: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-49949579-ad42-448a-843d-565781853f10 container test-container: <nil>
STEP: delete the pod
Jan 29 14:36:18.860: INFO: Waiting for pod pod-49949579-ad42-448a-843d-565781853f10 to disappear
Jan 29 14:36:18.872: INFO: Pod pod-49949579-ad42-448a-843d-565781853f10 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:36:18.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3384" for this suite.

• [SLOW TEST:6.691 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":276,"completed":217,"skipped":3457,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:36:18.986: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4731
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: set up a multi version CRD
Jan 29 14:36:19.783: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:36:41.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4731" for this suite.

• [SLOW TEST:23.010 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":276,"completed":218,"skipped":3461,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:36:42.000: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-5164
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:125
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jan 29 14:36:42.890: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jan 29 14:36:45.100: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715905403, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715905403, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715905403, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715905402, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 14:36:47.117: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715905403, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715905403, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715905403, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715905402, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 29 14:36:50.160: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 29 14:36:50.169: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:36:52.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5164" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:136

• [SLOW TEST:11.748 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":276,"completed":219,"skipped":3470,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:36:53.750: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8535
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-map-d3519127-2926-46aa-8047-6e3509834c1c
STEP: Creating a pod to test consume secrets
Jan 29 14:36:54.075: INFO: Waiting up to 5m0s for pod "pod-secrets-e3e365df-9de9-4f1c-b66c-fed0eed59e93" in namespace "secrets-8535" to be "success or failure"
Jan 29 14:36:54.104: INFO: Pod "pod-secrets-e3e365df-9de9-4f1c-b66c-fed0eed59e93": Phase="Pending", Reason="", readiness=false. Elapsed: 28.93752ms
Jan 29 14:36:56.152: INFO: Pod "pod-secrets-e3e365df-9de9-4f1c-b66c-fed0eed59e93": Phase="Pending", Reason="", readiness=false. Elapsed: 2.076834917s
Jan 29 14:36:58.170: INFO: Pod "pod-secrets-e3e365df-9de9-4f1c-b66c-fed0eed59e93": Phase="Pending", Reason="", readiness=false. Elapsed: 4.094850061s
Jan 29 14:37:00.180: INFO: Pod "pod-secrets-e3e365df-9de9-4f1c-b66c-fed0eed59e93": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.105039968s
STEP: Saw pod success
Jan 29 14:37:00.180: INFO: Pod "pod-secrets-e3e365df-9de9-4f1c-b66c-fed0eed59e93" satisfied condition "success or failure"
Jan 29 14:37:00.189: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-secrets-e3e365df-9de9-4f1c-b66c-fed0eed59e93 container secret-volume-test: <nil>
STEP: delete the pod
Jan 29 14:37:00.288: INFO: Waiting for pod pod-secrets-e3e365df-9de9-4f1c-b66c-fed0eed59e93 to disappear
Jan 29 14:37:00.304: INFO: Pod pod-secrets-e3e365df-9de9-4f1c-b66c-fed0eed59e93 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:37:00.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8535" for this suite.

• [SLOW TEST:6.585 seconds]
[sig-storage] Secrets
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":276,"completed":220,"skipped":3497,"failed":0}
SSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:37:00.336: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-1600
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test hostPath mode
Jan 29 14:37:00.786: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-1600" to be "success or failure"
Jan 29 14:37:00.800: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 14.103219ms
Jan 29 14:37:02.811: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024821669s
Jan 29 14:37:04.831: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04535494s
Jan 29 14:37:06.843: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.056876209s
STEP: Saw pod success
Jan 29 14:37:06.843: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Jan 29 14:37:06.851: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Jan 29 14:37:06.929: INFO: Waiting for pod pod-host-path-test to disappear
Jan 29 14:37:06.945: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:37:06.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-1600" for this suite.

• [SLOW TEST:6.710 seconds]
[sig-storage] HostPath
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] HostPath should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":276,"completed":221,"skipped":3501,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:37:07.047: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9671
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-6030dccd-72a3-45cf-adc6-dd1a86ded8fd
STEP: Creating a pod to test consume secrets
Jan 29 14:37:07.386: INFO: Waiting up to 5m0s for pod "pod-secrets-9c8f87a4-bbc3-4ddd-99f5-f1d3bb47ec3a" in namespace "secrets-9671" to be "success or failure"
Jan 29 14:37:07.407: INFO: Pod "pod-secrets-9c8f87a4-bbc3-4ddd-99f5-f1d3bb47ec3a": Phase="Pending", Reason="", readiness=false. Elapsed: 20.425023ms
Jan 29 14:37:09.426: INFO: Pod "pod-secrets-9c8f87a4-bbc3-4ddd-99f5-f1d3bb47ec3a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039901469s
Jan 29 14:37:11.442: INFO: Pod "pod-secrets-9c8f87a4-bbc3-4ddd-99f5-f1d3bb47ec3a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.055707508s
Jan 29 14:37:13.450: INFO: Pod "pod-secrets-9c8f87a4-bbc3-4ddd-99f5-f1d3bb47ec3a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.064018646s
Jan 29 14:37:15.466: INFO: Pod "pod-secrets-9c8f87a4-bbc3-4ddd-99f5-f1d3bb47ec3a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.080036536s
Jan 29 14:37:17.475: INFO: Pod "pod-secrets-9c8f87a4-bbc3-4ddd-99f5-f1d3bb47ec3a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.088420858s
Jan 29 14:37:19.486: INFO: Pod "pod-secrets-9c8f87a4-bbc3-4ddd-99f5-f1d3bb47ec3a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.099603434s
Jan 29 14:37:21.504: INFO: Pod "pod-secrets-9c8f87a4-bbc3-4ddd-99f5-f1d3bb47ec3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.117632063s
STEP: Saw pod success
Jan 29 14:37:21.504: INFO: Pod "pod-secrets-9c8f87a4-bbc3-4ddd-99f5-f1d3bb47ec3a" satisfied condition "success or failure"
Jan 29 14:37:21.522: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-secrets-9c8f87a4-bbc3-4ddd-99f5-f1d3bb47ec3a container secret-volume-test: <nil>
STEP: delete the pod
Jan 29 14:37:21.657: INFO: Waiting for pod pod-secrets-9c8f87a4-bbc3-4ddd-99f5-f1d3bb47ec3a to disappear
Jan 29 14:37:21.670: INFO: Pod pod-secrets-9c8f87a4-bbc3-4ddd-99f5-f1d3bb47ec3a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:37:21.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9671" for this suite.

• [SLOW TEST:14.649 seconds]
[sig-storage] Secrets
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":276,"completed":222,"skipped":3553,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:37:21.697: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4956
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 29 14:37:21.953: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8d5a5550-d671-46e9-a9c7-3de4b8856667" in namespace "projected-4956" to be "success or failure"
Jan 29 14:37:21.968: INFO: Pod "downwardapi-volume-8d5a5550-d671-46e9-a9c7-3de4b8856667": Phase="Pending", Reason="", readiness=false. Elapsed: 14.713721ms
Jan 29 14:37:23.981: INFO: Pod "downwardapi-volume-8d5a5550-d671-46e9-a9c7-3de4b8856667": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02832991s
Jan 29 14:37:25.993: INFO: Pod "downwardapi-volume-8d5a5550-d671-46e9-a9c7-3de4b8856667": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039743332s
STEP: Saw pod success
Jan 29 14:37:25.993: INFO: Pod "downwardapi-volume-8d5a5550-d671-46e9-a9c7-3de4b8856667" satisfied condition "success or failure"
Jan 29 14:37:26.002: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod downwardapi-volume-8d5a5550-d671-46e9-a9c7-3de4b8856667 container client-container: <nil>
STEP: delete the pod
Jan 29 14:37:26.075: INFO: Waiting for pod downwardapi-volume-8d5a5550-d671-46e9-a9c7-3de4b8856667 to disappear
Jan 29 14:37:26.098: INFO: Pod downwardapi-volume-8d5a5550-d671-46e9-a9c7-3de4b8856667 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:37:26.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4956" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":276,"completed":223,"skipped":3558,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:37:26.138: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5180
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 29 14:37:26.563: INFO: Number of nodes with available pods: 0
Jan 29 14:37:26.564: INFO: Node metakube-worker-cmccl-6d88bd94fc-87n7l is running more than one daemon pod
Jan 29 14:37:27.589: INFO: Number of nodes with available pods: 0
Jan 29 14:37:27.589: INFO: Node metakube-worker-cmccl-6d88bd94fc-87n7l is running more than one daemon pod
Jan 29 14:37:28.604: INFO: Number of nodes with available pods: 0
Jan 29 14:37:28.604: INFO: Node metakube-worker-cmccl-6d88bd94fc-87n7l is running more than one daemon pod
Jan 29 14:37:29.589: INFO: Number of nodes with available pods: 2
Jan 29 14:37:29.589: INFO: Node metakube-worker-cmccl-6d88bd94fc-lqfxz is running more than one daemon pod
Jan 29 14:37:30.604: INFO: Number of nodes with available pods: 3
Jan 29 14:37:30.604: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jan 29 14:37:30.752: INFO: Number of nodes with available pods: 2
Jan 29 14:37:30.752: INFO: Node metakube-worker-cmccl-6d88bd94fc-87n7l is running more than one daemon pod
Jan 29 14:37:31.797: INFO: Number of nodes with available pods: 2
Jan 29 14:37:31.797: INFO: Node metakube-worker-cmccl-6d88bd94fc-87n7l is running more than one daemon pod
Jan 29 14:37:32.819: INFO: Number of nodes with available pods: 2
Jan 29 14:37:32.819: INFO: Node metakube-worker-cmccl-6d88bd94fc-87n7l is running more than one daemon pod
Jan 29 14:37:33.771: INFO: Number of nodes with available pods: 2
Jan 29 14:37:33.771: INFO: Node metakube-worker-cmccl-6d88bd94fc-87n7l is running more than one daemon pod
Jan 29 14:37:34.811: INFO: Number of nodes with available pods: 2
Jan 29 14:37:34.811: INFO: Node metakube-worker-cmccl-6d88bd94fc-87n7l is running more than one daemon pod
Jan 29 14:37:35.967: INFO: Number of nodes with available pods: 2
Jan 29 14:37:35.967: INFO: Node metakube-worker-cmccl-6d88bd94fc-87n7l is running more than one daemon pod
Jan 29 14:37:36.771: INFO: Number of nodes with available pods: 2
Jan 29 14:37:36.772: INFO: Node metakube-worker-cmccl-6d88bd94fc-87n7l is running more than one daemon pod
Jan 29 14:37:37.772: INFO: Number of nodes with available pods: 2
Jan 29 14:37:37.772: INFO: Node metakube-worker-cmccl-6d88bd94fc-87n7l is running more than one daemon pod
Jan 29 14:37:38.776: INFO: Number of nodes with available pods: 2
Jan 29 14:37:38.776: INFO: Node metakube-worker-cmccl-6d88bd94fc-87n7l is running more than one daemon pod
Jan 29 14:37:39.807: INFO: Number of nodes with available pods: 2
Jan 29 14:37:39.807: INFO: Node metakube-worker-cmccl-6d88bd94fc-87n7l is running more than one daemon pod
Jan 29 14:37:40.903: INFO: Number of nodes with available pods: 2
Jan 29 14:37:40.903: INFO: Node metakube-worker-cmccl-6d88bd94fc-87n7l is running more than one daemon pod
Jan 29 14:37:41.910: INFO: Number of nodes with available pods: 2
Jan 29 14:37:41.910: INFO: Node metakube-worker-cmccl-6d88bd94fc-87n7l is running more than one daemon pod
Jan 29 14:37:42.798: INFO: Number of nodes with available pods: 2
Jan 29 14:37:42.798: INFO: Node metakube-worker-cmccl-6d88bd94fc-87n7l is running more than one daemon pod
Jan 29 14:37:43.775: INFO: Number of nodes with available pods: 2
Jan 29 14:37:43.775: INFO: Node metakube-worker-cmccl-6d88bd94fc-87n7l is running more than one daemon pod
Jan 29 14:37:44.783: INFO: Number of nodes with available pods: 2
Jan 29 14:37:44.783: INFO: Node metakube-worker-cmccl-6d88bd94fc-87n7l is running more than one daemon pod
Jan 29 14:37:45.789: INFO: Number of nodes with available pods: 3
Jan 29 14:37:45.789: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5180, will wait for the garbage collector to delete the pods
Jan 29 14:37:45.895: INFO: Deleting DaemonSet.extensions daemon-set took: 38.455933ms
Jan 29 14:37:46.495: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.350453ms
Jan 29 14:37:53.303: INFO: Number of nodes with available pods: 0
Jan 29 14:37:53.303: INFO: Number of running nodes: 0, number of available pods: 0
Jan 29 14:37:53.312: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5180/daemonsets","resourceVersion":"90250"},"items":null}

Jan 29 14:37:53.321: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5180/pods","resourceVersion":"90250"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:37:53.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5180" for this suite.

• [SLOW TEST:27.275 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":276,"completed":224,"skipped":3595,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:37:53.419: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-3207
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:172
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating server pod server in namespace prestop-3207
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-3207
STEP: Deleting pre-stop pod
Jan 29 14:38:09.091: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:38:09.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-3207" for this suite.

• [SLOW TEST:15.839 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":276,"completed":225,"skipped":3614,"failed":0}
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:38:09.263: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1319
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-map-a71391bc-9843-48c8-9c78-5603fbe160f7
STEP: Creating a pod to test consume secrets
Jan 29 14:38:09.674: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bcf90f32-271a-4c1b-8ee2-614f05117a26" in namespace "projected-1319" to be "success or failure"
Jan 29 14:38:09.705: INFO: Pod "pod-projected-secrets-bcf90f32-271a-4c1b-8ee2-614f05117a26": Phase="Pending", Reason="", readiness=false. Elapsed: 30.408522ms
Jan 29 14:38:11.716: INFO: Pod "pod-projected-secrets-bcf90f32-271a-4c1b-8ee2-614f05117a26": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041898541s
Jan 29 14:38:13.745: INFO: Pod "pod-projected-secrets-bcf90f32-271a-4c1b-8ee2-614f05117a26": Phase="Pending", Reason="", readiness=false. Elapsed: 4.07066596s
Jan 29 14:38:15.754: INFO: Pod "pod-projected-secrets-bcf90f32-271a-4c1b-8ee2-614f05117a26": Phase="Pending", Reason="", readiness=false. Elapsed: 6.079970072s
Jan 29 14:38:17.768: INFO: Pod "pod-projected-secrets-bcf90f32-271a-4c1b-8ee2-614f05117a26": Phase="Pending", Reason="", readiness=false. Elapsed: 8.093323712s
Jan 29 14:38:19.779: INFO: Pod "pod-projected-secrets-bcf90f32-271a-4c1b-8ee2-614f05117a26": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.10420744s
STEP: Saw pod success
Jan 29 14:38:19.779: INFO: Pod "pod-projected-secrets-bcf90f32-271a-4c1b-8ee2-614f05117a26" satisfied condition "success or failure"
Jan 29 14:38:19.788: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-projected-secrets-bcf90f32-271a-4c1b-8ee2-614f05117a26 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 29 14:38:19.904: INFO: Waiting for pod pod-projected-secrets-bcf90f32-271a-4c1b-8ee2-614f05117a26 to disappear
Jan 29 14:38:19.913: INFO: Pod pod-projected-secrets-bcf90f32-271a-4c1b-8ee2-614f05117a26 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:38:19.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1319" for this suite.

• [SLOW TEST:10.696 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":276,"completed":226,"skipped":3617,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:38:19.959: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1992
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Jan 29 14:39:00.346: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
W0129 14:39:00.346669      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jan 29 14:39:00.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1992" for this suite.

• [SLOW TEST:40.577 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":276,"completed":227,"skipped":3625,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:39:00.539: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2762
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Update Demo
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:330
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the initial replication controller
Jan 29 14:39:01.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 create -f - --namespace=kubectl-2762'
Jan 29 14:39:01.614: INFO: stderr: ""
Jan 29 14:39:01.614: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 29 14:39:01.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2762'
Jan 29 14:39:01.875: INFO: stderr: ""
Jan 29 14:39:01.875: INFO: stdout: "update-demo-nautilus-2xkwl update-demo-nautilus-jzqws "
Jan 29 14:39:01.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods update-demo-nautilus-2xkwl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2762'
Jan 29 14:39:02.250: INFO: stderr: ""
Jan 29 14:39:02.250: INFO: stdout: ""
Jan 29 14:39:02.250: INFO: update-demo-nautilus-2xkwl is created but not running
Jan 29 14:39:07.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2762'
Jan 29 14:39:07.413: INFO: stderr: ""
Jan 29 14:39:07.413: INFO: stdout: "update-demo-nautilus-2xkwl update-demo-nautilus-jzqws "
Jan 29 14:39:07.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods update-demo-nautilus-2xkwl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2762'
Jan 29 14:39:07.705: INFO: stderr: ""
Jan 29 14:39:07.705: INFO: stdout: "true"
Jan 29 14:39:07.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods update-demo-nautilus-2xkwl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2762'
Jan 29 14:39:07.906: INFO: stderr: ""
Jan 29 14:39:07.906: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 29 14:39:07.906: INFO: validating pod update-demo-nautilus-2xkwl
Jan 29 14:39:08.021: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 29 14:39:08.022: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 29 14:39:08.022: INFO: update-demo-nautilus-2xkwl is verified up and running
Jan 29 14:39:08.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods update-demo-nautilus-jzqws -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2762'
Jan 29 14:39:08.161: INFO: stderr: ""
Jan 29 14:39:08.161: INFO: stdout: "true"
Jan 29 14:39:08.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods update-demo-nautilus-jzqws -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2762'
Jan 29 14:39:08.317: INFO: stderr: ""
Jan 29 14:39:08.317: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 29 14:39:08.318: INFO: validating pod update-demo-nautilus-jzqws
Jan 29 14:39:08.425: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 29 14:39:08.426: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 29 14:39:08.426: INFO: update-demo-nautilus-jzqws is verified up and running
STEP: rolling-update to new replication controller
Jan 29 14:39:08.435: INFO: scanned /root for discovery docs: <nil>
Jan 29 14:39:08.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-2762'
Jan 29 14:39:34.771: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jan 29 14:39:34.771: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 29 14:39:34.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2762'
Jan 29 14:39:34.901: INFO: stderr: ""
Jan 29 14:39:34.902: INFO: stdout: "update-demo-kitten-cjq5l update-demo-kitten-ztzq7 update-demo-nautilus-jzqws "
STEP: Replicas for name=update-demo: expected=2 actual=3
Jan 29 14:39:39.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2762'
Jan 29 14:39:40.046: INFO: stderr: ""
Jan 29 14:39:40.046: INFO: stdout: "update-demo-kitten-cjq5l update-demo-kitten-ztzq7 update-demo-nautilus-jzqws "
STEP: Replicas for name=update-demo: expected=2 actual=3
Jan 29 14:39:45.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2762'
Jan 29 14:39:45.185: INFO: stderr: ""
Jan 29 14:39:45.185: INFO: stdout: "update-demo-kitten-cjq5l update-demo-kitten-ztzq7 update-demo-nautilus-jzqws "
STEP: Replicas for name=update-demo: expected=2 actual=3
Jan 29 14:39:50.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2762'
Jan 29 14:39:50.320: INFO: stderr: ""
Jan 29 14:39:50.320: INFO: stdout: "update-demo-kitten-cjq5l update-demo-kitten-ztzq7 "
Jan 29 14:39:50.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods update-demo-kitten-cjq5l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2762'
Jan 29 14:39:50.459: INFO: stderr: ""
Jan 29 14:39:50.459: INFO: stdout: "true"
Jan 29 14:39:50.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods update-demo-kitten-cjq5l -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2762'
Jan 29 14:39:50.598: INFO: stderr: ""
Jan 29 14:39:50.598: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jan 29 14:39:50.598: INFO: validating pod update-demo-kitten-cjq5l
Jan 29 14:39:50.741: INFO: got data: {
  "image": "kitten.jpg"
}

Jan 29 14:39:50.741: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jan 29 14:39:50.741: INFO: update-demo-kitten-cjq5l is verified up and running
Jan 29 14:39:50.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods update-demo-kitten-ztzq7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2762'
Jan 29 14:39:50.876: INFO: stderr: ""
Jan 29 14:39:50.876: INFO: stdout: "true"
Jan 29 14:39:50.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods update-demo-kitten-ztzq7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2762'
Jan 29 14:39:51.081: INFO: stderr: ""
Jan 29 14:39:51.081: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jan 29 14:39:51.082: INFO: validating pod update-demo-kitten-ztzq7
Jan 29 14:39:51.198: INFO: got data: {
  "image": "kitten.jpg"
}

Jan 29 14:39:51.198: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jan 29 14:39:51.198: INFO: update-demo-kitten-ztzq7 is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:39:51.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2762" for this suite.

• [SLOW TEST:50.724 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:328
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should do a rolling update of a replication controller  [Conformance]","total":276,"completed":228,"skipped":3663,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:39:51.272: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2709
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:40:51.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2709" for this suite.

• [SLOW TEST:60.533 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":276,"completed":229,"skipped":3676,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:40:51.806: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7609
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with configMap that has name projected-configmap-test-upd-015b8185-7aca-4c91-b2fd-f85504feeca0
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-015b8185-7aca-4c91-b2fd-f85504feeca0
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:42:22.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7609" for this suite.

• [SLOW TEST:90.813 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":276,"completed":230,"skipped":3688,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:42:22.626: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4068
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:42:29.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4068" for this suite.

• [SLOW TEST:7.430 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":276,"completed":231,"skipped":3731,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:42:30.062: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2814
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 29 14:42:30.886: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 29 14:42:32.919: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715905750, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715905750, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715905751, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715905750, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 29 14:42:35.969: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:42:36.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2814" for this suite.
STEP: Destroying namespace "webhook-2814-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.341 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":276,"completed":232,"skipped":3763,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:42:36.404: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4206
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 29 14:42:38.781: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 29 14:42:40.817: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715905758, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715905758, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715905758, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715905758, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 14:42:42.827: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715905758, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715905758, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715905758, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715905758, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 29 14:42:45.879: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:42:46.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4206" for this suite.
STEP: Destroying namespace "webhook-4206-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.996 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":276,"completed":233,"skipped":3786,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:42:46.401: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3857
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 29 14:42:46.728: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1081fb75-3d3b-4329-bf81-7cc99b3d5759" in namespace "downward-api-3857" to be "success or failure"
Jan 29 14:42:46.740: INFO: Pod "downwardapi-volume-1081fb75-3d3b-4329-bf81-7cc99b3d5759": Phase="Pending", Reason="", readiness=false. Elapsed: 12.290643ms
Jan 29 14:42:48.760: INFO: Pod "downwardapi-volume-1081fb75-3d3b-4329-bf81-7cc99b3d5759": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032000934s
Jan 29 14:42:50.771: INFO: Pod "downwardapi-volume-1081fb75-3d3b-4329-bf81-7cc99b3d5759": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043136087s
STEP: Saw pod success
Jan 29 14:42:50.772: INFO: Pod "downwardapi-volume-1081fb75-3d3b-4329-bf81-7cc99b3d5759" satisfied condition "success or failure"
Jan 29 14:42:50.784: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod downwardapi-volume-1081fb75-3d3b-4329-bf81-7cc99b3d5759 container client-container: <nil>
STEP: delete the pod
Jan 29 14:42:50.888: INFO: Waiting for pod downwardapi-volume-1081fb75-3d3b-4329-bf81-7cc99b3d5759 to disappear
Jan 29 14:42:50.895: INFO: Pod downwardapi-volume-1081fb75-3d3b-4329-bf81-7cc99b3d5759 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:42:50.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3857" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":276,"completed":234,"skipped":3817,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:42:50.920: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-623
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:43:07.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-623" for this suite.

• [SLOW TEST:16.810 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":276,"completed":235,"skipped":3832,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:43:07.731: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5958
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0129 14:43:15.782538      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jan 29 14:43:15.783: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:43:15.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5958" for this suite.

• [SLOW TEST:8.115 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":276,"completed":236,"skipped":3859,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:43:15.850: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1874
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Jan 29 14:43:16.227: INFO: Waiting up to 5m0s for pod "downward-api-756ecdd2-2b38-426b-879c-2707e2d7316c" in namespace "downward-api-1874" to be "success or failure"
Jan 29 14:43:16.267: INFO: Pod "downward-api-756ecdd2-2b38-426b-879c-2707e2d7316c": Phase="Pending", Reason="", readiness=false. Elapsed: 39.783925ms
Jan 29 14:43:18.277: INFO: Pod "downward-api-756ecdd2-2b38-426b-879c-2707e2d7316c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049582453s
Jan 29 14:43:20.289: INFO: Pod "downward-api-756ecdd2-2b38-426b-879c-2707e2d7316c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.06131571s
Jan 29 14:43:22.303: INFO: Pod "downward-api-756ecdd2-2b38-426b-879c-2707e2d7316c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.075681764s
Jan 29 14:43:24.318: INFO: Pod "downward-api-756ecdd2-2b38-426b-879c-2707e2d7316c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.090267721s
Jan 29 14:43:26.326: INFO: Pod "downward-api-756ecdd2-2b38-426b-879c-2707e2d7316c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.098651186s
STEP: Saw pod success
Jan 29 14:43:26.327: INFO: Pod "downward-api-756ecdd2-2b38-426b-879c-2707e2d7316c" satisfied condition "success or failure"
Jan 29 14:43:26.336: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod downward-api-756ecdd2-2b38-426b-879c-2707e2d7316c container dapi-container: <nil>
STEP: delete the pod
Jan 29 14:43:26.415: INFO: Waiting for pod downward-api-756ecdd2-2b38-426b-879c-2707e2d7316c to disappear
Jan 29 14:43:26.525: INFO: Pod downward-api-756ecdd2-2b38-426b-879c-2707e2d7316c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:43:26.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1874" for this suite.

• [SLOW TEST:10.705 seconds]
[sig-node] Downward API
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:33
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":276,"completed":237,"skipped":3870,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:43:26.557: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4850
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 29 14:43:27.876: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 29 14:43:29.912: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715905807, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715905807, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715905807, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715905807, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 29 14:43:32.996: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:43:37.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4850" for this suite.
STEP: Destroying namespace "webhook-4850-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:12.408 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":276,"completed":238,"skipped":3878,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:43:38.966: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2921
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Jan 29 14:43:46.479: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-2921 PodName:pod-sharedvolume-7f52f367-823d-4713-bad0-030ee8858b73 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 14:43:46.479: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
Jan 29 14:43:47.099: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:43:47.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2921" for this suite.

• [SLOW TEST:8.414 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":276,"completed":239,"skipped":3908,"failed":0}
SSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:43:47.381: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-7896
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:44:01.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7896" for this suite.

• [SLOW TEST:14.345 seconds]
[sig-apps] Job
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":276,"completed":240,"skipped":3913,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:44:01.732: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6179
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: executing a command with run --rm and attach with stdin
Jan 29 14:44:01.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 --namespace=kubectl-6179 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Jan 29 14:44:07.520: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Jan 29 14:44:07.520: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:44:09.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6179" for this suite.

• [SLOW TEST:7.882 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run --rm job
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1944
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run --rm job should create a job from an image, then delete the job  [Conformance]","total":276,"completed":241,"skipped":3938,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:44:09.614: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4352
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:44:21.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4352" for this suite.

• [SLOW TEST:12.418 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":276,"completed":242,"skipped":3949,"failed":0}
SSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:44:22.033: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-7186
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Jan 29 14:44:22.344: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:44:33.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7186" for this suite.

• [SLOW TEST:11.938 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":276,"completed":243,"skipped":3953,"failed":0}
S
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:44:33.972: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-6805
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jan 29 14:44:34.396: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6805 /api/v1/namespaces/watch-6805/configmaps/e2e-watch-test-label-changed 0e2b4f8a-2062-41ec-8e7e-9ed940643c0b 92974 0 2020-01-29 14:44:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 29 14:44:34.397: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6805 /api/v1/namespaces/watch-6805/configmaps/e2e-watch-test-label-changed 0e2b4f8a-2062-41ec-8e7e-9ed940643c0b 92975 0 2020-01-29 14:44:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jan 29 14:44:34.398: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6805 /api/v1/namespaces/watch-6805/configmaps/e2e-watch-test-label-changed 0e2b4f8a-2062-41ec-8e7e-9ed940643c0b 92976 0 2020-01-29 14:44:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jan 29 14:44:44.575: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6805 /api/v1/namespaces/watch-6805/configmaps/e2e-watch-test-label-changed 0e2b4f8a-2062-41ec-8e7e-9ed940643c0b 93038 0 2020-01-29 14:44:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 29 14:44:44.575: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6805 /api/v1/namespaces/watch-6805/configmaps/e2e-watch-test-label-changed 0e2b4f8a-2062-41ec-8e7e-9ed940643c0b 93039 0 2020-01-29 14:44:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Jan 29 14:44:44.575: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6805 /api/v1/namespaces/watch-6805/configmaps/e2e-watch-test-label-changed 0e2b4f8a-2062-41ec-8e7e-9ed940643c0b 93040 0 2020-01-29 14:44:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:44:44.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6805" for this suite.

• [SLOW TEST:10.650 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":276,"completed":244,"skipped":3954,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:44:44.627: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3765
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating secret secrets-3765/secret-test-36269cfe-030e-4738-89ae-c1f416f7b0d9
STEP: Creating a pod to test consume secrets
Jan 29 14:44:44.998: INFO: Waiting up to 5m0s for pod "pod-configmaps-365408b2-acf5-419c-86fb-0e863afe3200" in namespace "secrets-3765" to be "success or failure"
Jan 29 14:44:45.047: INFO: Pod "pod-configmaps-365408b2-acf5-419c-86fb-0e863afe3200": Phase="Pending", Reason="", readiness=false. Elapsed: 49.305018ms
Jan 29 14:44:47.064: INFO: Pod "pod-configmaps-365408b2-acf5-419c-86fb-0e863afe3200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.065860811s
Jan 29 14:44:49.077: INFO: Pod "pod-configmaps-365408b2-acf5-419c-86fb-0e863afe3200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.079228079s
STEP: Saw pod success
Jan 29 14:44:49.077: INFO: Pod "pod-configmaps-365408b2-acf5-419c-86fb-0e863afe3200" satisfied condition "success or failure"
Jan 29 14:44:49.087: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-configmaps-365408b2-acf5-419c-86fb-0e863afe3200 container env-test: <nil>
STEP: delete the pod
Jan 29 14:44:49.181: INFO: Waiting for pod pod-configmaps-365408b2-acf5-419c-86fb-0e863afe3200 to disappear
Jan 29 14:44:49.196: INFO: Pod pod-configmaps-365408b2-acf5-419c-86fb-0e863afe3200 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:44:49.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3765" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":276,"completed":245,"skipped":4001,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:44:49.238: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3046
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 29 14:44:49.600: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2ad1f966-dcf0-4725-b449-3fb571cd1873" in namespace "projected-3046" to be "success or failure"
Jan 29 14:44:49.609: INFO: Pod "downwardapi-volume-2ad1f966-dcf0-4725-b449-3fb571cd1873": Phase="Pending", Reason="", readiness=false. Elapsed: 8.264567ms
Jan 29 14:44:51.623: INFO: Pod "downwardapi-volume-2ad1f966-dcf0-4725-b449-3fb571cd1873": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022304805s
Jan 29 14:44:53.633: INFO: Pod "downwardapi-volume-2ad1f966-dcf0-4725-b449-3fb571cd1873": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032777171s
STEP: Saw pod success
Jan 29 14:44:53.633: INFO: Pod "downwardapi-volume-2ad1f966-dcf0-4725-b449-3fb571cd1873" satisfied condition "success or failure"
Jan 29 14:44:53.643: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod downwardapi-volume-2ad1f966-dcf0-4725-b449-3fb571cd1873 container client-container: <nil>
STEP: delete the pod
Jan 29 14:44:53.699: INFO: Waiting for pod downwardapi-volume-2ad1f966-dcf0-4725-b449-3fb571cd1873 to disappear
Jan 29 14:44:53.710: INFO: Pod downwardapi-volume-2ad1f966-dcf0-4725-b449-3fb571cd1873 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:44:53.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3046" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":276,"completed":246,"skipped":4006,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:44:53.758: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-3878
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Jan 29 14:44:53.979: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 29 14:44:54.020: INFO: Waiting for terminating namespaces to be deleted...
Jan 29 14:44:54.033: INFO: 
Logging pods the kubelet thinks is on node metakube-worker-cmccl-6d88bd94fc-87n7l before test
Jan 29 14:44:54.078: INFO: kube-proxy-7jg5r from kube-system started at 2020-01-29 10:31:06 +0000 UTC (1 container statuses recorded)
Jan 29 14:44:54.078: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 29 14:44:54.078: INFO: node-local-dns-nhzkw from kube-system started at 2020-01-29 10:31:06 +0000 UTC (1 container statuses recorded)
Jan 29 14:44:54.078: INFO: 	Container node-cache ready: true, restart count 0
Jan 29 14:44:54.078: INFO: sonobuoy-systemd-logs-daemon-set-6c9811082b824a7c-sbw8r from sonobuoy started at 2020-01-29 13:30:03 +0000 UTC (2 container statuses recorded)
Jan 29 14:44:54.078: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jan 29 14:44:54.078: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 29 14:44:54.078: INFO: tiller-deploy-78f78bb476-hz2w6 from kube-system started at 2020-01-29 10:31:35 +0000 UTC (1 container statuses recorded)
Jan 29 14:44:54.078: INFO: 	Container tiller ready: true, restart count 0
Jan 29 14:44:54.078: INFO: coredns-6f745f6d74-8q86x from kube-system started at 2020-01-29 10:31:43 +0000 UTC (1 container statuses recorded)
Jan 29 14:44:54.078: INFO: 	Container coredns ready: true, restart count 0
Jan 29 14:44:54.078: INFO: canal-lpkd9 from kube-system started at 2020-01-29 10:31:07 +0000 UTC (2 container statuses recorded)
Jan 29 14:44:54.078: INFO: 	Container calico-node ready: true, restart count 0
Jan 29 14:44:54.078: INFO: 	Container kube-flannel ready: true, restart count 0
Jan 29 14:44:54.078: INFO: node-exporter-4wc5c from kube-system started at 2020-01-29 10:31:07 +0000 UTC (1 container statuses recorded)
Jan 29 14:44:54.078: INFO: 	Container node-exporter ready: true, restart count 0
Jan 29 14:44:54.078: INFO: cluster-autoscaler-8c65c7d54-z9ddg from kube-system started at 2020-01-29 10:31:35 +0000 UTC (1 container statuses recorded)
Jan 29 14:44:54.078: INFO: 	Container cluster-autoscaler ready: true, restart count 1
Jan 29 14:44:54.078: INFO: openvpn-client-64df8b95c9-5fqpn from kube-system started at 2020-01-29 10:31:44 +0000 UTC (2 container statuses recorded)
Jan 29 14:44:54.078: INFO: 	Container dnat-controller ready: true, restart count 0
Jan 29 14:44:54.079: INFO: 	Container openvpn-client ready: true, restart count 1
Jan 29 14:44:54.079: INFO: coredns-6f745f6d74-kr8n2 from kube-system started at 2020-01-29 10:31:44 +0000 UTC (1 container statuses recorded)
Jan 29 14:44:54.079: INFO: 	Container coredns ready: true, restart count 0
Jan 29 14:44:54.079: INFO: 
Logging pods the kubelet thinks is on node metakube-worker-cmccl-6d88bd94fc-lqfxz before test
Jan 29 14:44:54.140: INFO: kube-proxy-rr5gp from kube-system started at 2020-01-29 10:32:02 +0000 UTC (1 container statuses recorded)
Jan 29 14:44:54.141: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 29 14:44:54.141: INFO: node-exporter-99p82 from kube-system started at 2020-01-29 10:32:02 +0000 UTC (1 container statuses recorded)
Jan 29 14:44:54.141: INFO: 	Container node-exporter ready: true, restart count 0
Jan 29 14:44:54.141: INFO: node-local-dns-72k8g from kube-system started at 2020-01-29 10:32:02 +0000 UTC (1 container statuses recorded)
Jan 29 14:44:54.141: INFO: 	Container node-cache ready: true, restart count 0
Jan 29 14:44:54.141: INFO: sonobuoy-systemd-logs-daemon-set-6c9811082b824a7c-9g4cq from sonobuoy started at 2020-01-29 13:30:03 +0000 UTC (2 container statuses recorded)
Jan 29 14:44:54.141: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jan 29 14:44:54.141: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 29 14:44:54.141: INFO: canal-4zhl8 from kube-system started at 2020-01-29 10:32:02 +0000 UTC (2 container statuses recorded)
Jan 29 14:44:54.141: INFO: 	Container calico-node ready: true, restart count 0
Jan 29 14:44:54.141: INFO: 	Container kube-flannel ready: true, restart count 0
Jan 29 14:44:54.141: INFO: 
Logging pods the kubelet thinks is on node metakube-worker-cmccl-6d88bd94fc-znv5g before test
Jan 29 14:44:54.208: INFO: sonobuoy from sonobuoy started at 2020-01-29 13:30:00 +0000 UTC (1 container statuses recorded)
Jan 29 14:44:54.208: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 29 14:44:54.208: INFO: node-exporter-xblbq from kube-system started at 2020-01-29 10:31:23 +0000 UTC (1 container statuses recorded)
Jan 29 14:44:54.208: INFO: 	Container node-exporter ready: true, restart count 0
Jan 29 14:44:54.208: INFO: sonobuoy-e2e-job-bb288164d824466f from sonobuoy started at 2020-01-29 13:30:03 +0000 UTC (2 container statuses recorded)
Jan 29 14:44:54.208: INFO: 	Container e2e ready: true, restart count 0
Jan 29 14:44:54.208: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 29 14:44:54.208: INFO: node-local-dns-2tvcw from kube-system started at 2020-01-29 10:31:23 +0000 UTC (1 container statuses recorded)
Jan 29 14:44:54.208: INFO: 	Container node-cache ready: true, restart count 0
Jan 29 14:44:54.208: INFO: canal-d5wpn from kube-system started at 2020-01-29 10:31:23 +0000 UTC (2 container statuses recorded)
Jan 29 14:44:54.208: INFO: 	Container calico-node ready: true, restart count 0
Jan 29 14:44:54.208: INFO: 	Container kube-flannel ready: true, restart count 0
Jan 29 14:44:54.208: INFO: sonobuoy-systemd-logs-daemon-set-6c9811082b824a7c-mxxcq from sonobuoy started at 2020-01-29 13:30:03 +0000 UTC (2 container statuses recorded)
Jan 29 14:44:54.208: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jan 29 14:44:54.208: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 29 14:44:54.208: INFO: kube-proxy-h6b8h from kube-system started at 2020-01-29 10:31:22 +0000 UTC (1 container statuses recorded)
Jan 29 14:44:54.208: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: verifying the node has the label node metakube-worker-cmccl-6d88bd94fc-87n7l
STEP: verifying the node has the label node metakube-worker-cmccl-6d88bd94fc-lqfxz
STEP: verifying the node has the label node metakube-worker-cmccl-6d88bd94fc-znv5g
Jan 29 14:44:54.408: INFO: Pod canal-4zhl8 requesting resource cpu=350m on Node metakube-worker-cmccl-6d88bd94fc-lqfxz
Jan 29 14:44:54.409: INFO: Pod canal-d5wpn requesting resource cpu=350m on Node metakube-worker-cmccl-6d88bd94fc-znv5g
Jan 29 14:44:54.409: INFO: Pod canal-lpkd9 requesting resource cpu=350m on Node metakube-worker-cmccl-6d88bd94fc-87n7l
Jan 29 14:44:54.409: INFO: Pod cluster-autoscaler-8c65c7d54-z9ddg requesting resource cpu=10m on Node metakube-worker-cmccl-6d88bd94fc-87n7l
Jan 29 14:44:54.410: INFO: Pod coredns-6f745f6d74-8q86x requesting resource cpu=100m on Node metakube-worker-cmccl-6d88bd94fc-87n7l
Jan 29 14:44:54.410: INFO: Pod coredns-6f745f6d74-kr8n2 requesting resource cpu=100m on Node metakube-worker-cmccl-6d88bd94fc-87n7l
Jan 29 14:44:54.410: INFO: Pod kube-proxy-7jg5r requesting resource cpu=75m on Node metakube-worker-cmccl-6d88bd94fc-87n7l
Jan 29 14:44:54.410: INFO: Pod kube-proxy-h6b8h requesting resource cpu=75m on Node metakube-worker-cmccl-6d88bd94fc-znv5g
Jan 29 14:44:54.411: INFO: Pod kube-proxy-rr5gp requesting resource cpu=75m on Node metakube-worker-cmccl-6d88bd94fc-lqfxz
Jan 29 14:44:54.411: INFO: Pod node-exporter-4wc5c requesting resource cpu=3m on Node metakube-worker-cmccl-6d88bd94fc-87n7l
Jan 29 14:44:54.411: INFO: Pod node-exporter-99p82 requesting resource cpu=3m on Node metakube-worker-cmccl-6d88bd94fc-lqfxz
Jan 29 14:44:54.411: INFO: Pod node-exporter-xblbq requesting resource cpu=3m on Node metakube-worker-cmccl-6d88bd94fc-znv5g
Jan 29 14:44:54.411: INFO: Pod node-local-dns-2tvcw requesting resource cpu=25m on Node metakube-worker-cmccl-6d88bd94fc-znv5g
Jan 29 14:44:54.412: INFO: Pod node-local-dns-72k8g requesting resource cpu=25m on Node metakube-worker-cmccl-6d88bd94fc-lqfxz
Jan 29 14:44:54.412: INFO: Pod node-local-dns-nhzkw requesting resource cpu=25m on Node metakube-worker-cmccl-6d88bd94fc-87n7l
Jan 29 14:44:54.412: INFO: Pod openvpn-client-64df8b95c9-5fqpn requesting resource cpu=30m on Node metakube-worker-cmccl-6d88bd94fc-87n7l
Jan 29 14:44:54.412: INFO: Pod tiller-deploy-78f78bb476-hz2w6 requesting resource cpu=0m on Node metakube-worker-cmccl-6d88bd94fc-87n7l
Jan 29 14:44:54.413: INFO: Pod sonobuoy requesting resource cpu=0m on Node metakube-worker-cmccl-6d88bd94fc-znv5g
Jan 29 14:44:54.413: INFO: Pod sonobuoy-e2e-job-bb288164d824466f requesting resource cpu=0m on Node metakube-worker-cmccl-6d88bd94fc-znv5g
Jan 29 14:44:54.413: INFO: Pod sonobuoy-systemd-logs-daemon-set-6c9811082b824a7c-9g4cq requesting resource cpu=0m on Node metakube-worker-cmccl-6d88bd94fc-lqfxz
Jan 29 14:44:54.414: INFO: Pod sonobuoy-systemd-logs-daemon-set-6c9811082b824a7c-mxxcq requesting resource cpu=0m on Node metakube-worker-cmccl-6d88bd94fc-znv5g
Jan 29 14:44:54.414: INFO: Pod sonobuoy-systemd-logs-daemon-set-6c9811082b824a7c-sbw8r requesting resource cpu=0m on Node metakube-worker-cmccl-6d88bd94fc-87n7l
STEP: Starting Pods to consume most of the cluster CPU.
Jan 29 14:44:54.414: INFO: Creating a pod which consumes cpu=774m on Node metakube-worker-cmccl-6d88bd94fc-87n7l
Jan 29 14:44:54.447: INFO: Creating a pod which consumes cpu=942m on Node metakube-worker-cmccl-6d88bd94fc-lqfxz
Jan 29 14:44:54.488: INFO: Creating a pod which consumes cpu=942m on Node metakube-worker-cmccl-6d88bd94fc-znv5g
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6a78cb64-e691-4023-b263-0741aa3e0225.15ee62bfa50c73ce], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3878/filler-pod-6a78cb64-e691-4023-b263-0741aa3e0225 to metakube-worker-cmccl-6d88bd94fc-lqfxz]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6a78cb64-e691-4023-b263-0741aa3e0225.15ee62c0165ddac0], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6a78cb64-e691-4023-b263-0741aa3e0225.15ee62c02972ad20], Reason = [Created], Message = [Created container filler-pod-6a78cb64-e691-4023-b263-0741aa3e0225]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6a78cb64-e691-4023-b263-0741aa3e0225.15ee62c040ff9a37], Reason = [Started], Message = [Started container filler-pod-6a78cb64-e691-4023-b263-0741aa3e0225]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ac493b34-d5a5-45ec-bce1-b22cf3f7a154.15ee62bfa1f76660], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3878/filler-pod-ac493b34-d5a5-45ec-bce1-b22cf3f7a154 to metakube-worker-cmccl-6d88bd94fc-87n7l]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ac493b34-d5a5-45ec-bce1-b22cf3f7a154.15ee62c014ca33ef], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ac493b34-d5a5-45ec-bce1-b22cf3f7a154.15ee62c029fd09c5], Reason = [Created], Message = [Created container filler-pod-ac493b34-d5a5-45ec-bce1-b22cf3f7a154]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ac493b34-d5a5-45ec-bce1-b22cf3f7a154.15ee62c03c64dd39], Reason = [Started], Message = [Started container filler-pod-ac493b34-d5a5-45ec-bce1-b22cf3f7a154]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bc399e29-164f-442c-bcf0-6530708fa0f5.15ee62bfa8a56531], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3878/filler-pod-bc399e29-164f-442c-bcf0-6530708fa0f5 to metakube-worker-cmccl-6d88bd94fc-znv5g]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bc399e29-164f-442c-bcf0-6530708fa0f5.15ee62c01b08d978], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bc399e29-164f-442c-bcf0-6530708fa0f5.15ee62c02ead90bb], Reason = [Created], Message = [Created container filler-pod-bc399e29-164f-442c-bcf0-6530708fa0f5]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bc399e29-164f-442c-bcf0-6530708fa0f5.15ee62c0435556f8], Reason = [Started], Message = [Started container filler-pod-bc399e29-164f-442c-bcf0-6530708fa0f5]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15ee62c09abd7d30], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15ee62c09c72138a], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node metakube-worker-cmccl-6d88bd94fc-87n7l
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node metakube-worker-cmccl-6d88bd94fc-lqfxz
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node metakube-worker-cmccl-6d88bd94fc-znv5g
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:44:59.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3878" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:6.067 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":276,"completed":247,"skipped":4049,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:44:59.826: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4736
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 29 14:45:00.221: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"50e04c7d-873b-439d-978f-246b3803a87a", Controller:(*bool)(0xc0051ccc0a), BlockOwnerDeletion:(*bool)(0xc0051ccc0b)}}
Jan 29 14:45:00.252: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"3cc05990-1d9d-4111-8723-d906072328cc", Controller:(*bool)(0xc004e001c6), BlockOwnerDeletion:(*bool)(0xc004e001c7)}}
Jan 29 14:45:00.267: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"c48159ac-78b0-4864-8a72-0f22a99de2fe", Controller:(*bool)(0xc0051cce06), BlockOwnerDeletion:(*bool)(0xc0051cce07)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:45:05.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4736" for this suite.

• [SLOW TEST:5.551 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":276,"completed":248,"skipped":4067,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:45:05.380: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9689
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on node default medium
Jan 29 14:45:05.756: INFO: Waiting up to 5m0s for pod "pod-e0bb0bf6-a54c-409d-a5da-23f9f47ae749" in namespace "emptydir-9689" to be "success or failure"
Jan 29 14:45:05.776: INFO: Pod "pod-e0bb0bf6-a54c-409d-a5da-23f9f47ae749": Phase="Pending", Reason="", readiness=false. Elapsed: 19.131574ms
Jan 29 14:45:07.784: INFO: Pod "pod-e0bb0bf6-a54c-409d-a5da-23f9f47ae749": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027619208s
Jan 29 14:45:09.803: INFO: Pod "pod-e0bb0bf6-a54c-409d-a5da-23f9f47ae749": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046198903s
STEP: Saw pod success
Jan 29 14:45:09.803: INFO: Pod "pod-e0bb0bf6-a54c-409d-a5da-23f9f47ae749" satisfied condition "success or failure"
Jan 29 14:45:09.845: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-e0bb0bf6-a54c-409d-a5da-23f9f47ae749 container test-container: <nil>
STEP: delete the pod
Jan 29 14:45:09.949: INFO: Waiting for pod pod-e0bb0bf6-a54c-409d-a5da-23f9f47ae749 to disappear
Jan 29 14:45:09.964: INFO: Pod pod-e0bb0bf6-a54c-409d-a5da-23f9f47ae749 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:45:09.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9689" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":276,"completed":249,"skipped":4108,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:45:10.076: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-4904
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-2198
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6805
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:45:16.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4904" for this suite.
STEP: Destroying namespace "nsdeletetest-2198" for this suite.
Jan 29 14:45:17.043: INFO: Namespace nsdeletetest-2198 was already deleted
STEP: Destroying namespace "nsdeletetest-6805" for this suite.

• [SLOW TEST:6.986 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":276,"completed":250,"skipped":4144,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:45:17.062: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7505
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Update Demo
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:330
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a replication controller
Jan 29 14:45:17.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 create -f - --namespace=kubectl-7505'
Jan 29 14:45:17.811: INFO: stderr: ""
Jan 29 14:45:17.811: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 29 14:45:17.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7505'
Jan 29 14:45:18.003: INFO: stderr: ""
Jan 29 14:45:18.003: INFO: stdout: "update-demo-nautilus-6rqk9 update-demo-nautilus-p522b "
Jan 29 14:45:18.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods update-demo-nautilus-6rqk9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7505'
Jan 29 14:45:18.147: INFO: stderr: ""
Jan 29 14:45:18.147: INFO: stdout: ""
Jan 29 14:45:18.147: INFO: update-demo-nautilus-6rqk9 is created but not running
Jan 29 14:45:23.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7505'
Jan 29 14:45:23.303: INFO: stderr: ""
Jan 29 14:45:23.303: INFO: stdout: "update-demo-nautilus-6rqk9 update-demo-nautilus-p522b "
Jan 29 14:45:23.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods update-demo-nautilus-6rqk9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7505'
Jan 29 14:45:23.426: INFO: stderr: ""
Jan 29 14:45:23.426: INFO: stdout: "true"
Jan 29 14:45:23.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods update-demo-nautilus-6rqk9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7505'
Jan 29 14:45:23.554: INFO: stderr: ""
Jan 29 14:45:23.554: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 29 14:45:23.554: INFO: validating pod update-demo-nautilus-6rqk9
Jan 29 14:45:24.241: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 29 14:45:24.241: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 29 14:45:24.242: INFO: update-demo-nautilus-6rqk9 is verified up and running
Jan 29 14:45:24.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods update-demo-nautilus-p522b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7505'
Jan 29 14:45:24.809: INFO: stderr: ""
Jan 29 14:45:24.809: INFO: stdout: "true"
Jan 29 14:45:24.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods update-demo-nautilus-p522b -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7505'
Jan 29 14:45:25.127: INFO: stderr: ""
Jan 29 14:45:25.127: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 29 14:45:25.127: INFO: validating pod update-demo-nautilus-p522b
Jan 29 14:45:25.232: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 29 14:45:25.232: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 29 14:45:25.232: INFO: update-demo-nautilus-p522b is verified up and running
STEP: using delete to clean up resources
Jan 29 14:45:25.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 delete --grace-period=0 --force -f - --namespace=kubectl-7505'
Jan 29 14:45:25.442: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 29 14:45:25.442: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 29 14:45:25.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7505'
Jan 29 14:45:25.647: INFO: stderr: "No resources found in kubectl-7505 namespace.\n"
Jan 29 14:45:25.647: INFO: stdout: ""
Jan 29 14:45:25.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods -l name=update-demo --namespace=kubectl-7505 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 29 14:45:25.804: INFO: stderr: ""
Jan 29 14:45:25.804: INFO: stdout: "update-demo-nautilus-6rqk9\nupdate-demo-nautilus-p522b\n"
Jan 29 14:45:26.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7505'
Jan 29 14:45:26.493: INFO: stderr: "No resources found in kubectl-7505 namespace.\n"
Jan 29 14:45:26.493: INFO: stdout: ""
Jan 29 14:45:26.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 get pods -l name=update-demo --namespace=kubectl-7505 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 29 14:45:26.677: INFO: stderr: ""
Jan 29 14:45:26.677: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:45:26.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7505" for this suite.

• [SLOW TEST:9.651 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:328
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":276,"completed":251,"skipped":4149,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:45:26.714: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7134
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-d2168f56-6df8-44f8-b3fe-60ff49f7b089
STEP: Creating a pod to test consume configMaps
Jan 29 14:45:27.019: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a2f900d9-93b1-402b-9cda-85a52af8f779" in namespace "projected-7134" to be "success or failure"
Jan 29 14:45:27.053: INFO: Pod "pod-projected-configmaps-a2f900d9-93b1-402b-9cda-85a52af8f779": Phase="Pending", Reason="", readiness=false. Elapsed: 33.17342ms
Jan 29 14:45:29.076: INFO: Pod "pod-projected-configmaps-a2f900d9-93b1-402b-9cda-85a52af8f779": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055602846s
Jan 29 14:45:31.084: INFO: Pod "pod-projected-configmaps-a2f900d9-93b1-402b-9cda-85a52af8f779": Phase="Pending", Reason="", readiness=false. Elapsed: 4.063870423s
Jan 29 14:45:33.244: INFO: Pod "pod-projected-configmaps-a2f900d9-93b1-402b-9cda-85a52af8f779": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.223823097s
STEP: Saw pod success
Jan 29 14:45:33.244: INFO: Pod "pod-projected-configmaps-a2f900d9-93b1-402b-9cda-85a52af8f779" satisfied condition "success or failure"
Jan 29 14:45:33.265: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-projected-configmaps-a2f900d9-93b1-402b-9cda-85a52af8f779 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 29 14:45:36.060: INFO: Waiting for pod pod-projected-configmaps-a2f900d9-93b1-402b-9cda-85a52af8f779 to disappear
Jan 29 14:45:36.725: INFO: Pod pod-projected-configmaps-a2f900d9-93b1-402b-9cda-85a52af8f779 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:45:36.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7134" for this suite.

• [SLOW TEST:11.821 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":276,"completed":252,"skipped":4171,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:45:38.537: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1361
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:45:49.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1361" for this suite.

• [SLOW TEST:10.783 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":276,"completed":253,"skipped":4180,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:45:49.326: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6818
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 29 14:45:52.763: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:45:52.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6818" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":276,"completed":254,"skipped":4221,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:45:52.892: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2616
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl run default
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1596
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jan 29 14:45:53.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-2616'
Jan 29 14:45:55.132: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan 29 14:45:55.132: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1602
Jan 29 14:45:57.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 delete deployment e2e-test-httpd-deployment --namespace=kubectl-2616'
Jan 29 14:45:57.321: INFO: stderr: ""
Jan 29 14:45:57.321: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:45:57.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2616" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run default should create an rc or deployment from an image  [Conformance]","total":276,"completed":255,"skipped":4254,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:45:57.371: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-2151
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jan 29 14:46:05.771: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 29 14:46:05.779: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 29 14:46:07.779: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 29 14:46:07.791: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 29 14:46:09.779: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 29 14:46:09.791: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 29 14:46:11.779: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 29 14:46:11.790: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 29 14:46:13.779: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 29 14:46:13.790: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 29 14:46:15.779: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 29 14:46:15.793: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 29 14:46:17.779: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 29 14:46:17.801: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:46:17.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2151" for this suite.

• [SLOW TEST:20.588 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":276,"completed":256,"skipped":4264,"failed":0}
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:46:17.961: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-1177
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-1177
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 29 14:46:18.255: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jan 29 14:46:46.743: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.0.105 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1177 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 14:46:46.743: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
Jan 29 14:46:48.465: INFO: Found all expected endpoints: [netserver-0]
Jan 29 14:46:48.487: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.2.17 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1177 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 14:46:48.487: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
Jan 29 14:46:50.090: INFO: Found all expected endpoints: [netserver-1]
Jan 29 14:46:50.109: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.1.195 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1177 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 14:46:50.109: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
Jan 29 14:46:51.754: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:46:51.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1177" for this suite.

• [SLOW TEST:34.036 seconds]
[sig-network] Networking
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":276,"completed":257,"skipped":4266,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:46:52.000: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1527
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 29 14:46:54.884: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 29 14:46:56.920: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715906014, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715906014, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715906014, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715906014, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 14:46:58.936: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715906014, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715906014, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715906014, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715906014, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 14:47:00.936: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715906014, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715906014, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715906014, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715906014, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 14:47:02.929: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715906014, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715906014, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715906014, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715906014, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 29 14:47:06.143: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:47:17.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1527" for this suite.
STEP: Destroying namespace "webhook-1527-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:25.582 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":276,"completed":258,"skipped":4268,"failed":0}
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:47:17.584: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-9741
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 29 14:47:23.004: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:47:23.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9741" for this suite.

• [SLOW TEST:5.604 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  blackbox test
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:131
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":276,"completed":259,"skipped":4268,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:47:23.188: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6289
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 29 14:47:24.116: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 29 14:47:26.173: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715906044, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715906044, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715906044, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715906044, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 14:47:28.204: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715906044, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715906044, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715906044, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715906044, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 29 14:47:31.208: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:47:31.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6289" for this suite.
STEP: Destroying namespace "webhook-6289-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.008 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":276,"completed":260,"skipped":4271,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:47:32.204: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6252
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Jan 29 14:47:39.284: INFO: Successfully updated pod "labelsupdate9a55bb1f-ac29-4cc8-934d-2f8e7824e631"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:47:43.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6252" for this suite.

• [SLOW TEST:11.389 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":276,"completed":261,"skipped":4309,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:47:43.595: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3669
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0129 14:48:14.228300      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jan 29 14:48:14.228: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:48:14.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3669" for this suite.

• [SLOW TEST:30.710 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":276,"completed":262,"skipped":4314,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:48:14.318: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2717
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 29 14:48:14.712: INFO: Waiting up to 5m0s for pod "downwardapi-volume-41aeca9d-e66f-4f71-99b0-064c6f88c029" in namespace "projected-2717" to be "success or failure"
Jan 29 14:48:14.751: INFO: Pod "downwardapi-volume-41aeca9d-e66f-4f71-99b0-064c6f88c029": Phase="Pending", Reason="", readiness=false. Elapsed: 37.144982ms
Jan 29 14:48:16.770: INFO: Pod "downwardapi-volume-41aeca9d-e66f-4f71-99b0-064c6f88c029": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055937727s
Jan 29 14:48:18.781: INFO: Pod "downwardapi-volume-41aeca9d-e66f-4f71-99b0-064c6f88c029": Phase="Pending", Reason="", readiness=false. Elapsed: 4.067707159s
Jan 29 14:48:20.796: INFO: Pod "downwardapi-volume-41aeca9d-e66f-4f71-99b0-064c6f88c029": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.082830853s
STEP: Saw pod success
Jan 29 14:48:20.797: INFO: Pod "downwardapi-volume-41aeca9d-e66f-4f71-99b0-064c6f88c029" satisfied condition "success or failure"
Jan 29 14:48:20.805: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod downwardapi-volume-41aeca9d-e66f-4f71-99b0-064c6f88c029 container client-container: <nil>
STEP: delete the pod
Jan 29 14:48:20.896: INFO: Waiting for pod downwardapi-volume-41aeca9d-e66f-4f71-99b0-064c6f88c029 to disappear
Jan 29 14:48:20.907: INFO: Pod downwardapi-volume-41aeca9d-e66f-4f71-99b0-064c6f88c029 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:48:20.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2717" for this suite.

• [SLOW TEST:6.620 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":276,"completed":263,"skipped":4389,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:48:20.939: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9522
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jan 29 14:48:21.214: INFO: Waiting up to 5m0s for pod "pod-e82a0c5a-c20f-4c43-8b5d-d0ec03961d9c" in namespace "emptydir-9522" to be "success or failure"
Jan 29 14:48:21.256: INFO: Pod "pod-e82a0c5a-c20f-4c43-8b5d-d0ec03961d9c": Phase="Pending", Reason="", readiness=false. Elapsed: 41.403262ms
Jan 29 14:48:23.276: INFO: Pod "pod-e82a0c5a-c20f-4c43-8b5d-d0ec03961d9c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061447346s
Jan 29 14:48:25.296: INFO: Pod "pod-e82a0c5a-c20f-4c43-8b5d-d0ec03961d9c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.082003229s
Jan 29 14:48:27.321: INFO: Pod "pod-e82a0c5a-c20f-4c43-8b5d-d0ec03961d9c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.106299477s
Jan 29 14:48:29.350: INFO: Pod "pod-e82a0c5a-c20f-4c43-8b5d-d0ec03961d9c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.13587938s
Jan 29 14:48:31.374: INFO: Pod "pod-e82a0c5a-c20f-4c43-8b5d-d0ec03961d9c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.159932457s
Jan 29 14:48:33.388: INFO: Pod "pod-e82a0c5a-c20f-4c43-8b5d-d0ec03961d9c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.173837684s
Jan 29 14:48:35.710: INFO: Pod "pod-e82a0c5a-c20f-4c43-8b5d-d0ec03961d9c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.495144071s
STEP: Saw pod success
Jan 29 14:48:35.710: INFO: Pod "pod-e82a0c5a-c20f-4c43-8b5d-d0ec03961d9c" satisfied condition "success or failure"
Jan 29 14:48:35.719: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-e82a0c5a-c20f-4c43-8b5d-d0ec03961d9c container test-container: <nil>
STEP: delete the pod
Jan 29 14:48:35.997: INFO: Waiting for pod pod-e82a0c5a-c20f-4c43-8b5d-d0ec03961d9c to disappear
Jan 29 14:48:36.120: INFO: Pod pod-e82a0c5a-c20f-4c43-8b5d-d0ec03961d9c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:48:36.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9522" for this suite.

• [SLOW TEST:15.228 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":276,"completed":264,"skipped":4395,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:48:36.168: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-815
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 29 14:48:37.169: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9d377770-cfd9-4d2f-93cb-6232c4cd1022" in namespace "projected-815" to be "success or failure"
Jan 29 14:48:37.178: INFO: Pod "downwardapi-volume-9d377770-cfd9-4d2f-93cb-6232c4cd1022": Phase="Pending", Reason="", readiness=false. Elapsed: 7.955897ms
Jan 29 14:48:39.189: INFO: Pod "downwardapi-volume-9d377770-cfd9-4d2f-93cb-6232c4cd1022": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019289912s
Jan 29 14:48:41.206: INFO: Pod "downwardapi-volume-9d377770-cfd9-4d2f-93cb-6232c4cd1022": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0365154s
STEP: Saw pod success
Jan 29 14:48:41.206: INFO: Pod "downwardapi-volume-9d377770-cfd9-4d2f-93cb-6232c4cd1022" satisfied condition "success or failure"
Jan 29 14:48:41.215: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod downwardapi-volume-9d377770-cfd9-4d2f-93cb-6232c4cd1022 container client-container: <nil>
STEP: delete the pod
Jan 29 14:48:41.424: INFO: Waiting for pod downwardapi-volume-9d377770-cfd9-4d2f-93cb-6232c4cd1022 to disappear
Jan 29 14:48:41.435: INFO: Pod downwardapi-volume-9d377770-cfd9-4d2f-93cb-6232c4cd1022 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:48:41.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-815" for this suite.

• [SLOW TEST:5.309 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":276,"completed":265,"skipped":4397,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:48:41.478: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1437
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 29 14:48:42.221: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 29 14:48:44.735: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715906122, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715906122, loc:(*time.Location)(0x7db4bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715906122, loc:(*time.Location)(0x7db4bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715906122, loc:(*time.Location)(0x7db4bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 29 14:48:47.808: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 29 14:48:47.817: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6584-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:48:49.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1437" for this suite.
STEP: Destroying namespace "webhook-1437-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.169 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":276,"completed":266,"skipped":4400,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:48:49.656: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6326
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-f91021de-8721-41b3-9365-c38282719ed6
STEP: Creating a pod to test consume configMaps
Jan 29 14:48:50.022: INFO: Waiting up to 5m0s for pod "pod-configmaps-0e0ae014-1cdd-4ba1-8974-3cea64126ca7" in namespace "configmap-6326" to be "success or failure"
Jan 29 14:48:50.045: INFO: Pod "pod-configmaps-0e0ae014-1cdd-4ba1-8974-3cea64126ca7": Phase="Pending", Reason="", readiness=false. Elapsed: 21.91261ms
Jan 29 14:48:52.075: INFO: Pod "pod-configmaps-0e0ae014-1cdd-4ba1-8974-3cea64126ca7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052769567s
Jan 29 14:48:54.089: INFO: Pod "pod-configmaps-0e0ae014-1cdd-4ba1-8974-3cea64126ca7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.066746279s
Jan 29 14:48:56.100: INFO: Pod "pod-configmaps-0e0ae014-1cdd-4ba1-8974-3cea64126ca7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.07702204s
STEP: Saw pod success
Jan 29 14:48:56.100: INFO: Pod "pod-configmaps-0e0ae014-1cdd-4ba1-8974-3cea64126ca7" satisfied condition "success or failure"
Jan 29 14:48:56.109: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-configmaps-0e0ae014-1cdd-4ba1-8974-3cea64126ca7 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 29 14:48:56.177: INFO: Waiting for pod pod-configmaps-0e0ae014-1cdd-4ba1-8974-3cea64126ca7 to disappear
Jan 29 14:48:56.189: INFO: Pod pod-configmaps-0e0ae014-1cdd-4ba1-8974-3cea64126ca7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:48:56.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6326" for this suite.

• [SLOW TEST:6.573 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]","total":276,"completed":267,"skipped":4436,"failed":0}
S
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:48:56.230: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1815
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:49:02.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1815" for this suite.

• [SLOW TEST:6.497 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when scheduling a busybox command in a pod
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":276,"completed":268,"skipped":4437,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:49:02.733: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7551
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-07dfb3e7-6a9b-4bad-8940-0308233aa786
STEP: Creating a pod to test consume configMaps
Jan 29 14:49:03.068: INFO: Waiting up to 5m0s for pod "pod-configmaps-0840dfc1-2569-4506-823f-9c8113575a56" in namespace "configmap-7551" to be "success or failure"
Jan 29 14:49:03.087: INFO: Pod "pod-configmaps-0840dfc1-2569-4506-823f-9c8113575a56": Phase="Pending", Reason="", readiness=false. Elapsed: 19.299641ms
Jan 29 14:49:05.106: INFO: Pod "pod-configmaps-0840dfc1-2569-4506-823f-9c8113575a56": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037845216s
Jan 29 14:49:07.129: INFO: Pod "pod-configmaps-0840dfc1-2569-4506-823f-9c8113575a56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060614867s
STEP: Saw pod success
Jan 29 14:49:07.129: INFO: Pod "pod-configmaps-0840dfc1-2569-4506-823f-9c8113575a56" satisfied condition "success or failure"
Jan 29 14:49:07.138: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-configmaps-0840dfc1-2569-4506-823f-9c8113575a56 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 29 14:49:07.229: INFO: Waiting for pod pod-configmaps-0840dfc1-2569-4506-823f-9c8113575a56 to disappear
Jan 29 14:49:07.238: INFO: Pod pod-configmaps-0840dfc1-2569-4506-823f-9c8113575a56 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:49:07.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7551" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":276,"completed":269,"skipped":4443,"failed":0}

------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:49:07.274: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5639
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-27af2adc-f169-4fad-b996-40bddaf80fe7
STEP: Creating a pod to test consume configMaps
Jan 29 14:49:07.613: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9bf69db1-d4f1-44c1-b6c1-358cb201adcc" in namespace "projected-5639" to be "success or failure"
Jan 29 14:49:07.630: INFO: Pod "pod-projected-configmaps-9bf69db1-d4f1-44c1-b6c1-358cb201adcc": Phase="Pending", Reason="", readiness=false. Elapsed: 16.954245ms
Jan 29 14:49:09.665: INFO: Pod "pod-projected-configmaps-9bf69db1-d4f1-44c1-b6c1-358cb201adcc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051507024s
Jan 29 14:49:11.685: INFO: Pod "pod-projected-configmaps-9bf69db1-d4f1-44c1-b6c1-358cb201adcc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.071941223s
STEP: Saw pod success
Jan 29 14:49:11.685: INFO: Pod "pod-projected-configmaps-9bf69db1-d4f1-44c1-b6c1-358cb201adcc" satisfied condition "success or failure"
Jan 29 14:49:11.710: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-projected-configmaps-9bf69db1-d4f1-44c1-b6c1-358cb201adcc container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 29 14:49:11.789: INFO: Waiting for pod pod-projected-configmaps-9bf69db1-d4f1-44c1-b6c1-358cb201adcc to disappear
Jan 29 14:49:11.797: INFO: Pod pod-projected-configmaps-9bf69db1-d4f1-44c1-b6c1-358cb201adcc no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:49:11.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5639" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":276,"completed":270,"skipped":4443,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:49:11.855: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3563
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:49:23.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3563" for this suite.

• [SLOW TEST:11.669 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":276,"completed":271,"skipped":4490,"failed":0}
S
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:49:23.526: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1948
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name s-test-opt-del-5ed05022-59bb-479f-9059-75b7cd2ab65f
STEP: Creating secret with name s-test-opt-upd-0b34c301-332a-42ad-aaea-e48810685234
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-5ed05022-59bb-479f-9059-75b7cd2ab65f
STEP: Updating secret s-test-opt-upd-0b34c301-332a-42ad-aaea-e48810685234
STEP: Creating secret with name s-test-opt-create-8ec4d9a3-1f70-4a13-99e3-da30a6873981
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:50:42.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1948" for this suite.

• [SLOW TEST:79.485 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":276,"completed":272,"skipped":4491,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:50:43.013: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4060
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-4060
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-4060
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4060
Jan 29 14:50:43.355: INFO: Found 0 stateful pods, waiting for 1
Jan 29 14:50:53.384: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jan 29 14:50:53.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 29 14:50:54.160: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 29 14:50:54.160: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 29 14:50:54.160: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 29 14:50:54.169: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 29 14:50:54.169: INFO: Waiting for statefulset status.replicas updated to 0
Jan 29 14:50:54.240: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999016s
Jan 29 14:50:55.248: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.984083218s
Jan 29 14:50:56.257: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.975759316s
Jan 29 14:50:57.268: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.966329349s
Jan 29 14:50:58.296: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.956301038s
Jan 29 14:50:59.741: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.927458448s
Jan 29 14:51:00.753: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.48275839s
Jan 29 14:51:01.764: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.470427782s
Jan 29 14:51:02.798: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.459780445s
Jan 29 14:51:03.818: INFO: Verifying statefulset ss doesn't scale past 1 for another 425.885958ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4060
Jan 29 14:51:04.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 14:51:05.755: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 29 14:51:05.755: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 29 14:51:05.755: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 29 14:51:05.778: INFO: Found 1 stateful pods, waiting for 3
Jan 29 14:51:15.792: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 14:51:15.792: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 14:51:15.792: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jan 29 14:51:15.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 29 14:51:16.670: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 29 14:51:16.670: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 29 14:51:16.670: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 29 14:51:16.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 29 14:51:17.487: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 29 14:51:17.487: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 29 14:51:17.487: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 29 14:51:17.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 29 14:51:18.282: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 29 14:51:18.282: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 29 14:51:18.282: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 29 14:51:18.282: INFO: Waiting for statefulset status.replicas updated to 0
Jan 29 14:51:18.304: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Jan 29 14:51:28.327: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 29 14:51:28.327: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 29 14:51:28.327: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 29 14:51:28.394: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999884s
Jan 29 14:51:29.412: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.968128132s
Jan 29 14:51:30.440: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.950457336s
Jan 29 14:51:31.525: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.922495465s
Jan 29 14:51:32.549: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.837251627s
Jan 29 14:51:33.561: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.812611536s
Jan 29 14:51:34.603: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.801450555s
Jan 29 14:51:35.621: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.75966787s
Jan 29 14:51:36.643: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.741790014s
Jan 29 14:51:37.661: INFO: Verifying statefulset ss doesn't scale past 3 for another 719.6165ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4060
Jan 29 14:51:38.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 14:51:39.385: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 29 14:51:39.385: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 29 14:51:39.385: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 29 14:51:39.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 14:51:40.083: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 29 14:51:40.083: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 29 14:51:40.083: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 29 14:51:40.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 14:51:40.654: INFO: rc: 1
Jan 29 14:51:40.655: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server: 

error:
exit status 1
Jan 29 14:51:50.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 14:51:50.903: INFO: rc: 1
Jan 29 14:51:50.903: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Jan 29 14:52:00.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 14:52:01.017: INFO: rc: 1
Jan 29 14:52:01.017: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 29 14:52:11.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 14:52:11.204: INFO: rc: 1
Jan 29 14:52:11.204: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 29 14:52:21.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 14:52:21.349: INFO: rc: 1
Jan 29 14:52:21.349: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 29 14:52:31.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 14:52:31.500: INFO: rc: 1
Jan 29 14:52:31.500: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 29 14:52:41.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 14:52:41.622: INFO: rc: 1
Jan 29 14:52:41.622: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 29 14:52:51.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 14:52:51.779: INFO: rc: 1
Jan 29 14:52:51.779: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 29 14:53:01.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 14:53:01.920: INFO: rc: 1
Jan 29 14:53:01.920: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 29 14:53:11.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 14:53:13.051: INFO: rc: 1
Jan 29 14:53:13.051: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 29 14:53:23.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 14:53:23.200: INFO: rc: 1
Jan 29 14:53:23.200: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 29 14:53:33.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 14:53:33.341: INFO: rc: 1
Jan 29 14:53:33.341: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 29 14:53:43.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 14:53:43.484: INFO: rc: 1
Jan 29 14:53:43.484: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 29 14:53:53.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 14:53:53.642: INFO: rc: 1
Jan 29 14:53:53.642: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 29 14:54:03.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 14:54:03.784: INFO: rc: 1
Jan 29 14:54:03.784: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 29 14:54:13.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 14:54:13.901: INFO: rc: 1
Jan 29 14:54:13.901: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 29 14:54:23.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 14:54:24.029: INFO: rc: 1
Jan 29 14:54:24.029: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 29 14:54:34.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 14:54:34.180: INFO: rc: 1
Jan 29 14:54:34.180: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 29 14:54:44.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 14:54:44.319: INFO: rc: 1
Jan 29 14:54:44.320: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 29 14:54:54.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 14:54:54.464: INFO: rc: 1
Jan 29 14:54:54.464: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 29 14:55:04.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 14:55:04.602: INFO: rc: 1
Jan 29 14:55:04.602: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 29 14:55:14.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 14:55:14.736: INFO: rc: 1
Jan 29 14:55:14.736: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 29 14:55:24.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 14:55:24.858: INFO: rc: 1
Jan 29 14:55:24.858: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 29 14:55:34.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 14:55:35.015: INFO: rc: 1
Jan 29 14:55:35.016: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 29 14:55:45.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 14:55:45.175: INFO: rc: 1
Jan 29 14:55:45.175: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 29 14:55:55.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 14:55:57.156: INFO: rc: 1
Jan 29 14:55:57.156: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 29 14:56:07.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 14:56:07.287: INFO: rc: 1
Jan 29 14:56:07.287: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 29 14:56:17.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 14:56:17.444: INFO: rc: 1
Jan 29 14:56:17.445: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 29 14:56:27.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 14:56:27.598: INFO: rc: 1
Jan 29 14:56:27.598: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 29 14:56:37.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 14:56:37.744: INFO: rc: 1
Jan 29 14:56:37.744: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 29 14:56:47.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-4060 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 14:56:47.863: INFO: rc: 1
Jan 29 14:56:47.864: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
Jan 29 14:56:47.864: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jan 29 14:56:47.901: INFO: Deleting all statefulset in ns statefulset-4060
Jan 29 14:56:47.924: INFO: Scaling statefulset ss to 0
Jan 29 14:56:47.944: INFO: Waiting for statefulset status.replicas updated to 0
Jan 29 14:56:47.950: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:56:47.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4060" for this suite.

• [SLOW TEST:365.030 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":276,"completed":273,"skipped":4500,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:56:48.045: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7526
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-7526
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating stateful set ss in namespace statefulset-7526
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7526
Jan 29 14:56:48.338: INFO: Found 0 stateful pods, waiting for 1
Jan 29 14:56:58.351: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jan 29 14:56:58.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-7526 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 29 14:56:59.125: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 29 14:56:59.125: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 29 14:56:59.125: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 29 14:56:59.133: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 29 14:57:09.150: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 29 14:57:09.150: INFO: Waiting for statefulset status.replicas updated to 0
Jan 29 14:57:09.216: INFO: POD   NODE                                    PHASE    GRACE  CONDITIONS
Jan 29 14:57:09.216: INFO: ss-0  metakube-worker-cmccl-6d88bd94fc-lqfxz  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:56:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:56:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:56:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:56:48 +0000 UTC  }]
Jan 29 14:57:09.216: INFO: 
Jan 29 14:57:09.216: INFO: StatefulSet ss has not reached scale 3, at 1
Jan 29 14:57:10.227: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.979736509s
Jan 29 14:57:11.270: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.968429252s
Jan 29 14:57:12.281: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.924761167s
Jan 29 14:57:13.289: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.914944335s
Jan 29 14:57:14.306: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.906338193s
Jan 29 14:57:15.314: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.889360294s
Jan 29 14:57:16.325: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.8813192s
Jan 29 14:57:17.339: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.870625023s
Jan 29 14:57:18.349: INFO: Verifying statefulset ss doesn't scale past 3 for another 856.373245ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7526
Jan 29 14:57:19.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-7526 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 14:57:20.160: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 29 14:57:20.160: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 29 14:57:20.160: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 29 14:57:20.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-7526 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 14:57:20.870: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 29 14:57:20.870: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 29 14:57:20.870: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 29 14:57:20.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-7526 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 14:57:21.548: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 29 14:57:21.548: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 29 14:57:21.548: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 29 14:57:21.560: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 14:57:21.560: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 14:57:21.560: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jan 29 14:57:21.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-7526 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 29 14:57:22.285: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 29 14:57:22.285: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 29 14:57:22.285: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 29 14:57:22.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-7526 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 29 14:57:23.016: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 29 14:57:23.016: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 29 14:57:23.016: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 29 14:57:23.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-836921002 exec --namespace=statefulset-7526 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 29 14:57:23.744: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 29 14:57:23.744: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 29 14:57:23.744: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 29 14:57:23.744: INFO: Waiting for statefulset status.replicas updated to 0
Jan 29 14:57:23.751: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Jan 29 14:57:33.772: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 29 14:57:33.773: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 29 14:57:33.773: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 29 14:57:33.804: INFO: POD   NODE                                    PHASE    GRACE  CONDITIONS
Jan 29 14:57:33.804: INFO: ss-0  metakube-worker-cmccl-6d88bd94fc-lqfxz  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:56:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:56:48 +0000 UTC  }]
Jan 29 14:57:33.804: INFO: ss-1  metakube-worker-cmccl-6d88bd94fc-znv5g  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:09 +0000 UTC  }]
Jan 29 14:57:33.805: INFO: ss-2  metakube-worker-cmccl-6d88bd94fc-87n7l  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:09 +0000 UTC  }]
Jan 29 14:57:33.805: INFO: 
Jan 29 14:57:33.805: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 29 14:57:34.813: INFO: POD   NODE                                    PHASE    GRACE  CONDITIONS
Jan 29 14:57:34.813: INFO: ss-0  metakube-worker-cmccl-6d88bd94fc-lqfxz  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:56:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:56:48 +0000 UTC  }]
Jan 29 14:57:34.813: INFO: ss-1  metakube-worker-cmccl-6d88bd94fc-znv5g  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:09 +0000 UTC  }]
Jan 29 14:57:34.813: INFO: ss-2  metakube-worker-cmccl-6d88bd94fc-87n7l  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:09 +0000 UTC  }]
Jan 29 14:57:34.813: INFO: 
Jan 29 14:57:34.813: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 29 14:57:35.821: INFO: POD   NODE                                    PHASE    GRACE  CONDITIONS
Jan 29 14:57:35.821: INFO: ss-0  metakube-worker-cmccl-6d88bd94fc-lqfxz  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:56:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:56:48 +0000 UTC  }]
Jan 29 14:57:35.821: INFO: ss-1  metakube-worker-cmccl-6d88bd94fc-znv5g  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:09 +0000 UTC  }]
Jan 29 14:57:35.821: INFO: ss-2  metakube-worker-cmccl-6d88bd94fc-87n7l  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:09 +0000 UTC  }]
Jan 29 14:57:35.821: INFO: 
Jan 29 14:57:35.821: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 29 14:57:36.829: INFO: POD   NODE                                    PHASE    GRACE  CONDITIONS
Jan 29 14:57:36.829: INFO: ss-0  metakube-worker-cmccl-6d88bd94fc-lqfxz  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:56:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:56:48 +0000 UTC  }]
Jan 29 14:57:36.829: INFO: ss-1  metakube-worker-cmccl-6d88bd94fc-znv5g  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:09 +0000 UTC  }]
Jan 29 14:57:36.829: INFO: ss-2  metakube-worker-cmccl-6d88bd94fc-87n7l  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:09 +0000 UTC  }]
Jan 29 14:57:36.829: INFO: 
Jan 29 14:57:36.829: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 29 14:57:37.843: INFO: POD   NODE                                    PHASE    GRACE  CONDITIONS
Jan 29 14:57:37.843: INFO: ss-1  metakube-worker-cmccl-6d88bd94fc-znv5g  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:09 +0000 UTC  }]
Jan 29 14:57:37.843: INFO: ss-2  metakube-worker-cmccl-6d88bd94fc-87n7l  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:09 +0000 UTC  }]
Jan 29 14:57:37.843: INFO: 
Jan 29 14:57:37.843: INFO: StatefulSet ss has not reached scale 0, at 2
Jan 29 14:57:38.852: INFO: POD   NODE                                    PHASE    GRACE  CONDITIONS
Jan 29 14:57:38.852: INFO: ss-1  metakube-worker-cmccl-6d88bd94fc-znv5g  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:09 +0000 UTC  }]
Jan 29 14:57:38.852: INFO: ss-2  metakube-worker-cmccl-6d88bd94fc-87n7l  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:09 +0000 UTC  }]
Jan 29 14:57:38.852: INFO: 
Jan 29 14:57:38.852: INFO: StatefulSet ss has not reached scale 0, at 2
Jan 29 14:57:39.860: INFO: POD   NODE                                    PHASE    GRACE  CONDITIONS
Jan 29 14:57:39.861: INFO: ss-1  metakube-worker-cmccl-6d88bd94fc-znv5g  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:09 +0000 UTC  }]
Jan 29 14:57:39.861: INFO: ss-2  metakube-worker-cmccl-6d88bd94fc-87n7l  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:09 +0000 UTC  }]
Jan 29 14:57:39.861: INFO: 
Jan 29 14:57:39.861: INFO: StatefulSet ss has not reached scale 0, at 2
Jan 29 14:57:40.868: INFO: POD   NODE                                    PHASE    GRACE  CONDITIONS
Jan 29 14:57:40.868: INFO: ss-1  metakube-worker-cmccl-6d88bd94fc-znv5g  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:09 +0000 UTC  }]
Jan 29 14:57:40.868: INFO: ss-2  metakube-worker-cmccl-6d88bd94fc-87n7l  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:09 +0000 UTC  }]
Jan 29 14:57:40.868: INFO: 
Jan 29 14:57:40.868: INFO: StatefulSet ss has not reached scale 0, at 2
Jan 29 14:57:41.879: INFO: POD   NODE                                    PHASE    GRACE  CONDITIONS
Jan 29 14:57:41.879: INFO: ss-2  metakube-worker-cmccl-6d88bd94fc-87n7l  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:09 +0000 UTC  }]
Jan 29 14:57:41.879: INFO: 
Jan 29 14:57:41.880: INFO: StatefulSet ss has not reached scale 0, at 1
Jan 29 14:57:42.891: INFO: POD   NODE                                    PHASE    GRACE  CONDITIONS
Jan 29 14:57:42.891: INFO: ss-2  metakube-worker-cmccl-6d88bd94fc-87n7l  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-29 14:57:09 +0000 UTC  }]
Jan 29 14:57:42.892: INFO: 
Jan 29 14:57:42.892: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7526
Jan 29 14:57:43.903: INFO: Scaling statefulset ss to 0
Jan 29 14:57:43.936: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jan 29 14:57:43.948: INFO: Deleting all statefulset in ns statefulset-7526
Jan 29 14:57:43.964: INFO: Scaling statefulset ss to 0
Jan 29 14:57:43.986: INFO: Waiting for statefulset status.replicas updated to 0
Jan 29 14:57:43.994: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:57:44.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7526" for this suite.

• [SLOW TEST:56.030 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":276,"completed":274,"skipped":4525,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:57:44.076: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-7734
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 29 14:57:44.416: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:57:46.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7734" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":276,"completed":275,"skipped":4530,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 29 14:57:46.134: INFO: >>> kubeConfig: /tmp/kubeconfig-836921002
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-439
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-map-70587f65-39aa-4ed9-bff7-ba386e7b8911
STEP: Creating a pod to test consume secrets
Jan 29 14:57:46.468: INFO: Waiting up to 5m0s for pod "pod-secrets-4ad38ada-491d-425c-b7ea-50747d7cbe50" in namespace "secrets-439" to be "success or failure"
Jan 29 14:57:46.475: INFO: Pod "pod-secrets-4ad38ada-491d-425c-b7ea-50747d7cbe50": Phase="Pending", Reason="", readiness=false. Elapsed: 6.728203ms
Jan 29 14:57:48.481: INFO: Pod "pod-secrets-4ad38ada-491d-425c-b7ea-50747d7cbe50": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01352683s
Jan 29 14:57:50.489: INFO: Pod "pod-secrets-4ad38ada-491d-425c-b7ea-50747d7cbe50": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021378875s
Jan 29 14:57:52.497: INFO: Pod "pod-secrets-4ad38ada-491d-425c-b7ea-50747d7cbe50": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029189161s
STEP: Saw pod success
Jan 29 14:57:52.497: INFO: Pod "pod-secrets-4ad38ada-491d-425c-b7ea-50747d7cbe50" satisfied condition "success or failure"
Jan 29 14:57:52.503: INFO: Trying to get logs from node metakube-worker-cmccl-6d88bd94fc-lqfxz pod pod-secrets-4ad38ada-491d-425c-b7ea-50747d7cbe50 container secret-volume-test: <nil>
STEP: delete the pod
Jan 29 14:57:52.593: INFO: Waiting for pod pod-secrets-4ad38ada-491d-425c-b7ea-50747d7cbe50 to disappear
Jan 29 14:57:52.598: INFO: Pod pod-secrets-4ad38ada-491d-425c-b7ea-50747d7cbe50 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 29 14:57:52.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-439" for this suite.

• [SLOW TEST:6.510 seconds]
[sig-storage] Secrets
/workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.2-beta.0.2+59603c6e503c87/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":276,"completed":276,"skipped":4561,"failed":0}
SSSSSSJan 29 14:57:52.648: INFO: Running AfterSuite actions on all nodes
Jan 29 14:57:52.649: INFO: Running AfterSuite actions on node 1
Jan 29 14:57:52.649: INFO: Skipping dumping logs from cluster
{"msg":"Test Suite completed","total":276,"completed":276,"skipped":4567,"failed":0}

Ran 276 of 4843 Specs in 5262.548 seconds
SUCCESS! -- 276 Passed | 0 Failed | 0 Pending | 4567 Skipped
PASS

Ginkgo ran 1 suite in 1h27m44.815097446s
Test Suite Passed
